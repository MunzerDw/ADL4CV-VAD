{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 197376  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 131584  \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | decoder1     | Sequential       | 8389376 \n",
      "6  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "7  | decoder1.1   | BatchNorm3d      | 512     \n",
      "8  | decoder1.2   | ReLU             | 0       \n",
      "9  | decoder2     | Sequential       | 2097536 \n",
      "10 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "11 | decoder2.1   | BatchNorm3d      | 256     \n",
      "12 | decoder2.2   | ReLU             | 0       \n",
      "13 | decoder3     | Sequential       | 524480  \n",
      "14 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "15 | decoder3.1   | BatchNorm3d      | 128     \n",
      "16 | decoder3.2   | ReLU             | 0       \n",
      "17 | decoder4     | Sequential       | 4097    \n",
      "18 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "19 | TOTAL        | ThreeDEPNDecoder | 11212865\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1854\n",
      "Length of val set: 232\n",
      "Length of test set: 232\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class='lamp')\n",
    "val_dataset = ShapeNet('val', filter_class='lamp')\n",
    "test_dataset = ShapeNet('test', filter_class='lamp')\n",
    "\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 153540\n",
    "print(f'Length of test set: {len(test_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n",
      "Target DF: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3989860bb341b69c3569a735ca6102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = test_dataset[231]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "print(f'Target DF: {type(sample[\"target_df\"])}')  # expected output: <class 'numpy.ndarray'>\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#                #\n",
    "#    TRAINING    #\n",
    "#                #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3236\n",
      "Training params: 2\n",
      "[001/00048] train_loss: 0.106201 kl_loss: 0.000000 normal_loss: 0.106201\n",
      "[003/00046] train_loss: 0.042423 kl_loss: 0.000000 normal_loss: 0.042423\n",
      "[005/00044] train_loss: 0.035614 kl_loss: 0.000000 normal_loss: 0.035614\n",
      "[007/00042] train_loss: 0.031330 kl_loss: 0.000000 normal_loss: 0.031330\n",
      "[009/00040] train_loss: 0.029408 kl_loss: 0.000000 normal_loss: 0.029408\n",
      "[011/00038] train_loss: 0.028273 kl_loss: 0.000000 normal_loss: 0.028273\n",
      "[013/00036] train_loss: 0.025522 kl_loss: 0.000000 normal_loss: 0.025522\n",
      "[015/00034] train_loss: 0.024653 kl_loss: 0.000000 normal_loss: 0.024653\n",
      "[017/00032] train_loss: 0.025657 kl_loss: 0.000000 normal_loss: 0.025657\n",
      "[019/00030] train_loss: 0.022715 kl_loss: 0.000000 normal_loss: 0.022715\n",
      "[021/00028] train_loss: 0.021348 kl_loss: 0.000000 normal_loss: 0.021348\n",
      "[023/00026] train_loss: 0.020369 kl_loss: 0.000000 normal_loss: 0.020369\n",
      "[025/00024] train_loss: 0.024364 kl_loss: 0.000000 normal_loss: 0.024364\n",
      "[027/00022] train_loss: 0.022951 kl_loss: 0.000000 normal_loss: 0.022951\n",
      "[029/00020] train_loss: 0.021597 kl_loss: 0.000000 normal_loss: 0.021597\n",
      "[031/00018] train_loss: 0.020342 kl_loss: 0.000000 normal_loss: 0.020342\n",
      "[033/00016] train_loss: 0.019189 kl_loss: 0.000000 normal_loss: 0.019189\n",
      "[035/00014] train_loss: 0.018313 kl_loss: 0.000000 normal_loss: 0.018313\n",
      "[037/00012] train_loss: 0.017717 kl_loss: 0.000000 normal_loss: 0.017717\n",
      "[039/00010] train_loss: 0.017121 kl_loss: 0.000000 normal_loss: 0.017121\n",
      "[041/00008] train_loss: 0.016550 kl_loss: 0.000000 normal_loss: 0.016550\n",
      "[043/00006] train_loss: 0.016037 kl_loss: 0.000000 normal_loss: 0.016037\n",
      "[045/00004] train_loss: 0.014177 kl_loss: 0.000000 normal_loss: 0.014177\n",
      "[047/00002] train_loss: 0.015520 kl_loss: 0.000000 normal_loss: 0.015520\n",
      "[049/00000] train_loss: 0.015192 kl_loss: 0.000000 normal_loss: 0.015192\n",
      "[049/00050] IOU 0.8063006274567914\n",
      "[050/00049] train_loss: 0.014601 kl_loss: 0.000000 normal_loss: 0.014601\n",
      "[052/00047] train_loss: 0.014220 kl_loss: 0.000000 normal_loss: 0.014220\n",
      "[054/00045] train_loss: 0.013703 kl_loss: 0.000000 normal_loss: 0.013703\n",
      "[056/00043] train_loss: 0.013695 kl_loss: 0.000000 normal_loss: 0.013695\n",
      "[058/00041] train_loss: 0.012883 kl_loss: 0.000000 normal_loss: 0.012883\n",
      "[060/00039] train_loss: 0.012819 kl_loss: 0.000000 normal_loss: 0.012819\n",
      "[062/00037] train_loss: 0.012836 kl_loss: 0.000000 normal_loss: 0.012836\n",
      "[064/00035] train_loss: 0.012791 kl_loss: 0.000000 normal_loss: 0.012791\n",
      "[066/00033] train_loss: 0.012476 kl_loss: 0.000000 normal_loss: 0.012476\n",
      "[068/00031] train_loss: 0.012316 kl_loss: 0.000000 normal_loss: 0.012316\n",
      "[070/00029] train_loss: 0.011803 kl_loss: 0.000000 normal_loss: 0.011803\n",
      "[072/00027] train_loss: 0.011014 kl_loss: 0.000000 normal_loss: 0.011014\n",
      "[074/00025] train_loss: 0.011772 kl_loss: 0.000000 normal_loss: 0.011772\n",
      "[076/00023] train_loss: 0.011582 kl_loss: 0.000000 normal_loss: 0.011582\n",
      "[078/00021] train_loss: 0.011229 kl_loss: 0.000000 normal_loss: 0.011229\n",
      "[080/00019] train_loss: 0.011229 kl_loss: 0.000000 normal_loss: 0.011229\n",
      "[082/00017] train_loss: 0.010978 kl_loss: 0.000000 normal_loss: 0.010978\n",
      "[084/00015] train_loss: 0.010512 kl_loss: 0.000000 normal_loss: 0.010512\n",
      "[086/00013] train_loss: 0.010199 kl_loss: 0.000000 normal_loss: 0.010199\n",
      "[088/00011] train_loss: 0.010301 kl_loss: 0.000000 normal_loss: 0.010301\n",
      "[090/00009] train_loss: 0.010272 kl_loss: 0.000000 normal_loss: 0.010272\n",
      "[092/00007] train_loss: 0.010392 kl_loss: 0.000000 normal_loss: 0.010392\n",
      "[094/00005] train_loss: 0.010156 kl_loss: 0.000000 normal_loss: 0.010156\n",
      "[096/00003] train_loss: 0.010084 kl_loss: 0.000000 normal_loss: 0.010084\n",
      "[098/00001] train_loss: 0.009890 kl_loss: 0.000000 normal_loss: 0.009890\n",
      "[099/00050] train_loss: 0.009625 kl_loss: 0.000000 normal_loss: 0.009625\n",
      "[099/00050] IOU 0.8885892022341232\n",
      "[101/00048] train_loss: 0.006687 kl_loss: 0.000000 normal_loss: 0.006687\n",
      "[103/00046] train_loss: 0.006355 kl_loss: 0.000000 normal_loss: 0.006355\n",
      "[105/00044] train_loss: 0.006316 kl_loss: 0.000000 normal_loss: 0.006316\n",
      "[107/00042] train_loss: 0.006515 kl_loss: 0.000000 normal_loss: 0.006515\n",
      "[109/00040] train_loss: 0.006439 kl_loss: 0.000000 normal_loss: 0.006439\n",
      "[111/00038] train_loss: 0.006445 kl_loss: 0.000000 normal_loss: 0.006445\n",
      "[113/00036] train_loss: 0.006852 kl_loss: 0.000000 normal_loss: 0.006852\n",
      "[115/00034] train_loss: 0.006719 kl_loss: 0.000000 normal_loss: 0.006719\n",
      "[117/00032] train_loss: 0.006506 kl_loss: 0.000000 normal_loss: 0.006506\n",
      "[119/00030] train_loss: 0.006467 kl_loss: 0.000000 normal_loss: 0.006467\n",
      "[121/00028] train_loss: 0.006596 kl_loss: 0.000000 normal_loss: 0.006596\n",
      "[123/00026] train_loss: 0.006502 kl_loss: 0.000000 normal_loss: 0.006502\n",
      "[125/00024] train_loss: 0.006630 kl_loss: 0.000000 normal_loss: 0.006630\n",
      "[127/00022] train_loss: 0.006474 kl_loss: 0.000000 normal_loss: 0.006474\n",
      "[129/00020] train_loss: 0.006653 kl_loss: 0.000000 normal_loss: 0.006653\n",
      "[131/00018] train_loss: 0.006524 kl_loss: 0.000000 normal_loss: 0.006524\n",
      "[133/00016] train_loss: 0.006183 kl_loss: 0.000000 normal_loss: 0.006183\n",
      "[135/00014] train_loss: 0.005946 kl_loss: 0.000000 normal_loss: 0.005946\n",
      "[137/00012] train_loss: 0.006370 kl_loss: 0.000000 normal_loss: 0.006370\n",
      "[139/00010] train_loss: 0.006504 kl_loss: 0.000000 normal_loss: 0.006504\n",
      "[141/00008] train_loss: 0.006158 kl_loss: 0.000000 normal_loss: 0.006158\n",
      "[143/00006] train_loss: 0.006420 kl_loss: 0.000000 normal_loss: 0.006420\n",
      "[145/00004] train_loss: 0.006318 kl_loss: 0.000000 normal_loss: 0.006318\n",
      "[147/00002] train_loss: 0.005959 kl_loss: 0.000000 normal_loss: 0.005959\n",
      "[149/00000] train_loss: 0.006167 kl_loss: 0.000000 normal_loss: 0.006167\n",
      "[149/00050] IOU 0.9118488400400937\n",
      "[150/00049] train_loss: 0.006267 kl_loss: 0.000000 normal_loss: 0.006267\n",
      "[152/00047] train_loss: 0.005839 kl_loss: 0.000000 normal_loss: 0.005839\n",
      "[154/00045] train_loss: 0.005782 kl_loss: 0.000000 normal_loss: 0.005782\n",
      "[156/00043] train_loss: 0.005699 kl_loss: 0.000000 normal_loss: 0.005699\n",
      "[158/00041] train_loss: 0.006232 kl_loss: 0.000000 normal_loss: 0.006232\n",
      "[160/00039] train_loss: 0.006133 kl_loss: 0.000000 normal_loss: 0.006133\n",
      "[162/00037] train_loss: 0.005951 kl_loss: 0.000000 normal_loss: 0.005951\n",
      "[164/00035] train_loss: 0.005979 kl_loss: 0.000000 normal_loss: 0.005979\n",
      "[166/00033] train_loss: 0.005948 kl_loss: 0.000000 normal_loss: 0.005948\n",
      "[168/00031] train_loss: 0.005905 kl_loss: 0.000000 normal_loss: 0.005905\n",
      "[170/00029] train_loss: 0.005624 kl_loss: 0.000000 normal_loss: 0.005624\n",
      "[172/00027] train_loss: 0.005861 kl_loss: 0.000000 normal_loss: 0.005861\n",
      "[174/00025] train_loss: 0.005777 kl_loss: 0.000000 normal_loss: 0.005777\n",
      "[176/00023] train_loss: 0.005548 kl_loss: 0.000000 normal_loss: 0.005548\n",
      "[178/00021] train_loss: 0.005477 kl_loss: 0.000000 normal_loss: 0.005477\n",
      "[180/00019] train_loss: 0.005532 kl_loss: 0.000000 normal_loss: 0.005532\n",
      "[182/00017] train_loss: 0.005863 kl_loss: 0.000000 normal_loss: 0.005863\n",
      "[184/00015] train_loss: 0.005802 kl_loss: 0.000000 normal_loss: 0.005802\n",
      "[186/00013] train_loss: 0.005245 kl_loss: 0.000000 normal_loss: 0.005245\n",
      "[188/00011] train_loss: 0.005779 kl_loss: 0.000000 normal_loss: 0.005779\n",
      "[190/00009] train_loss: 0.005693 kl_loss: 0.000000 normal_loss: 0.005693\n",
      "[192/00007] train_loss: 0.005587 kl_loss: 0.000000 normal_loss: 0.005587\n",
      "[194/00005] train_loss: 0.005504 kl_loss: 0.000000 normal_loss: 0.005504\n",
      "[196/00003] train_loss: 0.005209 kl_loss: 0.000000 normal_loss: 0.005209\n",
      "[198/00001] train_loss: 0.005176 kl_loss: 0.000000 normal_loss: 0.005176\n",
      "[199/00050] train_loss: 0.005617 kl_loss: 0.000000 normal_loss: 0.005617\n",
      "[199/00050] IOU 0.9256172117458286\n",
      "[201/00048] train_loss: 0.004190 kl_loss: 0.000000 normal_loss: 0.004190\n",
      "[203/00046] train_loss: 0.004088 kl_loss: 0.000000 normal_loss: 0.004088\n",
      "[205/00044] train_loss: 0.004151 kl_loss: 0.000000 normal_loss: 0.004151\n",
      "[207/00042] train_loss: 0.003992 kl_loss: 0.000000 normal_loss: 0.003992\n",
      "[209/00040] train_loss: 0.003967 kl_loss: 0.000000 normal_loss: 0.003967\n",
      "[211/00038] train_loss: 0.004063 kl_loss: 0.000000 normal_loss: 0.004063\n",
      "[213/00036] train_loss: 0.003931 kl_loss: 0.000000 normal_loss: 0.003931\n",
      "[215/00034] train_loss: 0.004004 kl_loss: 0.000000 normal_loss: 0.004004\n",
      "[217/00032] train_loss: 0.003941 kl_loss: 0.000000 normal_loss: 0.003941\n",
      "[219/00030] train_loss: 0.004066 kl_loss: 0.000000 normal_loss: 0.004066\n",
      "[221/00028] train_loss: 0.003951 kl_loss: 0.000000 normal_loss: 0.003951\n",
      "[223/00026] train_loss: 0.004050 kl_loss: 0.000000 normal_loss: 0.004050\n",
      "[225/00024] train_loss: 0.003784 kl_loss: 0.000000 normal_loss: 0.003784\n",
      "[227/00022] train_loss: 0.004005 kl_loss: 0.000000 normal_loss: 0.004005\n",
      "[229/00020] train_loss: 0.003875 kl_loss: 0.000000 normal_loss: 0.003875\n",
      "[231/00018] train_loss: 0.004033 kl_loss: 0.000000 normal_loss: 0.004033\n",
      "[233/00016] train_loss: 0.003944 kl_loss: 0.000000 normal_loss: 0.003944\n",
      "[235/00014] train_loss: 0.003814 kl_loss: 0.000000 normal_loss: 0.003814\n",
      "[237/00012] train_loss: 0.003859 kl_loss: 0.000000 normal_loss: 0.003859\n",
      "[239/00010] train_loss: 0.003986 kl_loss: 0.000000 normal_loss: 0.003986\n",
      "[241/00008] train_loss: 0.004073 kl_loss: 0.000000 normal_loss: 0.004073\n",
      "[243/00006] train_loss: 0.004000 kl_loss: 0.000000 normal_loss: 0.004000\n",
      "[245/00004] train_loss: 0.003843 kl_loss: 0.000000 normal_loss: 0.003843\n",
      "[247/00002] train_loss: 0.004078 kl_loss: 0.000000 normal_loss: 0.004078\n",
      "[249/00000] train_loss: 0.003961 kl_loss: 0.000000 normal_loss: 0.003961\n",
      "[249/00050] IOU 0.9346959705111123\n",
      "[250/00049] train_loss: 0.003924 kl_loss: 0.000000 normal_loss: 0.003924\n",
      "[252/00047] train_loss: 0.003846 kl_loss: 0.000000 normal_loss: 0.003846\n",
      "[254/00045] train_loss: 0.003735 kl_loss: 0.000000 normal_loss: 0.003735\n",
      "[256/00043] train_loss: 0.003948 kl_loss: 0.000000 normal_loss: 0.003948\n",
      "[258/00041] train_loss: 0.003995 kl_loss: 0.000000 normal_loss: 0.003995\n",
      "[260/00039] train_loss: 0.003818 kl_loss: 0.000000 normal_loss: 0.003818\n",
      "[262/00037] train_loss: 0.003759 kl_loss: 0.000000 normal_loss: 0.003759\n",
      "[264/00035] train_loss: 0.003703 kl_loss: 0.000000 normal_loss: 0.003703\n",
      "[266/00033] train_loss: 0.003788 kl_loss: 0.000000 normal_loss: 0.003788\n",
      "[268/00031] train_loss: 0.003723 kl_loss: 0.000000 normal_loss: 0.003723\n",
      "[270/00029] train_loss: 0.003873 kl_loss: 0.000000 normal_loss: 0.003873\n",
      "[272/00027] train_loss: 0.003660 kl_loss: 0.000000 normal_loss: 0.003660\n",
      "[274/00025] train_loss: 0.003853 kl_loss: 0.000000 normal_loss: 0.003853\n",
      "[276/00023] train_loss: 0.003877 kl_loss: 0.000000 normal_loss: 0.003877\n",
      "[278/00021] train_loss: 0.003803 kl_loss: 0.000000 normal_loss: 0.003803\n",
      "[280/00019] train_loss: 0.003703 kl_loss: 0.000000 normal_loss: 0.003703\n",
      "[282/00017] train_loss: 0.003841 kl_loss: 0.000000 normal_loss: 0.003841\n",
      "[284/00015] train_loss: 0.003895 kl_loss: 0.000000 normal_loss: 0.003895\n",
      "[286/00013] train_loss: 0.003555 kl_loss: 0.000000 normal_loss: 0.003555\n",
      "[288/00011] train_loss: 0.003735 kl_loss: 0.000000 normal_loss: 0.003735\n",
      "[290/00009] train_loss: 0.003763 kl_loss: 0.000000 normal_loss: 0.003763\n",
      "[292/00007] train_loss: 0.003704 kl_loss: 0.000000 normal_loss: 0.003704\n",
      "[294/00005] train_loss: 0.003652 kl_loss: 0.000000 normal_loss: 0.003652\n",
      "[296/00003] train_loss: 0.003586 kl_loss: 0.000000 normal_loss: 0.003586\n",
      "[298/00001] train_loss: 0.003862 kl_loss: 0.000000 normal_loss: 0.003862\n",
      "[299/00050] train_loss: 0.003801 kl_loss: 0.000000 normal_loss: 0.003801\n",
      "[299/00050] IOU 0.9409190300294878\n",
      "[301/00048] train_loss: 0.003255 kl_loss: 0.000000 normal_loss: 0.003255\n",
      "[303/00046] train_loss: 0.003081 kl_loss: 0.000000 normal_loss: 0.003081\n",
      "[305/00044] train_loss: 0.003031 kl_loss: 0.000000 normal_loss: 0.003031\n",
      "[307/00042] train_loss: 0.003017 kl_loss: 0.000000 normal_loss: 0.003017\n",
      "[309/00040] train_loss: 0.003090 kl_loss: 0.000000 normal_loss: 0.003090\n",
      "[311/00038] train_loss: 0.003025 kl_loss: 0.000000 normal_loss: 0.003025\n",
      "[313/00036] train_loss: 0.003138 kl_loss: 0.000000 normal_loss: 0.003138\n",
      "[315/00034] train_loss: 0.003145 kl_loss: 0.000000 normal_loss: 0.003145\n",
      "[317/00032] train_loss: 0.003234 kl_loss: 0.000000 normal_loss: 0.003234\n",
      "[319/00030] train_loss: 0.003039 kl_loss: 0.000000 normal_loss: 0.003039\n",
      "[321/00028] train_loss: 0.003081 kl_loss: 0.000000 normal_loss: 0.003081\n",
      "[323/00026] train_loss: 0.003025 kl_loss: 0.000000 normal_loss: 0.003025\n",
      "[325/00024] train_loss: 0.003036 kl_loss: 0.000000 normal_loss: 0.003036\n",
      "[327/00022] train_loss: 0.002954 kl_loss: 0.000000 normal_loss: 0.002954\n",
      "[329/00020] train_loss: 0.003022 kl_loss: 0.000000 normal_loss: 0.003022\n",
      "[331/00018] train_loss: 0.003104 kl_loss: 0.000000 normal_loss: 0.003104\n",
      "[333/00016] train_loss: 0.003053 kl_loss: 0.000000 normal_loss: 0.003053\n",
      "[335/00014] train_loss: 0.003131 kl_loss: 0.000000 normal_loss: 0.003131\n",
      "[337/00012] train_loss: 0.002940 kl_loss: 0.000000 normal_loss: 0.002940\n",
      "[339/00010] train_loss: 0.003013 kl_loss: 0.000000 normal_loss: 0.003013\n",
      "[341/00008] train_loss: 0.003054 kl_loss: 0.000000 normal_loss: 0.003054\n",
      "[343/00006] train_loss: 0.003020 kl_loss: 0.000000 normal_loss: 0.003020\n",
      "[345/00004] train_loss: 0.003072 kl_loss: 0.000000 normal_loss: 0.003072\n",
      "[347/00002] train_loss: 0.003019 kl_loss: 0.000000 normal_loss: 0.003019\n",
      "[349/00000] train_loss: 0.002970 kl_loss: 0.000000 normal_loss: 0.002970\n",
      "[349/00050] IOU 0.9448994673300438\n",
      "[350/00049] train_loss: 0.002997 kl_loss: 0.000000 normal_loss: 0.002997\n",
      "[352/00047] train_loss: 0.003048 kl_loss: 0.000000 normal_loss: 0.003048\n",
      "[354/00045] train_loss: 0.003040 kl_loss: 0.000000 normal_loss: 0.003040\n",
      "[356/00043] train_loss: 0.002957 kl_loss: 0.000000 normal_loss: 0.002957\n",
      "[358/00041] train_loss: 0.003000 kl_loss: 0.000000 normal_loss: 0.003000\n",
      "[360/00039] train_loss: 0.002947 kl_loss: 0.000000 normal_loss: 0.002947\n",
      "[362/00037] train_loss: 0.002954 kl_loss: 0.000000 normal_loss: 0.002954\n",
      "[364/00035] train_loss: 0.002963 kl_loss: 0.000000 normal_loss: 0.002963\n",
      "[366/00033] train_loss: 0.002984 kl_loss: 0.000000 normal_loss: 0.002984\n",
      "[368/00031] train_loss: 0.003052 kl_loss: 0.000000 normal_loss: 0.003052\n",
      "[370/00029] train_loss: 0.002905 kl_loss: 0.000000 normal_loss: 0.002905\n",
      "[372/00027] train_loss: 0.002975 kl_loss: 0.000000 normal_loss: 0.002975\n",
      "[374/00025] train_loss: 0.002945 kl_loss: 0.000000 normal_loss: 0.002945\n",
      "[376/00023] train_loss: 0.002922 kl_loss: 0.000000 normal_loss: 0.002922\n",
      "[378/00021] train_loss: 0.002909 kl_loss: 0.000000 normal_loss: 0.002909\n",
      "[380/00019] train_loss: 0.002939 kl_loss: 0.000000 normal_loss: 0.002939\n",
      "[382/00017] train_loss: 0.002971 kl_loss: 0.000000 normal_loss: 0.002971\n",
      "[384/00015] train_loss: 0.002892 kl_loss: 0.000000 normal_loss: 0.002892\n",
      "[386/00013] train_loss: 0.002953 kl_loss: 0.000000 normal_loss: 0.002953\n",
      "[388/00011] train_loss: 0.002892 kl_loss: 0.000000 normal_loss: 0.002892\n",
      "[390/00009] train_loss: 0.002880 kl_loss: 0.000000 normal_loss: 0.002880\n",
      "[392/00007] train_loss: 0.002916 kl_loss: 0.000000 normal_loss: 0.002916\n",
      "[394/00005] train_loss: 0.002930 kl_loss: 0.000000 normal_loss: 0.002930\n",
      "[396/00003] train_loss: 0.002926 kl_loss: 0.000000 normal_loss: 0.002926\n",
      "[398/00001] train_loss: 0.002911 kl_loss: 0.000000 normal_loss: 0.002911\n",
      "[399/00050] train_loss: 0.002905 kl_loss: 0.000000 normal_loss: 0.002905\n",
      "[399/00050] IOU 0.9468951126922046\n",
      "[401/00048] train_loss: 0.002722 kl_loss: 0.000000 normal_loss: 0.002722\n",
      "[403/00046] train_loss: 0.002626 kl_loss: 0.000000 normal_loss: 0.002626\n",
      "[405/00044] train_loss: 0.002706 kl_loss: 0.000000 normal_loss: 0.002706\n",
      "[407/00042] train_loss: 0.002621 kl_loss: 0.000000 normal_loss: 0.002621\n",
      "[409/00040] train_loss: 0.002653 kl_loss: 0.000000 normal_loss: 0.002653\n",
      "[411/00038] train_loss: 0.002629 kl_loss: 0.000000 normal_loss: 0.002629\n",
      "[413/00036] train_loss: 0.002617 kl_loss: 0.000000 normal_loss: 0.002617\n",
      "[415/00034] train_loss: 0.002619 kl_loss: 0.000000 normal_loss: 0.002619\n",
      "[417/00032] train_loss: 0.002685 kl_loss: 0.000000 normal_loss: 0.002685\n",
      "[419/00030] train_loss: 0.002677 kl_loss: 0.000000 normal_loss: 0.002677\n",
      "[421/00028] train_loss: 0.002639 kl_loss: 0.000000 normal_loss: 0.002639\n",
      "[423/00026] train_loss: 0.002647 kl_loss: 0.000000 normal_loss: 0.002647\n",
      "[425/00024] train_loss: 0.002607 kl_loss: 0.000000 normal_loss: 0.002607\n",
      "[427/00022] train_loss: 0.002590 kl_loss: 0.000000 normal_loss: 0.002590\n",
      "[429/00020] train_loss: 0.002619 kl_loss: 0.000000 normal_loss: 0.002619\n",
      "[431/00018] train_loss: 0.002590 kl_loss: 0.000000 normal_loss: 0.002590\n",
      "[433/00016] train_loss: 0.002600 kl_loss: 0.000000 normal_loss: 0.002600\n",
      "[435/00014] train_loss: 0.002608 kl_loss: 0.000000 normal_loss: 0.002608\n",
      "[437/00012] train_loss: 0.002611 kl_loss: 0.000000 normal_loss: 0.002611\n",
      "[439/00010] train_loss: 0.002590 kl_loss: 0.000000 normal_loss: 0.002590\n",
      "[441/00008] train_loss: 0.002608 kl_loss: 0.000000 normal_loss: 0.002608\n",
      "[443/00006] train_loss: 0.002603 kl_loss: 0.000000 normal_loss: 0.002603\n",
      "[445/00004] train_loss: 0.002616 kl_loss: 0.000000 normal_loss: 0.002616\n",
      "[447/00002] train_loss: 0.002620 kl_loss: 0.000000 normal_loss: 0.002620\n",
      "[449/00000] train_loss: 0.002606 kl_loss: 0.000000 normal_loss: 0.002606\n",
      "[449/00050] IOU 0.9495383341910665\n",
      "[450/00049] train_loss: 0.002602 kl_loss: 0.000000 normal_loss: 0.002602\n",
      "[452/00047] train_loss: 0.002635 kl_loss: 0.000000 normal_loss: 0.002635\n",
      "[454/00045] train_loss: 0.002679 kl_loss: 0.000000 normal_loss: 0.002679\n",
      "[456/00043] train_loss: 0.002605 kl_loss: 0.000000 normal_loss: 0.002605\n",
      "[458/00041] train_loss: 0.002596 kl_loss: 0.000000 normal_loss: 0.002596\n",
      "[460/00039] train_loss: 0.002581 kl_loss: 0.000000 normal_loss: 0.002581\n",
      "[462/00037] train_loss: 0.002634 kl_loss: 0.000000 normal_loss: 0.002634\n",
      "[464/00035] train_loss: 0.002574 kl_loss: 0.000000 normal_loss: 0.002574\n",
      "[466/00033] train_loss: 0.002574 kl_loss: 0.000000 normal_loss: 0.002574\n",
      "[468/00031] train_loss: 0.002642 kl_loss: 0.000000 normal_loss: 0.002642\n",
      "[470/00029] train_loss: 0.002573 kl_loss: 0.000000 normal_loss: 0.002573\n",
      "[472/00027] train_loss: 0.002558 kl_loss: 0.000000 normal_loss: 0.002558\n",
      "[474/00025] train_loss: 0.002524 kl_loss: 0.000000 normal_loss: 0.002524\n",
      "[476/00023] train_loss: 0.002595 kl_loss: 0.000000 normal_loss: 0.002595\n",
      "[478/00021] train_loss: 0.002551 kl_loss: 0.000000 normal_loss: 0.002551\n",
      "[480/00019] train_loss: 0.002536 kl_loss: 0.000000 normal_loss: 0.002536\n",
      "[482/00017] train_loss: 0.002541 kl_loss: 0.000000 normal_loss: 0.002541\n",
      "[484/00015] train_loss: 0.002522 kl_loss: 0.000000 normal_loss: 0.002522\n",
      "[486/00013] train_loss: 0.002624 kl_loss: 0.000000 normal_loss: 0.002624\n",
      "[488/00011] train_loss: 0.002531 kl_loss: 0.000000 normal_loss: 0.002531\n",
      "[490/00009] train_loss: 0.002525 kl_loss: 0.000000 normal_loss: 0.002525\n",
      "[492/00007] train_loss: 0.002606 kl_loss: 0.000000 normal_loss: 0.002606\n",
      "[494/00005] train_loss: 0.002620 kl_loss: 0.000000 normal_loss: 0.002620\n",
      "[496/00003] train_loss: 0.002541 kl_loss: 0.000000 normal_loss: 0.002541\n",
      "[498/00001] train_loss: 0.002546 kl_loss: 0.000000 normal_loss: 0.002546\n",
      "[499/00050] train_loss: 0.002573 kl_loss: 0.000000 normal_loss: 0.002573\n",
      "[499/00050] IOU 0.9503749401777135\n",
      "[501/00048] train_loss: 0.002475 kl_loss: 0.000000 normal_loss: 0.002475\n",
      "[503/00046] train_loss: 0.002424 kl_loss: 0.000000 normal_loss: 0.002424\n",
      "[505/00044] train_loss: 0.002428 kl_loss: 0.000000 normal_loss: 0.002428\n",
      "[507/00042] train_loss: 0.002423 kl_loss: 0.000000 normal_loss: 0.002423\n",
      "[509/00040] train_loss: 0.002429 kl_loss: 0.000000 normal_loss: 0.002429\n",
      "[511/00038] train_loss: 0.002437 kl_loss: 0.000000 normal_loss: 0.002437\n",
      "[513/00036] train_loss: 0.002428 kl_loss: 0.000000 normal_loss: 0.002428\n",
      "[515/00034] train_loss: 0.002434 kl_loss: 0.000000 normal_loss: 0.002434\n",
      "[517/00032] train_loss: 0.002470 kl_loss: 0.000000 normal_loss: 0.002470\n",
      "[519/00030] train_loss: 0.002429 kl_loss: 0.000000 normal_loss: 0.002429\n",
      "[521/00028] train_loss: 0.002435 kl_loss: 0.000000 normal_loss: 0.002435\n",
      "[523/00026] train_loss: 0.002443 kl_loss: 0.000000 normal_loss: 0.002443\n",
      "[525/00024] train_loss: 0.002432 kl_loss: 0.000000 normal_loss: 0.002432\n",
      "[527/00022] train_loss: 0.002453 kl_loss: 0.000000 normal_loss: 0.002453\n",
      "[529/00020] train_loss: 0.002408 kl_loss: 0.000000 normal_loss: 0.002408\n",
      "[531/00018] train_loss: 0.002432 kl_loss: 0.000000 normal_loss: 0.002432\n",
      "[533/00016] train_loss: 0.002431 kl_loss: 0.000000 normal_loss: 0.002431\n",
      "[535/00014] train_loss: 0.002428 kl_loss: 0.000000 normal_loss: 0.002428\n",
      "[537/00012] train_loss: 0.002444 kl_loss: 0.000000 normal_loss: 0.002444\n",
      "[539/00010] train_loss: 0.002421 kl_loss: 0.000000 normal_loss: 0.002421\n",
      "[541/00008] train_loss: 0.002461 kl_loss: 0.000000 normal_loss: 0.002461\n",
      "[543/00006] train_loss: 0.002450 kl_loss: 0.000000 normal_loss: 0.002450\n",
      "[545/00004] train_loss: 0.002411 kl_loss: 0.000000 normal_loss: 0.002411\n",
      "[547/00002] train_loss: 0.002439 kl_loss: 0.000000 normal_loss: 0.002439\n",
      "[549/00000] train_loss: 0.002399 kl_loss: 0.000000 normal_loss: 0.002399\n",
      "[549/00050] IOU 0.9521024298977351\n",
      "[550/00049] train_loss: 0.002439 kl_loss: 0.000000 normal_loss: 0.002439\n",
      "[552/00047] train_loss: 0.002406 kl_loss: 0.000000 normal_loss: 0.002406\n",
      "[554/00045] train_loss: 0.002416 kl_loss: 0.000000 normal_loss: 0.002416\n",
      "[556/00043] train_loss: 0.002406 kl_loss: 0.000000 normal_loss: 0.002406\n",
      "[558/00041] train_loss: 0.002396 kl_loss: 0.000000 normal_loss: 0.002396\n",
      "[560/00039] train_loss: 0.002409 kl_loss: 0.000000 normal_loss: 0.002409\n",
      "[562/00037] train_loss: 0.002398 kl_loss: 0.000000 normal_loss: 0.002398\n",
      "[564/00035] train_loss: 0.002433 kl_loss: 0.000000 normal_loss: 0.002433\n",
      "[566/00033] train_loss: 0.002401 kl_loss: 0.000000 normal_loss: 0.002401\n",
      "[568/00031] train_loss: 0.002402 kl_loss: 0.000000 normal_loss: 0.002402\n",
      "[570/00029] train_loss: 0.002395 kl_loss: 0.000000 normal_loss: 0.002395\n",
      "[572/00027] train_loss: 0.002397 kl_loss: 0.000000 normal_loss: 0.002397\n",
      "[574/00025] train_loss: 0.002431 kl_loss: 0.000000 normal_loss: 0.002431\n",
      "[576/00023] train_loss: 0.002410 kl_loss: 0.000000 normal_loss: 0.002410\n",
      "[578/00021] train_loss: 0.002402 kl_loss: 0.000000 normal_loss: 0.002402\n",
      "[580/00019] train_loss: 0.002373 kl_loss: 0.000000 normal_loss: 0.002373\n",
      "[582/00017] train_loss: 0.002418 kl_loss: 0.000000 normal_loss: 0.002418\n",
      "[584/00015] train_loss: 0.002380 kl_loss: 0.000000 normal_loss: 0.002380\n",
      "[586/00013] train_loss: 0.002376 kl_loss: 0.000000 normal_loss: 0.002376\n",
      "[588/00011] train_loss: 0.002387 kl_loss: 0.000000 normal_loss: 0.002387\n",
      "[590/00009] train_loss: 0.002386 kl_loss: 0.000000 normal_loss: 0.002386\n",
      "[592/00007] train_loss: 0.002387 kl_loss: 0.000000 normal_loss: 0.002387\n",
      "[594/00005] train_loss: 0.002383 kl_loss: 0.000000 normal_loss: 0.002383\n",
      "[596/00003] train_loss: 0.002383 kl_loss: 0.000000 normal_loss: 0.002383\n",
      "[598/00001] train_loss: 0.002381 kl_loss: 0.000000 normal_loss: 0.002381\n",
      "[599/00050] train_loss: 0.002367 kl_loss: 0.000000 normal_loss: 0.002367\n",
      "[599/00050] IOU 0.9526207224916027\n",
      "[601/00048] train_loss: 0.002342 kl_loss: 0.000000 normal_loss: 0.002342\n",
      "[603/00046] train_loss: 0.002340 kl_loss: 0.000000 normal_loss: 0.002340\n",
      "[605/00044] train_loss: 0.002341 kl_loss: 0.000000 normal_loss: 0.002341\n",
      "[607/00042] train_loss: 0.002323 kl_loss: 0.000000 normal_loss: 0.002323\n",
      "[609/00040] train_loss: 0.002339 kl_loss: 0.000000 normal_loss: 0.002339\n",
      "[611/00038] train_loss: 0.002327 kl_loss: 0.000000 normal_loss: 0.002327\n",
      "[613/00036] train_loss: 0.002320 kl_loss: 0.000000 normal_loss: 0.002320\n",
      "[615/00034] train_loss: 0.002334 kl_loss: 0.000000 normal_loss: 0.002334\n",
      "[617/00032] train_loss: 0.002341 kl_loss: 0.000000 normal_loss: 0.002341\n",
      "[619/00030] train_loss: 0.002330 kl_loss: 0.000000 normal_loss: 0.002330\n",
      "[621/00028] train_loss: 0.002361 kl_loss: 0.000000 normal_loss: 0.002361\n",
      "[623/00026] train_loss: 0.002335 kl_loss: 0.000000 normal_loss: 0.002335\n",
      "[625/00024] train_loss: 0.002327 kl_loss: 0.000000 normal_loss: 0.002327\n",
      "[627/00022] train_loss: 0.002332 kl_loss: 0.000000 normal_loss: 0.002332\n",
      "[629/00020] train_loss: 0.002319 kl_loss: 0.000000 normal_loss: 0.002319\n",
      "[631/00018] train_loss: 0.002322 kl_loss: 0.000000 normal_loss: 0.002322\n",
      "[633/00016] train_loss: 0.002344 kl_loss: 0.000000 normal_loss: 0.002344\n",
      "[635/00014] train_loss: 0.002336 kl_loss: 0.000000 normal_loss: 0.002336\n",
      "[637/00012] train_loss: 0.002314 kl_loss: 0.000000 normal_loss: 0.002314\n",
      "[639/00010] train_loss: 0.002334 kl_loss: 0.000000 normal_loss: 0.002334\n",
      "[641/00008] train_loss: 0.002338 kl_loss: 0.000000 normal_loss: 0.002338\n",
      "[643/00006] train_loss: 0.002299 kl_loss: 0.000000 normal_loss: 0.002299\n",
      "[645/00004] train_loss: 0.002320 kl_loss: 0.000000 normal_loss: 0.002320\n",
      "[647/00002] train_loss: 0.002352 kl_loss: 0.000000 normal_loss: 0.002352\n",
      "[649/00000] train_loss: 0.002341 kl_loss: 0.000000 normal_loss: 0.002341\n",
      "[649/00050] IOU 0.9529927011915427\n",
      "[650/00049] train_loss: 0.002325 kl_loss: 0.000000 normal_loss: 0.002325\n",
      "[652/00047] train_loss: 0.002331 kl_loss: 0.000000 normal_loss: 0.002331\n",
      "[654/00045] train_loss: 0.002317 kl_loss: 0.000000 normal_loss: 0.002317\n",
      "[656/00043] train_loss: 0.002316 kl_loss: 0.000000 normal_loss: 0.002316\n",
      "[658/00041] train_loss: 0.002320 kl_loss: 0.000000 normal_loss: 0.002320\n",
      "[660/00039] train_loss: 0.002321 kl_loss: 0.000000 normal_loss: 0.002321\n",
      "[662/00037] train_loss: 0.002316 kl_loss: 0.000000 normal_loss: 0.002316\n",
      "[664/00035] train_loss: 0.002335 kl_loss: 0.000000 normal_loss: 0.002335\n",
      "[666/00033] train_loss: 0.002310 kl_loss: 0.000000 normal_loss: 0.002310\n",
      "[668/00031] train_loss: 0.002309 kl_loss: 0.000000 normal_loss: 0.002309\n",
      "[670/00029] train_loss: 0.002299 kl_loss: 0.000000 normal_loss: 0.002299\n",
      "[672/00027] train_loss: 0.002338 kl_loss: 0.000000 normal_loss: 0.002338\n",
      "[674/00025] train_loss: 0.002312 kl_loss: 0.000000 normal_loss: 0.002312\n",
      "[676/00023] train_loss: 0.002328 kl_loss: 0.000000 normal_loss: 0.002328\n",
      "[678/00021] train_loss: 0.002328 kl_loss: 0.000000 normal_loss: 0.002328\n",
      "[680/00019] train_loss: 0.002296 kl_loss: 0.000000 normal_loss: 0.002296\n",
      "[682/00017] train_loss: 0.002320 kl_loss: 0.000000 normal_loss: 0.002320\n",
      "[684/00015] train_loss: 0.002309 kl_loss: 0.000000 normal_loss: 0.002309\n",
      "[686/00013] train_loss: 0.002308 kl_loss: 0.000000 normal_loss: 0.002308\n",
      "[688/00011] train_loss: 0.002327 kl_loss: 0.000000 normal_loss: 0.002327\n",
      "[690/00009] train_loss: 0.002322 kl_loss: 0.000000 normal_loss: 0.002322\n",
      "[692/00007] train_loss: 0.002306 kl_loss: 0.000000 normal_loss: 0.002306\n",
      "[694/00005] train_loss: 0.002316 kl_loss: 0.000000 normal_loss: 0.002316\n",
      "[696/00003] train_loss: 0.002305 kl_loss: 0.000000 normal_loss: 0.002305\n",
      "[698/00001] train_loss: 0.002288 kl_loss: 0.000000 normal_loss: 0.002288\n",
      "[699/00050] train_loss: 0.002295 kl_loss: 0.000000 normal_loss: 0.002295\n",
      "[699/00050] IOU 0.9531763271694572\n",
      "[701/00048] train_loss: 0.002288 kl_loss: 0.000000 normal_loss: 0.002288\n",
      "[703/00046] train_loss: 0.002288 kl_loss: 0.000000 normal_loss: 0.002288\n",
      "[705/00044] train_loss: 0.002282 kl_loss: 0.000000 normal_loss: 0.002282\n",
      "[707/00042] train_loss: 0.002268 kl_loss: 0.000000 normal_loss: 0.002268\n",
      "[709/00040] train_loss: 0.002296 kl_loss: 0.000000 normal_loss: 0.002296\n",
      "[711/00038] train_loss: 0.002284 kl_loss: 0.000000 normal_loss: 0.002284\n",
      "[713/00036] train_loss: 0.002287 kl_loss: 0.000000 normal_loss: 0.002287\n",
      "[715/00034] train_loss: 0.002267 kl_loss: 0.000000 normal_loss: 0.002267\n",
      "[717/00032] train_loss: 0.002283 kl_loss: 0.000000 normal_loss: 0.002283\n",
      "[719/00030] train_loss: 0.002273 kl_loss: 0.000000 normal_loss: 0.002273\n",
      "[721/00028] train_loss: 0.002288 kl_loss: 0.000000 normal_loss: 0.002288\n",
      "[723/00026] train_loss: 0.002270 kl_loss: 0.000000 normal_loss: 0.002270\n",
      "[725/00024] train_loss: 0.002286 kl_loss: 0.000000 normal_loss: 0.002286\n",
      "[727/00022] train_loss: 0.002278 kl_loss: 0.000000 normal_loss: 0.002278\n",
      "[729/00020] train_loss: 0.002288 kl_loss: 0.000000 normal_loss: 0.002288\n",
      "[731/00018] train_loss: 0.002272 kl_loss: 0.000000 normal_loss: 0.002272\n",
      "[733/00016] train_loss: 0.002286 kl_loss: 0.000000 normal_loss: 0.002286\n",
      "[735/00014] train_loss: 0.002277 kl_loss: 0.000000 normal_loss: 0.002277\n",
      "[737/00012] train_loss: 0.002294 kl_loss: 0.000000 normal_loss: 0.002294\n",
      "[739/00010] train_loss: 0.002266 kl_loss: 0.000000 normal_loss: 0.002266\n",
      "[741/00008] train_loss: 0.002275 kl_loss: 0.000000 normal_loss: 0.002275\n",
      "[743/00006] train_loss: 0.002277 kl_loss: 0.000000 normal_loss: 0.002277\n",
      "[745/00004] train_loss: 0.002281 kl_loss: 0.000000 normal_loss: 0.002281\n",
      "[747/00002] train_loss: 0.002273 kl_loss: 0.000000 normal_loss: 0.002273\n",
      "[749/00000] train_loss: 0.002277 kl_loss: 0.000000 normal_loss: 0.002277\n",
      "[749/00050] IOU 0.9535995807098075\n",
      "[750/00049] train_loss: 0.002268 kl_loss: 0.000000 normal_loss: 0.002268\n",
      "[752/00047] train_loss: 0.002282 kl_loss: 0.000000 normal_loss: 0.002282\n",
      "[754/00045] train_loss: 0.002279 kl_loss: 0.000000 normal_loss: 0.002279\n",
      "[756/00043] train_loss: 0.002270 kl_loss: 0.000000 normal_loss: 0.002270\n",
      "[758/00041] train_loss: 0.002273 kl_loss: 0.000000 normal_loss: 0.002273\n",
      "[760/00039] train_loss: 0.002280 kl_loss: 0.000000 normal_loss: 0.002280\n",
      "[762/00037] train_loss: 0.002285 kl_loss: 0.000000 normal_loss: 0.002285\n",
      "[764/00035] train_loss: 0.002268 kl_loss: 0.000000 normal_loss: 0.002268\n",
      "[766/00033] train_loss: 0.002265 kl_loss: 0.000000 normal_loss: 0.002265\n",
      "[768/00031] train_loss: 0.002272 kl_loss: 0.000000 normal_loss: 0.002272\n",
      "[770/00029] train_loss: 0.002267 kl_loss: 0.000000 normal_loss: 0.002267\n",
      "[772/00027] train_loss: 0.002270 kl_loss: 0.000000 normal_loss: 0.002270\n",
      "[774/00025] train_loss: 0.002271 kl_loss: 0.000000 normal_loss: 0.002271\n",
      "[776/00023] train_loss: 0.002276 kl_loss: 0.000000 normal_loss: 0.002276\n",
      "[778/00021] train_loss: 0.002266 kl_loss: 0.000000 normal_loss: 0.002266\n",
      "[780/00019] train_loss: 0.002262 kl_loss: 0.000000 normal_loss: 0.002262\n",
      "[782/00017] train_loss: 0.002263 kl_loss: 0.000000 normal_loss: 0.002263\n",
      "[784/00015] train_loss: 0.002270 kl_loss: 0.000000 normal_loss: 0.002270\n",
      "[786/00013] train_loss: 0.002262 kl_loss: 0.000000 normal_loss: 0.002262\n",
      "[788/00011] train_loss: 0.002269 kl_loss: 0.000000 normal_loss: 0.002269\n",
      "[790/00009] train_loss: 0.002265 kl_loss: 0.000000 normal_loss: 0.002265\n",
      "[792/00007] train_loss: 0.002261 kl_loss: 0.000000 normal_loss: 0.002261\n",
      "[794/00005] train_loss: 0.002270 kl_loss: 0.000000 normal_loss: 0.002270\n",
      "[796/00003] train_loss: 0.002255 kl_loss: 0.000000 normal_loss: 0.002255\n",
      "[798/00001] train_loss: 0.002266 kl_loss: 0.000000 normal_loss: 0.002266\n",
      "[799/00050] train_loss: 0.002266 kl_loss: 0.000000 normal_loss: 0.002266\n",
      "[799/00050] IOU 0.9538687331272734\n",
      "[801/00048] train_loss: 0.002250 kl_loss: 0.000000 normal_loss: 0.002250\n",
      "[803/00046] train_loss: 0.002257 kl_loss: 0.000000 normal_loss: 0.002257\n",
      "[805/00044] train_loss: 0.002251 kl_loss: 0.000000 normal_loss: 0.002251\n",
      "[807/00042] train_loss: 0.002244 kl_loss: 0.000000 normal_loss: 0.002244\n",
      "[809/00040] train_loss: 0.002251 kl_loss: 0.000000 normal_loss: 0.002251\n",
      "[811/00038] train_loss: 0.002250 kl_loss: 0.000000 normal_loss: 0.002250\n",
      "[813/00036] train_loss: 0.002259 kl_loss: 0.000000 normal_loss: 0.002259\n",
      "[815/00034] train_loss: 0.002253 kl_loss: 0.000000 normal_loss: 0.002253\n",
      "[817/00032] train_loss: 0.002257 kl_loss: 0.000000 normal_loss: 0.002257\n",
      "[819/00030] train_loss: 0.002242 kl_loss: 0.000000 normal_loss: 0.002242\n",
      "[821/00028] train_loss: 0.002260 kl_loss: 0.000000 normal_loss: 0.002260\n",
      "[823/00026] train_loss: 0.002243 kl_loss: 0.000000 normal_loss: 0.002243\n",
      "[825/00024] train_loss: 0.002261 kl_loss: 0.000000 normal_loss: 0.002261\n",
      "[827/00022] train_loss: 0.002256 kl_loss: 0.000000 normal_loss: 0.002256\n",
      "[829/00020] train_loss: 0.002241 kl_loss: 0.000000 normal_loss: 0.002241\n",
      "[831/00018] train_loss: 0.002257 kl_loss: 0.000000 normal_loss: 0.002257\n",
      "[833/00016] train_loss: 0.002249 kl_loss: 0.000000 normal_loss: 0.002249\n",
      "[835/00014] train_loss: 0.002245 kl_loss: 0.000000 normal_loss: 0.002245\n",
      "[837/00012] train_loss: 0.002250 kl_loss: 0.000000 normal_loss: 0.002250\n",
      "[839/00010] train_loss: 0.002246 kl_loss: 0.000000 normal_loss: 0.002246\n",
      "[841/00008] train_loss: 0.002246 kl_loss: 0.000000 normal_loss: 0.002246\n",
      "[843/00006] train_loss: 0.002241 kl_loss: 0.000000 normal_loss: 0.002241\n",
      "[845/00004] train_loss: 0.002256 kl_loss: 0.000000 normal_loss: 0.002256\n",
      "[847/00002] train_loss: 0.002244 kl_loss: 0.000000 normal_loss: 0.002244\n",
      "[849/00000] train_loss: 0.002255 kl_loss: 0.000000 normal_loss: 0.002255\n",
      "[849/00050] IOU 0.9540121351427436\n",
      "[850/00049] train_loss: 0.002252 kl_loss: 0.000000 normal_loss: 0.002252\n",
      "[852/00047] train_loss: 0.002254 kl_loss: 0.000000 normal_loss: 0.002254\n",
      "[854/00045] train_loss: 0.002251 kl_loss: 0.000000 normal_loss: 0.002251\n",
      "[856/00043] train_loss: 0.002239 kl_loss: 0.000000 normal_loss: 0.002239\n",
      "[858/00041] train_loss: 0.002243 kl_loss: 0.000000 normal_loss: 0.002243\n",
      "[860/00039] train_loss: 0.002257 kl_loss: 0.000000 normal_loss: 0.002257\n",
      "[862/00037] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[864/00035] train_loss: 0.002263 kl_loss: 0.000000 normal_loss: 0.002263\n",
      "[866/00033] train_loss: 0.002239 kl_loss: 0.000000 normal_loss: 0.002239\n",
      "[868/00031] train_loss: 0.002250 kl_loss: 0.000000 normal_loss: 0.002250\n",
      "[870/00029] train_loss: 0.002243 kl_loss: 0.000000 normal_loss: 0.002243\n",
      "[872/00027] train_loss: 0.002244 kl_loss: 0.000000 normal_loss: 0.002244\n",
      "[874/00025] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[876/00023] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[878/00021] train_loss: 0.002252 kl_loss: 0.000000 normal_loss: 0.002252\n",
      "[880/00019] train_loss: 0.002249 kl_loss: 0.000000 normal_loss: 0.002249\n",
      "[882/00017] train_loss: 0.002230 kl_loss: 0.000000 normal_loss: 0.002230\n",
      "[884/00015] train_loss: 0.002256 kl_loss: 0.000000 normal_loss: 0.002256\n",
      "[886/00013] train_loss: 0.002238 kl_loss: 0.000000 normal_loss: 0.002238\n",
      "[888/00011] train_loss: 0.002251 kl_loss: 0.000000 normal_loss: 0.002251\n",
      "[890/00009] train_loss: 0.002238 kl_loss: 0.000000 normal_loss: 0.002238\n",
      "[892/00007] train_loss: 0.002241 kl_loss: 0.000000 normal_loss: 0.002241\n",
      "[894/00005] train_loss: 0.002245 kl_loss: 0.000000 normal_loss: 0.002245\n",
      "[896/00003] train_loss: 0.002248 kl_loss: 0.000000 normal_loss: 0.002248\n",
      "[898/00001] train_loss: 0.002242 kl_loss: 0.000000 normal_loss: 0.002242\n",
      "[899/00050] train_loss: 0.002242 kl_loss: 0.000000 normal_loss: 0.002242\n",
      "[899/00050] IOU 0.954000507337496\n",
      "[901/00048] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[903/00046] train_loss: 0.002244 kl_loss: 0.000000 normal_loss: 0.002244\n",
      "[905/00044] train_loss: 0.002232 kl_loss: 0.000000 normal_loss: 0.002232\n",
      "[907/00042] train_loss: 0.002230 kl_loss: 0.000000 normal_loss: 0.002230\n",
      "[909/00040] train_loss: 0.002236 kl_loss: 0.000000 normal_loss: 0.002236\n",
      "[911/00038] train_loss: 0.002227 kl_loss: 0.000000 normal_loss: 0.002227\n",
      "[913/00036] train_loss: 0.002231 kl_loss: 0.000000 normal_loss: 0.002231\n",
      "[915/00034] train_loss: 0.002243 kl_loss: 0.000000 normal_loss: 0.002243\n",
      "[917/00032] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[919/00030] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[921/00028] train_loss: 0.002223 kl_loss: 0.000000 normal_loss: 0.002223\n",
      "[923/00026] train_loss: 0.002249 kl_loss: 0.000000 normal_loss: 0.002249\n",
      "[925/00024] train_loss: 0.002227 kl_loss: 0.000000 normal_loss: 0.002227\n",
      "[927/00022] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[929/00020] train_loss: 0.002241 kl_loss: 0.000000 normal_loss: 0.002241\n",
      "[931/00018] train_loss: 0.002243 kl_loss: 0.000000 normal_loss: 0.002243\n",
      "[933/00016] train_loss: 0.002236 kl_loss: 0.000000 normal_loss: 0.002236\n",
      "[935/00014] train_loss: 0.002225 kl_loss: 0.000000 normal_loss: 0.002225\n",
      "[937/00012] train_loss: 0.002230 kl_loss: 0.000000 normal_loss: 0.002230\n",
      "[939/00010] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[941/00008] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[943/00006] train_loss: 0.002232 kl_loss: 0.000000 normal_loss: 0.002232\n",
      "[945/00004] train_loss: 0.002229 kl_loss: 0.000000 normal_loss: 0.002229\n",
      "[947/00002] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[949/00000] train_loss: 0.002240 kl_loss: 0.000000 normal_loss: 0.002240\n",
      "[949/00050] IOU 0.9540956982944451\n",
      "[950/00049] train_loss: 0.002231 kl_loss: 0.000000 normal_loss: 0.002231\n",
      "[952/00047] train_loss: 0.002230 kl_loss: 0.000000 normal_loss: 0.002230\n",
      "[954/00045] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[956/00043] train_loss: 0.002227 kl_loss: 0.000000 normal_loss: 0.002227\n",
      "[958/00041] train_loss: 0.002240 kl_loss: 0.000000 normal_loss: 0.002240\n",
      "[960/00039] train_loss: 0.002229 kl_loss: 0.000000 normal_loss: 0.002229\n",
      "[962/00037] train_loss: 0.002232 kl_loss: 0.000000 normal_loss: 0.002232\n",
      "[964/00035] train_loss: 0.002237 kl_loss: 0.000000 normal_loss: 0.002237\n",
      "[966/00033] train_loss: 0.002227 kl_loss: 0.000000 normal_loss: 0.002227\n",
      "[968/00031] train_loss: 0.002230 kl_loss: 0.000000 normal_loss: 0.002230\n",
      "[970/00029] train_loss: 0.002240 kl_loss: 0.000000 normal_loss: 0.002240\n",
      "[972/00027] train_loss: 0.002231 kl_loss: 0.000000 normal_loss: 0.002231\n",
      "[974/00025] train_loss: 0.002229 kl_loss: 0.000000 normal_loss: 0.002229\n",
      "[976/00023] train_loss: 0.002246 kl_loss: 0.000000 normal_loss: 0.002246\n",
      "[978/00021] train_loss: 0.002238 kl_loss: 0.000000 normal_loss: 0.002238\n",
      "[980/00019] train_loss: 0.002237 kl_loss: 0.000000 normal_loss: 0.002237\n",
      "[982/00017] train_loss: 0.002220 kl_loss: 0.000000 normal_loss: 0.002220\n",
      "[984/00015] train_loss: 0.002242 kl_loss: 0.000000 normal_loss: 0.002242\n",
      "[986/00013] train_loss: 0.002223 kl_loss: 0.000000 normal_loss: 0.002223\n",
      "[988/00011] train_loss: 0.002238 kl_loss: 0.000000 normal_loss: 0.002238\n",
      "[990/00009] train_loss: 0.002227 kl_loss: 0.000000 normal_loss: 0.002227\n",
      "[992/00007] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[994/00005] train_loss: 0.002234 kl_loss: 0.000000 normal_loss: 0.002234\n",
      "[996/00003] train_loss: 0.002226 kl_loss: 0.000000 normal_loss: 0.002226\n",
      "[998/00001] train_loss: 0.002232 kl_loss: 0.000000 normal_loss: 0.002232\n",
      "[999/00050] train_loss: 0.002235 kl_loss: 0.000000 normal_loss: 0.002235\n",
      "[999/00050] IOU 0.9541702348421177\n"
     ]
    }
   ],
   "source": [
    "# AIRPLANE AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_ad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'iou_every_epoch': 50,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 2\n",
      "[001/00024] train_loss: 0.168495 kl_loss: 0.000000 normal_loss: 0.168495\n",
      "[002/00049] train_loss: 0.117954 kl_loss: 0.000000 normal_loss: 0.117954\n",
      "[003/00074] train_loss: 0.109562 kl_loss: 0.000000 normal_loss: 0.109562\n",
      "[005/00024] train_loss: 0.105227 kl_loss: 0.000000 normal_loss: 0.105227\n",
      "[006/00049] train_loss: 0.097987 kl_loss: 0.000000 normal_loss: 0.097987\n",
      "[007/00074] train_loss: 0.095581 kl_loss: 0.000000 normal_loss: 0.095581\n",
      "[009/00024] train_loss: 0.091399 kl_loss: 0.000000 normal_loss: 0.091399\n",
      "[010/00049] train_loss: 0.086116 kl_loss: 0.000000 normal_loss: 0.086116\n",
      "[011/00074] train_loss: 0.082822 kl_loss: 0.000000 normal_loss: 0.082822\n",
      "[013/00024] train_loss: 0.076092 kl_loss: 0.000000 normal_loss: 0.076092\n",
      "[014/00049] train_loss: 0.073706 kl_loss: 0.000000 normal_loss: 0.073706\n",
      "[015/00074] train_loss: 0.068664 kl_loss: 0.000000 normal_loss: 0.068664\n",
      "[017/00024] train_loss: 0.065321 kl_loss: 0.000000 normal_loss: 0.065321\n",
      "[018/00049] train_loss: 0.063247 kl_loss: 0.000000 normal_loss: 0.063247\n",
      "[019/00074] train_loss: 0.060584 kl_loss: 0.000000 normal_loss: 0.060584\n",
      "[021/00024] train_loss: 0.057878 kl_loss: 0.000000 normal_loss: 0.057878\n",
      "[022/00049] train_loss: 0.056367 kl_loss: 0.000000 normal_loss: 0.056367\n",
      "[023/00074] train_loss: 0.053790 kl_loss: 0.000000 normal_loss: 0.053790\n",
      "[025/00024] train_loss: 0.051930 kl_loss: 0.000000 normal_loss: 0.051930\n",
      "[026/00049] train_loss: 0.049985 kl_loss: 0.000000 normal_loss: 0.049985\n",
      "[027/00074] train_loss: 0.047572 kl_loss: 0.000000 normal_loss: 0.047572\n",
      "[029/00024] train_loss: 0.047217 kl_loss: 0.000000 normal_loss: 0.047217\n",
      "[030/00049] train_loss: 0.044996 kl_loss: 0.000000 normal_loss: 0.044996\n",
      "[031/00074] train_loss: 0.044526 kl_loss: 0.000000 normal_loss: 0.044526\n",
      "[033/00024] train_loss: 0.042396 kl_loss: 0.000000 normal_loss: 0.042396\n",
      "[034/00049] train_loss: 0.041516 kl_loss: 0.000000 normal_loss: 0.041516\n",
      "[035/00074] train_loss: 0.040286 kl_loss: 0.000000 normal_loss: 0.040286\n",
      "[037/00024] train_loss: 0.039482 kl_loss: 0.000000 normal_loss: 0.039482\n",
      "[038/00049] train_loss: 0.039414 kl_loss: 0.000000 normal_loss: 0.039414\n",
      "[039/00074] train_loss: 0.038704 kl_loss: 0.000000 normal_loss: 0.038704\n",
      "[041/00024] train_loss: 0.037147 kl_loss: 0.000000 normal_loss: 0.037147\n",
      "[042/00049] train_loss: 0.036394 kl_loss: 0.000000 normal_loss: 0.036394\n",
      "[043/00074] train_loss: 0.035746 kl_loss: 0.000000 normal_loss: 0.035746\n",
      "[045/00024] train_loss: 0.034762 kl_loss: 0.000000 normal_loss: 0.034762\n",
      "[046/00049] train_loss: 0.033424 kl_loss: 0.000000 normal_loss: 0.033424\n",
      "[047/00074] train_loss: 0.033722 kl_loss: 0.000000 normal_loss: 0.033722\n",
      "[049/00024] train_loss: 0.033844 kl_loss: 0.000000 normal_loss: 0.033844\n",
      "[049/00074] IOU 0.7622530335048213\n",
      "[050/00049] train_loss: 0.032229 kl_loss: 0.000000 normal_loss: 0.032229\n",
      "[051/00074] train_loss: 0.030781 kl_loss: 0.000000 normal_loss: 0.030781\n",
      "[053/00024] train_loss: 0.030600 kl_loss: 0.000000 normal_loss: 0.030600\n",
      "[054/00049] train_loss: 0.029736 kl_loss: 0.000000 normal_loss: 0.029736\n",
      "[055/00074] train_loss: 0.030456 kl_loss: 0.000000 normal_loss: 0.030456\n",
      "[057/00024] train_loss: 0.029372 kl_loss: 0.000000 normal_loss: 0.029372\n",
      "[058/00049] train_loss: 0.028534 kl_loss: 0.000000 normal_loss: 0.028534\n",
      "[059/00074] train_loss: 0.027996 kl_loss: 0.000000 normal_loss: 0.027996\n",
      "[061/00024] train_loss: 0.027135 kl_loss: 0.000000 normal_loss: 0.027135\n",
      "[062/00049] train_loss: 0.027203 kl_loss: 0.000000 normal_loss: 0.027203\n",
      "[063/00074] train_loss: 0.026554 kl_loss: 0.000000 normal_loss: 0.026554\n",
      "[065/00024] train_loss: 0.026542 kl_loss: 0.000000 normal_loss: 0.026542\n",
      "[066/00049] train_loss: 0.025803 kl_loss: 0.000000 normal_loss: 0.025803\n",
      "[067/00074] train_loss: 0.025319 kl_loss: 0.000000 normal_loss: 0.025319\n",
      "[069/00024] train_loss: 0.025430 kl_loss: 0.000000 normal_loss: 0.025430\n",
      "[070/00049] train_loss: 0.024336 kl_loss: 0.000000 normal_loss: 0.024336\n",
      "[071/00074] train_loss: 0.024208 kl_loss: 0.000000 normal_loss: 0.024208\n",
      "[073/00024] train_loss: 0.023858 kl_loss: 0.000000 normal_loss: 0.023858\n",
      "[074/00049] train_loss: 0.023339 kl_loss: 0.000000 normal_loss: 0.023339\n",
      "[075/00074] train_loss: 0.023397 kl_loss: 0.000000 normal_loss: 0.023397\n",
      "[077/00024] train_loss: 0.022137 kl_loss: 0.000000 normal_loss: 0.022137\n",
      "[078/00049] train_loss: 0.023627 kl_loss: 0.000000 normal_loss: 0.023627\n",
      "[079/00074] train_loss: 0.022821 kl_loss: 0.000000 normal_loss: 0.022821\n",
      "[081/00024] train_loss: 0.021747 kl_loss: 0.000000 normal_loss: 0.021747\n",
      "[082/00049] train_loss: 0.021771 kl_loss: 0.000000 normal_loss: 0.021771\n",
      "[083/00074] train_loss: 0.021447 kl_loss: 0.000000 normal_loss: 0.021447\n",
      "[085/00024] train_loss: 0.021570 kl_loss: 0.000000 normal_loss: 0.021570\n",
      "[086/00049] train_loss: 0.023595 kl_loss: 0.000000 normal_loss: 0.023595\n",
      "[087/00074] train_loss: 0.021081 kl_loss: 0.000000 normal_loss: 0.021081\n",
      "[089/00024] train_loss: 0.021024 kl_loss: 0.000000 normal_loss: 0.021024\n",
      "[090/00049] train_loss: 0.020293 kl_loss: 0.000000 normal_loss: 0.020293\n",
      "[091/00074] train_loss: 0.020695 kl_loss: 0.000000 normal_loss: 0.020695\n",
      "[093/00024] train_loss: 0.020079 kl_loss: 0.000000 normal_loss: 0.020079\n",
      "[094/00049] train_loss: 0.019900 kl_loss: 0.000000 normal_loss: 0.019900\n",
      "[095/00074] train_loss: 0.019607 kl_loss: 0.000000 normal_loss: 0.019607\n",
      "[097/00024] train_loss: 0.019325 kl_loss: 0.000000 normal_loss: 0.019325\n",
      "[098/00049] train_loss: 0.019085 kl_loss: 0.000000 normal_loss: 0.019085\n",
      "[099/00074] train_loss: 0.019873 kl_loss: 0.000000 normal_loss: 0.019873\n",
      "[099/00074] IOU 0.8655713971207539\n",
      "[101/00024] train_loss: 0.016658 kl_loss: 0.000000 normal_loss: 0.016658\n",
      "[102/00049] train_loss: 0.015480 kl_loss: 0.000000 normal_loss: 0.015480\n",
      "[103/00074] train_loss: 0.014648 kl_loss: 0.000000 normal_loss: 0.014648\n",
      "[105/00024] train_loss: 0.014769 kl_loss: 0.000000 normal_loss: 0.014769\n",
      "[106/00049] train_loss: 0.016084 kl_loss: 0.000000 normal_loss: 0.016084\n",
      "[107/00074] train_loss: 0.014534 kl_loss: 0.000000 normal_loss: 0.014534\n",
      "[109/00024] train_loss: 0.014817 kl_loss: 0.000000 normal_loss: 0.014817\n",
      "[110/00049] train_loss: 0.014652 kl_loss: 0.000000 normal_loss: 0.014652\n",
      "[111/00074] train_loss: 0.014367 kl_loss: 0.000000 normal_loss: 0.014367\n",
      "[113/00024] train_loss: 0.014340 kl_loss: 0.000000 normal_loss: 0.014340\n",
      "[114/00049] train_loss: 0.015159 kl_loss: 0.000000 normal_loss: 0.015159\n",
      "[115/00074] train_loss: 0.014448 kl_loss: 0.000000 normal_loss: 0.014448\n",
      "[117/00024] train_loss: 0.014429 kl_loss: 0.000000 normal_loss: 0.014429\n",
      "[118/00049] train_loss: 0.014190 kl_loss: 0.000000 normal_loss: 0.014190\n",
      "[119/00074] train_loss: 0.014278 kl_loss: 0.000000 normal_loss: 0.014278\n",
      "[121/00024] train_loss: 0.014228 kl_loss: 0.000000 normal_loss: 0.014228\n",
      "[122/00049] train_loss: 0.014223 kl_loss: 0.000000 normal_loss: 0.014223\n",
      "[123/00074] train_loss: 0.013981 kl_loss: 0.000000 normal_loss: 0.013981\n",
      "[125/00024] train_loss: 0.013909 kl_loss: 0.000000 normal_loss: 0.013909\n",
      "[126/00049] train_loss: 0.014281 kl_loss: 0.000000 normal_loss: 0.014281\n",
      "[127/00074] train_loss: 0.013830 kl_loss: 0.000000 normal_loss: 0.013830\n",
      "[129/00024] train_loss: 0.014006 kl_loss: 0.000000 normal_loss: 0.014006\n",
      "[130/00049] train_loss: 0.014121 kl_loss: 0.000000 normal_loss: 0.014121\n",
      "[131/00074] train_loss: 0.013807 kl_loss: 0.000000 normal_loss: 0.013807\n",
      "[133/00024] train_loss: 0.013787 kl_loss: 0.000000 normal_loss: 0.013787\n",
      "[134/00049] train_loss: 0.013484 kl_loss: 0.000000 normal_loss: 0.013484\n",
      "[135/00074] train_loss: 0.013499 kl_loss: 0.000000 normal_loss: 0.013499\n",
      "[137/00024] train_loss: 0.013692 kl_loss: 0.000000 normal_loss: 0.013692\n",
      "[138/00049] train_loss: 0.013088 kl_loss: 0.000000 normal_loss: 0.013088\n",
      "[139/00074] train_loss: 0.013704 kl_loss: 0.000000 normal_loss: 0.013704\n",
      "[141/00024] train_loss: 0.013468 kl_loss: 0.000000 normal_loss: 0.013468\n",
      "[142/00049] train_loss: 0.013239 kl_loss: 0.000000 normal_loss: 0.013239\n",
      "[143/00074] train_loss: 0.013199 kl_loss: 0.000000 normal_loss: 0.013199\n",
      "[145/00024] train_loss: 0.013233 kl_loss: 0.000000 normal_loss: 0.013233\n",
      "[146/00049] train_loss: 0.013188 kl_loss: 0.000000 normal_loss: 0.013188\n",
      "[147/00074] train_loss: 0.013026 kl_loss: 0.000000 normal_loss: 0.013026\n",
      "[149/00024] train_loss: 0.013072 kl_loss: 0.000000 normal_loss: 0.013072\n",
      "[149/00074] IOU 0.8918958876344065\n",
      "[150/00049] train_loss: 0.012995 kl_loss: 0.000000 normal_loss: 0.012995\n",
      "[151/00074] train_loss: 0.012918 kl_loss: 0.000000 normal_loss: 0.012918\n",
      "[153/00024] train_loss: 0.013256 kl_loss: 0.000000 normal_loss: 0.013256\n",
      "[154/00049] train_loss: 0.012769 kl_loss: 0.000000 normal_loss: 0.012769\n",
      "[155/00074] train_loss: 0.012903 kl_loss: 0.000000 normal_loss: 0.012903\n",
      "[157/00024] train_loss: 0.012863 kl_loss: 0.000000 normal_loss: 0.012863\n",
      "[158/00049] train_loss: 0.012738 kl_loss: 0.000000 normal_loss: 0.012738\n",
      "[159/00074] train_loss: 0.013069 kl_loss: 0.000000 normal_loss: 0.013069\n",
      "[161/00024] train_loss: 0.012759 kl_loss: 0.000000 normal_loss: 0.012759\n",
      "[162/00049] train_loss: 0.012512 kl_loss: 0.000000 normal_loss: 0.012512\n",
      "[163/00074] train_loss: 0.012702 kl_loss: 0.000000 normal_loss: 0.012702\n",
      "[165/00024] train_loss: 0.012281 kl_loss: 0.000000 normal_loss: 0.012281\n",
      "[166/00049] train_loss: 0.012643 kl_loss: 0.000000 normal_loss: 0.012643\n",
      "[167/00074] train_loss: 0.012493 kl_loss: 0.000000 normal_loss: 0.012493\n",
      "[169/00024] train_loss: 0.012528 kl_loss: 0.000000 normal_loss: 0.012528\n",
      "[170/00049] train_loss: 0.012374 kl_loss: 0.000000 normal_loss: 0.012374\n",
      "[171/00074] train_loss: 0.012414 kl_loss: 0.000000 normal_loss: 0.012414\n",
      "[173/00024] train_loss: 0.012233 kl_loss: 0.000000 normal_loss: 0.012233\n",
      "[174/00049] train_loss: 0.012403 kl_loss: 0.000000 normal_loss: 0.012403\n",
      "[175/00074] train_loss: 0.012256 kl_loss: 0.000000 normal_loss: 0.012256\n",
      "[177/00024] train_loss: 0.012279 kl_loss: 0.000000 normal_loss: 0.012279\n",
      "[178/00049] train_loss: 0.011979 kl_loss: 0.000000 normal_loss: 0.011979\n",
      "[179/00074] train_loss: 0.011892 kl_loss: 0.000000 normal_loss: 0.011892\n",
      "[181/00024] train_loss: 0.012069 kl_loss: 0.000000 normal_loss: 0.012069\n",
      "[182/00049] train_loss: 0.011950 kl_loss: 0.000000 normal_loss: 0.011950\n",
      "[183/00074] train_loss: 0.012135 kl_loss: 0.000000 normal_loss: 0.012135\n",
      "[185/00024] train_loss: 0.012050 kl_loss: 0.000000 normal_loss: 0.012050\n",
      "[186/00049] train_loss: 0.012243 kl_loss: 0.000000 normal_loss: 0.012243\n",
      "[187/00074] train_loss: 0.012127 kl_loss: 0.000000 normal_loss: 0.012127\n",
      "[189/00024] train_loss: 0.011887 kl_loss: 0.000000 normal_loss: 0.011887\n",
      "[190/00049] train_loss: 0.011886 kl_loss: 0.000000 normal_loss: 0.011886\n",
      "[191/00074] train_loss: 0.011742 kl_loss: 0.000000 normal_loss: 0.011742\n",
      "[193/00024] train_loss: 0.011814 kl_loss: 0.000000 normal_loss: 0.011814\n",
      "[194/00049] train_loss: 0.011699 kl_loss: 0.000000 normal_loss: 0.011699\n",
      "[195/00074] train_loss: 0.011788 kl_loss: 0.000000 normal_loss: 0.011788\n",
      "[197/00024] train_loss: 0.011886 kl_loss: 0.000000 normal_loss: 0.011886\n",
      "[198/00049] train_loss: 0.011643 kl_loss: 0.000000 normal_loss: 0.011643\n",
      "[199/00074] train_loss: 0.011573 kl_loss: 0.000000 normal_loss: 0.011573\n",
      "[199/00074] IOU 0.905182747785002\n",
      "[201/00024] train_loss: 0.010377 kl_loss: 0.000000 normal_loss: 0.010377\n",
      "[202/00049] train_loss: 0.009947 kl_loss: 0.000000 normal_loss: 0.009947\n",
      "[203/00074] train_loss: 0.009725 kl_loss: 0.000000 normal_loss: 0.009725\n",
      "[205/00024] train_loss: 0.009883 kl_loss: 0.000000 normal_loss: 0.009883\n",
      "[206/00049] train_loss: 0.009887 kl_loss: 0.000000 normal_loss: 0.009887\n",
      "[207/00074] train_loss: 0.009953 kl_loss: 0.000000 normal_loss: 0.009953\n",
      "[209/00024] train_loss: 0.009872 kl_loss: 0.000000 normal_loss: 0.009872\n",
      "[210/00049] train_loss: 0.010071 kl_loss: 0.000000 normal_loss: 0.010071\n",
      "[211/00074] train_loss: 0.009833 kl_loss: 0.000000 normal_loss: 0.009833\n",
      "[213/00024] train_loss: 0.009793 kl_loss: 0.000000 normal_loss: 0.009793\n",
      "[214/00049] train_loss: 0.009823 kl_loss: 0.000000 normal_loss: 0.009823\n",
      "[215/00074] train_loss: 0.009684 kl_loss: 0.000000 normal_loss: 0.009684\n",
      "[217/00024] train_loss: 0.009797 kl_loss: 0.000000 normal_loss: 0.009797\n",
      "[218/00049] train_loss: 0.009706 kl_loss: 0.000000 normal_loss: 0.009706\n",
      "[219/00074] train_loss: 0.009859 kl_loss: 0.000000 normal_loss: 0.009859\n",
      "[221/00024] train_loss: 0.009833 kl_loss: 0.000000 normal_loss: 0.009833\n",
      "[222/00049] train_loss: 0.009789 kl_loss: 0.000000 normal_loss: 0.009789\n",
      "[223/00074] train_loss: 0.009782 kl_loss: 0.000000 normal_loss: 0.009782\n",
      "[225/00024] train_loss: 0.009711 kl_loss: 0.000000 normal_loss: 0.009711\n",
      "[226/00049] train_loss: 0.009779 kl_loss: 0.000000 normal_loss: 0.009779\n",
      "[227/00074] train_loss: 0.009647 kl_loss: 0.000000 normal_loss: 0.009647\n",
      "[229/00024] train_loss: 0.009610 kl_loss: 0.000000 normal_loss: 0.009610\n",
      "[230/00049] train_loss: 0.009632 kl_loss: 0.000000 normal_loss: 0.009632\n",
      "[231/00074] train_loss: 0.009604 kl_loss: 0.000000 normal_loss: 0.009604\n",
      "[233/00024] train_loss: 0.009519 kl_loss: 0.000000 normal_loss: 0.009519\n",
      "[234/00049] train_loss: 0.009646 kl_loss: 0.000000 normal_loss: 0.009646\n",
      "[235/00074] train_loss: 0.009728 kl_loss: 0.000000 normal_loss: 0.009728\n",
      "[237/00024] train_loss: 0.009822 kl_loss: 0.000000 normal_loss: 0.009822\n",
      "[238/00049] train_loss: 0.009763 kl_loss: 0.000000 normal_loss: 0.009763\n",
      "[239/00074] train_loss: 0.009536 kl_loss: 0.000000 normal_loss: 0.009536\n",
      "[241/00024] train_loss: 0.009630 kl_loss: 0.000000 normal_loss: 0.009630\n",
      "[242/00049] train_loss: 0.009532 kl_loss: 0.000000 normal_loss: 0.009532\n",
      "[243/00074] train_loss: 0.009560 kl_loss: 0.000000 normal_loss: 0.009560\n",
      "[245/00024] train_loss: 0.009768 kl_loss: 0.000000 normal_loss: 0.009768\n",
      "[246/00049] train_loss: 0.009695 kl_loss: 0.000000 normal_loss: 0.009695\n",
      "[247/00074] train_loss: 0.009559 kl_loss: 0.000000 normal_loss: 0.009559\n",
      "[249/00024] train_loss: 0.009581 kl_loss: 0.000000 normal_loss: 0.009581\n",
      "[249/00074] IOU 0.9117270767937103\n",
      "[250/00049] train_loss: 0.009403 kl_loss: 0.000000 normal_loss: 0.009403\n",
      "[251/00074] train_loss: 0.009483 kl_loss: 0.000000 normal_loss: 0.009483\n",
      "[253/00024] train_loss: 0.009401 kl_loss: 0.000000 normal_loss: 0.009401\n",
      "[254/00049] train_loss: 0.009466 kl_loss: 0.000000 normal_loss: 0.009466\n",
      "[255/00074] train_loss: 0.009541 kl_loss: 0.000000 normal_loss: 0.009541\n",
      "[257/00024] train_loss: 0.009438 kl_loss: 0.000000 normal_loss: 0.009438\n",
      "[258/00049] train_loss: 0.009436 kl_loss: 0.000000 normal_loss: 0.009436\n",
      "[259/00074] train_loss: 0.009550 kl_loss: 0.000000 normal_loss: 0.009550\n",
      "[261/00024] train_loss: 0.009406 kl_loss: 0.000000 normal_loss: 0.009406\n",
      "[262/00049] train_loss: 0.009444 kl_loss: 0.000000 normal_loss: 0.009444\n",
      "[263/00074] train_loss: 0.009470 kl_loss: 0.000000 normal_loss: 0.009470\n",
      "[265/00024] train_loss: 0.009471 kl_loss: 0.000000 normal_loss: 0.009471\n",
      "[266/00049] train_loss: 0.009330 kl_loss: 0.000000 normal_loss: 0.009330\n",
      "[267/00074] train_loss: 0.009408 kl_loss: 0.000000 normal_loss: 0.009408\n",
      "[269/00024] train_loss: 0.009331 kl_loss: 0.000000 normal_loss: 0.009331\n",
      "[270/00049] train_loss: 0.009181 kl_loss: 0.000000 normal_loss: 0.009181\n",
      "[271/00074] train_loss: 0.009260 kl_loss: 0.000000 normal_loss: 0.009260\n",
      "[273/00024] train_loss: 0.009280 kl_loss: 0.000000 normal_loss: 0.009280\n",
      "[274/00049] train_loss: 0.009348 kl_loss: 0.000000 normal_loss: 0.009348\n",
      "[275/00074] train_loss: 0.009229 kl_loss: 0.000000 normal_loss: 0.009229\n",
      "[277/00024] train_loss: 0.009139 kl_loss: 0.000000 normal_loss: 0.009139\n",
      "[278/00049] train_loss: 0.009318 kl_loss: 0.000000 normal_loss: 0.009318\n",
      "[279/00074] train_loss: 0.009370 kl_loss: 0.000000 normal_loss: 0.009370\n",
      "[281/00024] train_loss: 0.009266 kl_loss: 0.000000 normal_loss: 0.009266\n",
      "[282/00049] train_loss: 0.009135 kl_loss: 0.000000 normal_loss: 0.009135\n",
      "[283/00074] train_loss: 0.009251 kl_loss: 0.000000 normal_loss: 0.009251\n",
      "[285/00024] train_loss: 0.009162 kl_loss: 0.000000 normal_loss: 0.009162\n",
      "[286/00049] train_loss: 0.009173 kl_loss: 0.000000 normal_loss: 0.009173\n",
      "[287/00074] train_loss: 0.009311 kl_loss: 0.000000 normal_loss: 0.009311\n",
      "[289/00024] train_loss: 0.009150 kl_loss: 0.000000 normal_loss: 0.009150\n",
      "[290/00049] train_loss: 0.009117 kl_loss: 0.000000 normal_loss: 0.009117\n",
      "[291/00074] train_loss: 0.009457 kl_loss: 0.000000 normal_loss: 0.009457\n",
      "[293/00024] train_loss: 0.009151 kl_loss: 0.000000 normal_loss: 0.009151\n",
      "[294/00049] train_loss: 0.009170 kl_loss: 0.000000 normal_loss: 0.009170\n",
      "[295/00074] train_loss: 0.009162 kl_loss: 0.000000 normal_loss: 0.009162\n",
      "[297/00024] train_loss: 0.009070 kl_loss: 0.000000 normal_loss: 0.009070\n",
      "[298/00049] train_loss: 0.009079 kl_loss: 0.000000 normal_loss: 0.009079\n",
      "[299/00074] train_loss: 0.009035 kl_loss: 0.000000 normal_loss: 0.009035\n",
      "[299/00074] IOU 0.918188418423136\n",
      "[301/00024] train_loss: 0.008645 kl_loss: 0.000000 normal_loss: 0.008645\n",
      "[302/00049] train_loss: 0.008626 kl_loss: 0.000000 normal_loss: 0.008626\n",
      "[303/00074] train_loss: 0.008374 kl_loss: 0.000000 normal_loss: 0.008374\n",
      "[305/00024] train_loss: 0.008354 kl_loss: 0.000000 normal_loss: 0.008354\n",
      "[306/00049] train_loss: 0.008309 kl_loss: 0.000000 normal_loss: 0.008309\n",
      "[307/00074] train_loss: 0.008409 kl_loss: 0.000000 normal_loss: 0.008409\n",
      "[309/00024] train_loss: 0.008408 kl_loss: 0.000000 normal_loss: 0.008408\n",
      "[310/00049] train_loss: 0.008270 kl_loss: 0.000000 normal_loss: 0.008270\n",
      "[311/00074] train_loss: 0.008391 kl_loss: 0.000000 normal_loss: 0.008391\n",
      "[313/00024] train_loss: 0.008340 kl_loss: 0.000000 normal_loss: 0.008340\n",
      "[314/00049] train_loss: 0.008402 kl_loss: 0.000000 normal_loss: 0.008402\n",
      "[315/00074] train_loss: 0.008339 kl_loss: 0.000000 normal_loss: 0.008339\n",
      "[317/00024] train_loss: 0.008278 kl_loss: 0.000000 normal_loss: 0.008278\n",
      "[318/00049] train_loss: 0.008322 kl_loss: 0.000000 normal_loss: 0.008322\n",
      "[319/00074] train_loss: 0.008336 kl_loss: 0.000000 normal_loss: 0.008336\n",
      "[321/00024] train_loss: 0.008316 kl_loss: 0.000000 normal_loss: 0.008316\n",
      "[322/00049] train_loss: 0.008351 kl_loss: 0.000000 normal_loss: 0.008351\n",
      "[323/00074] train_loss: 0.008341 kl_loss: 0.000000 normal_loss: 0.008341\n",
      "[325/00024] train_loss: 0.008288 kl_loss: 0.000000 normal_loss: 0.008288\n",
      "[326/00049] train_loss: 0.008339 kl_loss: 0.000000 normal_loss: 0.008339\n",
      "[327/00074] train_loss: 0.008353 kl_loss: 0.000000 normal_loss: 0.008353\n",
      "[329/00024] train_loss: 0.008343 kl_loss: 0.000000 normal_loss: 0.008343\n",
      "[330/00049] train_loss: 0.008359 kl_loss: 0.000000 normal_loss: 0.008359\n",
      "[331/00074] train_loss: 0.008310 kl_loss: 0.000000 normal_loss: 0.008310\n",
      "[333/00024] train_loss: 0.008287 kl_loss: 0.000000 normal_loss: 0.008287\n",
      "[334/00049] train_loss: 0.008249 kl_loss: 0.000000 normal_loss: 0.008249\n",
      "[335/00074] train_loss: 0.008330 kl_loss: 0.000000 normal_loss: 0.008330\n",
      "[337/00024] train_loss: 0.008247 kl_loss: 0.000000 normal_loss: 0.008247\n",
      "[338/00049] train_loss: 0.008313 kl_loss: 0.000000 normal_loss: 0.008313\n",
      "[339/00074] train_loss: 0.008270 kl_loss: 0.000000 normal_loss: 0.008270\n",
      "[341/00024] train_loss: 0.008267 kl_loss: 0.000000 normal_loss: 0.008267\n",
      "[342/00049] train_loss: 0.008220 kl_loss: 0.000000 normal_loss: 0.008220\n",
      "[343/00074] train_loss: 0.008186 kl_loss: 0.000000 normal_loss: 0.008186\n",
      "[345/00024] train_loss: 0.008237 kl_loss: 0.000000 normal_loss: 0.008237\n",
      "[346/00049] train_loss: 0.008186 kl_loss: 0.000000 normal_loss: 0.008186\n",
      "[347/00074] train_loss: 0.008210 kl_loss: 0.000000 normal_loss: 0.008210\n",
      "[349/00024] train_loss: 0.008151 kl_loss: 0.000000 normal_loss: 0.008151\n",
      "[349/00074] IOU 0.9216015450966855\n",
      "[350/00049] train_loss: 0.008175 kl_loss: 0.000000 normal_loss: 0.008175\n",
      "[351/00074] train_loss: 0.008192 kl_loss: 0.000000 normal_loss: 0.008192\n",
      "[353/00024] train_loss: 0.008168 kl_loss: 0.000000 normal_loss: 0.008168\n",
      "[354/00049] train_loss: 0.008103 kl_loss: 0.000000 normal_loss: 0.008103\n",
      "[355/00074] train_loss: 0.008230 kl_loss: 0.000000 normal_loss: 0.008230\n",
      "[357/00024] train_loss: 0.008196 kl_loss: 0.000000 normal_loss: 0.008196\n",
      "[358/00049] train_loss: 0.008185 kl_loss: 0.000000 normal_loss: 0.008185\n",
      "[359/00074] train_loss: 0.008179 kl_loss: 0.000000 normal_loss: 0.008179\n",
      "[361/00024] train_loss: 0.008169 kl_loss: 0.000000 normal_loss: 0.008169\n",
      "[362/00049] train_loss: 0.008159 kl_loss: 0.000000 normal_loss: 0.008159\n",
      "[363/00074] train_loss: 0.008145 kl_loss: 0.000000 normal_loss: 0.008145\n",
      "[365/00024] train_loss: 0.008176 kl_loss: 0.000000 normal_loss: 0.008176\n",
      "[366/00049] train_loss: 0.008138 kl_loss: 0.000000 normal_loss: 0.008138\n",
      "[367/00074] train_loss: 0.008125 kl_loss: 0.000000 normal_loss: 0.008125\n",
      "[369/00024] train_loss: 0.008222 kl_loss: 0.000000 normal_loss: 0.008222\n",
      "[370/00049] train_loss: 0.008111 kl_loss: 0.000000 normal_loss: 0.008111\n",
      "[371/00074] train_loss: 0.008109 kl_loss: 0.000000 normal_loss: 0.008109\n",
      "[373/00024] train_loss: 0.008090 kl_loss: 0.000000 normal_loss: 0.008090\n",
      "[374/00049] train_loss: 0.008116 kl_loss: 0.000000 normal_loss: 0.008116\n",
      "[375/00074] train_loss: 0.008121 kl_loss: 0.000000 normal_loss: 0.008121\n",
      "[377/00024] train_loss: 0.008143 kl_loss: 0.000000 normal_loss: 0.008143\n",
      "[378/00049] train_loss: 0.008087 kl_loss: 0.000000 normal_loss: 0.008087\n",
      "[379/00074] train_loss: 0.008109 kl_loss: 0.000000 normal_loss: 0.008109\n",
      "[381/00024] train_loss: 0.008032 kl_loss: 0.000000 normal_loss: 0.008032\n",
      "[382/00049] train_loss: 0.008130 kl_loss: 0.000000 normal_loss: 0.008130\n",
      "[383/00074] train_loss: 0.008082 kl_loss: 0.000000 normal_loss: 0.008082\n",
      "[385/00024] train_loss: 0.008399 kl_loss: 0.000000 normal_loss: 0.008399\n",
      "[386/00049] train_loss: 0.008161 kl_loss: 0.000000 normal_loss: 0.008161\n",
      "[387/00074] train_loss: 0.008120 kl_loss: 0.000000 normal_loss: 0.008120\n",
      "[389/00024] train_loss: 0.007964 kl_loss: 0.000000 normal_loss: 0.007964\n",
      "[390/00049] train_loss: 0.008060 kl_loss: 0.000000 normal_loss: 0.008060\n",
      "[391/00074] train_loss: 0.008054 kl_loss: 0.000000 normal_loss: 0.008054\n",
      "[393/00024] train_loss: 0.008057 kl_loss: 0.000000 normal_loss: 0.008057\n",
      "[394/00049] train_loss: 0.007999 kl_loss: 0.000000 normal_loss: 0.007999\n",
      "[395/00074] train_loss: 0.008074 kl_loss: 0.000000 normal_loss: 0.008074\n",
      "[397/00024] train_loss: 0.007996 kl_loss: 0.000000 normal_loss: 0.007996\n",
      "[398/00049] train_loss: 0.007980 kl_loss: 0.000000 normal_loss: 0.007980\n",
      "[399/00074] train_loss: 0.007986 kl_loss: 0.000000 normal_loss: 0.007986\n",
      "[399/00074] IOU 0.9229857251731058\n",
      "[401/00024] train_loss: 0.007774 kl_loss: 0.000000 normal_loss: 0.007774\n",
      "[402/00049] train_loss: 0.007787 kl_loss: 0.000000 normal_loss: 0.007787\n",
      "[403/00074] train_loss: 0.007738 kl_loss: 0.000000 normal_loss: 0.007738\n",
      "[405/00024] train_loss: 0.007694 kl_loss: 0.000000 normal_loss: 0.007694\n",
      "[406/00049] train_loss: 0.007777 kl_loss: 0.000000 normal_loss: 0.007777\n",
      "[407/00074] train_loss: 0.007658 kl_loss: 0.000000 normal_loss: 0.007658\n",
      "[409/00024] train_loss: 0.007714 kl_loss: 0.000000 normal_loss: 0.007714\n",
      "[410/00049] train_loss: 0.007657 kl_loss: 0.000000 normal_loss: 0.007657\n",
      "[411/00074] train_loss: 0.007752 kl_loss: 0.000000 normal_loss: 0.007752\n",
      "[413/00024] train_loss: 0.007709 kl_loss: 0.000000 normal_loss: 0.007709\n",
      "[414/00049] train_loss: 0.007831 kl_loss: 0.000000 normal_loss: 0.007831\n",
      "[415/00074] train_loss: 0.007738 kl_loss: 0.000000 normal_loss: 0.007738\n",
      "[417/00024] train_loss: 0.007736 kl_loss: 0.000000 normal_loss: 0.007736\n",
      "[418/00049] train_loss: 0.007679 kl_loss: 0.000000 normal_loss: 0.007679\n",
      "[419/00074] train_loss: 0.007703 kl_loss: 0.000000 normal_loss: 0.007703\n",
      "[421/00024] train_loss: 0.007694 kl_loss: 0.000000 normal_loss: 0.007694\n",
      "[422/00049] train_loss: 0.007706 kl_loss: 0.000000 normal_loss: 0.007706\n",
      "[423/00074] train_loss: 0.007672 kl_loss: 0.000000 normal_loss: 0.007672\n",
      "[425/00024] train_loss: 0.007741 kl_loss: 0.000000 normal_loss: 0.007741\n",
      "[426/00049] train_loss: 0.007712 kl_loss: 0.000000 normal_loss: 0.007712\n",
      "[427/00074] train_loss: 0.007709 kl_loss: 0.000000 normal_loss: 0.007709\n",
      "[429/00024] train_loss: 0.007665 kl_loss: 0.000000 normal_loss: 0.007665\n",
      "[430/00049] train_loss: 0.007666 kl_loss: 0.000000 normal_loss: 0.007666\n",
      "[431/00074] train_loss: 0.007667 kl_loss: 0.000000 normal_loss: 0.007667\n",
      "[433/00024] train_loss: 0.007704 kl_loss: 0.000000 normal_loss: 0.007704\n",
      "[434/00049] train_loss: 0.007652 kl_loss: 0.000000 normal_loss: 0.007652\n",
      "[435/00074] train_loss: 0.007652 kl_loss: 0.000000 normal_loss: 0.007652\n",
      "[437/00024] train_loss: 0.007715 kl_loss: 0.000000 normal_loss: 0.007715\n",
      "[438/00049] train_loss: 0.007689 kl_loss: 0.000000 normal_loss: 0.007689\n",
      "[439/00074] train_loss: 0.007709 kl_loss: 0.000000 normal_loss: 0.007709\n",
      "[441/00024] train_loss: 0.007662 kl_loss: 0.000000 normal_loss: 0.007662\n",
      "[442/00049] train_loss: 0.007702 kl_loss: 0.000000 normal_loss: 0.007702\n",
      "[443/00074] train_loss: 0.007661 kl_loss: 0.000000 normal_loss: 0.007661\n",
      "[445/00024] train_loss: 0.007714 kl_loss: 0.000000 normal_loss: 0.007714\n",
      "[446/00049] train_loss: 0.007593 kl_loss: 0.000000 normal_loss: 0.007593\n",
      "[447/00074] train_loss: 0.007666 kl_loss: 0.000000 normal_loss: 0.007666\n",
      "[449/00024] train_loss: 0.007611 kl_loss: 0.000000 normal_loss: 0.007611\n",
      "[449/00074] IOU 0.9239503654030462\n",
      "[450/00049] train_loss: 0.007604 kl_loss: 0.000000 normal_loss: 0.007604\n",
      "[451/00074] train_loss: 0.007650 kl_loss: 0.000000 normal_loss: 0.007650\n",
      "[453/00024] train_loss: 0.007750 kl_loss: 0.000000 normal_loss: 0.007750\n",
      "[454/00049] train_loss: 0.007654 kl_loss: 0.000000 normal_loss: 0.007654\n",
      "[455/00074] train_loss: 0.007612 kl_loss: 0.000000 normal_loss: 0.007612\n",
      "[457/00024] train_loss: 0.007651 kl_loss: 0.000000 normal_loss: 0.007651\n",
      "[458/00049] train_loss: 0.007642 kl_loss: 0.000000 normal_loss: 0.007642\n",
      "[459/00074] train_loss: 0.007595 kl_loss: 0.000000 normal_loss: 0.007595\n",
      "[461/00024] train_loss: 0.007605 kl_loss: 0.000000 normal_loss: 0.007605\n",
      "[462/00049] train_loss: 0.007568 kl_loss: 0.000000 normal_loss: 0.007568\n",
      "[463/00074] train_loss: 0.007596 kl_loss: 0.000000 normal_loss: 0.007596\n",
      "[465/00024] train_loss: 0.007601 kl_loss: 0.000000 normal_loss: 0.007601\n",
      "[466/00049] train_loss: 0.007634 kl_loss: 0.000000 normal_loss: 0.007634\n",
      "[467/00074] train_loss: 0.007598 kl_loss: 0.000000 normal_loss: 0.007598\n",
      "[469/00024] train_loss: 0.007605 kl_loss: 0.000000 normal_loss: 0.007605\n",
      "[470/00049] train_loss: 0.007650 kl_loss: 0.000000 normal_loss: 0.007650\n",
      "[471/00074] train_loss: 0.007589 kl_loss: 0.000000 normal_loss: 0.007589\n",
      "[473/00024] train_loss: 0.007680 kl_loss: 0.000000 normal_loss: 0.007680\n",
      "[474/00049] train_loss: 0.007506 kl_loss: 0.000000 normal_loss: 0.007506\n",
      "[475/00074] train_loss: 0.007719 kl_loss: 0.000000 normal_loss: 0.007719\n",
      "[477/00024] train_loss: 0.007574 kl_loss: 0.000000 normal_loss: 0.007574\n",
      "[478/00049] train_loss: 0.007602 kl_loss: 0.000000 normal_loss: 0.007602\n",
      "[479/00074] train_loss: 0.007552 kl_loss: 0.000000 normal_loss: 0.007552\n",
      "[481/00024] train_loss: 0.007595 kl_loss: 0.000000 normal_loss: 0.007595\n",
      "[482/00049] train_loss: 0.007643 kl_loss: 0.000000 normal_loss: 0.007643\n",
      "[483/00074] train_loss: 0.007583 kl_loss: 0.000000 normal_loss: 0.007583\n",
      "[485/00024] train_loss: 0.007576 kl_loss: 0.000000 normal_loss: 0.007576\n",
      "[486/00049] train_loss: 0.007607 kl_loss: 0.000000 normal_loss: 0.007607\n",
      "[487/00074] train_loss: 0.007605 kl_loss: 0.000000 normal_loss: 0.007605\n",
      "[489/00024] train_loss: 0.007563 kl_loss: 0.000000 normal_loss: 0.007563\n",
      "[490/00049] train_loss: 0.007519 kl_loss: 0.000000 normal_loss: 0.007519\n",
      "[491/00074] train_loss: 0.007551 kl_loss: 0.000000 normal_loss: 0.007551\n",
      "[493/00024] train_loss: 0.007536 kl_loss: 0.000000 normal_loss: 0.007536\n",
      "[494/00049] train_loss: 0.007555 kl_loss: 0.000000 normal_loss: 0.007555\n",
      "[495/00074] train_loss: 0.007599 kl_loss: 0.000000 normal_loss: 0.007599\n",
      "[497/00024] train_loss: 0.007508 kl_loss: 0.000000 normal_loss: 0.007508\n",
      "[498/00049] train_loss: 0.007557 kl_loss: 0.000000 normal_loss: 0.007557\n",
      "[499/00074] train_loss: 0.007536 kl_loss: 0.000000 normal_loss: 0.007536\n",
      "[499/00074] IOU 0.9246742397360503\n",
      "[501/00024] train_loss: 0.007434 kl_loss: 0.000000 normal_loss: 0.007434\n",
      "[502/00049] train_loss: 0.007427 kl_loss: 0.000000 normal_loss: 0.007427\n",
      "[503/00074] train_loss: 0.007451 kl_loss: 0.000000 normal_loss: 0.007451\n",
      "[505/00024] train_loss: 0.007393 kl_loss: 0.000000 normal_loss: 0.007393\n",
      "[506/00049] train_loss: 0.007461 kl_loss: 0.000000 normal_loss: 0.007461\n",
      "[507/00074] train_loss: 0.007396 kl_loss: 0.000000 normal_loss: 0.007396\n",
      "[509/00024] train_loss: 0.007435 kl_loss: 0.000000 normal_loss: 0.007435\n",
      "[510/00049] train_loss: 0.007399 kl_loss: 0.000000 normal_loss: 0.007399\n",
      "[511/00074] train_loss: 0.007397 kl_loss: 0.000000 normal_loss: 0.007397\n",
      "[513/00024] train_loss: 0.007392 kl_loss: 0.000000 normal_loss: 0.007392\n",
      "[514/00049] train_loss: 0.007383 kl_loss: 0.000000 normal_loss: 0.007383\n",
      "[515/00074] train_loss: 0.007414 kl_loss: 0.000000 normal_loss: 0.007414\n",
      "[517/00024] train_loss: 0.007451 kl_loss: 0.000000 normal_loss: 0.007451\n",
      "[518/00049] train_loss: 0.007426 kl_loss: 0.000000 normal_loss: 0.007426\n",
      "[519/00074] train_loss: 0.007383 kl_loss: 0.000000 normal_loss: 0.007383\n",
      "[521/00024] train_loss: 0.007374 kl_loss: 0.000000 normal_loss: 0.007374\n",
      "[522/00049] train_loss: 0.007368 kl_loss: 0.000000 normal_loss: 0.007368\n",
      "[523/00074] train_loss: 0.007395 kl_loss: 0.000000 normal_loss: 0.007395\n",
      "[525/00024] train_loss: 0.007411 kl_loss: 0.000000 normal_loss: 0.007411\n",
      "[526/00049] train_loss: 0.007370 kl_loss: 0.000000 normal_loss: 0.007370\n",
      "[527/00074] train_loss: 0.007391 kl_loss: 0.000000 normal_loss: 0.007391\n",
      "[529/00024] train_loss: 0.007445 kl_loss: 0.000000 normal_loss: 0.007445\n",
      "[530/00049] train_loss: 0.007336 kl_loss: 0.000000 normal_loss: 0.007336\n",
      "[531/00074] train_loss: 0.007395 kl_loss: 0.000000 normal_loss: 0.007395\n",
      "[533/00024] train_loss: 0.007419 kl_loss: 0.000000 normal_loss: 0.007419\n",
      "[534/00049] train_loss: 0.007384 kl_loss: 0.000000 normal_loss: 0.007384\n",
      "[535/00074] train_loss: 0.007372 kl_loss: 0.000000 normal_loss: 0.007372\n",
      "[537/00024] train_loss: 0.007379 kl_loss: 0.000000 normal_loss: 0.007379\n",
      "[538/00049] train_loss: 0.007354 kl_loss: 0.000000 normal_loss: 0.007354\n",
      "[539/00074] train_loss: 0.007384 kl_loss: 0.000000 normal_loss: 0.007384\n",
      "[541/00024] train_loss: 0.007398 kl_loss: 0.000000 normal_loss: 0.007398\n",
      "[542/00049] train_loss: 0.007352 kl_loss: 0.000000 normal_loss: 0.007352\n",
      "[543/00074] train_loss: 0.007407 kl_loss: 0.000000 normal_loss: 0.007407\n",
      "[545/00024] train_loss: 0.007361 kl_loss: 0.000000 normal_loss: 0.007361\n",
      "[546/00049] train_loss: 0.007383 kl_loss: 0.000000 normal_loss: 0.007383\n",
      "[547/00074] train_loss: 0.007347 kl_loss: 0.000000 normal_loss: 0.007347\n",
      "[549/00024] train_loss: 0.007396 kl_loss: 0.000000 normal_loss: 0.007396\n",
      "[549/00074] IOU 0.9263209449810287\n",
      "[550/00049] train_loss: 0.007383 kl_loss: 0.000000 normal_loss: 0.007383\n",
      "[551/00074] train_loss: 0.007368 kl_loss: 0.000000 normal_loss: 0.007368\n",
      "[553/00024] train_loss: 0.007386 kl_loss: 0.000000 normal_loss: 0.007386\n",
      "[554/00049] train_loss: 0.007384 kl_loss: 0.000000 normal_loss: 0.007384\n",
      "[555/00074] train_loss: 0.007351 kl_loss: 0.000000 normal_loss: 0.007351\n",
      "[557/00024] train_loss: 0.007321 kl_loss: 0.000000 normal_loss: 0.007321\n",
      "[558/00049] train_loss: 0.007414 kl_loss: 0.000000 normal_loss: 0.007414\n",
      "[559/00074] train_loss: 0.007342 kl_loss: 0.000000 normal_loss: 0.007342\n",
      "[561/00024] train_loss: 0.007379 kl_loss: 0.000000 normal_loss: 0.007379\n",
      "[562/00049] train_loss: 0.007362 kl_loss: 0.000000 normal_loss: 0.007362\n",
      "[563/00074] train_loss: 0.007373 kl_loss: 0.000000 normal_loss: 0.007373\n",
      "[565/00024] train_loss: 0.007366 kl_loss: 0.000000 normal_loss: 0.007366\n",
      "[566/00049] train_loss: 0.007343 kl_loss: 0.000000 normal_loss: 0.007343\n",
      "[567/00074] train_loss: 0.007324 kl_loss: 0.000000 normal_loss: 0.007324\n",
      "[569/00024] train_loss: 0.007333 kl_loss: 0.000000 normal_loss: 0.007333\n",
      "[570/00049] train_loss: 0.007323 kl_loss: 0.000000 normal_loss: 0.007323\n",
      "[571/00074] train_loss: 0.007354 kl_loss: 0.000000 normal_loss: 0.007354\n",
      "[573/00024] train_loss: 0.007307 kl_loss: 0.000000 normal_loss: 0.007307\n",
      "[574/00049] train_loss: 0.007354 kl_loss: 0.000000 normal_loss: 0.007354\n",
      "[575/00074] train_loss: 0.007339 kl_loss: 0.000000 normal_loss: 0.007339\n",
      "[577/00024] train_loss: 0.007347 kl_loss: 0.000000 normal_loss: 0.007347\n",
      "[578/00049] train_loss: 0.007352 kl_loss: 0.000000 normal_loss: 0.007352\n",
      "[579/00074] train_loss: 0.007356 kl_loss: 0.000000 normal_loss: 0.007356\n",
      "[581/00024] train_loss: 0.007323 kl_loss: 0.000000 normal_loss: 0.007323\n",
      "[582/00049] train_loss: 0.007302 kl_loss: 0.000000 normal_loss: 0.007302\n",
      "[583/00074] train_loss: 0.007347 kl_loss: 0.000000 normal_loss: 0.007347\n",
      "[585/00024] train_loss: 0.007364 kl_loss: 0.000000 normal_loss: 0.007364\n",
      "[586/00049] train_loss: 0.007350 kl_loss: 0.000000 normal_loss: 0.007350\n",
      "[587/00074] train_loss: 0.007331 kl_loss: 0.000000 normal_loss: 0.007331\n",
      "[589/00024] train_loss: 0.007362 kl_loss: 0.000000 normal_loss: 0.007362\n",
      "[590/00049] train_loss: 0.007291 kl_loss: 0.000000 normal_loss: 0.007291\n",
      "[591/00074] train_loss: 0.007343 kl_loss: 0.000000 normal_loss: 0.007343\n",
      "[593/00024] train_loss: 0.007328 kl_loss: 0.000000 normal_loss: 0.007328\n",
      "[594/00049] train_loss: 0.007301 kl_loss: 0.000000 normal_loss: 0.007301\n",
      "[595/00074] train_loss: 0.007308 kl_loss: 0.000000 normal_loss: 0.007308\n",
      "[597/00024] train_loss: 0.007264 kl_loss: 0.000000 normal_loss: 0.007264\n",
      "[598/00049] train_loss: 0.007352 kl_loss: 0.000000 normal_loss: 0.007352\n",
      "[599/00074] train_loss: 0.007317 kl_loss: 0.000000 normal_loss: 0.007317\n",
      "[599/00074] IOU 0.9263666389447948\n",
      "[601/00024] train_loss: 0.007286 kl_loss: 0.000000 normal_loss: 0.007286\n",
      "[602/00049] train_loss: 0.007254 kl_loss: 0.000000 normal_loss: 0.007254\n",
      "[603/00074] train_loss: 0.007248 kl_loss: 0.000000 normal_loss: 0.007248\n",
      "[605/00024] train_loss: 0.007230 kl_loss: 0.000000 normal_loss: 0.007230\n",
      "[606/00049] train_loss: 0.007265 kl_loss: 0.000000 normal_loss: 0.007265\n",
      "[607/00074] train_loss: 0.007258 kl_loss: 0.000000 normal_loss: 0.007258\n",
      "[609/00024] train_loss: 0.007254 kl_loss: 0.000000 normal_loss: 0.007254\n",
      "[610/00049] train_loss: 0.007280 kl_loss: 0.000000 normal_loss: 0.007280\n",
      "[611/00074] train_loss: 0.007224 kl_loss: 0.000000 normal_loss: 0.007224\n",
      "[613/00024] train_loss: 0.007263 kl_loss: 0.000000 normal_loss: 0.007263\n",
      "[614/00049] train_loss: 0.007219 kl_loss: 0.000000 normal_loss: 0.007219\n",
      "[615/00074] train_loss: 0.007283 kl_loss: 0.000000 normal_loss: 0.007283\n",
      "[617/00024] train_loss: 0.007239 kl_loss: 0.000000 normal_loss: 0.007239\n",
      "[618/00049] train_loss: 0.007256 kl_loss: 0.000000 normal_loss: 0.007256\n",
      "[619/00074] train_loss: 0.007247 kl_loss: 0.000000 normal_loss: 0.007247\n",
      "[621/00024] train_loss: 0.007257 kl_loss: 0.000000 normal_loss: 0.007257\n",
      "[622/00049] train_loss: 0.007282 kl_loss: 0.000000 normal_loss: 0.007282\n",
      "[623/00074] train_loss: 0.007188 kl_loss: 0.000000 normal_loss: 0.007188\n",
      "[625/00024] train_loss: 0.007239 kl_loss: 0.000000 normal_loss: 0.007239\n",
      "[626/00049] train_loss: 0.007256 kl_loss: 0.000000 normal_loss: 0.007256\n",
      "[627/00074] train_loss: 0.007222 kl_loss: 0.000000 normal_loss: 0.007222\n",
      "[629/00024] train_loss: 0.007269 kl_loss: 0.000000 normal_loss: 0.007269\n",
      "[630/00049] train_loss: 0.007226 kl_loss: 0.000000 normal_loss: 0.007226\n",
      "[631/00074] train_loss: 0.007234 kl_loss: 0.000000 normal_loss: 0.007234\n",
      "[633/00024] train_loss: 0.007239 kl_loss: 0.000000 normal_loss: 0.007239\n",
      "[634/00049] train_loss: 0.007219 kl_loss: 0.000000 normal_loss: 0.007219\n",
      "[635/00074] train_loss: 0.007259 kl_loss: 0.000000 normal_loss: 0.007259\n",
      "[637/00024] train_loss: 0.007246 kl_loss: 0.000000 normal_loss: 0.007246\n",
      "[638/00049] train_loss: 0.007253 kl_loss: 0.000000 normal_loss: 0.007253\n",
      "[639/00074] train_loss: 0.007205 kl_loss: 0.000000 normal_loss: 0.007205\n",
      "[641/00024] train_loss: 0.007216 kl_loss: 0.000000 normal_loss: 0.007216\n",
      "[642/00049] train_loss: 0.007246 kl_loss: 0.000000 normal_loss: 0.007246\n",
      "[643/00074] train_loss: 0.007217 kl_loss: 0.000000 normal_loss: 0.007217\n",
      "[645/00024] train_loss: 0.007301 kl_loss: 0.000000 normal_loss: 0.007301\n",
      "[646/00049] train_loss: 0.007160 kl_loss: 0.000000 normal_loss: 0.007160\n",
      "[647/00074] train_loss: 0.007253 kl_loss: 0.000000 normal_loss: 0.007253\n",
      "[649/00024] train_loss: 0.007237 kl_loss: 0.000000 normal_loss: 0.007237\n",
      "[649/00074] IOU 0.9272706339942912\n",
      "[650/00049] train_loss: 0.007217 kl_loss: 0.000000 normal_loss: 0.007217\n",
      "[651/00074] train_loss: 0.007246 kl_loss: 0.000000 normal_loss: 0.007246\n",
      "[653/00024] train_loss: 0.007237 kl_loss: 0.000000 normal_loss: 0.007237\n",
      "[654/00049] train_loss: 0.007201 kl_loss: 0.000000 normal_loss: 0.007201\n",
      "[655/00074] train_loss: 0.007228 kl_loss: 0.000000 normal_loss: 0.007228\n",
      "[657/00024] train_loss: 0.007245 kl_loss: 0.000000 normal_loss: 0.007245\n",
      "[658/00049] train_loss: 0.007193 kl_loss: 0.000000 normal_loss: 0.007193\n",
      "[659/00074] train_loss: 0.007256 kl_loss: 0.000000 normal_loss: 0.007256\n",
      "[661/00024] train_loss: 0.007233 kl_loss: 0.000000 normal_loss: 0.007233\n",
      "[662/00049] train_loss: 0.007232 kl_loss: 0.000000 normal_loss: 0.007232\n",
      "[663/00074] train_loss: 0.007186 kl_loss: 0.000000 normal_loss: 0.007186\n",
      "[665/00024] train_loss: 0.007236 kl_loss: 0.000000 normal_loss: 0.007236\n",
      "[666/00049] train_loss: 0.007266 kl_loss: 0.000000 normal_loss: 0.007266\n",
      "[667/00074] train_loss: 0.007185 kl_loss: 0.000000 normal_loss: 0.007185\n",
      "[669/00024] train_loss: 0.007198 kl_loss: 0.000000 normal_loss: 0.007198\n",
      "[670/00049] train_loss: 0.007224 kl_loss: 0.000000 normal_loss: 0.007224\n",
      "[671/00074] train_loss: 0.007239 kl_loss: 0.000000 normal_loss: 0.007239\n",
      "[673/00024] train_loss: 0.007196 kl_loss: 0.000000 normal_loss: 0.007196\n",
      "[674/00049] train_loss: 0.007254 kl_loss: 0.000000 normal_loss: 0.007254\n",
      "[675/00074] train_loss: 0.007178 kl_loss: 0.000000 normal_loss: 0.007178\n",
      "[677/00024] train_loss: 0.007219 kl_loss: 0.000000 normal_loss: 0.007219\n",
      "[678/00049] train_loss: 0.007209 kl_loss: 0.000000 normal_loss: 0.007209\n",
      "[679/00074] train_loss: 0.007236 kl_loss: 0.000000 normal_loss: 0.007236\n",
      "[681/00024] train_loss: 0.007212 kl_loss: 0.000000 normal_loss: 0.007212\n",
      "[682/00049] train_loss: 0.007176 kl_loss: 0.000000 normal_loss: 0.007176\n",
      "[683/00074] train_loss: 0.007217 kl_loss: 0.000000 normal_loss: 0.007217\n",
      "[685/00024] train_loss: 0.007218 kl_loss: 0.000000 normal_loss: 0.007218\n",
      "[686/00049] train_loss: 0.007216 kl_loss: 0.000000 normal_loss: 0.007216\n",
      "[687/00074] train_loss: 0.007208 kl_loss: 0.000000 normal_loss: 0.007208\n",
      "[689/00024] train_loss: 0.007226 kl_loss: 0.000000 normal_loss: 0.007226\n",
      "[690/00049] train_loss: 0.007194 kl_loss: 0.000000 normal_loss: 0.007194\n",
      "[691/00074] train_loss: 0.007245 kl_loss: 0.000000 normal_loss: 0.007245\n",
      "[693/00024] train_loss: 0.007257 kl_loss: 0.000000 normal_loss: 0.007257\n",
      "[694/00049] train_loss: 0.007140 kl_loss: 0.000000 normal_loss: 0.007140\n",
      "[695/00074] train_loss: 0.007188 kl_loss: 0.000000 normal_loss: 0.007188\n",
      "[697/00024] train_loss: 0.007218 kl_loss: 0.000000 normal_loss: 0.007218\n",
      "[698/00049] train_loss: 0.007209 kl_loss: 0.000000 normal_loss: 0.007209\n",
      "[699/00074] train_loss: 0.007198 kl_loss: 0.000000 normal_loss: 0.007198\n",
      "[699/00074] IOU 0.9275575063253443\n",
      "[701/00024] train_loss: 0.007197 kl_loss: 0.000000 normal_loss: 0.007197\n",
      "[702/00049] train_loss: 0.007110 kl_loss: 0.000000 normal_loss: 0.007110\n",
      "[703/00074] train_loss: 0.007209 kl_loss: 0.000000 normal_loss: 0.007209\n",
      "[705/00024] train_loss: 0.007179 kl_loss: 0.000000 normal_loss: 0.007179\n",
      "[706/00049] train_loss: 0.007147 kl_loss: 0.000000 normal_loss: 0.007147\n",
      "[707/00074] train_loss: 0.007166 kl_loss: 0.000000 normal_loss: 0.007166\n",
      "[709/00024] train_loss: 0.007143 kl_loss: 0.000000 normal_loss: 0.007143\n",
      "[710/00049] train_loss: 0.007157 kl_loss: 0.000000 normal_loss: 0.007157\n",
      "[711/00074] train_loss: 0.007195 kl_loss: 0.000000 normal_loss: 0.007195\n",
      "[713/00024] train_loss: 0.007158 kl_loss: 0.000000 normal_loss: 0.007158\n",
      "[714/00049] train_loss: 0.007186 kl_loss: 0.000000 normal_loss: 0.007186\n",
      "[715/00074] train_loss: 0.007153 kl_loss: 0.000000 normal_loss: 0.007153\n",
      "[717/00024] train_loss: 0.007174 kl_loss: 0.000000 normal_loss: 0.007174\n",
      "[718/00049] train_loss: 0.007123 kl_loss: 0.000000 normal_loss: 0.007123\n",
      "[719/00074] train_loss: 0.007186 kl_loss: 0.000000 normal_loss: 0.007186\n",
      "[721/00024] train_loss: 0.007171 kl_loss: 0.000000 normal_loss: 0.007171\n",
      "[722/00049] train_loss: 0.007155 kl_loss: 0.000000 normal_loss: 0.007155\n",
      "[723/00074] train_loss: 0.007176 kl_loss: 0.000000 normal_loss: 0.007176\n",
      "[725/00024] train_loss: 0.007141 kl_loss: 0.000000 normal_loss: 0.007141\n",
      "[726/00049] train_loss: 0.007173 kl_loss: 0.000000 normal_loss: 0.007173\n",
      "[727/00074] train_loss: 0.007193 kl_loss: 0.000000 normal_loss: 0.007193\n",
      "[729/00024] train_loss: 0.007154 kl_loss: 0.000000 normal_loss: 0.007154\n",
      "[730/00049] train_loss: 0.007188 kl_loss: 0.000000 normal_loss: 0.007188\n",
      "[731/00074] train_loss: 0.007152 kl_loss: 0.000000 normal_loss: 0.007152\n",
      "[733/00024] train_loss: 0.007128 kl_loss: 0.000000 normal_loss: 0.007128\n",
      "[734/00049] train_loss: 0.007203 kl_loss: 0.000000 normal_loss: 0.007203\n",
      "[735/00074] train_loss: 0.007149 kl_loss: 0.000000 normal_loss: 0.007149\n",
      "[737/00024] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[738/00049] train_loss: 0.007196 kl_loss: 0.000000 normal_loss: 0.007196\n",
      "[739/00074] train_loss: 0.007152 kl_loss: 0.000000 normal_loss: 0.007152\n",
      "[741/00024] train_loss: 0.007159 kl_loss: 0.000000 normal_loss: 0.007159\n",
      "[742/00049] train_loss: 0.007177 kl_loss: 0.000000 normal_loss: 0.007177\n",
      "[743/00074] train_loss: 0.007160 kl_loss: 0.000000 normal_loss: 0.007160\n",
      "[745/00024] train_loss: 0.007150 kl_loss: 0.000000 normal_loss: 0.007150\n",
      "[746/00049] train_loss: 0.007154 kl_loss: 0.000000 normal_loss: 0.007154\n",
      "[747/00074] train_loss: 0.007176 kl_loss: 0.000000 normal_loss: 0.007176\n",
      "[749/00024] train_loss: 0.007144 kl_loss: 0.000000 normal_loss: 0.007144\n",
      "[749/00074] IOU 0.9276946818456053\n",
      "[750/00049] train_loss: 0.007147 kl_loss: 0.000000 normal_loss: 0.007147\n",
      "[751/00074] train_loss: 0.007175 kl_loss: 0.000000 normal_loss: 0.007175\n",
      "[753/00024] train_loss: 0.007150 kl_loss: 0.000000 normal_loss: 0.007150\n",
      "[754/00049] train_loss: 0.007184 kl_loss: 0.000000 normal_loss: 0.007184\n",
      "[755/00074] train_loss: 0.007156 kl_loss: 0.000000 normal_loss: 0.007156\n",
      "[757/00024] train_loss: 0.007161 kl_loss: 0.000000 normal_loss: 0.007161\n",
      "[758/00049] train_loss: 0.007139 kl_loss: 0.000000 normal_loss: 0.007139\n",
      "[759/00074] train_loss: 0.007170 kl_loss: 0.000000 normal_loss: 0.007170\n",
      "[761/00024] train_loss: 0.007202 kl_loss: 0.000000 normal_loss: 0.007202\n",
      "[762/00049] train_loss: 0.007122 kl_loss: 0.000000 normal_loss: 0.007122\n",
      "[763/00074] train_loss: 0.007149 kl_loss: 0.000000 normal_loss: 0.007149\n",
      "[765/00024] train_loss: 0.007152 kl_loss: 0.000000 normal_loss: 0.007152\n",
      "[766/00049] train_loss: 0.007146 kl_loss: 0.000000 normal_loss: 0.007146\n",
      "[767/00074] train_loss: 0.007151 kl_loss: 0.000000 normal_loss: 0.007151\n",
      "[769/00024] train_loss: 0.007163 kl_loss: 0.000000 normal_loss: 0.007163\n",
      "[770/00049] train_loss: 0.007175 kl_loss: 0.000000 normal_loss: 0.007175\n",
      "[771/00074] train_loss: 0.007128 kl_loss: 0.000000 normal_loss: 0.007128\n",
      "[773/00024] train_loss: 0.007127 kl_loss: 0.000000 normal_loss: 0.007127\n",
      "[774/00049] train_loss: 0.007169 kl_loss: 0.000000 normal_loss: 0.007169\n",
      "[775/00074] train_loss: 0.007155 kl_loss: 0.000000 normal_loss: 0.007155\n",
      "[777/00024] train_loss: 0.007157 kl_loss: 0.000000 normal_loss: 0.007157\n",
      "[778/00049] train_loss: 0.007163 kl_loss: 0.000000 normal_loss: 0.007163\n",
      "[779/00074] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[781/00024] train_loss: 0.007150 kl_loss: 0.000000 normal_loss: 0.007150\n",
      "[782/00049] train_loss: 0.007141 kl_loss: 0.000000 normal_loss: 0.007141\n",
      "[783/00074] train_loss: 0.007136 kl_loss: 0.000000 normal_loss: 0.007136\n",
      "[785/00024] train_loss: 0.007149 kl_loss: 0.000000 normal_loss: 0.007149\n",
      "[786/00049] train_loss: 0.007129 kl_loss: 0.000000 normal_loss: 0.007129\n",
      "[787/00074] train_loss: 0.007152 kl_loss: 0.000000 normal_loss: 0.007152\n",
      "[789/00024] train_loss: 0.007137 kl_loss: 0.000000 normal_loss: 0.007137\n",
      "[790/00049] train_loss: 0.007197 kl_loss: 0.000000 normal_loss: 0.007197\n",
      "[791/00074] train_loss: 0.007119 kl_loss: 0.000000 normal_loss: 0.007119\n",
      "[793/00024] train_loss: 0.007129 kl_loss: 0.000000 normal_loss: 0.007129\n",
      "[794/00049] train_loss: 0.007191 kl_loss: 0.000000 normal_loss: 0.007191\n",
      "[795/00074] train_loss: 0.007124 kl_loss: 0.000000 normal_loss: 0.007124\n",
      "[797/00024] train_loss: 0.007143 kl_loss: 0.000000 normal_loss: 0.007143\n",
      "[798/00049] train_loss: 0.007144 kl_loss: 0.000000 normal_loss: 0.007144\n",
      "[799/00074] train_loss: 0.007158 kl_loss: 0.000000 normal_loss: 0.007158\n",
      "[799/00074] IOU 0.927694671265781\n",
      "[801/00024] train_loss: 0.007139 kl_loss: 0.000000 normal_loss: 0.007139\n",
      "[802/00049] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[803/00074] train_loss: 0.007102 kl_loss: 0.000000 normal_loss: 0.007102\n",
      "[805/00024] train_loss: 0.007113 kl_loss: 0.000000 normal_loss: 0.007113\n",
      "[806/00049] train_loss: 0.007144 kl_loss: 0.000000 normal_loss: 0.007144\n",
      "[807/00074] train_loss: 0.007125 kl_loss: 0.000000 normal_loss: 0.007125\n",
      "[809/00024] train_loss: 0.007122 kl_loss: 0.000000 normal_loss: 0.007122\n",
      "[810/00049] train_loss: 0.007106 kl_loss: 0.000000 normal_loss: 0.007106\n",
      "[811/00074] train_loss: 0.007134 kl_loss: 0.000000 normal_loss: 0.007134\n",
      "[813/00024] train_loss: 0.007111 kl_loss: 0.000000 normal_loss: 0.007111\n",
      "[814/00049] train_loss: 0.007136 kl_loss: 0.000000 normal_loss: 0.007136\n",
      "[815/00074] train_loss: 0.007112 kl_loss: 0.000000 normal_loss: 0.007112\n",
      "[817/00024] train_loss: 0.007137 kl_loss: 0.000000 normal_loss: 0.007137\n",
      "[818/00049] train_loss: 0.007094 kl_loss: 0.000000 normal_loss: 0.007094\n",
      "[819/00074] train_loss: 0.007149 kl_loss: 0.000000 normal_loss: 0.007149\n",
      "[821/00024] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[822/00049] train_loss: 0.007130 kl_loss: 0.000000 normal_loss: 0.007130\n",
      "[823/00074] train_loss: 0.007136 kl_loss: 0.000000 normal_loss: 0.007136\n",
      "[825/00024] train_loss: 0.007115 kl_loss: 0.000000 normal_loss: 0.007115\n",
      "[826/00049] train_loss: 0.007140 kl_loss: 0.000000 normal_loss: 0.007140\n",
      "[827/00074] train_loss: 0.007091 kl_loss: 0.000000 normal_loss: 0.007091\n",
      "[829/00024] train_loss: 0.007117 kl_loss: 0.000000 normal_loss: 0.007117\n",
      "[830/00049] train_loss: 0.007109 kl_loss: 0.000000 normal_loss: 0.007109\n",
      "[831/00074] train_loss: 0.007138 kl_loss: 0.000000 normal_loss: 0.007138\n",
      "[833/00024] train_loss: 0.007095 kl_loss: 0.000000 normal_loss: 0.007095\n",
      "[834/00049] train_loss: 0.007161 kl_loss: 0.000000 normal_loss: 0.007161\n",
      "[835/00074] train_loss: 0.007124 kl_loss: 0.000000 normal_loss: 0.007124\n",
      "[837/00024] train_loss: 0.007131 kl_loss: 0.000000 normal_loss: 0.007131\n",
      "[838/00049] train_loss: 0.007098 kl_loss: 0.000000 normal_loss: 0.007098\n",
      "[839/00074] train_loss: 0.007124 kl_loss: 0.000000 normal_loss: 0.007124\n",
      "[841/00024] train_loss: 0.007127 kl_loss: 0.000000 normal_loss: 0.007127\n",
      "[842/00049] train_loss: 0.007117 kl_loss: 0.000000 normal_loss: 0.007117\n",
      "[843/00074] train_loss: 0.007112 kl_loss: 0.000000 normal_loss: 0.007112\n",
      "[845/00024] train_loss: 0.007125 kl_loss: 0.000000 normal_loss: 0.007125\n",
      "[846/00049] train_loss: 0.007089 kl_loss: 0.000000 normal_loss: 0.007089\n",
      "[847/00074] train_loss: 0.007136 kl_loss: 0.000000 normal_loss: 0.007136\n",
      "[849/00024] train_loss: 0.007087 kl_loss: 0.000000 normal_loss: 0.007087\n",
      "[849/00074] IOU 0.9279553627471129\n",
      "[850/00049] train_loss: 0.007167 kl_loss: 0.000000 normal_loss: 0.007167\n",
      "[851/00074] train_loss: 0.007110 kl_loss: 0.000000 normal_loss: 0.007110\n",
      "[853/00024] train_loss: 0.007117 kl_loss: 0.000000 normal_loss: 0.007117\n",
      "[854/00049] train_loss: 0.007135 kl_loss: 0.000000 normal_loss: 0.007135\n",
      "[855/00074] train_loss: 0.007108 kl_loss: 0.000000 normal_loss: 0.007108\n",
      "[857/00024] train_loss: 0.007126 kl_loss: 0.000000 normal_loss: 0.007126\n",
      "[858/00049] train_loss: 0.007116 kl_loss: 0.000000 normal_loss: 0.007116\n",
      "[859/00074] train_loss: 0.007101 kl_loss: 0.000000 normal_loss: 0.007101\n",
      "[861/00024] train_loss: 0.007134 kl_loss: 0.000000 normal_loss: 0.007134\n",
      "[862/00049] train_loss: 0.007105 kl_loss: 0.000000 normal_loss: 0.007105\n",
      "[863/00074] train_loss: 0.007096 kl_loss: 0.000000 normal_loss: 0.007096\n",
      "[865/00024] train_loss: 0.007107 kl_loss: 0.000000 normal_loss: 0.007107\n",
      "[866/00049] train_loss: 0.007134 kl_loss: 0.000000 normal_loss: 0.007134\n",
      "[867/00074] train_loss: 0.007116 kl_loss: 0.000000 normal_loss: 0.007116\n",
      "[869/00024] train_loss: 0.007073 kl_loss: 0.000000 normal_loss: 0.007073\n",
      "[870/00049] train_loss: 0.007152 kl_loss: 0.000000 normal_loss: 0.007152\n",
      "[871/00074] train_loss: 0.007114 kl_loss: 0.000000 normal_loss: 0.007114\n",
      "[873/00024] train_loss: 0.007111 kl_loss: 0.000000 normal_loss: 0.007111\n",
      "[874/00049] train_loss: 0.007131 kl_loss: 0.000000 normal_loss: 0.007131\n",
      "[875/00074] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[877/00024] train_loss: 0.007109 kl_loss: 0.000000 normal_loss: 0.007109\n",
      "[878/00049] train_loss: 0.007112 kl_loss: 0.000000 normal_loss: 0.007112\n",
      "[879/00074] train_loss: 0.007132 kl_loss: 0.000000 normal_loss: 0.007132\n",
      "[881/00024] train_loss: 0.007114 kl_loss: 0.000000 normal_loss: 0.007114\n",
      "[882/00049] train_loss: 0.007116 kl_loss: 0.000000 normal_loss: 0.007116\n",
      "[883/00074] train_loss: 0.007122 kl_loss: 0.000000 normal_loss: 0.007122\n",
      "[885/00024] train_loss: 0.007133 kl_loss: 0.000000 normal_loss: 0.007133\n",
      "[886/00049] train_loss: 0.007099 kl_loss: 0.000000 normal_loss: 0.007099\n",
      "[887/00074] train_loss: 0.007109 kl_loss: 0.000000 normal_loss: 0.007109\n",
      "[889/00024] train_loss: 0.007099 kl_loss: 0.000000 normal_loss: 0.007099\n",
      "[890/00049] train_loss: 0.007119 kl_loss: 0.000000 normal_loss: 0.007119\n",
      "[891/00074] train_loss: 0.007108 kl_loss: 0.000000 normal_loss: 0.007108\n",
      "[893/00024] train_loss: 0.007116 kl_loss: 0.000000 normal_loss: 0.007116\n",
      "[894/00049] train_loss: 0.007112 kl_loss: 0.000000 normal_loss: 0.007112\n",
      "[895/00074] train_loss: 0.007110 kl_loss: 0.000000 normal_loss: 0.007110\n",
      "[897/00024] train_loss: 0.007111 kl_loss: 0.000000 normal_loss: 0.007111\n",
      "[898/00049] train_loss: 0.007138 kl_loss: 0.000000 normal_loss: 0.007138\n",
      "[899/00074] train_loss: 0.007094 kl_loss: 0.000000 normal_loss: 0.007094\n",
      "[899/00074] IOU 0.9280677935667336\n",
      "[901/00024] train_loss: 0.007085 kl_loss: 0.000000 normal_loss: 0.007085\n",
      "[902/00049] train_loss: 0.007107 kl_loss: 0.000000 normal_loss: 0.007107\n",
      "[903/00074] train_loss: 0.007112 kl_loss: 0.000000 normal_loss: 0.007112\n",
      "[905/00024] train_loss: 0.007090 kl_loss: 0.000000 normal_loss: 0.007090\n",
      "[906/00049] train_loss: 0.007121 kl_loss: 0.000000 normal_loss: 0.007121\n",
      "[907/00074] train_loss: 0.007096 kl_loss: 0.000000 normal_loss: 0.007096\n",
      "[909/00024] train_loss: 0.007116 kl_loss: 0.000000 normal_loss: 0.007116\n",
      "[910/00049] train_loss: 0.007074 kl_loss: 0.000000 normal_loss: 0.007074\n",
      "[911/00074] train_loss: 0.007118 kl_loss: 0.000000 normal_loss: 0.007118\n",
      "[913/00024] train_loss: 0.007102 kl_loss: 0.000000 normal_loss: 0.007102\n",
      "[914/00049] train_loss: 0.007091 kl_loss: 0.000000 normal_loss: 0.007091\n",
      "[915/00074] train_loss: 0.007105 kl_loss: 0.000000 normal_loss: 0.007105\n",
      "[917/00024] train_loss: 0.007085 kl_loss: 0.000000 normal_loss: 0.007085\n",
      "[918/00049] train_loss: 0.007091 kl_loss: 0.000000 normal_loss: 0.007091\n",
      "[919/00074] train_loss: 0.007129 kl_loss: 0.000000 normal_loss: 0.007129\n",
      "[921/00024] train_loss: 0.007074 kl_loss: 0.000000 normal_loss: 0.007074\n",
      "[922/00049] train_loss: 0.007101 kl_loss: 0.000000 normal_loss: 0.007101\n",
      "[923/00074] train_loss: 0.007127 kl_loss: 0.000000 normal_loss: 0.007127\n",
      "[925/00024] train_loss: 0.007107 kl_loss: 0.000000 normal_loss: 0.007107\n",
      "[926/00049] train_loss: 0.007103 kl_loss: 0.000000 normal_loss: 0.007103\n",
      "[927/00074] train_loss: 0.007088 kl_loss: 0.000000 normal_loss: 0.007088\n",
      "[929/00024] train_loss: 0.007075 kl_loss: 0.000000 normal_loss: 0.007075\n",
      "[930/00049] train_loss: 0.007123 kl_loss: 0.000000 normal_loss: 0.007123\n",
      "[931/00074] train_loss: 0.007108 kl_loss: 0.000000 normal_loss: 0.007108\n",
      "[933/00024] train_loss: 0.007118 kl_loss: 0.000000 normal_loss: 0.007118\n",
      "[934/00049] train_loss: 0.007085 kl_loss: 0.000000 normal_loss: 0.007085\n",
      "[935/00074] train_loss: 0.007096 kl_loss: 0.000000 normal_loss: 0.007096\n",
      "[937/00024] train_loss: 0.007094 kl_loss: 0.000000 normal_loss: 0.007094\n",
      "[938/00049] train_loss: 0.007088 kl_loss: 0.000000 normal_loss: 0.007088\n",
      "[939/00074] train_loss: 0.007106 kl_loss: 0.000000 normal_loss: 0.007106\n",
      "[941/00024] train_loss: 0.007097 kl_loss: 0.000000 normal_loss: 0.007097\n",
      "[942/00049] train_loss: 0.007104 kl_loss: 0.000000 normal_loss: 0.007104\n",
      "[943/00074] train_loss: 0.007086 kl_loss: 0.000000 normal_loss: 0.007086\n",
      "[945/00024] train_loss: 0.007087 kl_loss: 0.000000 normal_loss: 0.007087\n",
      "[946/00049] train_loss: 0.007130 kl_loss: 0.000000 normal_loss: 0.007130\n",
      "[947/00074] train_loss: 0.007092 kl_loss: 0.000000 normal_loss: 0.007092\n",
      "[949/00024] train_loss: 0.007082 kl_loss: 0.000000 normal_loss: 0.007082\n",
      "[949/00074] IOU 0.9280678423928718\n",
      "[950/00049] train_loss: 0.007075 kl_loss: 0.000000 normal_loss: 0.007075\n",
      "[951/00074] train_loss: 0.007134 kl_loss: 0.000000 normal_loss: 0.007134\n",
      "[953/00024] train_loss: 0.007093 kl_loss: 0.000000 normal_loss: 0.007093\n",
      "[954/00049] train_loss: 0.007099 kl_loss: 0.000000 normal_loss: 0.007099\n",
      "[955/00074] train_loss: 0.007108 kl_loss: 0.000000 normal_loss: 0.007108\n",
      "[957/00024] train_loss: 0.007111 kl_loss: 0.000000 normal_loss: 0.007111\n",
      "[958/00049] train_loss: 0.007074 kl_loss: 0.000000 normal_loss: 0.007074\n",
      "[959/00074] train_loss: 0.007098 kl_loss: 0.000000 normal_loss: 0.007098\n",
      "[961/00024] train_loss: 0.007080 kl_loss: 0.000000 normal_loss: 0.007080\n",
      "[962/00049] train_loss: 0.007126 kl_loss: 0.000000 normal_loss: 0.007126\n",
      "[963/00074] train_loss: 0.007079 kl_loss: 0.000000 normal_loss: 0.007079\n",
      "[965/00024] train_loss: 0.007088 kl_loss: 0.000000 normal_loss: 0.007088\n",
      "[966/00049] train_loss: 0.007100 kl_loss: 0.000000 normal_loss: 0.007100\n",
      "[967/00074] train_loss: 0.007113 kl_loss: 0.000000 normal_loss: 0.007113\n",
      "[969/00024] train_loss: 0.007089 kl_loss: 0.000000 normal_loss: 0.007089\n",
      "[970/00049] train_loss: 0.007104 kl_loss: 0.000000 normal_loss: 0.007104\n",
      "[971/00074] train_loss: 0.007089 kl_loss: 0.000000 normal_loss: 0.007089\n",
      "[973/00024] train_loss: 0.007092 kl_loss: 0.000000 normal_loss: 0.007092\n",
      "[974/00049] train_loss: 0.007093 kl_loss: 0.000000 normal_loss: 0.007093\n",
      "[975/00074] train_loss: 0.007088 kl_loss: 0.000000 normal_loss: 0.007088\n",
      "[977/00024] train_loss: 0.007105 kl_loss: 0.000000 normal_loss: 0.007105\n",
      "[978/00049] train_loss: 0.007074 kl_loss: 0.000000 normal_loss: 0.007074\n",
      "[979/00074] train_loss: 0.007100 kl_loss: 0.000000 normal_loss: 0.007100\n",
      "[981/00024] train_loss: 0.007094 kl_loss: 0.000000 normal_loss: 0.007094\n",
      "[982/00049] train_loss: 0.007081 kl_loss: 0.000000 normal_loss: 0.007081\n",
      "[983/00074] train_loss: 0.007102 kl_loss: 0.000000 normal_loss: 0.007102\n",
      "[985/00024] train_loss: 0.007093 kl_loss: 0.000000 normal_loss: 0.007093\n",
      "[986/00049] train_loss: 0.007119 kl_loss: 0.000000 normal_loss: 0.007119\n",
      "[987/00074] train_loss: 0.007078 kl_loss: 0.000000 normal_loss: 0.007078\n",
      "[989/00024] train_loss: 0.007069 kl_loss: 0.000000 normal_loss: 0.007069\n",
      "[990/00049] train_loss: 0.007095 kl_loss: 0.000000 normal_loss: 0.007095\n",
      "[991/00074] train_loss: 0.007117 kl_loss: 0.000000 normal_loss: 0.007117\n",
      "[993/00024] train_loss: 0.007079 kl_loss: 0.000000 normal_loss: 0.007079\n",
      "[994/00049] train_loss: 0.007115 kl_loss: 0.000000 normal_loss: 0.007115\n",
      "[995/00074] train_loss: 0.007091 kl_loss: 0.000000 normal_loss: 0.007091\n",
      "[997/00024] train_loss: 0.007060 kl_loss: 0.000000 normal_loss: 0.007060\n",
      "[998/00049] train_loss: 0.007135 kl_loss: 0.000000 normal_loss: 0.007135\n",
      "[999/00074] train_loss: 0.007098 kl_loss: 0.000000 normal_loss: 0.007098\n",
      "[999/00074] IOU 0.9280581993671755\n"
     ]
    }
   ],
   "source": [
    "# TABLE AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'table_ad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'iou_every_epoch': 50,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'table',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 2\n",
      "[001/00024] train_loss: 0.164451 kl_loss: 0.000000 normal_loss: 0.164451\n",
      "[002/00049] train_loss: 0.116169 kl_loss: 0.000000 normal_loss: 0.116169\n",
      "[003/00074] train_loss: 0.113166 kl_loss: 0.000000 normal_loss: 0.113166\n",
      "[005/00024] train_loss: 0.106087 kl_loss: 0.000000 normal_loss: 0.106087\n",
      "[006/00049] train_loss: 0.100100 kl_loss: 0.000000 normal_loss: 0.100100\n",
      "[007/00074] train_loss: 0.095202 kl_loss: 0.000000 normal_loss: 0.095202\n",
      "[009/00024] train_loss: 0.089955 kl_loss: 0.000000 normal_loss: 0.089955\n",
      "[010/00049] train_loss: 0.085373 kl_loss: 0.000000 normal_loss: 0.085373\n",
      "[011/00074] train_loss: 0.082074 kl_loss: 0.000000 normal_loss: 0.082074\n",
      "[013/00024] train_loss: 0.077538 kl_loss: 0.000000 normal_loss: 0.077538\n",
      "[014/00049] train_loss: 0.075404 kl_loss: 0.000000 normal_loss: 0.075404\n",
      "[015/00074] train_loss: 0.071963 kl_loss: 0.000000 normal_loss: 0.071963\n",
      "[017/00024] train_loss: 0.069499 kl_loss: 0.000000 normal_loss: 0.069499\n",
      "[018/00049] train_loss: 0.066746 kl_loss: 0.000000 normal_loss: 0.066746\n",
      "[019/00074] train_loss: 0.064414 kl_loss: 0.000000 normal_loss: 0.064414\n",
      "[021/00024] train_loss: 0.061414 kl_loss: 0.000000 normal_loss: 0.061414\n",
      "[022/00049] train_loss: 0.058761 kl_loss: 0.000000 normal_loss: 0.058761\n",
      "[023/00074] train_loss: 0.057564 kl_loss: 0.000000 normal_loss: 0.057564\n",
      "[025/00024] train_loss: 0.054771 kl_loss: 0.000000 normal_loss: 0.054771\n",
      "[026/00049] train_loss: 0.052876 kl_loss: 0.000000 normal_loss: 0.052876\n",
      "[027/00074] train_loss: 0.051078 kl_loss: 0.000000 normal_loss: 0.051078\n",
      "[029/00024] train_loss: 0.050608 kl_loss: 0.000000 normal_loss: 0.050608\n",
      "[030/00049] train_loss: 0.048213 kl_loss: 0.000000 normal_loss: 0.048213\n",
      "[031/00074] train_loss: 0.046769 kl_loss: 0.000000 normal_loss: 0.046769\n",
      "[033/00024] train_loss: 0.045782 kl_loss: 0.000000 normal_loss: 0.045782\n",
      "[034/00049] train_loss: 0.043948 kl_loss: 0.000000 normal_loss: 0.043948\n",
      "[035/00074] train_loss: 0.044078 kl_loss: 0.000000 normal_loss: 0.044078\n",
      "[037/00024] train_loss: 0.043027 kl_loss: 0.000000 normal_loss: 0.043027\n",
      "[038/00049] train_loss: 0.041322 kl_loss: 0.000000 normal_loss: 0.041322\n",
      "[039/00074] train_loss: 0.040937 kl_loss: 0.000000 normal_loss: 0.040937\n",
      "[041/00024] train_loss: 0.039977 kl_loss: 0.000000 normal_loss: 0.039977\n",
      "[042/00049] train_loss: 0.039429 kl_loss: 0.000000 normal_loss: 0.039429\n",
      "[043/00074] train_loss: 0.037095 kl_loss: 0.000000 normal_loss: 0.037095\n",
      "[045/00024] train_loss: 0.036566 kl_loss: 0.000000 normal_loss: 0.036566\n",
      "[046/00049] train_loss: 0.035893 kl_loss: 0.000000 normal_loss: 0.035893\n",
      "[047/00074] train_loss: 0.034910 kl_loss: 0.000000 normal_loss: 0.034910\n",
      "[049/00024] train_loss: 0.033562 kl_loss: 0.000000 normal_loss: 0.033562\n",
      "[049/00074] IOU 0.7890190244900683\n",
      "[050/00049] train_loss: 0.034636 kl_loss: 0.000000 normal_loss: 0.034636\n",
      "[051/00074] train_loss: 0.032916 kl_loss: 0.000000 normal_loss: 0.032916\n",
      "[053/00024] train_loss: 0.031271 kl_loss: 0.000000 normal_loss: 0.031271\n",
      "[054/00049] train_loss: 0.032771 kl_loss: 0.000000 normal_loss: 0.032771\n",
      "[055/00074] train_loss: 0.030839 kl_loss: 0.000000 normal_loss: 0.030839\n",
      "[057/00024] train_loss: 0.030091 kl_loss: 0.000000 normal_loss: 0.030091\n",
      "[058/00049] train_loss: 0.029720 kl_loss: 0.000000 normal_loss: 0.029720\n",
      "[059/00074] train_loss: 0.028828 kl_loss: 0.000000 normal_loss: 0.028828\n",
      "[061/00024] train_loss: 0.028370 kl_loss: 0.000000 normal_loss: 0.028370\n",
      "[062/00049] train_loss: 0.028777 kl_loss: 0.000000 normal_loss: 0.028777\n",
      "[063/00074] train_loss: 0.027703 kl_loss: 0.000000 normal_loss: 0.027703\n",
      "[065/00024] train_loss: 0.027142 kl_loss: 0.000000 normal_loss: 0.027142\n",
      "[066/00049] train_loss: 0.026743 kl_loss: 0.000000 normal_loss: 0.026743\n",
      "[067/00074] train_loss: 0.026170 kl_loss: 0.000000 normal_loss: 0.026170\n",
      "[069/00024] train_loss: 0.025965 kl_loss: 0.000000 normal_loss: 0.025965\n",
      "[070/00049] train_loss: 0.025977 kl_loss: 0.000000 normal_loss: 0.025977\n",
      "[071/00074] train_loss: 0.025848 kl_loss: 0.000000 normal_loss: 0.025848\n",
      "[073/00024] train_loss: 0.025788 kl_loss: 0.000000 normal_loss: 0.025788\n",
      "[074/00049] train_loss: 0.024837 kl_loss: 0.000000 normal_loss: 0.024837\n",
      "[075/00074] train_loss: 0.025637 kl_loss: 0.000000 normal_loss: 0.025637\n",
      "[077/00024] train_loss: 0.024126 kl_loss: 0.000000 normal_loss: 0.024126\n",
      "[078/00049] train_loss: 0.024836 kl_loss: 0.000000 normal_loss: 0.024836\n",
      "[079/00074] train_loss: 0.023747 kl_loss: 0.000000 normal_loss: 0.023747\n",
      "[081/00024] train_loss: 0.023330 kl_loss: 0.000000 normal_loss: 0.023330\n",
      "[082/00049] train_loss: 0.024536 kl_loss: 0.000000 normal_loss: 0.024536\n",
      "[083/00074] train_loss: 0.023570 kl_loss: 0.000000 normal_loss: 0.023570\n",
      "[085/00024] train_loss: 0.023637 kl_loss: 0.000000 normal_loss: 0.023637\n",
      "[086/00049] train_loss: 0.022628 kl_loss: 0.000000 normal_loss: 0.022628\n",
      "[087/00074] train_loss: 0.022251 kl_loss: 0.000000 normal_loss: 0.022251\n",
      "[089/00024] train_loss: 0.022363 kl_loss: 0.000000 normal_loss: 0.022363\n",
      "[090/00049] train_loss: 0.022950 kl_loss: 0.000000 normal_loss: 0.022950\n",
      "[091/00074] train_loss: 0.022131 kl_loss: 0.000000 normal_loss: 0.022131\n",
      "[093/00024] train_loss: 0.021984 kl_loss: 0.000000 normal_loss: 0.021984\n",
      "[094/00049] train_loss: 0.022061 kl_loss: 0.000000 normal_loss: 0.022061\n",
      "[095/00074] train_loss: 0.021452 kl_loss: 0.000000 normal_loss: 0.021452\n",
      "[097/00024] train_loss: 0.021802 kl_loss: 0.000000 normal_loss: 0.021802\n",
      "[098/00049] train_loss: 0.021225 kl_loss: 0.000000 normal_loss: 0.021225\n",
      "[099/00074] train_loss: 0.020889 kl_loss: 0.000000 normal_loss: 0.020889\n",
      "[099/00074] IOU 0.8662025389696161\n",
      "[101/00024] train_loss: 0.018751 kl_loss: 0.000000 normal_loss: 0.018751\n",
      "[102/00049] train_loss: 0.016807 kl_loss: 0.000000 normal_loss: 0.016807\n",
      "[103/00074] train_loss: 0.016106 kl_loss: 0.000000 normal_loss: 0.016106\n",
      "[105/00024] train_loss: 0.015961 kl_loss: 0.000000 normal_loss: 0.015961\n",
      "[106/00049] train_loss: 0.016184 kl_loss: 0.000000 normal_loss: 0.016184\n",
      "[107/00074] train_loss: 0.015720 kl_loss: 0.000000 normal_loss: 0.015720\n",
      "[109/00024] train_loss: 0.015845 kl_loss: 0.000000 normal_loss: 0.015845\n",
      "[110/00049] train_loss: 0.015781 kl_loss: 0.000000 normal_loss: 0.015781\n",
      "[111/00074] train_loss: 0.016113 kl_loss: 0.000000 normal_loss: 0.016113\n",
      "[113/00024] train_loss: 0.016199 kl_loss: 0.000000 normal_loss: 0.016199\n",
      "[114/00049] train_loss: 0.016084 kl_loss: 0.000000 normal_loss: 0.016084\n",
      "[115/00074] train_loss: 0.015986 kl_loss: 0.000000 normal_loss: 0.015986\n",
      "[117/00024] train_loss: 0.015418 kl_loss: 0.000000 normal_loss: 0.015418\n",
      "[118/00049] train_loss: 0.016162 kl_loss: 0.000000 normal_loss: 0.016162\n",
      "[119/00074] train_loss: 0.016461 kl_loss: 0.000000 normal_loss: 0.016461\n",
      "[121/00024] train_loss: 0.015995 kl_loss: 0.000000 normal_loss: 0.015995\n",
      "[122/00049] train_loss: 0.015734 kl_loss: 0.000000 normal_loss: 0.015734\n",
      "[123/00074] train_loss: 0.015335 kl_loss: 0.000000 normal_loss: 0.015335\n",
      "[125/00024] train_loss: 0.015976 kl_loss: 0.000000 normal_loss: 0.015976\n",
      "[126/00049] train_loss: 0.015941 kl_loss: 0.000000 normal_loss: 0.015941\n",
      "[127/00074] train_loss: 0.015573 kl_loss: 0.000000 normal_loss: 0.015573\n",
      "[129/00024] train_loss: 0.015858 kl_loss: 0.000000 normal_loss: 0.015858\n",
      "[130/00049] train_loss: 0.015334 kl_loss: 0.000000 normal_loss: 0.015334\n",
      "[131/00074] train_loss: 0.015677 kl_loss: 0.000000 normal_loss: 0.015677\n",
      "[133/00024] train_loss: 0.015271 kl_loss: 0.000000 normal_loss: 0.015271\n",
      "[134/00049] train_loss: 0.015364 kl_loss: 0.000000 normal_loss: 0.015364\n",
      "[135/00074] train_loss: 0.015314 kl_loss: 0.000000 normal_loss: 0.015314\n",
      "[137/00024] train_loss: 0.015494 kl_loss: 0.000000 normal_loss: 0.015494\n",
      "[138/00049] train_loss: 0.015551 kl_loss: 0.000000 normal_loss: 0.015551\n",
      "[139/00074] train_loss: 0.015190 kl_loss: 0.000000 normal_loss: 0.015190\n",
      "[141/00024] train_loss: 0.014862 kl_loss: 0.000000 normal_loss: 0.014862\n",
      "[142/00049] train_loss: 0.015344 kl_loss: 0.000000 normal_loss: 0.015344\n",
      "[143/00074] train_loss: 0.015097 kl_loss: 0.000000 normal_loss: 0.015097\n",
      "[145/00024] train_loss: 0.015198 kl_loss: 0.000000 normal_loss: 0.015198\n",
      "[146/00049] train_loss: 0.015145 kl_loss: 0.000000 normal_loss: 0.015145\n",
      "[147/00074] train_loss: 0.014797 kl_loss: 0.000000 normal_loss: 0.014797\n",
      "[149/00024] train_loss: 0.015085 kl_loss: 0.000000 normal_loss: 0.015085\n",
      "[149/00074] IOU 0.8902636351746818\n",
      "[150/00049] train_loss: 0.015042 kl_loss: 0.000000 normal_loss: 0.015042\n",
      "[151/00074] train_loss: 0.014795 kl_loss: 0.000000 normal_loss: 0.014795\n",
      "[153/00024] train_loss: 0.014889 kl_loss: 0.000000 normal_loss: 0.014889\n",
      "[154/00049] train_loss: 0.014328 kl_loss: 0.000000 normal_loss: 0.014328\n",
      "[155/00074] train_loss: 0.014663 kl_loss: 0.000000 normal_loss: 0.014663\n",
      "[157/00024] train_loss: 0.014570 kl_loss: 0.000000 normal_loss: 0.014570\n",
      "[158/00049] train_loss: 0.014872 kl_loss: 0.000000 normal_loss: 0.014872\n",
      "[159/00074] train_loss: 0.014569 kl_loss: 0.000000 normal_loss: 0.014569\n",
      "[161/00024] train_loss: 0.014658 kl_loss: 0.000000 normal_loss: 0.014658\n",
      "[162/00049] train_loss: 0.014701 kl_loss: 0.000000 normal_loss: 0.014701\n",
      "[163/00074] train_loss: 0.014564 kl_loss: 0.000000 normal_loss: 0.014564\n",
      "[165/00024] train_loss: 0.014121 kl_loss: 0.000000 normal_loss: 0.014121\n",
      "[166/00049] train_loss: 0.014525 kl_loss: 0.000000 normal_loss: 0.014525\n",
      "[167/00074] train_loss: 0.014238 kl_loss: 0.000000 normal_loss: 0.014238\n",
      "[169/00024] train_loss: 0.014586 kl_loss: 0.000000 normal_loss: 0.014586\n",
      "[170/00049] train_loss: 0.014109 kl_loss: 0.000000 normal_loss: 0.014109\n",
      "[171/00074] train_loss: 0.014192 kl_loss: 0.000000 normal_loss: 0.014192\n",
      "[173/00024] train_loss: 0.014367 kl_loss: 0.000000 normal_loss: 0.014367\n",
      "[174/00049] train_loss: 0.014173 kl_loss: 0.000000 normal_loss: 0.014173\n",
      "[175/00074] train_loss: 0.014156 kl_loss: 0.000000 normal_loss: 0.014156\n",
      "[177/00024] train_loss: 0.014205 kl_loss: 0.000000 normal_loss: 0.014205\n",
      "[178/00049] train_loss: 0.014131 kl_loss: 0.000000 normal_loss: 0.014131\n",
      "[179/00074] train_loss: 0.014310 kl_loss: 0.000000 normal_loss: 0.014310\n",
      "[181/00024] train_loss: 0.013722 kl_loss: 0.000000 normal_loss: 0.013722\n",
      "[182/00049] train_loss: 0.013985 kl_loss: 0.000000 normal_loss: 0.013985\n",
      "[183/00074] train_loss: 0.014210 kl_loss: 0.000000 normal_loss: 0.014210\n",
      "[185/00024] train_loss: 0.013881 kl_loss: 0.000000 normal_loss: 0.013881\n",
      "[186/00049] train_loss: 0.013712 kl_loss: 0.000000 normal_loss: 0.013712\n",
      "[187/00074] train_loss: 0.013481 kl_loss: 0.000000 normal_loss: 0.013481\n",
      "[189/00024] train_loss: 0.014046 kl_loss: 0.000000 normal_loss: 0.014046\n",
      "[190/00049] train_loss: 0.014026 kl_loss: 0.000000 normal_loss: 0.014026\n",
      "[191/00074] train_loss: 0.014068 kl_loss: 0.000000 normal_loss: 0.014068\n",
      "[193/00024] train_loss: 0.013848 kl_loss: 0.000000 normal_loss: 0.013848\n",
      "[194/00049] train_loss: 0.013905 kl_loss: 0.000000 normal_loss: 0.013905\n",
      "[195/00074] train_loss: 0.013573 kl_loss: 0.000000 normal_loss: 0.013573\n",
      "[197/00024] train_loss: 0.013619 kl_loss: 0.000000 normal_loss: 0.013619\n",
      "[198/00049] train_loss: 0.013699 kl_loss: 0.000000 normal_loss: 0.013699\n",
      "[199/00074] train_loss: 0.013374 kl_loss: 0.000000 normal_loss: 0.013374\n",
      "[199/00074] IOU 0.8991929771751166\n",
      "[201/00024] train_loss: 0.012351 kl_loss: 0.000000 normal_loss: 0.012351\n",
      "[202/00049] train_loss: 0.011701 kl_loss: 0.000000 normal_loss: 0.011701\n",
      "[203/00074] train_loss: 0.011644 kl_loss: 0.000000 normal_loss: 0.011644\n",
      "[205/00024] train_loss: 0.011500 kl_loss: 0.000000 normal_loss: 0.011500\n",
      "[206/00049] train_loss: 0.011616 kl_loss: 0.000000 normal_loss: 0.011616\n",
      "[207/00074] train_loss: 0.011539 kl_loss: 0.000000 normal_loss: 0.011539\n",
      "[209/00024] train_loss: 0.011481 kl_loss: 0.000000 normal_loss: 0.011481\n",
      "[210/00049] train_loss: 0.011602 kl_loss: 0.000000 normal_loss: 0.011602\n",
      "[211/00074] train_loss: 0.011510 kl_loss: 0.000000 normal_loss: 0.011510\n",
      "[213/00024] train_loss: 0.011490 kl_loss: 0.000000 normal_loss: 0.011490\n",
      "[214/00049] train_loss: 0.011511 kl_loss: 0.000000 normal_loss: 0.011511\n",
      "[215/00074] train_loss: 0.011491 kl_loss: 0.000000 normal_loss: 0.011491\n",
      "[217/00024] train_loss: 0.011423 kl_loss: 0.000000 normal_loss: 0.011423\n",
      "[218/00049] train_loss: 0.011675 kl_loss: 0.000000 normal_loss: 0.011675\n",
      "[219/00074] train_loss: 0.011741 kl_loss: 0.000000 normal_loss: 0.011741\n",
      "[221/00024] train_loss: 0.011548 kl_loss: 0.000000 normal_loss: 0.011548\n",
      "[222/00049] train_loss: 0.011584 kl_loss: 0.000000 normal_loss: 0.011584\n",
      "[223/00074] train_loss: 0.011517 kl_loss: 0.000000 normal_loss: 0.011517\n",
      "[225/00024] train_loss: 0.011517 kl_loss: 0.000000 normal_loss: 0.011517\n",
      "[226/00049] train_loss: 0.011440 kl_loss: 0.000000 normal_loss: 0.011440\n",
      "[227/00074] train_loss: 0.011536 kl_loss: 0.000000 normal_loss: 0.011536\n",
      "[229/00024] train_loss: 0.011458 kl_loss: 0.000000 normal_loss: 0.011458\n",
      "[230/00049] train_loss: 0.011424 kl_loss: 0.000000 normal_loss: 0.011424\n",
      "[231/00074] train_loss: 0.011441 kl_loss: 0.000000 normal_loss: 0.011441\n",
      "[233/00024] train_loss: 0.011437 kl_loss: 0.000000 normal_loss: 0.011437\n",
      "[234/00049] train_loss: 0.011520 kl_loss: 0.000000 normal_loss: 0.011520\n",
      "[235/00074] train_loss: 0.011382 kl_loss: 0.000000 normal_loss: 0.011382\n",
      "[237/00024] train_loss: 0.011432 kl_loss: 0.000000 normal_loss: 0.011432\n",
      "[238/00049] train_loss: 0.011508 kl_loss: 0.000000 normal_loss: 0.011508\n",
      "[239/00074] train_loss: 0.011330 kl_loss: 0.000000 normal_loss: 0.011330\n",
      "[241/00024] train_loss: 0.011474 kl_loss: 0.000000 normal_loss: 0.011474\n",
      "[242/00049] train_loss: 0.011459 kl_loss: 0.000000 normal_loss: 0.011459\n",
      "[243/00074] train_loss: 0.011282 kl_loss: 0.000000 normal_loss: 0.011282\n",
      "[245/00024] train_loss: 0.011403 kl_loss: 0.000000 normal_loss: 0.011403\n",
      "[246/00049] train_loss: 0.011354 kl_loss: 0.000000 normal_loss: 0.011354\n",
      "[247/00074] train_loss: 0.011400 kl_loss: 0.000000 normal_loss: 0.011400\n",
      "[249/00024] train_loss: 0.011352 kl_loss: 0.000000 normal_loss: 0.011352\n",
      "[249/00074] IOU 0.9097692750145991\n",
      "[250/00049] train_loss: 0.011393 kl_loss: 0.000000 normal_loss: 0.011393\n",
      "[251/00074] train_loss: 0.011180 kl_loss: 0.000000 normal_loss: 0.011180\n",
      "[253/00024] train_loss: 0.011163 kl_loss: 0.000000 normal_loss: 0.011163\n",
      "[254/00049] train_loss: 0.011223 kl_loss: 0.000000 normal_loss: 0.011223\n",
      "[255/00074] train_loss: 0.011185 kl_loss: 0.000000 normal_loss: 0.011185\n",
      "[257/00024] train_loss: 0.011283 kl_loss: 0.000000 normal_loss: 0.011283\n",
      "[258/00049] train_loss: 0.011216 kl_loss: 0.000000 normal_loss: 0.011216\n",
      "[259/00074] train_loss: 0.011251 kl_loss: 0.000000 normal_loss: 0.011251\n",
      "[261/00024] train_loss: 0.011064 kl_loss: 0.000000 normal_loss: 0.011064\n",
      "[262/00049] train_loss: 0.011278 kl_loss: 0.000000 normal_loss: 0.011278\n",
      "[263/00074] train_loss: 0.011164 kl_loss: 0.000000 normal_loss: 0.011164\n",
      "[265/00024] train_loss: 0.011301 kl_loss: 0.000000 normal_loss: 0.011301\n",
      "[266/00049] train_loss: 0.011068 kl_loss: 0.000000 normal_loss: 0.011068\n",
      "[267/00074] train_loss: 0.011145 kl_loss: 0.000000 normal_loss: 0.011145\n",
      "[269/00024] train_loss: 0.011347 kl_loss: 0.000000 normal_loss: 0.011347\n",
      "[270/00049] train_loss: 0.011098 kl_loss: 0.000000 normal_loss: 0.011098\n",
      "[271/00074] train_loss: 0.011144 kl_loss: 0.000000 normal_loss: 0.011144\n",
      "[273/00024] train_loss: 0.010963 kl_loss: 0.000000 normal_loss: 0.010963\n",
      "[274/00049] train_loss: 0.011172 kl_loss: 0.000000 normal_loss: 0.011172\n",
      "[275/00074] train_loss: 0.011058 kl_loss: 0.000000 normal_loss: 0.011058\n",
      "[277/00024] train_loss: 0.011133 kl_loss: 0.000000 normal_loss: 0.011133\n",
      "[278/00049] train_loss: 0.011219 kl_loss: 0.000000 normal_loss: 0.011219\n",
      "[279/00074] train_loss: 0.011025 kl_loss: 0.000000 normal_loss: 0.011025\n",
      "[281/00024] train_loss: 0.011000 kl_loss: 0.000000 normal_loss: 0.011000\n",
      "[282/00049] train_loss: 0.011118 kl_loss: 0.000000 normal_loss: 0.011118\n",
      "[283/00074] train_loss: 0.011107 kl_loss: 0.000000 normal_loss: 0.011107\n",
      "[285/00024] train_loss: 0.010989 kl_loss: 0.000000 normal_loss: 0.010989\n",
      "[286/00049] train_loss: 0.011106 kl_loss: 0.000000 normal_loss: 0.011106\n",
      "[287/00074] train_loss: 0.010985 kl_loss: 0.000000 normal_loss: 0.010985\n",
      "[289/00024] train_loss: 0.011186 kl_loss: 0.000000 normal_loss: 0.011186\n",
      "[290/00049] train_loss: 0.010873 kl_loss: 0.000000 normal_loss: 0.010873\n",
      "[291/00074] train_loss: 0.010929 kl_loss: 0.000000 normal_loss: 0.010929\n",
      "[293/00024] train_loss: 0.010951 kl_loss: 0.000000 normal_loss: 0.010951\n",
      "[294/00049] train_loss: 0.011059 kl_loss: 0.000000 normal_loss: 0.011059\n",
      "[295/00074] train_loss: 0.011040 kl_loss: 0.000000 normal_loss: 0.011040\n",
      "[297/00024] train_loss: 0.010972 kl_loss: 0.000000 normal_loss: 0.010972\n",
      "[298/00049] train_loss: 0.010871 kl_loss: 0.000000 normal_loss: 0.010871\n",
      "[299/00074] train_loss: 0.010959 kl_loss: 0.000000 normal_loss: 0.010959\n",
      "[299/00074] IOU 0.9090687726127604\n",
      "[301/00024] train_loss: 0.010345 kl_loss: 0.000000 normal_loss: 0.010345\n",
      "[302/00049] train_loss: 0.010154 kl_loss: 0.000000 normal_loss: 0.010154\n",
      "[303/00074] train_loss: 0.010115 kl_loss: 0.000000 normal_loss: 0.010115\n",
      "[305/00024] train_loss: 0.010201 kl_loss: 0.000000 normal_loss: 0.010201\n",
      "[306/00049] train_loss: 0.010100 kl_loss: 0.000000 normal_loss: 0.010100\n",
      "[307/00074] train_loss: 0.010107 kl_loss: 0.000000 normal_loss: 0.010107\n",
      "[309/00024] train_loss: 0.010163 kl_loss: 0.000000 normal_loss: 0.010163\n",
      "[310/00049] train_loss: 0.010034 kl_loss: 0.000000 normal_loss: 0.010034\n",
      "[311/00074] train_loss: 0.010108 kl_loss: 0.000000 normal_loss: 0.010108\n",
      "[313/00024] train_loss: 0.010115 kl_loss: 0.000000 normal_loss: 0.010115\n",
      "[314/00049] train_loss: 0.010149 kl_loss: 0.000000 normal_loss: 0.010149\n",
      "[315/00074] train_loss: 0.010075 kl_loss: 0.000000 normal_loss: 0.010075\n",
      "[317/00024] train_loss: 0.010188 kl_loss: 0.000000 normal_loss: 0.010188\n",
      "[318/00049] train_loss: 0.010170 kl_loss: 0.000000 normal_loss: 0.010170\n",
      "[319/00074] train_loss: 0.010041 kl_loss: 0.000000 normal_loss: 0.010041\n",
      "[321/00024] train_loss: 0.010063 kl_loss: 0.000000 normal_loss: 0.010063\n",
      "[322/00049] train_loss: 0.009991 kl_loss: 0.000000 normal_loss: 0.009991\n",
      "[323/00074] train_loss: 0.010043 kl_loss: 0.000000 normal_loss: 0.010043\n",
      "[325/00024] train_loss: 0.010064 kl_loss: 0.000000 normal_loss: 0.010064\n",
      "[326/00049] train_loss: 0.010069 kl_loss: 0.000000 normal_loss: 0.010069\n",
      "[327/00074] train_loss: 0.010033 kl_loss: 0.000000 normal_loss: 0.010033\n",
      "[329/00024] train_loss: 0.010040 kl_loss: 0.000000 normal_loss: 0.010040\n",
      "[330/00049] train_loss: 0.010082 kl_loss: 0.000000 normal_loss: 0.010082\n",
      "[331/00074] train_loss: 0.010083 kl_loss: 0.000000 normal_loss: 0.010083\n",
      "[333/00024] train_loss: 0.010194 kl_loss: 0.000000 normal_loss: 0.010194\n",
      "[334/00049] train_loss: 0.009973 kl_loss: 0.000000 normal_loss: 0.009973\n",
      "[335/00074] train_loss: 0.010036 kl_loss: 0.000000 normal_loss: 0.010036\n",
      "[337/00024] train_loss: 0.010017 kl_loss: 0.000000 normal_loss: 0.010017\n",
      "[338/00049] train_loss: 0.009979 kl_loss: 0.000000 normal_loss: 0.009979\n",
      "[339/00074] train_loss: 0.010012 kl_loss: 0.000000 normal_loss: 0.010012\n",
      "[341/00024] train_loss: 0.009935 kl_loss: 0.000000 normal_loss: 0.009935\n",
      "[342/00049] train_loss: 0.010071 kl_loss: 0.000000 normal_loss: 0.010071\n",
      "[343/00074] train_loss: 0.009963 kl_loss: 0.000000 normal_loss: 0.009963\n",
      "[345/00024] train_loss: 0.009981 kl_loss: 0.000000 normal_loss: 0.009981\n",
      "[346/00049] train_loss: 0.009955 kl_loss: 0.000000 normal_loss: 0.009955\n",
      "[347/00074] train_loss: 0.009958 kl_loss: 0.000000 normal_loss: 0.009958\n",
      "[349/00024] train_loss: 0.010044 kl_loss: 0.000000 normal_loss: 0.010044\n",
      "[349/00074] IOU 0.9136460893414915\n",
      "[350/00049] train_loss: 0.009924 kl_loss: 0.000000 normal_loss: 0.009924\n",
      "[351/00074] train_loss: 0.009981 kl_loss: 0.000000 normal_loss: 0.009981\n",
      "[353/00024] train_loss: 0.009933 kl_loss: 0.000000 normal_loss: 0.009933\n",
      "[354/00049] train_loss: 0.009890 kl_loss: 0.000000 normal_loss: 0.009890\n",
      "[355/00074] train_loss: 0.009957 kl_loss: 0.000000 normal_loss: 0.009957\n",
      "[357/00024] train_loss: 0.009963 kl_loss: 0.000000 normal_loss: 0.009963\n",
      "[358/00049] train_loss: 0.009960 kl_loss: 0.000000 normal_loss: 0.009960\n",
      "[359/00074] train_loss: 0.009859 kl_loss: 0.000000 normal_loss: 0.009859\n",
      "[361/00024] train_loss: 0.009998 kl_loss: 0.000000 normal_loss: 0.009998\n",
      "[362/00049] train_loss: 0.009995 kl_loss: 0.000000 normal_loss: 0.009995\n",
      "[363/00074] train_loss: 0.009922 kl_loss: 0.000000 normal_loss: 0.009922\n",
      "[365/00024] train_loss: 0.009864 kl_loss: 0.000000 normal_loss: 0.009864\n",
      "[366/00049] train_loss: 0.009954 kl_loss: 0.000000 normal_loss: 0.009954\n",
      "[367/00074] train_loss: 0.009985 kl_loss: 0.000000 normal_loss: 0.009985\n",
      "[369/00024] train_loss: 0.009921 kl_loss: 0.000000 normal_loss: 0.009921\n",
      "[370/00049] train_loss: 0.009901 kl_loss: 0.000000 normal_loss: 0.009901\n",
      "[371/00074] train_loss: 0.009925 kl_loss: 0.000000 normal_loss: 0.009925\n",
      "[373/00024] train_loss: 0.009891 kl_loss: 0.000000 normal_loss: 0.009891\n",
      "[374/00049] train_loss: 0.009865 kl_loss: 0.000000 normal_loss: 0.009865\n",
      "[375/00074] train_loss: 0.009865 kl_loss: 0.000000 normal_loss: 0.009865\n",
      "[377/00024] train_loss: 0.009924 kl_loss: 0.000000 normal_loss: 0.009924\n",
      "[378/00049] train_loss: 0.009788 kl_loss: 0.000000 normal_loss: 0.009788\n",
      "[379/00074] train_loss: 0.009802 kl_loss: 0.000000 normal_loss: 0.009802\n",
      "[381/00024] train_loss: 0.009820 kl_loss: 0.000000 normal_loss: 0.009820\n",
      "[382/00049] train_loss: 0.009806 kl_loss: 0.000000 normal_loss: 0.009806\n",
      "[383/00074] train_loss: 0.009812 kl_loss: 0.000000 normal_loss: 0.009812\n",
      "[385/00024] train_loss: 0.009851 kl_loss: 0.000000 normal_loss: 0.009851\n",
      "[386/00049] train_loss: 0.009822 kl_loss: 0.000000 normal_loss: 0.009822\n",
      "[387/00074] train_loss: 0.009862 kl_loss: 0.000000 normal_loss: 0.009862\n",
      "[389/00024] train_loss: 0.009803 kl_loss: 0.000000 normal_loss: 0.009803\n",
      "[390/00049] train_loss: 0.009755 kl_loss: 0.000000 normal_loss: 0.009755\n",
      "[391/00074] train_loss: 0.009870 kl_loss: 0.000000 normal_loss: 0.009870\n",
      "[393/00024] train_loss: 0.009904 kl_loss: 0.000000 normal_loss: 0.009904\n",
      "[394/00049] train_loss: 0.009875 kl_loss: 0.000000 normal_loss: 0.009875\n",
      "[395/00074] train_loss: 0.009751 kl_loss: 0.000000 normal_loss: 0.009751\n",
      "[397/00024] train_loss: 0.009760 kl_loss: 0.000000 normal_loss: 0.009760\n",
      "[398/00049] train_loss: 0.009743 kl_loss: 0.000000 normal_loss: 0.009743\n",
      "[399/00074] train_loss: 0.009700 kl_loss: 0.000000 normal_loss: 0.009700\n",
      "[399/00074] IOU 0.9167061796349784\n",
      "[401/00024] train_loss: 0.009582 kl_loss: 0.000000 normal_loss: 0.009582\n",
      "[402/00049] train_loss: 0.009484 kl_loss: 0.000000 normal_loss: 0.009484\n",
      "[403/00074] train_loss: 0.009537 kl_loss: 0.000000 normal_loss: 0.009537\n",
      "[405/00024] train_loss: 0.009503 kl_loss: 0.000000 normal_loss: 0.009503\n",
      "[406/00049] train_loss: 0.009487 kl_loss: 0.000000 normal_loss: 0.009487\n",
      "[407/00074] train_loss: 0.009437 kl_loss: 0.000000 normal_loss: 0.009437\n",
      "[409/00024] train_loss: 0.009476 kl_loss: 0.000000 normal_loss: 0.009476\n",
      "[410/00049] train_loss: 0.009451 kl_loss: 0.000000 normal_loss: 0.009451\n",
      "[411/00074] train_loss: 0.009444 kl_loss: 0.000000 normal_loss: 0.009444\n",
      "[413/00024] train_loss: 0.009420 kl_loss: 0.000000 normal_loss: 0.009420\n",
      "[414/00049] train_loss: 0.009469 kl_loss: 0.000000 normal_loss: 0.009469\n",
      "[415/00074] train_loss: 0.009595 kl_loss: 0.000000 normal_loss: 0.009595\n",
      "[417/00024] train_loss: 0.009462 kl_loss: 0.000000 normal_loss: 0.009462\n",
      "[418/00049] train_loss: 0.009431 kl_loss: 0.000000 normal_loss: 0.009431\n",
      "[419/00074] train_loss: 0.009461 kl_loss: 0.000000 normal_loss: 0.009461\n",
      "[421/00024] train_loss: 0.009500 kl_loss: 0.000000 normal_loss: 0.009500\n",
      "[422/00049] train_loss: 0.009395 kl_loss: 0.000000 normal_loss: 0.009395\n",
      "[423/00074] train_loss: 0.009445 kl_loss: 0.000000 normal_loss: 0.009445\n",
      "[425/00024] train_loss: 0.009456 kl_loss: 0.000000 normal_loss: 0.009456\n",
      "[426/00049] train_loss: 0.009399 kl_loss: 0.000000 normal_loss: 0.009399\n",
      "[427/00074] train_loss: 0.009494 kl_loss: 0.000000 normal_loss: 0.009494\n",
      "[429/00024] train_loss: 0.009382 kl_loss: 0.000000 normal_loss: 0.009382\n",
      "[430/00049] train_loss: 0.009525 kl_loss: 0.000000 normal_loss: 0.009525\n",
      "[431/00074] train_loss: 0.009366 kl_loss: 0.000000 normal_loss: 0.009366\n",
      "[433/00024] train_loss: 0.009379 kl_loss: 0.000000 normal_loss: 0.009379\n",
      "[434/00049] train_loss: 0.009400 kl_loss: 0.000000 normal_loss: 0.009400\n",
      "[435/00074] train_loss: 0.009426 kl_loss: 0.000000 normal_loss: 0.009426\n",
      "[437/00024] train_loss: 0.009387 kl_loss: 0.000000 normal_loss: 0.009387\n",
      "[438/00049] train_loss: 0.009469 kl_loss: 0.000000 normal_loss: 0.009469\n",
      "[439/00074] train_loss: 0.009370 kl_loss: 0.000000 normal_loss: 0.009370\n",
      "[441/00024] train_loss: 0.009428 kl_loss: 0.000000 normal_loss: 0.009428\n",
      "[442/00049] train_loss: 0.009426 kl_loss: 0.000000 normal_loss: 0.009426\n",
      "[443/00074] train_loss: 0.009386 kl_loss: 0.000000 normal_loss: 0.009386\n",
      "[445/00024] train_loss: 0.009415 kl_loss: 0.000000 normal_loss: 0.009415\n",
      "[446/00049] train_loss: 0.009388 kl_loss: 0.000000 normal_loss: 0.009388\n",
      "[447/00074] train_loss: 0.009373 kl_loss: 0.000000 normal_loss: 0.009373\n",
      "[449/00024] train_loss: 0.009362 kl_loss: 0.000000 normal_loss: 0.009362\n",
      "[449/00074] IOU 0.9176198591167728\n",
      "[450/00049] train_loss: 0.009342 kl_loss: 0.000000 normal_loss: 0.009342\n",
      "[451/00074] train_loss: 0.009419 kl_loss: 0.000000 normal_loss: 0.009419\n",
      "[453/00024] train_loss: 0.009396 kl_loss: 0.000000 normal_loss: 0.009396\n",
      "[454/00049] train_loss: 0.009279 kl_loss: 0.000000 normal_loss: 0.009279\n",
      "[455/00074] train_loss: 0.009350 kl_loss: 0.000000 normal_loss: 0.009350\n",
      "[457/00024] train_loss: 0.009387 kl_loss: 0.000000 normal_loss: 0.009387\n",
      "[458/00049] train_loss: 0.009380 kl_loss: 0.000000 normal_loss: 0.009380\n",
      "[459/00074] train_loss: 0.009369 kl_loss: 0.000000 normal_loss: 0.009369\n",
      "[461/00024] train_loss: 0.009380 kl_loss: 0.000000 normal_loss: 0.009380\n",
      "[462/00049] train_loss: 0.009346 kl_loss: 0.000000 normal_loss: 0.009346\n",
      "[463/00074] train_loss: 0.009486 kl_loss: 0.000000 normal_loss: 0.009486\n",
      "[465/00024] train_loss: 0.009400 kl_loss: 0.000000 normal_loss: 0.009400\n",
      "[466/00049] train_loss: 0.009361 kl_loss: 0.000000 normal_loss: 0.009361\n",
      "[467/00074] train_loss: 0.009257 kl_loss: 0.000000 normal_loss: 0.009257\n",
      "[469/00024] train_loss: 0.009341 kl_loss: 0.000000 normal_loss: 0.009341\n",
      "[470/00049] train_loss: 0.009389 kl_loss: 0.000000 normal_loss: 0.009389\n",
      "[471/00074] train_loss: 0.009313 kl_loss: 0.000000 normal_loss: 0.009313\n",
      "[473/00024] train_loss: 0.009314 kl_loss: 0.000000 normal_loss: 0.009314\n",
      "[474/00049] train_loss: 0.009491 kl_loss: 0.000000 normal_loss: 0.009491\n",
      "[475/00074] train_loss: 0.009285 kl_loss: 0.000000 normal_loss: 0.009285\n",
      "[477/00024] train_loss: 0.009291 kl_loss: 0.000000 normal_loss: 0.009291\n",
      "[478/00049] train_loss: 0.009256 kl_loss: 0.000000 normal_loss: 0.009256\n",
      "[479/00074] train_loss: 0.009355 kl_loss: 0.000000 normal_loss: 0.009355\n",
      "[481/00024] train_loss: 0.009277 kl_loss: 0.000000 normal_loss: 0.009277\n",
      "[482/00049] train_loss: 0.009281 kl_loss: 0.000000 normal_loss: 0.009281\n",
      "[483/00074] train_loss: 0.009314 kl_loss: 0.000000 normal_loss: 0.009314\n",
      "[485/00024] train_loss: 0.009307 kl_loss: 0.000000 normal_loss: 0.009307\n",
      "[486/00049] train_loss: 0.009324 kl_loss: 0.000000 normal_loss: 0.009324\n",
      "[487/00074] train_loss: 0.009298 kl_loss: 0.000000 normal_loss: 0.009298\n",
      "[489/00024] train_loss: 0.009349 kl_loss: 0.000000 normal_loss: 0.009349\n",
      "[490/00049] train_loss: 0.009310 kl_loss: 0.000000 normal_loss: 0.009310\n",
      "[491/00074] train_loss: 0.009278 kl_loss: 0.000000 normal_loss: 0.009278\n",
      "[493/00024] train_loss: 0.009312 kl_loss: 0.000000 normal_loss: 0.009312\n",
      "[494/00049] train_loss: 0.009280 kl_loss: 0.000000 normal_loss: 0.009280\n",
      "[495/00074] train_loss: 0.009283 kl_loss: 0.000000 normal_loss: 0.009283\n",
      "[497/00024] train_loss: 0.009348 kl_loss: 0.000000 normal_loss: 0.009348\n",
      "[498/00049] train_loss: 0.009276 kl_loss: 0.000000 normal_loss: 0.009276\n",
      "[499/00074] train_loss: 0.009316 kl_loss: 0.000000 normal_loss: 0.009316\n",
      "[499/00074] IOU 0.9187757863290608\n",
      "[501/00024] train_loss: 0.009178 kl_loss: 0.000000 normal_loss: 0.009178\n",
      "[502/00049] train_loss: 0.009143 kl_loss: 0.000000 normal_loss: 0.009143\n",
      "[503/00074] train_loss: 0.009154 kl_loss: 0.000000 normal_loss: 0.009154\n",
      "[505/00024] train_loss: 0.009155 kl_loss: 0.000000 normal_loss: 0.009155\n",
      "[506/00049] train_loss: 0.009105 kl_loss: 0.000000 normal_loss: 0.009105\n",
      "[507/00074] train_loss: 0.009165 kl_loss: 0.000000 normal_loss: 0.009165\n",
      "[509/00024] train_loss: 0.009148 kl_loss: 0.000000 normal_loss: 0.009148\n",
      "[510/00049] train_loss: 0.009087 kl_loss: 0.000000 normal_loss: 0.009087\n",
      "[511/00074] train_loss: 0.009185 kl_loss: 0.000000 normal_loss: 0.009185\n",
      "[513/00024] train_loss: 0.009153 kl_loss: 0.000000 normal_loss: 0.009153\n",
      "[514/00049] train_loss: 0.009177 kl_loss: 0.000000 normal_loss: 0.009177\n",
      "[515/00074] train_loss: 0.009111 kl_loss: 0.000000 normal_loss: 0.009111\n",
      "[517/00024] train_loss: 0.009079 kl_loss: 0.000000 normal_loss: 0.009079\n",
      "[518/00049] train_loss: 0.009156 kl_loss: 0.000000 normal_loss: 0.009156\n",
      "[519/00074] train_loss: 0.009155 kl_loss: 0.000000 normal_loss: 0.009155\n",
      "[521/00024] train_loss: 0.009109 kl_loss: 0.000000 normal_loss: 0.009109\n",
      "[522/00049] train_loss: 0.009177 kl_loss: 0.000000 normal_loss: 0.009177\n",
      "[523/00074] train_loss: 0.009139 kl_loss: 0.000000 normal_loss: 0.009139\n",
      "[525/00024] train_loss: 0.009088 kl_loss: 0.000000 normal_loss: 0.009088\n",
      "[526/00049] train_loss: 0.009088 kl_loss: 0.000000 normal_loss: 0.009088\n",
      "[527/00074] train_loss: 0.009167 kl_loss: 0.000000 normal_loss: 0.009167\n",
      "[529/00024] train_loss: 0.009094 kl_loss: 0.000000 normal_loss: 0.009094\n",
      "[530/00049] train_loss: 0.009168 kl_loss: 0.000000 normal_loss: 0.009168\n",
      "[531/00074] train_loss: 0.009083 kl_loss: 0.000000 normal_loss: 0.009083\n",
      "[533/00024] train_loss: 0.009093 kl_loss: 0.000000 normal_loss: 0.009093\n",
      "[534/00049] train_loss: 0.009144 kl_loss: 0.000000 normal_loss: 0.009144\n",
      "[535/00074] train_loss: 0.009079 kl_loss: 0.000000 normal_loss: 0.009079\n",
      "[537/00024] train_loss: 0.009097 kl_loss: 0.000000 normal_loss: 0.009097\n",
      "[538/00049] train_loss: 0.009092 kl_loss: 0.000000 normal_loss: 0.009092\n",
      "[539/00074] train_loss: 0.009147 kl_loss: 0.000000 normal_loss: 0.009147\n",
      "[541/00024] train_loss: 0.009102 kl_loss: 0.000000 normal_loss: 0.009102\n",
      "[542/00049] train_loss: 0.009146 kl_loss: 0.000000 normal_loss: 0.009146\n",
      "[543/00074] train_loss: 0.009091 kl_loss: 0.000000 normal_loss: 0.009091\n",
      "[545/00024] train_loss: 0.009117 kl_loss: 0.000000 normal_loss: 0.009117\n",
      "[546/00049] train_loss: 0.009158 kl_loss: 0.000000 normal_loss: 0.009158\n",
      "[547/00074] train_loss: 0.009088 kl_loss: 0.000000 normal_loss: 0.009088\n",
      "[549/00024] train_loss: 0.009081 kl_loss: 0.000000 normal_loss: 0.009081\n",
      "[549/00074] IOU 0.9194678045560917\n",
      "[550/00049] train_loss: 0.009057 kl_loss: 0.000000 normal_loss: 0.009057\n",
      "[551/00074] train_loss: 0.009121 kl_loss: 0.000000 normal_loss: 0.009121\n",
      "[553/00024] train_loss: 0.009082 kl_loss: 0.000000 normal_loss: 0.009082\n",
      "[554/00049] train_loss: 0.009158 kl_loss: 0.000000 normal_loss: 0.009158\n",
      "[555/00074] train_loss: 0.009105 kl_loss: 0.000000 normal_loss: 0.009105\n",
      "[557/00024] train_loss: 0.009068 kl_loss: 0.000000 normal_loss: 0.009068\n",
      "[558/00049] train_loss: 0.009112 kl_loss: 0.000000 normal_loss: 0.009112\n",
      "[559/00074] train_loss: 0.009099 kl_loss: 0.000000 normal_loss: 0.009099\n",
      "[561/00024] train_loss: 0.009077 kl_loss: 0.000000 normal_loss: 0.009077\n",
      "[562/00049] train_loss: 0.009059 kl_loss: 0.000000 normal_loss: 0.009059\n",
      "[563/00074] train_loss: 0.009095 kl_loss: 0.000000 normal_loss: 0.009095\n",
      "[565/00024] train_loss: 0.009057 kl_loss: 0.000000 normal_loss: 0.009057\n",
      "[566/00049] train_loss: 0.009075 kl_loss: 0.000000 normal_loss: 0.009075\n",
      "[567/00074] train_loss: 0.009095 kl_loss: 0.000000 normal_loss: 0.009095\n",
      "[569/00024] train_loss: 0.009080 kl_loss: 0.000000 normal_loss: 0.009080\n",
      "[570/00049] train_loss: 0.009121 kl_loss: 0.000000 normal_loss: 0.009121\n",
      "[571/00074] train_loss: 0.009088 kl_loss: 0.000000 normal_loss: 0.009088\n",
      "[573/00024] train_loss: 0.009042 kl_loss: 0.000000 normal_loss: 0.009042\n",
      "[574/00049] train_loss: 0.009119 kl_loss: 0.000000 normal_loss: 0.009119\n",
      "[575/00074] train_loss: 0.009072 kl_loss: 0.000000 normal_loss: 0.009072\n",
      "[577/00024] train_loss: 0.009044 kl_loss: 0.000000 normal_loss: 0.009044\n",
      "[578/00049] train_loss: 0.009084 kl_loss: 0.000000 normal_loss: 0.009084\n",
      "[579/00074] train_loss: 0.009063 kl_loss: 0.000000 normal_loss: 0.009063\n",
      "[581/00024] train_loss: 0.009070 kl_loss: 0.000000 normal_loss: 0.009070\n",
      "[582/00049] train_loss: 0.009110 kl_loss: 0.000000 normal_loss: 0.009110\n",
      "[583/00074] train_loss: 0.009051 kl_loss: 0.000000 normal_loss: 0.009051\n",
      "[585/00024] train_loss: 0.009094 kl_loss: 0.000000 normal_loss: 0.009094\n",
      "[586/00049] train_loss: 0.009072 kl_loss: 0.000000 normal_loss: 0.009072\n",
      "[587/00074] train_loss: 0.009042 kl_loss: 0.000000 normal_loss: 0.009042\n",
      "[589/00024] train_loss: 0.009039 kl_loss: 0.000000 normal_loss: 0.009039\n",
      "[590/00049] train_loss: 0.009043 kl_loss: 0.000000 normal_loss: 0.009043\n",
      "[591/00074] train_loss: 0.009093 kl_loss: 0.000000 normal_loss: 0.009093\n",
      "[593/00024] train_loss: 0.009024 kl_loss: 0.000000 normal_loss: 0.009024\n",
      "[594/00049] train_loss: 0.009068 kl_loss: 0.000000 normal_loss: 0.009068\n",
      "[595/00074] train_loss: 0.009055 kl_loss: 0.000000 normal_loss: 0.009055\n",
      "[597/00024] train_loss: 0.009086 kl_loss: 0.000000 normal_loss: 0.009086\n",
      "[598/00049] train_loss: 0.009089 kl_loss: 0.000000 normal_loss: 0.009089\n",
      "[599/00074] train_loss: 0.009002 kl_loss: 0.000000 normal_loss: 0.009002\n",
      "[599/00074] IOU 0.9207689363136887\n",
      "[601/00024] train_loss: 0.008974 kl_loss: 0.000000 normal_loss: 0.008974\n",
      "[602/00049] train_loss: 0.008986 kl_loss: 0.000000 normal_loss: 0.008986\n",
      "[603/00074] train_loss: 0.009004 kl_loss: 0.000000 normal_loss: 0.009004\n",
      "[605/00024] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[606/00049] train_loss: 0.008920 kl_loss: 0.000000 normal_loss: 0.008920\n",
      "[607/00074] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[609/00024] train_loss: 0.008979 kl_loss: 0.000000 normal_loss: 0.008979\n",
      "[610/00049] train_loss: 0.008967 kl_loss: 0.000000 normal_loss: 0.008967\n",
      "[611/00074] train_loss: 0.008987 kl_loss: 0.000000 normal_loss: 0.008987\n",
      "[613/00024] train_loss: 0.008939 kl_loss: 0.000000 normal_loss: 0.008939\n",
      "[614/00049] train_loss: 0.009030 kl_loss: 0.000000 normal_loss: 0.009030\n",
      "[615/00074] train_loss: 0.008957 kl_loss: 0.000000 normal_loss: 0.008957\n",
      "[617/00024] train_loss: 0.008989 kl_loss: 0.000000 normal_loss: 0.008989\n",
      "[618/00049] train_loss: 0.008979 kl_loss: 0.000000 normal_loss: 0.008979\n",
      "[619/00074] train_loss: 0.008958 kl_loss: 0.000000 normal_loss: 0.008958\n",
      "[621/00024] train_loss: 0.008922 kl_loss: 0.000000 normal_loss: 0.008922\n",
      "[622/00049] train_loss: 0.008992 kl_loss: 0.000000 normal_loss: 0.008992\n",
      "[623/00074] train_loss: 0.008990 kl_loss: 0.000000 normal_loss: 0.008990\n",
      "[625/00024] train_loss: 0.008965 kl_loss: 0.000000 normal_loss: 0.008965\n",
      "[626/00049] train_loss: 0.008937 kl_loss: 0.000000 normal_loss: 0.008937\n",
      "[627/00074] train_loss: 0.008999 kl_loss: 0.000000 normal_loss: 0.008999\n",
      "[629/00024] train_loss: 0.008934 kl_loss: 0.000000 normal_loss: 0.008934\n",
      "[630/00049] train_loss: 0.009042 kl_loss: 0.000000 normal_loss: 0.009042\n",
      "[631/00074] train_loss: 0.008947 kl_loss: 0.000000 normal_loss: 0.008947\n",
      "[633/00024] train_loss: 0.008940 kl_loss: 0.000000 normal_loss: 0.008940\n",
      "[634/00049] train_loss: 0.008986 kl_loss: 0.000000 normal_loss: 0.008986\n",
      "[635/00074] train_loss: 0.008986 kl_loss: 0.000000 normal_loss: 0.008986\n",
      "[637/00024] train_loss: 0.008987 kl_loss: 0.000000 normal_loss: 0.008987\n",
      "[638/00049] train_loss: 0.008953 kl_loss: 0.000000 normal_loss: 0.008953\n",
      "[639/00074] train_loss: 0.008958 kl_loss: 0.000000 normal_loss: 0.008958\n",
      "[641/00024] train_loss: 0.008921 kl_loss: 0.000000 normal_loss: 0.008921\n",
      "[642/00049] train_loss: 0.008987 kl_loss: 0.000000 normal_loss: 0.008987\n",
      "[643/00074] train_loss: 0.008983 kl_loss: 0.000000 normal_loss: 0.008983\n",
      "[645/00024] train_loss: 0.008982 kl_loss: 0.000000 normal_loss: 0.008982\n",
      "[646/00049] train_loss: 0.008947 kl_loss: 0.000000 normal_loss: 0.008947\n",
      "[647/00074] train_loss: 0.008970 kl_loss: 0.000000 normal_loss: 0.008970\n",
      "[649/00024] train_loss: 0.008954 kl_loss: 0.000000 normal_loss: 0.008954\n",
      "[649/00074] IOU 0.9200765140975515\n",
      "[650/00049] train_loss: 0.008913 kl_loss: 0.000000 normal_loss: 0.008913\n",
      "[651/00074] train_loss: 0.008990 kl_loss: 0.000000 normal_loss: 0.008990\n",
      "[653/00024] train_loss: 0.008923 kl_loss: 0.000000 normal_loss: 0.008923\n",
      "[654/00049] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[655/00074] train_loss: 0.008922 kl_loss: 0.000000 normal_loss: 0.008922\n",
      "[657/00024] train_loss: 0.008969 kl_loss: 0.000000 normal_loss: 0.008969\n",
      "[658/00049] train_loss: 0.008977 kl_loss: 0.000000 normal_loss: 0.008977\n",
      "[659/00074] train_loss: 0.008952 kl_loss: 0.000000 normal_loss: 0.008952\n",
      "[661/00024] train_loss: 0.008921 kl_loss: 0.000000 normal_loss: 0.008921\n",
      "[662/00049] train_loss: 0.008931 kl_loss: 0.000000 normal_loss: 0.008931\n",
      "[663/00074] train_loss: 0.008972 kl_loss: 0.000000 normal_loss: 0.008972\n",
      "[665/00024] train_loss: 0.008936 kl_loss: 0.000000 normal_loss: 0.008936\n",
      "[666/00049] train_loss: 0.009000 kl_loss: 0.000000 normal_loss: 0.009000\n",
      "[667/00074] train_loss: 0.008916 kl_loss: 0.000000 normal_loss: 0.008916\n",
      "[669/00024] train_loss: 0.008925 kl_loss: 0.000000 normal_loss: 0.008925\n",
      "[670/00049] train_loss: 0.008915 kl_loss: 0.000000 normal_loss: 0.008915\n",
      "[671/00074] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[673/00024] train_loss: 0.008948 kl_loss: 0.000000 normal_loss: 0.008948\n",
      "[674/00049] train_loss: 0.008964 kl_loss: 0.000000 normal_loss: 0.008964\n",
      "[675/00074] train_loss: 0.008935 kl_loss: 0.000000 normal_loss: 0.008935\n",
      "[677/00024] train_loss: 0.008906 kl_loss: 0.000000 normal_loss: 0.008906\n",
      "[678/00049] train_loss: 0.008982 kl_loss: 0.000000 normal_loss: 0.008982\n",
      "[679/00074] train_loss: 0.008942 kl_loss: 0.000000 normal_loss: 0.008942\n",
      "[681/00024] train_loss: 0.008948 kl_loss: 0.000000 normal_loss: 0.008948\n",
      "[682/00049] train_loss: 0.008919 kl_loss: 0.000000 normal_loss: 0.008919\n",
      "[683/00074] train_loss: 0.008972 kl_loss: 0.000000 normal_loss: 0.008972\n",
      "[685/00024] train_loss: 0.008942 kl_loss: 0.000000 normal_loss: 0.008942\n",
      "[686/00049] train_loss: 0.008912 kl_loss: 0.000000 normal_loss: 0.008912\n",
      "[687/00074] train_loss: 0.008972 kl_loss: 0.000000 normal_loss: 0.008972\n",
      "[689/00024] train_loss: 0.008890 kl_loss: 0.000000 normal_loss: 0.008890\n",
      "[690/00049] train_loss: 0.008965 kl_loss: 0.000000 normal_loss: 0.008965\n",
      "[691/00074] train_loss: 0.008940 kl_loss: 0.000000 normal_loss: 0.008940\n",
      "[693/00024] train_loss: 0.008908 kl_loss: 0.000000 normal_loss: 0.008908\n",
      "[694/00049] train_loss: 0.008959 kl_loss: 0.000000 normal_loss: 0.008959\n",
      "[695/00074] train_loss: 0.008904 kl_loss: 0.000000 normal_loss: 0.008904\n",
      "[697/00024] train_loss: 0.008909 kl_loss: 0.000000 normal_loss: 0.008909\n",
      "[698/00049] train_loss: 0.008904 kl_loss: 0.000000 normal_loss: 0.008904\n",
      "[699/00074] train_loss: 0.008957 kl_loss: 0.000000 normal_loss: 0.008957\n",
      "[699/00074] IOU 0.9203456947021187\n",
      "[701/00024] train_loss: 0.008897 kl_loss: 0.000000 normal_loss: 0.008897\n",
      "[702/00049] train_loss: 0.008879 kl_loss: 0.000000 normal_loss: 0.008879\n",
      "[703/00074] train_loss: 0.008902 kl_loss: 0.000000 normal_loss: 0.008902\n",
      "[705/00024] train_loss: 0.008885 kl_loss: 0.000000 normal_loss: 0.008885\n",
      "[706/00049] train_loss: 0.008929 kl_loss: 0.000000 normal_loss: 0.008929\n",
      "[707/00074] train_loss: 0.008870 kl_loss: 0.000000 normal_loss: 0.008870\n",
      "[709/00024] train_loss: 0.008878 kl_loss: 0.000000 normal_loss: 0.008878\n",
      "[710/00049] train_loss: 0.008897 kl_loss: 0.000000 normal_loss: 0.008897\n",
      "[711/00074] train_loss: 0.008915 kl_loss: 0.000000 normal_loss: 0.008915\n",
      "[713/00024] train_loss: 0.008842 kl_loss: 0.000000 normal_loss: 0.008842\n",
      "[714/00049] train_loss: 0.008908 kl_loss: 0.000000 normal_loss: 0.008908\n",
      "[715/00074] train_loss: 0.008912 kl_loss: 0.000000 normal_loss: 0.008912\n",
      "[717/00024] train_loss: 0.008860 kl_loss: 0.000000 normal_loss: 0.008860\n",
      "[718/00049] train_loss: 0.008933 kl_loss: 0.000000 normal_loss: 0.008933\n",
      "[719/00074] train_loss: 0.008890 kl_loss: 0.000000 normal_loss: 0.008890\n",
      "[721/00024] train_loss: 0.008871 kl_loss: 0.000000 normal_loss: 0.008871\n",
      "[722/00049] train_loss: 0.008932 kl_loss: 0.000000 normal_loss: 0.008932\n",
      "[723/00074] train_loss: 0.008848 kl_loss: 0.000000 normal_loss: 0.008848\n",
      "[725/00024] train_loss: 0.008893 kl_loss: 0.000000 normal_loss: 0.008893\n",
      "[726/00049] train_loss: 0.008910 kl_loss: 0.000000 normal_loss: 0.008910\n",
      "[727/00074] train_loss: 0.008876 kl_loss: 0.000000 normal_loss: 0.008876\n",
      "[729/00024] train_loss: 0.008910 kl_loss: 0.000000 normal_loss: 0.008910\n",
      "[730/00049] train_loss: 0.008856 kl_loss: 0.000000 normal_loss: 0.008856\n",
      "[731/00074] train_loss: 0.008893 kl_loss: 0.000000 normal_loss: 0.008893\n",
      "[733/00024] train_loss: 0.008882 kl_loss: 0.000000 normal_loss: 0.008882\n",
      "[734/00049] train_loss: 0.008915 kl_loss: 0.000000 normal_loss: 0.008915\n",
      "[735/00074] train_loss: 0.008858 kl_loss: 0.000000 normal_loss: 0.008858\n",
      "[737/00024] train_loss: 0.008913 kl_loss: 0.000000 normal_loss: 0.008913\n",
      "[738/00049] train_loss: 0.008847 kl_loss: 0.000000 normal_loss: 0.008847\n",
      "[739/00074] train_loss: 0.008891 kl_loss: 0.000000 normal_loss: 0.008891\n",
      "[741/00024] train_loss: 0.008855 kl_loss: 0.000000 normal_loss: 0.008855\n",
      "[742/00049] train_loss: 0.008925 kl_loss: 0.000000 normal_loss: 0.008925\n",
      "[743/00074] train_loss: 0.008869 kl_loss: 0.000000 normal_loss: 0.008869\n",
      "[745/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[746/00049] train_loss: 0.008849 kl_loss: 0.000000 normal_loss: 0.008849\n",
      "[747/00074] train_loss: 0.008954 kl_loss: 0.000000 normal_loss: 0.008954\n",
      "[749/00024] train_loss: 0.008868 kl_loss: 0.000000 normal_loss: 0.008868\n",
      "[749/00074] IOU 0.9206733241987725\n",
      "[750/00049] train_loss: 0.008890 kl_loss: 0.000000 normal_loss: 0.008890\n",
      "[751/00074] train_loss: 0.008913 kl_loss: 0.000000 normal_loss: 0.008913\n",
      "[753/00024] train_loss: 0.008889 kl_loss: 0.000000 normal_loss: 0.008889\n",
      "[754/00049] train_loss: 0.008861 kl_loss: 0.000000 normal_loss: 0.008861\n",
      "[755/00074] train_loss: 0.008901 kl_loss: 0.000000 normal_loss: 0.008901\n",
      "[757/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[758/00049] train_loss: 0.008924 kl_loss: 0.000000 normal_loss: 0.008924\n",
      "[759/00074] train_loss: 0.008848 kl_loss: 0.000000 normal_loss: 0.008848\n",
      "[761/00024] train_loss: 0.008893 kl_loss: 0.000000 normal_loss: 0.008893\n",
      "[762/00049] train_loss: 0.008885 kl_loss: 0.000000 normal_loss: 0.008885\n",
      "[763/00074] train_loss: 0.008865 kl_loss: 0.000000 normal_loss: 0.008865\n",
      "[765/00024] train_loss: 0.008838 kl_loss: 0.000000 normal_loss: 0.008838\n",
      "[766/00049] train_loss: 0.008929 kl_loss: 0.000000 normal_loss: 0.008929\n",
      "[767/00074] train_loss: 0.008846 kl_loss: 0.000000 normal_loss: 0.008846\n",
      "[769/00024] train_loss: 0.008891 kl_loss: 0.000000 normal_loss: 0.008891\n",
      "[770/00049] train_loss: 0.008853 kl_loss: 0.000000 normal_loss: 0.008853\n",
      "[771/00074] train_loss: 0.008862 kl_loss: 0.000000 normal_loss: 0.008862\n",
      "[773/00024] train_loss: 0.008867 kl_loss: 0.000000 normal_loss: 0.008867\n",
      "[774/00049] train_loss: 0.008847 kl_loss: 0.000000 normal_loss: 0.008847\n",
      "[775/00074] train_loss: 0.008913 kl_loss: 0.000000 normal_loss: 0.008913\n",
      "[777/00024] train_loss: 0.008846 kl_loss: 0.000000 normal_loss: 0.008846\n",
      "[778/00049] train_loss: 0.008896 kl_loss: 0.000000 normal_loss: 0.008896\n",
      "[779/00074] train_loss: 0.008870 kl_loss: 0.000000 normal_loss: 0.008870\n",
      "[781/00024] train_loss: 0.008844 kl_loss: 0.000000 normal_loss: 0.008844\n",
      "[782/00049] train_loss: 0.008896 kl_loss: 0.000000 normal_loss: 0.008896\n",
      "[783/00074] train_loss: 0.008865 kl_loss: 0.000000 normal_loss: 0.008865\n",
      "[785/00024] train_loss: 0.008866 kl_loss: 0.000000 normal_loss: 0.008866\n",
      "[786/00049] train_loss: 0.008879 kl_loss: 0.000000 normal_loss: 0.008879\n",
      "[787/00074] train_loss: 0.008874 kl_loss: 0.000000 normal_loss: 0.008874\n",
      "[789/00024] train_loss: 0.008821 kl_loss: 0.000000 normal_loss: 0.008821\n",
      "[790/00049] train_loss: 0.008920 kl_loss: 0.000000 normal_loss: 0.008920\n",
      "[791/00074] train_loss: 0.008867 kl_loss: 0.000000 normal_loss: 0.008867\n",
      "[793/00024] train_loss: 0.008870 kl_loss: 0.000000 normal_loss: 0.008870\n",
      "[794/00049] train_loss: 0.008847 kl_loss: 0.000000 normal_loss: 0.008847\n",
      "[795/00074] train_loss: 0.008875 kl_loss: 0.000000 normal_loss: 0.008875\n",
      "[797/00024] train_loss: 0.008860 kl_loss: 0.000000 normal_loss: 0.008860\n",
      "[798/00049] train_loss: 0.008857 kl_loss: 0.000000 normal_loss: 0.008857\n",
      "[799/00074] train_loss: 0.008879 kl_loss: 0.000000 normal_loss: 0.008879\n",
      "[799/00074] IOU 0.9212639772891998\n",
      "[801/00024] train_loss: 0.008889 kl_loss: 0.000000 normal_loss: 0.008889\n",
      "[802/00049] train_loss: 0.008834 kl_loss: 0.000000 normal_loss: 0.008834\n",
      "[803/00074] train_loss: 0.008850 kl_loss: 0.000000 normal_loss: 0.008850\n",
      "[805/00024] train_loss: 0.008861 kl_loss: 0.000000 normal_loss: 0.008861\n",
      "[806/00049] train_loss: 0.008835 kl_loss: 0.000000 normal_loss: 0.008835\n",
      "[807/00074] train_loss: 0.008834 kl_loss: 0.000000 normal_loss: 0.008834\n",
      "[809/00024] train_loss: 0.008861 kl_loss: 0.000000 normal_loss: 0.008861\n",
      "[810/00049] train_loss: 0.008841 kl_loss: 0.000000 normal_loss: 0.008841\n",
      "[811/00074] train_loss: 0.008848 kl_loss: 0.000000 normal_loss: 0.008848\n",
      "[813/00024] train_loss: 0.008859 kl_loss: 0.000000 normal_loss: 0.008859\n",
      "[814/00049] train_loss: 0.008855 kl_loss: 0.000000 normal_loss: 0.008855\n",
      "[815/00074] train_loss: 0.008856 kl_loss: 0.000000 normal_loss: 0.008856\n",
      "[817/00024] train_loss: 0.008859 kl_loss: 0.000000 normal_loss: 0.008859\n",
      "[818/00049] train_loss: 0.008881 kl_loss: 0.000000 normal_loss: 0.008881\n",
      "[819/00074] train_loss: 0.008811 kl_loss: 0.000000 normal_loss: 0.008811\n",
      "[821/00024] train_loss: 0.008852 kl_loss: 0.000000 normal_loss: 0.008852\n",
      "[822/00049] train_loss: 0.008847 kl_loss: 0.000000 normal_loss: 0.008847\n",
      "[823/00074] train_loss: 0.008841 kl_loss: 0.000000 normal_loss: 0.008841\n",
      "[825/00024] train_loss: 0.008850 kl_loss: 0.000000 normal_loss: 0.008850\n",
      "[826/00049] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[827/00074] train_loss: 0.008829 kl_loss: 0.000000 normal_loss: 0.008829\n",
      "[829/00024] train_loss: 0.008869 kl_loss: 0.000000 normal_loss: 0.008869\n",
      "[830/00049] train_loss: 0.008801 kl_loss: 0.000000 normal_loss: 0.008801\n",
      "[831/00074] train_loss: 0.008871 kl_loss: 0.000000 normal_loss: 0.008871\n",
      "[833/00024] train_loss: 0.008859 kl_loss: 0.000000 normal_loss: 0.008859\n",
      "[834/00049] train_loss: 0.008835 kl_loss: 0.000000 normal_loss: 0.008835\n",
      "[835/00074] train_loss: 0.008843 kl_loss: 0.000000 normal_loss: 0.008843\n",
      "[837/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[838/00049] train_loss: 0.008798 kl_loss: 0.000000 normal_loss: 0.008798\n",
      "[839/00074] train_loss: 0.008880 kl_loss: 0.000000 normal_loss: 0.008880\n",
      "[841/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[842/00049] train_loss: 0.008808 kl_loss: 0.000000 normal_loss: 0.008808\n",
      "[843/00074] train_loss: 0.008853 kl_loss: 0.000000 normal_loss: 0.008853\n",
      "[845/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[846/00049] train_loss: 0.008835 kl_loss: 0.000000 normal_loss: 0.008835\n",
      "[847/00074] train_loss: 0.008820 kl_loss: 0.000000 normal_loss: 0.008820\n",
      "[849/00024] train_loss: 0.008864 kl_loss: 0.000000 normal_loss: 0.008864\n",
      "[849/00074] IOU 0.9209131120455761\n",
      "[850/00049] train_loss: 0.008826 kl_loss: 0.000000 normal_loss: 0.008826\n",
      "[851/00074] train_loss: 0.008846 kl_loss: 0.000000 normal_loss: 0.008846\n",
      "[853/00024] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[854/00049] train_loss: 0.008852 kl_loss: 0.000000 normal_loss: 0.008852\n",
      "[855/00074] train_loss: 0.008835 kl_loss: 0.000000 normal_loss: 0.008835\n",
      "[857/00024] train_loss: 0.008852 kl_loss: 0.000000 normal_loss: 0.008852\n",
      "[858/00049] train_loss: 0.008877 kl_loss: 0.000000 normal_loss: 0.008877\n",
      "[859/00074] train_loss: 0.008828 kl_loss: 0.000000 normal_loss: 0.008828\n",
      "[861/00024] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[862/00049] train_loss: 0.008820 kl_loss: 0.000000 normal_loss: 0.008820\n",
      "[863/00074] train_loss: 0.008895 kl_loss: 0.000000 normal_loss: 0.008895\n",
      "[865/00024] train_loss: 0.008860 kl_loss: 0.000000 normal_loss: 0.008860\n",
      "[866/00049] train_loss: 0.008825 kl_loss: 0.000000 normal_loss: 0.008825\n",
      "[867/00074] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[869/00024] train_loss: 0.008843 kl_loss: 0.000000 normal_loss: 0.008843\n",
      "[870/00049] train_loss: 0.008852 kl_loss: 0.000000 normal_loss: 0.008852\n",
      "[871/00074] train_loss: 0.008830 kl_loss: 0.000000 normal_loss: 0.008830\n",
      "[873/00024] train_loss: 0.008818 kl_loss: 0.000000 normal_loss: 0.008818\n",
      "[874/00049] train_loss: 0.008880 kl_loss: 0.000000 normal_loss: 0.008880\n",
      "[875/00074] train_loss: 0.008827 kl_loss: 0.000000 normal_loss: 0.008827\n",
      "[877/00024] train_loss: 0.008828 kl_loss: 0.000000 normal_loss: 0.008828\n",
      "[878/00049] train_loss: 0.008855 kl_loss: 0.000000 normal_loss: 0.008855\n",
      "[879/00074] train_loss: 0.008834 kl_loss: 0.000000 normal_loss: 0.008834\n",
      "[881/00024] train_loss: 0.008890 kl_loss: 0.000000 normal_loss: 0.008890\n",
      "[882/00049] train_loss: 0.008822 kl_loss: 0.000000 normal_loss: 0.008822\n",
      "[883/00074] train_loss: 0.008796 kl_loss: 0.000000 normal_loss: 0.008796\n",
      "[885/00024] train_loss: 0.008867 kl_loss: 0.000000 normal_loss: 0.008867\n",
      "[886/00049] train_loss: 0.008846 kl_loss: 0.000000 normal_loss: 0.008846\n",
      "[887/00074] train_loss: 0.008789 kl_loss: 0.000000 normal_loss: 0.008789\n",
      "[889/00024] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[890/00049] train_loss: 0.008831 kl_loss: 0.000000 normal_loss: 0.008831\n",
      "[891/00074] train_loss: 0.008844 kl_loss: 0.000000 normal_loss: 0.008844\n",
      "[893/00024] train_loss: 0.008832 kl_loss: 0.000000 normal_loss: 0.008832\n",
      "[894/00049] train_loss: 0.008781 kl_loss: 0.000000 normal_loss: 0.008781\n",
      "[895/00074] train_loss: 0.008888 kl_loss: 0.000000 normal_loss: 0.008888\n",
      "[897/00024] train_loss: 0.008863 kl_loss: 0.000000 normal_loss: 0.008863\n",
      "[898/00049] train_loss: 0.008773 kl_loss: 0.000000 normal_loss: 0.008773\n",
      "[899/00074] train_loss: 0.008882 kl_loss: 0.000000 normal_loss: 0.008882\n",
      "[899/00074] IOU 0.920868968218565\n",
      "[901/00024] train_loss: 0.008815 kl_loss: 0.000000 normal_loss: 0.008815\n",
      "[902/00049] train_loss: 0.008821 kl_loss: 0.000000 normal_loss: 0.008821\n",
      "[903/00074] train_loss: 0.008823 kl_loss: 0.000000 normal_loss: 0.008823\n",
      "[905/00024] train_loss: 0.008846 kl_loss: 0.000000 normal_loss: 0.008846\n",
      "[906/00049] train_loss: 0.008785 kl_loss: 0.000000 normal_loss: 0.008785\n",
      "[907/00074] train_loss: 0.008859 kl_loss: 0.000000 normal_loss: 0.008859\n",
      "[909/00024] train_loss: 0.008804 kl_loss: 0.000000 normal_loss: 0.008804\n",
      "[910/00049] train_loss: 0.008860 kl_loss: 0.000000 normal_loss: 0.008860\n",
      "[911/00074] train_loss: 0.008784 kl_loss: 0.000000 normal_loss: 0.008784\n",
      "[913/00024] train_loss: 0.008809 kl_loss: 0.000000 normal_loss: 0.008809\n",
      "[914/00049] train_loss: 0.008815 kl_loss: 0.000000 normal_loss: 0.008815\n",
      "[915/00074] train_loss: 0.008861 kl_loss: 0.000000 normal_loss: 0.008861\n",
      "[917/00024] train_loss: 0.008834 kl_loss: 0.000000 normal_loss: 0.008834\n",
      "[918/00049] train_loss: 0.008780 kl_loss: 0.000000 normal_loss: 0.008780\n",
      "[919/00074] train_loss: 0.008858 kl_loss: 0.000000 normal_loss: 0.008858\n",
      "[921/00024] train_loss: 0.008825 kl_loss: 0.000000 normal_loss: 0.008825\n",
      "[922/00049] train_loss: 0.008843 kl_loss: 0.000000 normal_loss: 0.008843\n",
      "[923/00074] train_loss: 0.008824 kl_loss: 0.000000 normal_loss: 0.008824\n",
      "[925/00024] train_loss: 0.008837 kl_loss: 0.000000 normal_loss: 0.008837\n",
      "[926/00049] train_loss: 0.008758 kl_loss: 0.000000 normal_loss: 0.008758\n",
      "[927/00074] train_loss: 0.008868 kl_loss: 0.000000 normal_loss: 0.008868\n",
      "[929/00024] train_loss: 0.008844 kl_loss: 0.000000 normal_loss: 0.008844\n",
      "[930/00049] train_loss: 0.008777 kl_loss: 0.000000 normal_loss: 0.008777\n",
      "[931/00074] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[933/00024] train_loss: 0.008813 kl_loss: 0.000000 normal_loss: 0.008813\n",
      "[934/00049] train_loss: 0.008866 kl_loss: 0.000000 normal_loss: 0.008866\n",
      "[935/00074] train_loss: 0.008775 kl_loss: 0.000000 normal_loss: 0.008775\n",
      "[937/00024] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[938/00049] train_loss: 0.008792 kl_loss: 0.000000 normal_loss: 0.008792\n",
      "[939/00074] train_loss: 0.008833 kl_loss: 0.000000 normal_loss: 0.008833\n",
      "[941/00024] train_loss: 0.008806 kl_loss: 0.000000 normal_loss: 0.008806\n",
      "[942/00049] train_loss: 0.008847 kl_loss: 0.000000 normal_loss: 0.008847\n",
      "[943/00074] train_loss: 0.008811 kl_loss: 0.000000 normal_loss: 0.008811\n",
      "[945/00024] train_loss: 0.008791 kl_loss: 0.000000 normal_loss: 0.008791\n",
      "[946/00049] train_loss: 0.008838 kl_loss: 0.000000 normal_loss: 0.008838\n",
      "[947/00074] train_loss: 0.008817 kl_loss: 0.000000 normal_loss: 0.008817\n",
      "[949/00024] train_loss: 0.008826 kl_loss: 0.000000 normal_loss: 0.008826\n",
      "[949/00074] IOU 0.9210172387460868\n",
      "[950/00049] train_loss: 0.008805 kl_loss: 0.000000 normal_loss: 0.008805\n",
      "[951/00074] train_loss: 0.008831 kl_loss: 0.000000 normal_loss: 0.008831\n",
      "[953/00024] train_loss: 0.008813 kl_loss: 0.000000 normal_loss: 0.008813\n",
      "[954/00049] train_loss: 0.008855 kl_loss: 0.000000 normal_loss: 0.008855\n",
      "[955/00074] train_loss: 0.008788 kl_loss: 0.000000 normal_loss: 0.008788\n",
      "[957/00024] train_loss: 0.008814 kl_loss: 0.000000 normal_loss: 0.008814\n",
      "[958/00049] train_loss: 0.008825 kl_loss: 0.000000 normal_loss: 0.008825\n",
      "[959/00074] train_loss: 0.008822 kl_loss: 0.000000 normal_loss: 0.008822\n",
      "[961/00024] train_loss: 0.008824 kl_loss: 0.000000 normal_loss: 0.008824\n",
      "[962/00049] train_loss: 0.008787 kl_loss: 0.000000 normal_loss: 0.008787\n",
      "[963/00074] train_loss: 0.008850 kl_loss: 0.000000 normal_loss: 0.008850\n",
      "[965/00024] train_loss: 0.008786 kl_loss: 0.000000 normal_loss: 0.008786\n",
      "[966/00049] train_loss: 0.008848 kl_loss: 0.000000 normal_loss: 0.008848\n",
      "[967/00074] train_loss: 0.008827 kl_loss: 0.000000 normal_loss: 0.008827\n",
      "[969/00024] train_loss: 0.008783 kl_loss: 0.000000 normal_loss: 0.008783\n",
      "[970/00049] train_loss: 0.008854 kl_loss: 0.000000 normal_loss: 0.008854\n",
      "[971/00074] train_loss: 0.008793 kl_loss: 0.000000 normal_loss: 0.008793\n",
      "[973/00024] train_loss: 0.008816 kl_loss: 0.000000 normal_loss: 0.008816\n",
      "[974/00049] train_loss: 0.008807 kl_loss: 0.000000 normal_loss: 0.008807\n",
      "[975/00074] train_loss: 0.008841 kl_loss: 0.000000 normal_loss: 0.008841\n",
      "[977/00024] train_loss: 0.008831 kl_loss: 0.000000 normal_loss: 0.008831\n",
      "[978/00049] train_loss: 0.008759 kl_loss: 0.000000 normal_loss: 0.008759\n",
      "[979/00074] train_loss: 0.008853 kl_loss: 0.000000 normal_loss: 0.008853\n",
      "[981/00024] train_loss: 0.008830 kl_loss: 0.000000 normal_loss: 0.008830\n",
      "[982/00049] train_loss: 0.008862 kl_loss: 0.000000 normal_loss: 0.008862\n",
      "[983/00074] train_loss: 0.008783 kl_loss: 0.000000 normal_loss: 0.008783\n",
      "[985/00024] train_loss: 0.008787 kl_loss: 0.000000 normal_loss: 0.008787\n",
      "[986/00049] train_loss: 0.008811 kl_loss: 0.000000 normal_loss: 0.008811\n",
      "[987/00074] train_loss: 0.008839 kl_loss: 0.000000 normal_loss: 0.008839\n",
      "[989/00024] train_loss: 0.008858 kl_loss: 0.000000 normal_loss: 0.008858\n",
      "[990/00049] train_loss: 0.008791 kl_loss: 0.000000 normal_loss: 0.008791\n",
      "[991/00074] train_loss: 0.008812 kl_loss: 0.000000 normal_loss: 0.008812\n",
      "[993/00024] train_loss: 0.008866 kl_loss: 0.000000 normal_loss: 0.008866\n",
      "[994/00049] train_loss: 0.008793 kl_loss: 0.000000 normal_loss: 0.008793\n",
      "[995/00074] train_loss: 0.008821 kl_loss: 0.000000 normal_loss: 0.008821\n",
      "[997/00024] train_loss: 0.008815 kl_loss: 0.000000 normal_loss: 0.008815\n",
      "[998/00049] train_loss: 0.008824 kl_loss: 0.000000 normal_loss: 0.008824\n",
      "[999/00074] train_loss: 0.008800 kl_loss: 0.000000 normal_loss: 0.008800\n",
      "[999/00074] IOU 0.9210172180334727\n"
     ]
    }
   ],
   "source": [
    "# CHAIR AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_ad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'iou_every_epoch': 50,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TABLE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'table_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.01,\n",
    "    'resume_ckpt': 'table_vad',\n",
    "    'filter_class': 'table',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRPLANE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'resume_ckpt': 'airplane_vad',\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 3\n",
      "[001/00024] train_loss: 0.178638 kl_loss: 0.492069 normal_loss: 0.163876\n",
      "[002/00049] train_loss: 0.133184 kl_loss: 0.440588 normal_loss: 0.119966\n",
      "[003/00074] train_loss: 0.126972 kl_loss: 0.387870 normal_loss: 0.115336\n",
      "[005/00024] train_loss: 0.122300 kl_loss: 0.344194 normal_loss: 0.111974\n",
      "[006/00049] train_loss: 0.117572 kl_loss: 0.326850 normal_loss: 0.107766\n",
      "[007/00074] train_loss: 0.114495 kl_loss: 0.316428 normal_loss: 0.105002\n",
      "[009/00024] train_loss: 0.109007 kl_loss: 0.302268 normal_loss: 0.099939\n",
      "[010/00049] train_loss: 0.107026 kl_loss: 0.290253 normal_loss: 0.098318\n",
      "[011/00074] train_loss: 0.105014 kl_loss: 0.278051 normal_loss: 0.096672\n",
      "[013/00024] train_loss: 0.103401 kl_loss: 0.266038 normal_loss: 0.095420\n",
      "[014/00049] train_loss: 0.099887 kl_loss: 0.255598 normal_loss: 0.092219\n",
      "[015/00074] train_loss: 0.098646 kl_loss: 0.247489 normal_loss: 0.091221\n",
      "[017/00024] train_loss: 0.097429 kl_loss: 0.241518 normal_loss: 0.090184\n",
      "[018/00049] train_loss: 0.094537 kl_loss: 0.240299 normal_loss: 0.087328\n",
      "[019/00074] train_loss: 0.092248 kl_loss: 0.237083 normal_loss: 0.085135\n",
      "[021/00024] train_loss: 0.090566 kl_loss: 0.233879 normal_loss: 0.083550\n",
      "[022/00049] train_loss: 0.088628 kl_loss: 0.230393 normal_loss: 0.081716\n",
      "[023/00074] train_loss: 0.087367 kl_loss: 0.229643 normal_loss: 0.080477\n",
      "[025/00024] train_loss: 0.085515 kl_loss: 0.227364 normal_loss: 0.078694\n",
      "[026/00049] train_loss: 0.084186 kl_loss: 0.225406 normal_loss: 0.077423\n",
      "[027/00074] train_loss: 0.082387 kl_loss: 0.224574 normal_loss: 0.075650\n",
      "[029/00024] train_loss: 0.080859 kl_loss: 0.223576 normal_loss: 0.074151\n",
      "[030/00049] train_loss: 0.080671 kl_loss: 0.225389 normal_loss: 0.073910\n",
      "[031/00074] train_loss: 0.078297 kl_loss: 0.224095 normal_loss: 0.071574\n",
      "[033/00024] train_loss: 0.077416 kl_loss: 0.225772 normal_loss: 0.070643\n",
      "[034/00049] train_loss: 0.076505 kl_loss: 0.226039 normal_loss: 0.069724\n",
      "[035/00074] train_loss: 0.076279 kl_loss: 0.226972 normal_loss: 0.069470\n",
      "[037/00024] train_loss: 0.073701 kl_loss: 0.227346 normal_loss: 0.066881\n",
      "[038/00049] train_loss: 0.072637 kl_loss: 0.227411 normal_loss: 0.065814\n",
      "[039/00074] train_loss: 0.071753 kl_loss: 0.228318 normal_loss: 0.064904\n",
      "[041/00024] train_loss: 0.071381 kl_loss: 0.229485 normal_loss: 0.064497\n",
      "[042/00049] train_loss: 0.070086 kl_loss: 0.228875 normal_loss: 0.063220\n",
      "[043/00074] train_loss: 0.068728 kl_loss: 0.229004 normal_loss: 0.061858\n",
      "[045/00024] train_loss: 0.068534 kl_loss: 0.230920 normal_loss: 0.061606\n",
      "[046/00049] train_loss: 0.068384 kl_loss: 0.230847 normal_loss: 0.061459\n",
      "[047/00074] train_loss: 0.066707 kl_loss: 0.232220 normal_loss: 0.059740\n",
      "[049/00024] train_loss: 0.067064 kl_loss: 0.232719 normal_loss: 0.060082\n",
      "[049/00074] MMD 0.00483490526676178\n",
      "[049/00074] TMD 0.058382462710142136\n",
      "[050/00049] train_loss: 0.066090 kl_loss: 0.233780 normal_loss: 0.059077\n",
      "[051/00074] train_loss: 0.065735 kl_loss: 0.235657 normal_loss: 0.058665\n",
      "[053/00024] train_loss: 0.064367 kl_loss: 0.235278 normal_loss: 0.057309\n",
      "[054/00049] train_loss: 0.064141 kl_loss: 0.238040 normal_loss: 0.057000\n",
      "[055/00074] train_loss: 0.064249 kl_loss: 0.238480 normal_loss: 0.057094\n",
      "[057/00024] train_loss: 0.062686 kl_loss: 0.239607 normal_loss: 0.055498\n",
      "[058/00049] train_loss: 0.062306 kl_loss: 0.240559 normal_loss: 0.055089\n",
      "[059/00074] train_loss: 0.062390 kl_loss: 0.242489 normal_loss: 0.055115\n",
      "[061/00024] train_loss: 0.061092 kl_loss: 0.244442 normal_loss: 0.053759\n",
      "[062/00049] train_loss: 0.060578 kl_loss: 0.243492 normal_loss: 0.053273\n",
      "[063/00074] train_loss: 0.060285 kl_loss: 0.247513 normal_loss: 0.052859\n",
      "[065/00024] train_loss: 0.059986 kl_loss: 0.248322 normal_loss: 0.052536\n",
      "[066/00049] train_loss: 0.059700 kl_loss: 0.249850 normal_loss: 0.052205\n",
      "[067/00074] train_loss: 0.058685 kl_loss: 0.250920 normal_loss: 0.051158\n",
      "[069/00024] train_loss: 0.058846 kl_loss: 0.252642 normal_loss: 0.051267\n",
      "[070/00049] train_loss: 0.056994 kl_loss: 0.253884 normal_loss: 0.049377\n",
      "[071/00074] train_loss: 0.058436 kl_loss: 0.255965 normal_loss: 0.050757\n",
      "[073/00024] train_loss: 0.057254 kl_loss: 0.257246 normal_loss: 0.049537\n",
      "[074/00049] train_loss: 0.057324 kl_loss: 0.258666 normal_loss: 0.049564\n",
      "[075/00074] train_loss: 0.056233 kl_loss: 0.259782 normal_loss: 0.048440\n",
      "[077/00024] train_loss: 0.055803 kl_loss: 0.261438 normal_loss: 0.047960\n",
      "[078/00049] train_loss: 0.055530 kl_loss: 0.262924 normal_loss: 0.047642\n",
      "[079/00074] train_loss: 0.055365 kl_loss: 0.263876 normal_loss: 0.047448\n",
      "[081/00024] train_loss: 0.054437 kl_loss: 0.265252 normal_loss: 0.046479\n",
      "[082/00049] train_loss: 0.053687 kl_loss: 0.267364 normal_loss: 0.045666\n",
      "[083/00074] train_loss: 0.053845 kl_loss: 0.268546 normal_loss: 0.045789\n",
      "[085/00024] train_loss: 0.053205 kl_loss: 0.268384 normal_loss: 0.045153\n",
      "[086/00049] train_loss: 0.053422 kl_loss: 0.273187 normal_loss: 0.045226\n",
      "[087/00074] train_loss: 0.052590 kl_loss: 0.270658 normal_loss: 0.044470\n",
      "[089/00024] train_loss: 0.051889 kl_loss: 0.272924 normal_loss: 0.043702\n",
      "[090/00049] train_loss: 0.051732 kl_loss: 0.274924 normal_loss: 0.043484\n",
      "[091/00074] train_loss: 0.051126 kl_loss: 0.275591 normal_loss: 0.042858\n",
      "[093/00024] train_loss: 0.050967 kl_loss: 0.276019 normal_loss: 0.042686\n",
      "[094/00049] train_loss: 0.049827 kl_loss: 0.278587 normal_loss: 0.041469\n",
      "[095/00074] train_loss: 0.050372 kl_loss: 0.277962 normal_loss: 0.042033\n",
      "[097/00024] train_loss: 0.049956 kl_loss: 0.279586 normal_loss: 0.041569\n",
      "[098/00049] train_loss: 0.049669 kl_loss: 0.281057 normal_loss: 0.041237\n",
      "[099/00000] updated kl_weight: 0.03\n",
      "[099/00001] updated kl_weight: 0.03\n",
      "[099/00002] updated kl_weight: 0.03\n",
      "[099/00003] updated kl_weight: 0.03\n",
      "[099/00004] updated kl_weight: 0.03\n",
      "[099/00005] updated kl_weight: 0.03\n",
      "[099/00006] updated kl_weight: 0.03\n",
      "[099/00007] updated kl_weight: 0.03\n",
      "[099/00008] updated kl_weight: 0.03\n",
      "[099/00009] updated kl_weight: 0.03\n",
      "[099/00010] updated kl_weight: 0.03\n",
      "[099/00011] updated kl_weight: 0.03\n",
      "[099/00012] updated kl_weight: 0.03\n",
      "[099/00013] updated kl_weight: 0.03\n",
      "[099/00014] updated kl_weight: 0.03\n",
      "[099/00015] updated kl_weight: 0.03\n",
      "[099/00016] updated kl_weight: 0.03\n",
      "[099/00017] updated kl_weight: 0.03\n",
      "[099/00018] updated kl_weight: 0.03\n",
      "[099/00019] updated kl_weight: 0.03\n",
      "[099/00020] updated kl_weight: 0.03\n",
      "[099/00021] updated kl_weight: 0.03\n",
      "[099/00022] updated kl_weight: 0.03\n",
      "[099/00023] updated kl_weight: 0.03\n",
      "[099/00024] updated kl_weight: 0.03\n",
      "[099/00025] updated kl_weight: 0.03\n",
      "[099/00026] updated kl_weight: 0.03\n",
      "[099/00027] updated kl_weight: 0.03\n",
      "[099/00028] updated kl_weight: 0.03\n",
      "[099/00029] updated kl_weight: 0.03\n",
      "[099/00030] updated kl_weight: 0.03\n",
      "[099/00031] updated kl_weight: 0.03\n",
      "[099/00032] updated kl_weight: 0.03\n",
      "[099/00033] updated kl_weight: 0.03\n",
      "[099/00034] updated kl_weight: 0.03\n",
      "[099/00035] updated kl_weight: 0.03\n",
      "[099/00036] updated kl_weight: 0.03\n",
      "[099/00037] updated kl_weight: 0.03\n",
      "[099/00038] updated kl_weight: 0.03\n",
      "[099/00039] updated kl_weight: 0.03\n",
      "[099/00040] updated kl_weight: 0.03\n",
      "[099/00041] updated kl_weight: 0.03\n",
      "[099/00042] updated kl_weight: 0.03\n",
      "[099/00043] updated kl_weight: 0.03\n",
      "[099/00044] updated kl_weight: 0.03\n",
      "[099/00045] updated kl_weight: 0.03\n",
      "[099/00046] updated kl_weight: 0.03\n",
      "[099/00047] updated kl_weight: 0.03\n",
      "[099/00048] updated kl_weight: 0.03\n",
      "[099/00049] updated kl_weight: 0.03\n",
      "[099/00050] updated kl_weight: 0.03\n",
      "[099/00051] updated kl_weight: 0.03\n",
      "[099/00052] updated kl_weight: 0.03\n",
      "[099/00053] updated kl_weight: 0.03\n",
      "[099/00054] updated kl_weight: 0.03\n",
      "[099/00055] updated kl_weight: 0.03\n",
      "[099/00056] updated kl_weight: 0.03\n",
      "[099/00057] updated kl_weight: 0.03\n",
      "[099/00058] updated kl_weight: 0.03\n",
      "[099/00059] updated kl_weight: 0.03\n",
      "[099/00060] updated kl_weight: 0.03\n",
      "[099/00061] updated kl_weight: 0.03\n",
      "[099/00062] updated kl_weight: 0.03\n",
      "[099/00063] updated kl_weight: 0.03\n",
      "[099/00064] updated kl_weight: 0.03\n",
      "[099/00065] updated kl_weight: 0.03\n",
      "[099/00066] updated kl_weight: 0.03\n",
      "[099/00067] updated kl_weight: 0.03\n",
      "[099/00068] updated kl_weight: 0.03\n",
      "[099/00069] updated kl_weight: 0.03\n",
      "[099/00070] updated kl_weight: 0.03\n",
      "[099/00071] updated kl_weight: 0.03\n",
      "[099/00072] updated kl_weight: 0.03\n",
      "[099/00073] updated kl_weight: 0.03\n",
      "[099/00074] updated kl_weight: 0.03\n",
      "[099/00074] train_loss: 0.049135 kl_loss: 0.282469 normal_loss: 0.040661\n",
      "[099/00074] MMD 0.005458594299852848\n",
      "[099/00074] TMD 0.07454124838113785\n",
      "[101/00024] train_loss: 0.045897 kl_loss: 0.283105 normal_loss: 0.037404\n",
      "[102/00049] train_loss: 0.044594 kl_loss: 0.280281 normal_loss: 0.036186\n",
      "[103/00074] train_loss: 0.044643 kl_loss: 0.278864 normal_loss: 0.036277\n",
      "[105/00024] train_loss: 0.043844 kl_loss: 0.278227 normal_loss: 0.035497\n",
      "[106/00049] train_loss: 0.043249 kl_loss: 0.275947 normal_loss: 0.034970\n",
      "[107/00074] train_loss: 0.043278 kl_loss: 0.273650 normal_loss: 0.035069\n",
      "[109/00024] train_loss: 0.042940 kl_loss: 0.273073 normal_loss: 0.034748\n",
      "[110/00049] train_loss: 0.042730 kl_loss: 0.272346 normal_loss: 0.034560\n",
      "[111/00074] train_loss: 0.042597 kl_loss: 0.271824 normal_loss: 0.034442\n",
      "[113/00024] train_loss: 0.041805 kl_loss: 0.270188 normal_loss: 0.033699\n",
      "[114/00049] train_loss: 0.041912 kl_loss: 0.270082 normal_loss: 0.033810\n",
      "[115/00074] train_loss: 0.041798 kl_loss: 0.269676 normal_loss: 0.033708\n",
      "[117/00024] train_loss: 0.041288 kl_loss: 0.268114 normal_loss: 0.033244\n",
      "[118/00049] train_loss: 0.041133 kl_loss: 0.268787 normal_loss: 0.033069\n",
      "[119/00074] train_loss: 0.040812 kl_loss: 0.267440 normal_loss: 0.032789\n",
      "[121/00024] train_loss: 0.040566 kl_loss: 0.267458 normal_loss: 0.032542\n",
      "[122/00049] train_loss: 0.040457 kl_loss: 0.266486 normal_loss: 0.032462\n",
      "[123/00074] train_loss: 0.040162 kl_loss: 0.267135 normal_loss: 0.032148\n",
      "[125/00024] train_loss: 0.039990 kl_loss: 0.265269 normal_loss: 0.032032\n",
      "[126/00049] train_loss: 0.040169 kl_loss: 0.267219 normal_loss: 0.032152\n",
      "[127/00074] train_loss: 0.039488 kl_loss: 0.266125 normal_loss: 0.031504\n",
      "[129/00024] train_loss: 0.039476 kl_loss: 0.265784 normal_loss: 0.031502\n",
      "[130/00049] train_loss: 0.039218 kl_loss: 0.265952 normal_loss: 0.031240\n",
      "[131/00074] train_loss: 0.039199 kl_loss: 0.265835 normal_loss: 0.031224\n",
      "[133/00024] train_loss: 0.038480 kl_loss: 0.265401 normal_loss: 0.030518\n",
      "[134/00049] train_loss: 0.038635 kl_loss: 0.265674 normal_loss: 0.030665\n",
      "[135/00074] train_loss: 0.038567 kl_loss: 0.265379 normal_loss: 0.030605\n",
      "[137/00024] train_loss: 0.037968 kl_loss: 0.263610 normal_loss: 0.030060\n",
      "[138/00049] train_loss: 0.038265 kl_loss: 0.266732 normal_loss: 0.030263\n",
      "[139/00074] train_loss: 0.038121 kl_loss: 0.266179 normal_loss: 0.030135\n",
      "[141/00024] train_loss: 0.037794 kl_loss: 0.265885 normal_loss: 0.029818\n",
      "[142/00049] train_loss: 0.037855 kl_loss: 0.264694 normal_loss: 0.029914\n",
      "[143/00074] train_loss: 0.037912 kl_loss: 0.266056 normal_loss: 0.029930\n",
      "[145/00024] train_loss: 0.037566 kl_loss: 0.265422 normal_loss: 0.029603\n",
      "[146/00049] train_loss: 0.037347 kl_loss: 0.265990 normal_loss: 0.029367\n",
      "[147/00074] train_loss: 0.037070 kl_loss: 0.266142 normal_loss: 0.029085\n",
      "[149/00024] train_loss: 0.037022 kl_loss: 0.264675 normal_loss: 0.029082\n",
      "[149/00074] MMD 0.005198840983211994\n",
      "[149/00074] TMD 0.06507346779108047\n",
      "[150/00049] train_loss: 0.036466 kl_loss: 0.266540 normal_loss: 0.028470\n",
      "[151/00074] train_loss: 0.036551 kl_loss: 0.266324 normal_loss: 0.028562\n",
      "[153/00024] train_loss: 0.036595 kl_loss: 0.266010 normal_loss: 0.028615\n",
      "[154/00049] train_loss: 0.035992 kl_loss: 0.264564 normal_loss: 0.028055\n",
      "[155/00074] train_loss: 0.036044 kl_loss: 0.266370 normal_loss: 0.028053\n",
      "[157/00024] train_loss: 0.035807 kl_loss: 0.265795 normal_loss: 0.027833\n",
      "[158/00049] train_loss: 0.036194 kl_loss: 0.265590 normal_loss: 0.028226\n",
      "[159/00074] train_loss: 0.035838 kl_loss: 0.265665 normal_loss: 0.027868\n",
      "[161/00024] train_loss: 0.035929 kl_loss: 0.265387 normal_loss: 0.027967\n",
      "[162/00049] train_loss: 0.035451 kl_loss: 0.266251 normal_loss: 0.027463\n",
      "[163/00074] train_loss: 0.035457 kl_loss: 0.265121 normal_loss: 0.027504\n",
      "[165/00024] train_loss: 0.035308 kl_loss: 0.265390 normal_loss: 0.027347\n",
      "[166/00049] train_loss: 0.035376 kl_loss: 0.265349 normal_loss: 0.027416\n",
      "[167/00074] train_loss: 0.034914 kl_loss: 0.265276 normal_loss: 0.026956\n",
      "[169/00024] train_loss: 0.035041 kl_loss: 0.264930 normal_loss: 0.027093\n",
      "[170/00049] train_loss: 0.034796 kl_loss: 0.266332 normal_loss: 0.026806\n",
      "[171/00074] train_loss: 0.034633 kl_loss: 0.264469 normal_loss: 0.026699\n",
      "[173/00024] train_loss: 0.034528 kl_loss: 0.265171 normal_loss: 0.026573\n",
      "[174/00049] train_loss: 0.034311 kl_loss: 0.264892 normal_loss: 0.026365\n",
      "[175/00074] train_loss: 0.034277 kl_loss: 0.264508 normal_loss: 0.026342\n",
      "[177/00024] train_loss: 0.033872 kl_loss: 0.264346 normal_loss: 0.025941\n",
      "[178/00049] train_loss: 0.034102 kl_loss: 0.264509 normal_loss: 0.026167\n",
      "[179/00074] train_loss: 0.034085 kl_loss: 0.264219 normal_loss: 0.026158\n",
      "[181/00024] train_loss: 0.033979 kl_loss: 0.263673 normal_loss: 0.026068\n",
      "[182/00049] train_loss: 0.033731 kl_loss: 0.264715 normal_loss: 0.025790\n",
      "[183/00074] train_loss: 0.033665 kl_loss: 0.263269 normal_loss: 0.025767\n",
      "[185/00024] train_loss: 0.033684 kl_loss: 0.264140 normal_loss: 0.025759\n",
      "[186/00049] train_loss: 0.033647 kl_loss: 0.263288 normal_loss: 0.025748\n",
      "[187/00074] train_loss: 0.033153 kl_loss: 0.262563 normal_loss: 0.025276\n",
      "[189/00024] train_loss: 0.033020 kl_loss: 0.263619 normal_loss: 0.025111\n",
      "[190/00049] train_loss: 0.032779 kl_loss: 0.261699 normal_loss: 0.024928\n",
      "[191/00074] train_loss: 0.033452 kl_loss: 0.263479 normal_loss: 0.025548\n",
      "[193/00024] train_loss: 0.033161 kl_loss: 0.262318 normal_loss: 0.025291\n",
      "[194/00049] train_loss: 0.032559 kl_loss: 0.262425 normal_loss: 0.024686\n",
      "[195/00074] train_loss: 0.032742 kl_loss: 0.262129 normal_loss: 0.024878\n",
      "[197/00024] train_loss: 0.032592 kl_loss: 0.261776 normal_loss: 0.024738\n",
      "[198/00049] train_loss: 0.032493 kl_loss: 0.261043 normal_loss: 0.024662\n",
      "[199/00000] updated kl_weight: 0.03\n",
      "[199/00001] updated kl_weight: 0.03\n",
      "[199/00002] updated kl_weight: 0.03\n",
      "[199/00003] updated kl_weight: 0.03\n",
      "[199/00004] updated kl_weight: 0.03\n",
      "[199/00005] updated kl_weight: 0.03\n",
      "[199/00006] updated kl_weight: 0.03\n",
      "[199/00007] updated kl_weight: 0.03\n",
      "[199/00008] updated kl_weight: 0.03\n",
      "[199/00009] updated kl_weight: 0.03\n",
      "[199/00010] updated kl_weight: 0.03\n",
      "[199/00011] updated kl_weight: 0.03\n",
      "[199/00012] updated kl_weight: 0.03\n",
      "[199/00013] updated kl_weight: 0.03\n",
      "[199/00014] updated kl_weight: 0.03\n",
      "[199/00015] updated kl_weight: 0.03\n",
      "[199/00016] updated kl_weight: 0.03\n",
      "[199/00017] updated kl_weight: 0.03\n",
      "[199/00018] updated kl_weight: 0.03\n",
      "[199/00019] updated kl_weight: 0.03\n",
      "[199/00020] updated kl_weight: 0.03\n",
      "[199/00021] updated kl_weight: 0.03\n",
      "[199/00022] updated kl_weight: 0.03\n",
      "[199/00023] updated kl_weight: 0.03\n",
      "[199/00024] updated kl_weight: 0.03\n",
      "[199/00025] updated kl_weight: 0.03\n",
      "[199/00026] updated kl_weight: 0.03\n",
      "[199/00027] updated kl_weight: 0.03\n",
      "[199/00028] updated kl_weight: 0.03\n",
      "[199/00029] updated kl_weight: 0.03\n",
      "[199/00030] updated kl_weight: 0.03\n",
      "[199/00031] updated kl_weight: 0.03\n",
      "[199/00032] updated kl_weight: 0.03\n",
      "[199/00033] updated kl_weight: 0.03\n",
      "[199/00034] updated kl_weight: 0.03\n",
      "[199/00035] updated kl_weight: 0.03\n",
      "[199/00036] updated kl_weight: 0.03\n",
      "[199/00037] updated kl_weight: 0.03\n",
      "[199/00038] updated kl_weight: 0.03\n",
      "[199/00039] updated kl_weight: 0.03\n",
      "[199/00040] updated kl_weight: 0.03\n",
      "[199/00041] updated kl_weight: 0.03\n",
      "[199/00042] updated kl_weight: 0.03\n",
      "[199/00043] updated kl_weight: 0.03\n",
      "[199/00044] updated kl_weight: 0.03\n",
      "[199/00045] updated kl_weight: 0.03\n",
      "[199/00046] updated kl_weight: 0.03\n",
      "[199/00047] updated kl_weight: 0.03\n",
      "[199/00048] updated kl_weight: 0.03\n",
      "[199/00049] updated kl_weight: 0.03\n",
      "[199/00050] updated kl_weight: 0.03\n",
      "[199/00051] updated kl_weight: 0.03\n",
      "[199/00052] updated kl_weight: 0.03\n",
      "[199/00053] updated kl_weight: 0.03\n",
      "[199/00054] updated kl_weight: 0.03\n",
      "[199/00055] updated kl_weight: 0.03\n",
      "[199/00056] updated kl_weight: 0.03\n",
      "[199/00057] updated kl_weight: 0.03\n",
      "[199/00058] updated kl_weight: 0.03\n",
      "[199/00059] updated kl_weight: 0.03\n",
      "[199/00060] updated kl_weight: 0.03\n",
      "[199/00061] updated kl_weight: 0.03\n",
      "[199/00062] updated kl_weight: 0.03\n",
      "[199/00063] updated kl_weight: 0.03\n",
      "[199/00064] updated kl_weight: 0.03\n",
      "[199/00065] updated kl_weight: 0.03\n",
      "[199/00066] updated kl_weight: 0.03\n",
      "[199/00067] updated kl_weight: 0.03\n",
      "[199/00068] updated kl_weight: 0.03\n",
      "[199/00069] updated kl_weight: 0.03\n",
      "[199/00070] updated kl_weight: 0.03\n",
      "[199/00071] updated kl_weight: 0.03\n",
      "[199/00072] updated kl_weight: 0.03\n",
      "[199/00073] updated kl_weight: 0.03\n",
      "[199/00074] updated kl_weight: 0.03\n",
      "[199/00074] train_loss: 0.032708 kl_loss: 0.261414 normal_loss: 0.024865\n",
      "[199/00074] MMD 0.005202102940529585\n",
      "[199/00074] TMD 0.05481155216693878\n",
      "[201/00024] train_loss: 0.031082 kl_loss: 0.261171 normal_loss: 0.023247\n",
      "[202/00049] train_loss: 0.030734 kl_loss: 0.259696 normal_loss: 0.022943\n",
      "[203/00074] train_loss: 0.030549 kl_loss: 0.259764 normal_loss: 0.022756\n",
      "[205/00024] train_loss: 0.030134 kl_loss: 0.257910 normal_loss: 0.022397\n",
      "[206/00049] train_loss: 0.030215 kl_loss: 0.258801 normal_loss: 0.022451\n",
      "[207/00074] train_loss: 0.030172 kl_loss: 0.258132 normal_loss: 0.022428\n",
      "[209/00024] train_loss: 0.030000 kl_loss: 0.257421 normal_loss: 0.022278\n",
      "[210/00049] train_loss: 0.029797 kl_loss: 0.255861 normal_loss: 0.022121\n",
      "[211/00074] train_loss: 0.029880 kl_loss: 0.255749 normal_loss: 0.022207\n",
      "[213/00024] train_loss: 0.029612 kl_loss: 0.254567 normal_loss: 0.021975\n",
      "[214/00049] train_loss: 0.029673 kl_loss: 0.254710 normal_loss: 0.022031\n",
      "[215/00074] train_loss: 0.029650 kl_loss: 0.254181 normal_loss: 0.022025\n",
      "[217/00024] train_loss: 0.029368 kl_loss: 0.252701 normal_loss: 0.021787\n",
      "[218/00049] train_loss: 0.029619 kl_loss: 0.253140 normal_loss: 0.022024\n",
      "[219/00074] train_loss: 0.029515 kl_loss: 0.252472 normal_loss: 0.021941\n",
      "[221/00024] train_loss: 0.029300 kl_loss: 0.251730 normal_loss: 0.021748\n",
      "[222/00049] train_loss: 0.029453 kl_loss: 0.251196 normal_loss: 0.021917\n",
      "[223/00074] train_loss: 0.029344 kl_loss: 0.250903 normal_loss: 0.021817\n",
      "[225/00024] train_loss: 0.029219 kl_loss: 0.249470 normal_loss: 0.021735\n",
      "[226/00049] train_loss: 0.029164 kl_loss: 0.250925 normal_loss: 0.021636\n",
      "[227/00074] train_loss: 0.029236 kl_loss: 0.249009 normal_loss: 0.021766\n",
      "[229/00024] train_loss: 0.029181 kl_loss: 0.249691 normal_loss: 0.021691\n",
      "[230/00049] train_loss: 0.029030 kl_loss: 0.247650 normal_loss: 0.021600\n",
      "[231/00074] train_loss: 0.028946 kl_loss: 0.248279 normal_loss: 0.021498\n",
      "[233/00024] train_loss: 0.028774 kl_loss: 0.247810 normal_loss: 0.021340\n",
      "[234/00049] train_loss: 0.028894 kl_loss: 0.247338 normal_loss: 0.021474\n",
      "[235/00074] train_loss: 0.028843 kl_loss: 0.246467 normal_loss: 0.021449\n",
      "[237/00024] train_loss: 0.028683 kl_loss: 0.246519 normal_loss: 0.021288\n",
      "[238/00049] train_loss: 0.028816 kl_loss: 0.246357 normal_loss: 0.021426\n",
      "[239/00074] train_loss: 0.028430 kl_loss: 0.245068 normal_loss: 0.021078\n",
      "[241/00024] train_loss: 0.028649 kl_loss: 0.245898 normal_loss: 0.021272\n",
      "[242/00049] train_loss: 0.028471 kl_loss: 0.244009 normal_loss: 0.021150\n",
      "[243/00074] train_loss: 0.028465 kl_loss: 0.244498 normal_loss: 0.021130\n",
      "[245/00024] train_loss: 0.028347 kl_loss: 0.243593 normal_loss: 0.021039\n",
      "[246/00049] train_loss: 0.028472 kl_loss: 0.244354 normal_loss: 0.021142\n",
      "[247/00074] train_loss: 0.028335 kl_loss: 0.243542 normal_loss: 0.021029\n",
      "[249/00024] train_loss: 0.028080 kl_loss: 0.243219 normal_loss: 0.020783\n",
      "[249/00074] MMD 0.005108760204166174\n",
      "[249/00074] TMD 0.05944669991731644\n",
      "[250/00049] train_loss: 0.028192 kl_loss: 0.242270 normal_loss: 0.020924\n",
      "[251/00074] train_loss: 0.028320 kl_loss: 0.242823 normal_loss: 0.021035\n",
      "[253/00024] train_loss: 0.028004 kl_loss: 0.241568 normal_loss: 0.020757\n",
      "[254/00049] train_loss: 0.028353 kl_loss: 0.242681 normal_loss: 0.021072\n",
      "[255/00074] train_loss: 0.028117 kl_loss: 0.241256 normal_loss: 0.020879\n",
      "[257/00024] train_loss: 0.028050 kl_loss: 0.241570 normal_loss: 0.020803\n",
      "[258/00049] train_loss: 0.028076 kl_loss: 0.240221 normal_loss: 0.020870\n",
      "[259/00074] train_loss: 0.027945 kl_loss: 0.241189 normal_loss: 0.020710\n",
      "[261/00024] train_loss: 0.027854 kl_loss: 0.240548 normal_loss: 0.020638\n",
      "[262/00049] train_loss: 0.027956 kl_loss: 0.239937 normal_loss: 0.020758\n",
      "[263/00074] train_loss: 0.027776 kl_loss: 0.239979 normal_loss: 0.020577\n",
      "[265/00024] train_loss: 0.027635 kl_loss: 0.239121 normal_loss: 0.020462\n",
      "[266/00049] train_loss: 0.027725 kl_loss: 0.239911 normal_loss: 0.020527\n",
      "[267/00074] train_loss: 0.027668 kl_loss: 0.238846 normal_loss: 0.020502\n",
      "[269/00024] train_loss: 0.027640 kl_loss: 0.238325 normal_loss: 0.020490\n",
      "[270/00049] train_loss: 0.027602 kl_loss: 0.238505 normal_loss: 0.020447\n",
      "[271/00074] train_loss: 0.027660 kl_loss: 0.238499 normal_loss: 0.020505\n",
      "[273/00024] train_loss: 0.027560 kl_loss: 0.237732 normal_loss: 0.020428\n",
      "[274/00049] train_loss: 0.027600 kl_loss: 0.237858 normal_loss: 0.020464\n",
      "[275/00074] train_loss: 0.027393 kl_loss: 0.237333 normal_loss: 0.020273\n",
      "[277/00024] train_loss: 0.027403 kl_loss: 0.237212 normal_loss: 0.020287\n",
      "[278/00049] train_loss: 0.027360 kl_loss: 0.236706 normal_loss: 0.020259\n",
      "[279/00074] train_loss: 0.027432 kl_loss: 0.236556 normal_loss: 0.020336\n",
      "[281/00024] train_loss: 0.027384 kl_loss: 0.236702 normal_loss: 0.020282\n",
      "[282/00049] train_loss: 0.027254 kl_loss: 0.235805 normal_loss: 0.020180\n",
      "[283/00074] train_loss: 0.026988 kl_loss: 0.235773 normal_loss: 0.019914\n",
      "[285/00024] train_loss: 0.026883 kl_loss: 0.234735 normal_loss: 0.019841\n",
      "[286/00049] train_loss: 0.027239 kl_loss: 0.235838 normal_loss: 0.020164\n",
      "[287/00074] train_loss: 0.027067 kl_loss: 0.235262 normal_loss: 0.020009\n",
      "[289/00024] train_loss: 0.026977 kl_loss: 0.234286 normal_loss: 0.019948\n",
      "[290/00049] train_loss: 0.027222 kl_loss: 0.235221 normal_loss: 0.020166\n",
      "[291/00074] train_loss: 0.026977 kl_loss: 0.234017 normal_loss: 0.019956\n",
      "[293/00024] train_loss: 0.026720 kl_loss: 0.233526 normal_loss: 0.019715\n",
      "[294/00049] train_loss: 0.026898 kl_loss: 0.234191 normal_loss: 0.019872\n",
      "[295/00074] train_loss: 0.026777 kl_loss: 0.233500 normal_loss: 0.019772\n",
      "[297/00024] train_loss: 0.026791 kl_loss: 0.233492 normal_loss: 0.019786\n",
      "[298/00049] train_loss: 0.026856 kl_loss: 0.232661 normal_loss: 0.019876\n",
      "[299/00000] updated kl_weight: 0.03\n",
      "[299/00001] updated kl_weight: 0.03\n",
      "[299/00002] updated kl_weight: 0.03\n",
      "[299/00003] updated kl_weight: 0.03\n",
      "[299/00004] updated kl_weight: 0.03\n",
      "[299/00005] updated kl_weight: 0.03\n",
      "[299/00006] updated kl_weight: 0.03\n",
      "[299/00007] updated kl_weight: 0.03\n",
      "[299/00008] updated kl_weight: 0.03\n",
      "[299/00009] updated kl_weight: 0.03\n",
      "[299/00010] updated kl_weight: 0.03\n",
      "[299/00011] updated kl_weight: 0.03\n",
      "[299/00012] updated kl_weight: 0.03\n",
      "[299/00013] updated kl_weight: 0.03\n",
      "[299/00014] updated kl_weight: 0.03\n",
      "[299/00015] updated kl_weight: 0.03\n",
      "[299/00016] updated kl_weight: 0.03\n",
      "[299/00017] updated kl_weight: 0.03\n",
      "[299/00018] updated kl_weight: 0.03\n",
      "[299/00019] updated kl_weight: 0.03\n",
      "[299/00020] updated kl_weight: 0.03\n",
      "[299/00021] updated kl_weight: 0.03\n",
      "[299/00022] updated kl_weight: 0.03\n",
      "[299/00023] updated kl_weight: 0.03\n",
      "[299/00024] updated kl_weight: 0.03\n",
      "[299/00025] updated kl_weight: 0.03\n",
      "[299/00026] updated kl_weight: 0.03\n",
      "[299/00027] updated kl_weight: 0.03\n",
      "[299/00028] updated kl_weight: 0.03\n",
      "[299/00029] updated kl_weight: 0.03\n",
      "[299/00030] updated kl_weight: 0.03\n",
      "[299/00031] updated kl_weight: 0.03\n",
      "[299/00032] updated kl_weight: 0.03\n",
      "[299/00033] updated kl_weight: 0.03\n",
      "[299/00034] updated kl_weight: 0.03\n",
      "[299/00035] updated kl_weight: 0.03\n",
      "[299/00036] updated kl_weight: 0.03\n",
      "[299/00037] updated kl_weight: 0.03\n",
      "[299/00038] updated kl_weight: 0.03\n",
      "[299/00039] updated kl_weight: 0.03\n",
      "[299/00040] updated kl_weight: 0.03\n",
      "[299/00041] updated kl_weight: 0.03\n",
      "[299/00042] updated kl_weight: 0.03\n",
      "[299/00043] updated kl_weight: 0.03\n",
      "[299/00044] updated kl_weight: 0.03\n",
      "[299/00045] updated kl_weight: 0.03\n",
      "[299/00046] updated kl_weight: 0.03\n",
      "[299/00047] updated kl_weight: 0.03\n",
      "[299/00048] updated kl_weight: 0.03\n",
      "[299/00049] updated kl_weight: 0.03\n",
      "[299/00050] updated kl_weight: 0.03\n",
      "[299/00051] updated kl_weight: 0.03\n",
      "[299/00052] updated kl_weight: 0.03\n",
      "[299/00053] updated kl_weight: 0.03\n",
      "[299/00054] updated kl_weight: 0.03\n",
      "[299/00055] updated kl_weight: 0.03\n",
      "[299/00056] updated kl_weight: 0.03\n",
      "[299/00057] updated kl_weight: 0.03\n",
      "[299/00058] updated kl_weight: 0.03\n",
      "[299/00059] updated kl_weight: 0.03\n",
      "[299/00060] updated kl_weight: 0.03\n",
      "[299/00061] updated kl_weight: 0.03\n",
      "[299/00062] updated kl_weight: 0.03\n",
      "[299/00063] updated kl_weight: 0.03\n",
      "[299/00064] updated kl_weight: 0.03\n",
      "[299/00065] updated kl_weight: 0.03\n",
      "[299/00066] updated kl_weight: 0.03\n",
      "[299/00067] updated kl_weight: 0.03\n",
      "[299/00068] updated kl_weight: 0.03\n",
      "[299/00069] updated kl_weight: 0.03\n",
      "[299/00070] updated kl_weight: 0.03\n",
      "[299/00071] updated kl_weight: 0.03\n",
      "[299/00072] updated kl_weight: 0.03\n",
      "[299/00073] updated kl_weight: 0.03\n",
      "[299/00074] updated kl_weight: 0.03\n",
      "[299/00074] train_loss: 0.026658 kl_loss: 0.232660 normal_loss: 0.019678\n",
      "[299/00074] MMD 0.005370283965021372\n",
      "[299/00074] TMD 0.05886409431695938\n",
      "[301/00024] train_loss: 0.026104 kl_loss: 0.232139 normal_loss: 0.019140\n",
      "[302/00049] train_loss: 0.026049 kl_loss: 0.232662 normal_loss: 0.019069\n",
      "[303/00074] train_loss: 0.025857 kl_loss: 0.231675 normal_loss: 0.018906\n",
      "[305/00024] train_loss: 0.025756 kl_loss: 0.231592 normal_loss: 0.018808\n",
      "[306/00049] train_loss: 0.025864 kl_loss: 0.231065 normal_loss: 0.018932\n",
      "[307/00074] train_loss: 0.025805 kl_loss: 0.231397 normal_loss: 0.018864\n",
      "[309/00024] train_loss: 0.025632 kl_loss: 0.230762 normal_loss: 0.018709\n",
      "[310/00049] train_loss: 0.025700 kl_loss: 0.230884 normal_loss: 0.018773\n",
      "[311/00074] train_loss: 0.025656 kl_loss: 0.230178 normal_loss: 0.018751\n",
      "[313/00024] train_loss: 0.025582 kl_loss: 0.229850 normal_loss: 0.018686\n",
      "[314/00049] train_loss: 0.025607 kl_loss: 0.230137 normal_loss: 0.018702\n",
      "[315/00074] train_loss: 0.025604 kl_loss: 0.229590 normal_loss: 0.018716\n",
      "[317/00024] train_loss: 0.025460 kl_loss: 0.229220 normal_loss: 0.018584\n",
      "[318/00049] train_loss: 0.025527 kl_loss: 0.229412 normal_loss: 0.018645\n",
      "[319/00074] train_loss: 0.025453 kl_loss: 0.228766 normal_loss: 0.018590\n",
      "[321/00024] train_loss: 0.025341 kl_loss: 0.227929 normal_loss: 0.018503\n",
      "[322/00049] train_loss: 0.025472 kl_loss: 0.229070 normal_loss: 0.018600\n",
      "[323/00074] train_loss: 0.025384 kl_loss: 0.228138 normal_loss: 0.018539\n",
      "[325/00024] train_loss: 0.025283 kl_loss: 0.227579 normal_loss: 0.018456\n",
      "[326/00049] train_loss: 0.025400 kl_loss: 0.227878 normal_loss: 0.018564\n",
      "[327/00074] train_loss: 0.025249 kl_loss: 0.227310 normal_loss: 0.018429\n",
      "[329/00024] train_loss: 0.025218 kl_loss: 0.227143 normal_loss: 0.018403\n",
      "[330/00049] train_loss: 0.025257 kl_loss: 0.227026 normal_loss: 0.018446\n",
      "[331/00074] train_loss: 0.025295 kl_loss: 0.226313 normal_loss: 0.018506\n",
      "[333/00024] train_loss: 0.025277 kl_loss: 0.226529 normal_loss: 0.018481\n",
      "[334/00049] train_loss: 0.025153 kl_loss: 0.225826 normal_loss: 0.018379\n",
      "[335/00074] train_loss: 0.025143 kl_loss: 0.226152 normal_loss: 0.018359\n",
      "[337/00024] train_loss: 0.025159 kl_loss: 0.225821 normal_loss: 0.018384\n",
      "[338/00049] train_loss: 0.025207 kl_loss: 0.225659 normal_loss: 0.018438\n",
      "[339/00074] train_loss: 0.025123 kl_loss: 0.224956 normal_loss: 0.018375\n",
      "[341/00024] train_loss: 0.025064 kl_loss: 0.224799 normal_loss: 0.018320\n",
      "[342/00049] train_loss: 0.025139 kl_loss: 0.225702 normal_loss: 0.018368\n",
      "[343/00074] train_loss: 0.024966 kl_loss: 0.224101 normal_loss: 0.018243\n",
      "[345/00024] train_loss: 0.024880 kl_loss: 0.224277 normal_loss: 0.018151\n",
      "[346/00049] train_loss: 0.025045 kl_loss: 0.224640 normal_loss: 0.018306\n",
      "[347/00074] train_loss: 0.024892 kl_loss: 0.223764 normal_loss: 0.018179\n",
      "[349/00024] train_loss: 0.024955 kl_loss: 0.223719 normal_loss: 0.018244\n",
      "[349/00074] MMD 0.004884112626314163\n",
      "[349/00074] TMD 0.0640522688627243\n",
      "[350/00049] train_loss: 0.024894 kl_loss: 0.223650 normal_loss: 0.018184\n",
      "[351/00074] train_loss: 0.024909 kl_loss: 0.223482 normal_loss: 0.018205\n",
      "[353/00024] train_loss: 0.024888 kl_loss: 0.223474 normal_loss: 0.018184\n",
      "[354/00049] train_loss: 0.024730 kl_loss: 0.222558 normal_loss: 0.018053\n",
      "[355/00074] train_loss: 0.024869 kl_loss: 0.222944 normal_loss: 0.018181\n",
      "[357/00024] train_loss: 0.024795 kl_loss: 0.222451 normal_loss: 0.018121\n",
      "[358/00049] train_loss: 0.024850 kl_loss: 0.222371 normal_loss: 0.018179\n",
      "[359/00074] train_loss: 0.024881 kl_loss: 0.222427 normal_loss: 0.018208\n",
      "[361/00024] train_loss: 0.024795 kl_loss: 0.222391 normal_loss: 0.018123\n",
      "[362/00049] train_loss: 0.024721 kl_loss: 0.222334 normal_loss: 0.018051\n",
      "[363/00074] train_loss: 0.024555 kl_loss: 0.220793 normal_loss: 0.017932\n",
      "[365/00024] train_loss: 0.024670 kl_loss: 0.221668 normal_loss: 0.018020\n",
      "[366/00049] train_loss: 0.024615 kl_loss: 0.220942 normal_loss: 0.017987\n",
      "[367/00074] train_loss: 0.024638 kl_loss: 0.221064 normal_loss: 0.018006\n",
      "[369/00024] train_loss: 0.024634 kl_loss: 0.220812 normal_loss: 0.018009\n",
      "[370/00049] train_loss: 0.024548 kl_loss: 0.220429 normal_loss: 0.017935\n",
      "[371/00074] train_loss: 0.024547 kl_loss: 0.220769 normal_loss: 0.017924\n",
      "[373/00024] train_loss: 0.024592 kl_loss: 0.220504 normal_loss: 0.017977\n",
      "[374/00049] train_loss: 0.024534 kl_loss: 0.219897 normal_loss: 0.017937\n",
      "[375/00074] train_loss: 0.024524 kl_loss: 0.219849 normal_loss: 0.017929\n",
      "[377/00024] train_loss: 0.024513 kl_loss: 0.219344 normal_loss: 0.017933\n",
      "[378/00049] train_loss: 0.024515 kl_loss: 0.220105 normal_loss: 0.017912\n",
      "[379/00074] train_loss: 0.024400 kl_loss: 0.219199 normal_loss: 0.017824\n",
      "[381/00024] train_loss: 0.024417 kl_loss: 0.219015 normal_loss: 0.017847\n",
      "[382/00049] train_loss: 0.024427 kl_loss: 0.219013 normal_loss: 0.017856\n",
      "[383/00074] train_loss: 0.024390 kl_loss: 0.219008 normal_loss: 0.017820\n",
      "[385/00024] train_loss: 0.024346 kl_loss: 0.218221 normal_loss: 0.017799\n",
      "[386/00049] train_loss: 0.024354 kl_loss: 0.219091 normal_loss: 0.017781\n",
      "[387/00074] train_loss: 0.024382 kl_loss: 0.218194 normal_loss: 0.017836\n",
      "[389/00024] train_loss: 0.024300 kl_loss: 0.217866 normal_loss: 0.017764\n",
      "[390/00049] train_loss: 0.024410 kl_loss: 0.218435 normal_loss: 0.017857\n",
      "[391/00074] train_loss: 0.024219 kl_loss: 0.217742 normal_loss: 0.017687\n",
      "[393/00024] train_loss: 0.024268 kl_loss: 0.217946 normal_loss: 0.017730\n",
      "[394/00049] train_loss: 0.024216 kl_loss: 0.217171 normal_loss: 0.017701\n",
      "[395/00074] train_loss: 0.024148 kl_loss: 0.217513 normal_loss: 0.017623\n",
      "[397/00024] train_loss: 0.024178 kl_loss: 0.216754 normal_loss: 0.017676\n",
      "[398/00049] train_loss: 0.024324 kl_loss: 0.217747 normal_loss: 0.017791\n",
      "[399/00000] updated kl_weight: 0.03\n",
      "[399/00001] updated kl_weight: 0.03\n",
      "[399/00002] updated kl_weight: 0.03\n",
      "[399/00003] updated kl_weight: 0.03\n",
      "[399/00004] updated kl_weight: 0.03\n",
      "[399/00005] updated kl_weight: 0.03\n",
      "[399/00006] updated kl_weight: 0.03\n",
      "[399/00007] updated kl_weight: 0.03\n",
      "[399/00008] updated kl_weight: 0.03\n",
      "[399/00009] updated kl_weight: 0.03\n",
      "[399/00010] updated kl_weight: 0.03\n",
      "[399/00011] updated kl_weight: 0.03\n",
      "[399/00012] updated kl_weight: 0.03\n",
      "[399/00013] updated kl_weight: 0.03\n",
      "[399/00014] updated kl_weight: 0.03\n",
      "[399/00015] updated kl_weight: 0.03\n",
      "[399/00016] updated kl_weight: 0.03\n",
      "[399/00017] updated kl_weight: 0.03\n",
      "[399/00018] updated kl_weight: 0.03\n",
      "[399/00019] updated kl_weight: 0.03\n",
      "[399/00020] updated kl_weight: 0.03\n",
      "[399/00021] updated kl_weight: 0.03\n",
      "[399/00022] updated kl_weight: 0.03\n",
      "[399/00023] updated kl_weight: 0.03\n",
      "[399/00024] updated kl_weight: 0.03\n",
      "[399/00025] updated kl_weight: 0.03\n",
      "[399/00026] updated kl_weight: 0.03\n",
      "[399/00027] updated kl_weight: 0.03\n",
      "[399/00028] updated kl_weight: 0.03\n",
      "[399/00029] updated kl_weight: 0.03\n",
      "[399/00030] updated kl_weight: 0.03\n",
      "[399/00031] updated kl_weight: 0.03\n",
      "[399/00032] updated kl_weight: 0.03\n",
      "[399/00033] updated kl_weight: 0.03\n",
      "[399/00034] updated kl_weight: 0.03\n",
      "[399/00035] updated kl_weight: 0.03\n",
      "[399/00036] updated kl_weight: 0.03\n",
      "[399/00037] updated kl_weight: 0.03\n",
      "[399/00038] updated kl_weight: 0.03\n",
      "[399/00039] updated kl_weight: 0.03\n",
      "[399/00040] updated kl_weight: 0.03\n",
      "[399/00041] updated kl_weight: 0.03\n",
      "[399/00042] updated kl_weight: 0.03\n",
      "[399/00043] updated kl_weight: 0.03\n",
      "[399/00044] updated kl_weight: 0.03\n",
      "[399/00045] updated kl_weight: 0.03\n",
      "[399/00046] updated kl_weight: 0.03\n",
      "[399/00047] updated kl_weight: 0.03\n",
      "[399/00048] updated kl_weight: 0.03\n",
      "[399/00049] updated kl_weight: 0.03\n",
      "[399/00050] updated kl_weight: 0.03\n",
      "[399/00051] updated kl_weight: 0.03\n",
      "[399/00052] updated kl_weight: 0.03\n",
      "[399/00053] updated kl_weight: 0.03\n",
      "[399/00054] updated kl_weight: 0.03\n",
      "[399/00055] updated kl_weight: 0.03\n",
      "[399/00056] updated kl_weight: 0.03\n",
      "[399/00057] updated kl_weight: 0.03\n",
      "[399/00058] updated kl_weight: 0.03\n",
      "[399/00059] updated kl_weight: 0.03\n",
      "[399/00060] updated kl_weight: 0.03\n",
      "[399/00061] updated kl_weight: 0.03\n",
      "[399/00062] updated kl_weight: 0.03\n",
      "[399/00063] updated kl_weight: 0.03\n",
      "[399/00064] updated kl_weight: 0.03\n",
      "[399/00065] updated kl_weight: 0.03\n",
      "[399/00066] updated kl_weight: 0.03\n",
      "[399/00067] updated kl_weight: 0.03\n",
      "[399/00068] updated kl_weight: 0.03\n",
      "[399/00069] updated kl_weight: 0.03\n",
      "[399/00070] updated kl_weight: 0.03\n",
      "[399/00071] updated kl_weight: 0.03\n",
      "[399/00072] updated kl_weight: 0.03\n",
      "[399/00073] updated kl_weight: 0.03\n",
      "[399/00074] updated kl_weight: 0.03\n",
      "[399/00074] train_loss: 0.024137 kl_loss: 0.216698 normal_loss: 0.017636\n",
      "[399/00074] MMD 0.004945943597704172\n",
      "[399/00074] TMD 0.05976533144712448\n",
      "[401/00024] train_loss: 0.023902 kl_loss: 0.216592 normal_loss: 0.017404\n",
      "[402/00049] train_loss: 0.023836 kl_loss: 0.216462 normal_loss: 0.017342\n",
      "[403/00074] train_loss: 0.023851 kl_loss: 0.216892 normal_loss: 0.017345\n",
      "[405/00024] train_loss: 0.023707 kl_loss: 0.216410 normal_loss: 0.017215\n",
      "[406/00049] train_loss: 0.023811 kl_loss: 0.216322 normal_loss: 0.017322\n",
      "[407/00074] train_loss: 0.023798 kl_loss: 0.216137 normal_loss: 0.017314\n",
      "[409/00024] train_loss: 0.023782 kl_loss: 0.215486 normal_loss: 0.017317\n",
      "[410/00049] train_loss: 0.023729 kl_loss: 0.216108 normal_loss: 0.017246\n",
      "[411/00074] train_loss: 0.023697 kl_loss: 0.216238 normal_loss: 0.017210\n",
      "[413/00024] train_loss: 0.023715 kl_loss: 0.215715 normal_loss: 0.017244\n",
      "[414/00049] train_loss: 0.023708 kl_loss: 0.215934 normal_loss: 0.017230\n",
      "[415/00074] train_loss: 0.023721 kl_loss: 0.215099 normal_loss: 0.017268\n",
      "[417/00024] train_loss: 0.023641 kl_loss: 0.214911 normal_loss: 0.017193\n",
      "[418/00049] train_loss: 0.023709 kl_loss: 0.215611 normal_loss: 0.017241\n",
      "[419/00074] train_loss: 0.023622 kl_loss: 0.215224 normal_loss: 0.017165\n",
      "[421/00024] train_loss: 0.023551 kl_loss: 0.214741 normal_loss: 0.017109\n",
      "[422/00049] train_loss: 0.023616 kl_loss: 0.215133 normal_loss: 0.017162\n",
      "[423/00074] train_loss: 0.023586 kl_loss: 0.214868 normal_loss: 0.017140\n",
      "[425/00024] train_loss: 0.023529 kl_loss: 0.214673 normal_loss: 0.017089\n",
      "[426/00049] train_loss: 0.023609 kl_loss: 0.214655 normal_loss: 0.017169\n",
      "[427/00074] train_loss: 0.023583 kl_loss: 0.214332 normal_loss: 0.017153\n",
      "[429/00024] train_loss: 0.023464 kl_loss: 0.213837 normal_loss: 0.017049\n",
      "[430/00049] train_loss: 0.023629 kl_loss: 0.214824 normal_loss: 0.017184\n",
      "[431/00074] train_loss: 0.023534 kl_loss: 0.214015 normal_loss: 0.017113\n",
      "[433/00024] train_loss: 0.023463 kl_loss: 0.213575 normal_loss: 0.017056\n",
      "[434/00049] train_loss: 0.023476 kl_loss: 0.214047 normal_loss: 0.017055\n",
      "[435/00074] train_loss: 0.023517 kl_loss: 0.214040 normal_loss: 0.017096\n",
      "[437/00024] train_loss: 0.023471 kl_loss: 0.213425 normal_loss: 0.017068\n",
      "[438/00049] train_loss: 0.023464 kl_loss: 0.213490 normal_loss: 0.017059\n",
      "[439/00074] train_loss: 0.023468 kl_loss: 0.213732 normal_loss: 0.017056\n",
      "[441/00024] train_loss: 0.023467 kl_loss: 0.213502 normal_loss: 0.017062\n",
      "[442/00049] train_loss: 0.023451 kl_loss: 0.213459 normal_loss: 0.017048\n",
      "[443/00074] train_loss: 0.023413 kl_loss: 0.212742 normal_loss: 0.017031\n",
      "[445/00024] train_loss: 0.023313 kl_loss: 0.212825 normal_loss: 0.016929\n",
      "[446/00049] train_loss: 0.023376 kl_loss: 0.212580 normal_loss: 0.016999\n",
      "[447/00074] train_loss: 0.023450 kl_loss: 0.213320 normal_loss: 0.017050\n",
      "[449/00024] train_loss: 0.023344 kl_loss: 0.212856 normal_loss: 0.016959\n",
      "[449/00074] MMD 0.005230878945440054\n",
      "[449/00074] TMD 0.0789671465754509\n",
      "[450/00049] train_loss: 0.023429 kl_loss: 0.212711 normal_loss: 0.017048\n",
      "[451/00074] train_loss: 0.023323 kl_loss: 0.212227 normal_loss: 0.016956\n",
      "[453/00024] train_loss: 0.023287 kl_loss: 0.212451 normal_loss: 0.016914\n",
      "[454/00049] train_loss: 0.023258 kl_loss: 0.211959 normal_loss: 0.016899\n",
      "[455/00074] train_loss: 0.023361 kl_loss: 0.212424 normal_loss: 0.016988\n",
      "[457/00024] train_loss: 0.023279 kl_loss: 0.212252 normal_loss: 0.016912\n",
      "[458/00049] train_loss: 0.023238 kl_loss: 0.211582 normal_loss: 0.016891\n",
      "[459/00074] train_loss: 0.023291 kl_loss: 0.212044 normal_loss: 0.016930\n",
      "[461/00024] train_loss: 0.023162 kl_loss: 0.211218 normal_loss: 0.016825\n",
      "[462/00049] train_loss: 0.023320 kl_loss: 0.212132 normal_loss: 0.016956\n",
      "[463/00074] train_loss: 0.023269 kl_loss: 0.211598 normal_loss: 0.016921\n",
      "[465/00024] train_loss: 0.023213 kl_loss: 0.211234 normal_loss: 0.016876\n",
      "[466/00049] train_loss: 0.023281 kl_loss: 0.211361 normal_loss: 0.016940\n",
      "[467/00074] train_loss: 0.023199 kl_loss: 0.211447 normal_loss: 0.016856\n",
      "[469/00024] train_loss: 0.023339 kl_loss: 0.211660 normal_loss: 0.016989\n",
      "[470/00049] train_loss: 0.023043 kl_loss: 0.209940 normal_loss: 0.016745\n",
      "[471/00074] train_loss: 0.023265 kl_loss: 0.211571 normal_loss: 0.016918\n",
      "[473/00024] train_loss: 0.023102 kl_loss: 0.210651 normal_loss: 0.016782\n",
      "[474/00049] train_loss: 0.023238 kl_loss: 0.210973 normal_loss: 0.016909\n",
      "[475/00074] train_loss: 0.023221 kl_loss: 0.210702 normal_loss: 0.016900\n",
      "[477/00024] train_loss: 0.023042 kl_loss: 0.209973 normal_loss: 0.016743\n",
      "[478/00049] train_loss: 0.023196 kl_loss: 0.210898 normal_loss: 0.016869\n",
      "[479/00074] train_loss: 0.023195 kl_loss: 0.210594 normal_loss: 0.016877\n",
      "[481/00024] train_loss: 0.023095 kl_loss: 0.210207 normal_loss: 0.016789\n",
      "[482/00049] train_loss: 0.023120 kl_loss: 0.210594 normal_loss: 0.016802\n",
      "[483/00074] train_loss: 0.023111 kl_loss: 0.209805 normal_loss: 0.016817\n",
      "[485/00024] train_loss: 0.023049 kl_loss: 0.210037 normal_loss: 0.016748\n",
      "[486/00049] train_loss: 0.023150 kl_loss: 0.210125 normal_loss: 0.016846\n",
      "[487/00074] train_loss: 0.023101 kl_loss: 0.209611 normal_loss: 0.016813\n",
      "[489/00024] train_loss: 0.023083 kl_loss: 0.209893 normal_loss: 0.016786\n",
      "[490/00049] train_loss: 0.022924 kl_loss: 0.208981 normal_loss: 0.016655\n",
      "[491/00074] train_loss: 0.023134 kl_loss: 0.210077 normal_loss: 0.016831\n",
      "[493/00024] train_loss: 0.023033 kl_loss: 0.209375 normal_loss: 0.016752\n",
      "[494/00049] train_loss: 0.023081 kl_loss: 0.210094 normal_loss: 0.016778\n",
      "[495/00074] train_loss: 0.022992 kl_loss: 0.208677 normal_loss: 0.016732\n",
      "[497/00024] train_loss: 0.022966 kl_loss: 0.209233 normal_loss: 0.016689\n",
      "[498/00049] train_loss: 0.022955 kl_loss: 0.209501 normal_loss: 0.016669\n",
      "[499/00000] updated kl_weight: 0.03\n",
      "[499/00001] updated kl_weight: 0.03\n",
      "[499/00002] updated kl_weight: 0.03\n",
      "[499/00003] updated kl_weight: 0.03\n",
      "[499/00004] updated kl_weight: 0.03\n",
      "[499/00005] updated kl_weight: 0.03\n",
      "[499/00006] updated kl_weight: 0.03\n",
      "[499/00007] updated kl_weight: 0.03\n",
      "[499/00008] updated kl_weight: 0.03\n",
      "[499/00009] updated kl_weight: 0.03\n",
      "[499/00010] updated kl_weight: 0.03\n",
      "[499/00011] updated kl_weight: 0.03\n",
      "[499/00012] updated kl_weight: 0.03\n",
      "[499/00013] updated kl_weight: 0.03\n",
      "[499/00014] updated kl_weight: 0.03\n",
      "[499/00015] updated kl_weight: 0.03\n",
      "[499/00016] updated kl_weight: 0.03\n",
      "[499/00017] updated kl_weight: 0.03\n",
      "[499/00018] updated kl_weight: 0.03\n",
      "[499/00019] updated kl_weight: 0.03\n",
      "[499/00020] updated kl_weight: 0.03\n",
      "[499/00021] updated kl_weight: 0.03\n",
      "[499/00022] updated kl_weight: 0.03\n",
      "[499/00023] updated kl_weight: 0.03\n",
      "[499/00024] updated kl_weight: 0.03\n",
      "[499/00025] updated kl_weight: 0.03\n",
      "[499/00026] updated kl_weight: 0.03\n",
      "[499/00027] updated kl_weight: 0.03\n",
      "[499/00028] updated kl_weight: 0.03\n",
      "[499/00029] updated kl_weight: 0.03\n",
      "[499/00030] updated kl_weight: 0.03\n",
      "[499/00031] updated kl_weight: 0.03\n",
      "[499/00032] updated kl_weight: 0.03\n",
      "[499/00033] updated kl_weight: 0.03\n",
      "[499/00034] updated kl_weight: 0.03\n",
      "[499/00035] updated kl_weight: 0.03\n",
      "[499/00036] updated kl_weight: 0.03\n",
      "[499/00037] updated kl_weight: 0.03\n",
      "[499/00038] updated kl_weight: 0.03\n",
      "[499/00039] updated kl_weight: 0.03\n",
      "[499/00040] updated kl_weight: 0.03\n",
      "[499/00041] updated kl_weight: 0.03\n",
      "[499/00042] updated kl_weight: 0.03\n",
      "[499/00043] updated kl_weight: 0.03\n",
      "[499/00044] updated kl_weight: 0.03\n",
      "[499/00045] updated kl_weight: 0.03\n",
      "[499/00046] updated kl_weight: 0.03\n",
      "[499/00047] updated kl_weight: 0.03\n",
      "[499/00048] updated kl_weight: 0.03\n",
      "[499/00049] updated kl_weight: 0.03\n",
      "[499/00050] updated kl_weight: 0.03\n",
      "[499/00051] updated kl_weight: 0.03\n",
      "[499/00052] updated kl_weight: 0.03\n",
      "[499/00053] updated kl_weight: 0.03\n",
      "[499/00054] updated kl_weight: 0.03\n",
      "[499/00055] updated kl_weight: 0.03\n",
      "[499/00056] updated kl_weight: 0.03\n",
      "[499/00057] updated kl_weight: 0.03\n",
      "[499/00058] updated kl_weight: 0.03\n",
      "[499/00059] updated kl_weight: 0.03\n",
      "[499/00060] updated kl_weight: 0.03\n",
      "[499/00061] updated kl_weight: 0.03\n",
      "[499/00062] updated kl_weight: 0.03\n",
      "[499/00063] updated kl_weight: 0.03\n",
      "[499/00064] updated kl_weight: 0.03\n",
      "[499/00065] updated kl_weight: 0.03\n",
      "[499/00066] updated kl_weight: 0.03\n",
      "[499/00067] updated kl_weight: 0.03\n",
      "[499/00068] updated kl_weight: 0.03\n",
      "[499/00069] updated kl_weight: 0.03\n",
      "[499/00070] updated kl_weight: 0.03\n",
      "[499/00071] updated kl_weight: 0.03\n",
      "[499/00072] updated kl_weight: 0.03\n",
      "[499/00073] updated kl_weight: 0.03\n",
      "[499/00074] updated kl_weight: 0.03\n",
      "[499/00074] train_loss: 0.022958 kl_loss: 0.208578 normal_loss: 0.016701\n",
      "[499/00074] MMD 0.0050292606465518475\n",
      "[499/00074] TMD 0.07114793360233307\n",
      "[501/00024] train_loss: 0.022878 kl_loss: 0.208997 normal_loss: 0.016609\n",
      "[502/00049] train_loss: 0.022843 kl_loss: 0.208846 normal_loss: 0.016578\n",
      "[503/00074] train_loss: 0.022865 kl_loss: 0.208797 normal_loss: 0.016601\n",
      "[505/00024] train_loss: 0.022783 kl_loss: 0.208710 normal_loss: 0.016522\n",
      "[506/00049] train_loss: 0.022810 kl_loss: 0.208573 normal_loss: 0.016553\n",
      "[507/00074] train_loss: 0.022861 kl_loss: 0.208903 normal_loss: 0.016594\n",
      "[509/00024] train_loss: 0.022796 kl_loss: 0.208205 normal_loss: 0.016550\n",
      "[510/00049] train_loss: 0.022792 kl_loss: 0.209191 normal_loss: 0.016516\n",
      "[511/00074] train_loss: 0.022722 kl_loss: 0.208273 normal_loss: 0.016474\n",
      "[513/00024] train_loss: 0.022747 kl_loss: 0.208093 normal_loss: 0.016505\n",
      "[514/00049] train_loss: 0.022776 kl_loss: 0.208802 normal_loss: 0.016512\n",
      "[515/00074] train_loss: 0.022746 kl_loss: 0.208249 normal_loss: 0.016499\n",
      "[517/00024] train_loss: 0.022772 kl_loss: 0.208067 normal_loss: 0.016530\n",
      "[518/00049] train_loss: 0.022737 kl_loss: 0.208503 normal_loss: 0.016482\n",
      "[519/00074] train_loss: 0.022738 kl_loss: 0.208087 normal_loss: 0.016496\n",
      "[521/00024] train_loss: 0.022760 kl_loss: 0.208318 normal_loss: 0.016511\n",
      "[522/00049] train_loss: 0.022716 kl_loss: 0.207961 normal_loss: 0.016478\n",
      "[523/00074] train_loss: 0.022711 kl_loss: 0.207868 normal_loss: 0.016475\n",
      "[525/00024] train_loss: 0.022689 kl_loss: 0.208140 normal_loss: 0.016444\n",
      "[526/00049] train_loss: 0.022796 kl_loss: 0.208333 normal_loss: 0.016546\n",
      "[527/00074] train_loss: 0.022584 kl_loss: 0.207168 normal_loss: 0.016369\n",
      "[529/00024] train_loss: 0.022774 kl_loss: 0.208106 normal_loss: 0.016530\n",
      "[530/00049] train_loss: 0.022723 kl_loss: 0.207877 normal_loss: 0.016487\n",
      "[531/00074] train_loss: 0.022579 kl_loss: 0.207171 normal_loss: 0.016364\n",
      "[533/00024] train_loss: 0.022626 kl_loss: 0.207695 normal_loss: 0.016395\n",
      "[534/00049] train_loss: 0.022691 kl_loss: 0.207591 normal_loss: 0.016464\n",
      "[535/00074] train_loss: 0.022654 kl_loss: 0.207345 normal_loss: 0.016433\n",
      "[537/00024] train_loss: 0.022689 kl_loss: 0.207674 normal_loss: 0.016459\n",
      "[538/00049] train_loss: 0.022538 kl_loss: 0.206831 normal_loss: 0.016333\n",
      "[539/00074] train_loss: 0.022739 kl_loss: 0.207643 normal_loss: 0.016509\n",
      "[541/00024] train_loss: 0.022606 kl_loss: 0.207794 normal_loss: 0.016372\n",
      "[542/00049] train_loss: 0.022584 kl_loss: 0.206753 normal_loss: 0.016382\n",
      "[543/00074] train_loss: 0.022618 kl_loss: 0.207112 normal_loss: 0.016405\n",
      "[545/00024] train_loss: 0.022642 kl_loss: 0.207414 normal_loss: 0.016420\n",
      "[546/00049] train_loss: 0.022539 kl_loss: 0.206185 normal_loss: 0.016353\n",
      "[547/00074] train_loss: 0.022662 kl_loss: 0.207572 normal_loss: 0.016435\n",
      "[549/00024] train_loss: 0.022543 kl_loss: 0.206222 normal_loss: 0.016356\n",
      "[549/00074] MMD 0.005278234835714102\n",
      "[549/00074] TMD 0.05892890691757202\n",
      "[550/00049] train_loss: 0.022586 kl_loss: 0.207289 normal_loss: 0.016367\n",
      "[551/00074] train_loss: 0.022633 kl_loss: 0.207181 normal_loss: 0.016418\n",
      "[553/00024] train_loss: 0.022584 kl_loss: 0.206867 normal_loss: 0.016378\n",
      "[554/00049] train_loss: 0.022654 kl_loss: 0.206981 normal_loss: 0.016444\n",
      "[555/00074] train_loss: 0.022558 kl_loss: 0.206352 normal_loss: 0.016367\n",
      "[557/00024] train_loss: 0.022570 kl_loss: 0.206474 normal_loss: 0.016375\n",
      "[558/00049] train_loss: 0.022582 kl_loss: 0.206672 normal_loss: 0.016382\n",
      "[559/00074] train_loss: 0.022602 kl_loss: 0.206595 normal_loss: 0.016404\n",
      "[561/00024] train_loss: 0.022595 kl_loss: 0.206681 normal_loss: 0.016394\n",
      "[562/00049] train_loss: 0.022665 kl_loss: 0.206672 normal_loss: 0.016465\n",
      "[563/00074] train_loss: 0.022501 kl_loss: 0.205961 normal_loss: 0.016322\n",
      "[565/00024] train_loss: 0.022533 kl_loss: 0.206145 normal_loss: 0.016349\n",
      "[566/00049] train_loss: 0.022537 kl_loss: 0.206442 normal_loss: 0.016344\n",
      "[567/00074] train_loss: 0.022537 kl_loss: 0.206302 normal_loss: 0.016348\n",
      "[569/00024] train_loss: 0.022479 kl_loss: 0.206188 normal_loss: 0.016294\n",
      "[570/00049] train_loss: 0.022581 kl_loss: 0.206224 normal_loss: 0.016394\n",
      "[571/00074] train_loss: 0.022507 kl_loss: 0.206008 normal_loss: 0.016327\n",
      "[573/00024] train_loss: 0.022521 kl_loss: 0.206170 normal_loss: 0.016336\n",
      "[574/00049] train_loss: 0.022464 kl_loss: 0.205632 normal_loss: 0.016295\n",
      "[575/00074] train_loss: 0.022532 kl_loss: 0.206191 normal_loss: 0.016347\n",
      "[577/00024] train_loss: 0.022415 kl_loss: 0.205356 normal_loss: 0.016255\n",
      "[578/00049] train_loss: 0.022531 kl_loss: 0.206232 normal_loss: 0.016344\n",
      "[579/00074] train_loss: 0.022499 kl_loss: 0.205944 normal_loss: 0.016320\n",
      "[581/00024] train_loss: 0.022447 kl_loss: 0.205736 normal_loss: 0.016275\n",
      "[582/00049] train_loss: 0.022483 kl_loss: 0.205964 normal_loss: 0.016304\n",
      "[583/00074] train_loss: 0.022441 kl_loss: 0.205387 normal_loss: 0.016280\n",
      "[585/00024] train_loss: 0.022499 kl_loss: 0.205628 normal_loss: 0.016331\n",
      "[586/00049] train_loss: 0.022423 kl_loss: 0.205432 normal_loss: 0.016260\n",
      "[587/00074] train_loss: 0.022453 kl_loss: 0.205577 normal_loss: 0.016286\n",
      "[589/00024] train_loss: 0.022439 kl_loss: 0.204929 normal_loss: 0.016291\n",
      "[590/00049] train_loss: 0.022554 kl_loss: 0.206632 normal_loss: 0.016355\n",
      "[591/00074] train_loss: 0.022369 kl_loss: 0.204644 normal_loss: 0.016229\n",
      "[593/00024] train_loss: 0.022354 kl_loss: 0.205346 normal_loss: 0.016194\n",
      "[594/00049] train_loss: 0.022469 kl_loss: 0.205178 normal_loss: 0.016314\n",
      "[595/00074] train_loss: 0.022364 kl_loss: 0.205248 normal_loss: 0.016207\n",
      "[597/00024] train_loss: 0.022405 kl_loss: 0.205538 normal_loss: 0.016239\n",
      "[598/00049] train_loss: 0.022395 kl_loss: 0.204641 normal_loss: 0.016255\n",
      "[599/00000] updated kl_weight: 0.03\n",
      "[599/00001] updated kl_weight: 0.03\n",
      "[599/00002] updated kl_weight: 0.03\n",
      "[599/00003] updated kl_weight: 0.03\n",
      "[599/00004] updated kl_weight: 0.03\n",
      "[599/00005] updated kl_weight: 0.03\n",
      "[599/00006] updated kl_weight: 0.03\n",
      "[599/00007] updated kl_weight: 0.03\n",
      "[599/00008] updated kl_weight: 0.03\n",
      "[599/00009] updated kl_weight: 0.03\n",
      "[599/00010] updated kl_weight: 0.03\n",
      "[599/00011] updated kl_weight: 0.03\n",
      "[599/00012] updated kl_weight: 0.03\n",
      "[599/00013] updated kl_weight: 0.03\n",
      "[599/00014] updated kl_weight: 0.03\n",
      "[599/00015] updated kl_weight: 0.03\n",
      "[599/00016] updated kl_weight: 0.03\n",
      "[599/00017] updated kl_weight: 0.03\n",
      "[599/00018] updated kl_weight: 0.03\n",
      "[599/00019] updated kl_weight: 0.03\n",
      "[599/00020] updated kl_weight: 0.03\n",
      "[599/00021] updated kl_weight: 0.03\n",
      "[599/00022] updated kl_weight: 0.03\n",
      "[599/00023] updated kl_weight: 0.03\n",
      "[599/00024] updated kl_weight: 0.03\n",
      "[599/00025] updated kl_weight: 0.03\n",
      "[599/00026] updated kl_weight: 0.03\n",
      "[599/00027] updated kl_weight: 0.03\n",
      "[599/00028] updated kl_weight: 0.03\n",
      "[599/00029] updated kl_weight: 0.03\n",
      "[599/00030] updated kl_weight: 0.03\n",
      "[599/00031] updated kl_weight: 0.03\n",
      "[599/00032] updated kl_weight: 0.03\n",
      "[599/00033] updated kl_weight: 0.03\n",
      "[599/00034] updated kl_weight: 0.03\n",
      "[599/00035] updated kl_weight: 0.03\n",
      "[599/00036] updated kl_weight: 0.03\n",
      "[599/00037] updated kl_weight: 0.03\n",
      "[599/00038] updated kl_weight: 0.03\n",
      "[599/00039] updated kl_weight: 0.03\n",
      "[599/00040] updated kl_weight: 0.03\n",
      "[599/00041] updated kl_weight: 0.03\n",
      "[599/00042] updated kl_weight: 0.03\n",
      "[599/00043] updated kl_weight: 0.03\n",
      "[599/00044] updated kl_weight: 0.03\n",
      "[599/00045] updated kl_weight: 0.03\n",
      "[599/00046] updated kl_weight: 0.03\n",
      "[599/00047] updated kl_weight: 0.03\n",
      "[599/00048] updated kl_weight: 0.03\n",
      "[599/00049] updated kl_weight: 0.03\n",
      "[599/00050] updated kl_weight: 0.03\n",
      "[599/00051] updated kl_weight: 0.03\n",
      "[599/00052] updated kl_weight: 0.03\n",
      "[599/00053] updated kl_weight: 0.03\n",
      "[599/00054] updated kl_weight: 0.03\n",
      "[599/00055] updated kl_weight: 0.03\n",
      "[599/00056] updated kl_weight: 0.03\n",
      "[599/00057] updated kl_weight: 0.03\n",
      "[599/00058] updated kl_weight: 0.03\n",
      "[599/00059] updated kl_weight: 0.03\n",
      "[599/00060] updated kl_weight: 0.03\n",
      "[599/00061] updated kl_weight: 0.03\n",
      "[599/00062] updated kl_weight: 0.03\n",
      "[599/00063] updated kl_weight: 0.03\n",
      "[599/00064] updated kl_weight: 0.03\n",
      "[599/00065] updated kl_weight: 0.03\n",
      "[599/00066] updated kl_weight: 0.03\n",
      "[599/00067] updated kl_weight: 0.03\n",
      "[599/00068] updated kl_weight: 0.03\n",
      "[599/00069] updated kl_weight: 0.03\n",
      "[599/00070] updated kl_weight: 0.03\n",
      "[599/00071] updated kl_weight: 0.03\n",
      "[599/00072] updated kl_weight: 0.03\n",
      "[599/00073] updated kl_weight: 0.03\n",
      "[599/00074] updated kl_weight: 0.03\n",
      "[599/00074] train_loss: 0.022380 kl_loss: 0.205128 normal_loss: 0.016226\n",
      "[599/00074] MMD 0.004921461921185255\n",
      "[599/00074] TMD 0.05919303745031357\n",
      "[601/00024] train_loss: 0.022314 kl_loss: 0.204713 normal_loss: 0.016173\n",
      "[602/00049] train_loss: 0.022311 kl_loss: 0.205386 normal_loss: 0.016149\n",
      "[603/00074] train_loss: 0.022363 kl_loss: 0.204854 normal_loss: 0.016217\n",
      "[605/00024] train_loss: 0.022278 kl_loss: 0.204744 normal_loss: 0.016135\n",
      "[606/00049] train_loss: 0.022347 kl_loss: 0.204964 normal_loss: 0.016198\n",
      "[607/00074] train_loss: 0.022292 kl_loss: 0.205004 normal_loss: 0.016142\n",
      "[609/00024] train_loss: 0.022383 kl_loss: 0.205440 normal_loss: 0.016219\n",
      "[610/00049] train_loss: 0.022312 kl_loss: 0.204581 normal_loss: 0.016174\n",
      "[611/00074] train_loss: 0.022222 kl_loss: 0.204445 normal_loss: 0.016089\n",
      "[613/00024] train_loss: 0.022276 kl_loss: 0.204417 normal_loss: 0.016143\n",
      "[614/00049] train_loss: 0.022393 kl_loss: 0.205491 normal_loss: 0.016228\n",
      "[615/00074] train_loss: 0.022266 kl_loss: 0.204313 normal_loss: 0.016136\n",
      "[617/00024] train_loss: 0.022246 kl_loss: 0.204516 normal_loss: 0.016111\n",
      "[618/00049] train_loss: 0.022350 kl_loss: 0.205068 normal_loss: 0.016198\n",
      "[619/00074] train_loss: 0.022280 kl_loss: 0.204408 normal_loss: 0.016148\n",
      "[621/00024] train_loss: 0.022272 kl_loss: 0.204488 normal_loss: 0.016137\n",
      "[622/00049] train_loss: 0.022230 kl_loss: 0.204338 normal_loss: 0.016100\n",
      "[623/00074] train_loss: 0.022318 kl_loss: 0.204933 normal_loss: 0.016170\n",
      "[625/00024] train_loss: 0.022207 kl_loss: 0.204479 normal_loss: 0.016072\n",
      "[626/00049] train_loss: 0.022262 kl_loss: 0.204560 normal_loss: 0.016125\n",
      "[627/00074] train_loss: 0.022265 kl_loss: 0.204478 normal_loss: 0.016131\n",
      "[629/00024] train_loss: 0.022213 kl_loss: 0.203762 normal_loss: 0.016100\n",
      "[630/00049] train_loss: 0.022336 kl_loss: 0.205459 normal_loss: 0.016172\n",
      "[631/00074] train_loss: 0.022213 kl_loss: 0.204049 normal_loss: 0.016092\n",
      "[633/00024] train_loss: 0.022266 kl_loss: 0.204416 normal_loss: 0.016133\n",
      "[634/00049] train_loss: 0.022228 kl_loss: 0.204251 normal_loss: 0.016100\n",
      "[635/00074] train_loss: 0.022205 kl_loss: 0.204374 normal_loss: 0.016074\n",
      "[637/00024] train_loss: 0.022194 kl_loss: 0.203950 normal_loss: 0.016076\n",
      "[638/00049] train_loss: 0.022244 kl_loss: 0.204767 normal_loss: 0.016101\n",
      "[639/00074] train_loss: 0.022228 kl_loss: 0.204066 normal_loss: 0.016106\n",
      "[641/00024] train_loss: 0.022181 kl_loss: 0.204110 normal_loss: 0.016057\n",
      "[642/00049] train_loss: 0.022212 kl_loss: 0.204162 normal_loss: 0.016087\n",
      "[643/00074] train_loss: 0.022199 kl_loss: 0.204268 normal_loss: 0.016071\n",
      "[645/00024] train_loss: 0.022222 kl_loss: 0.203984 normal_loss: 0.016103\n",
      "[646/00049] train_loss: 0.022185 kl_loss: 0.204066 normal_loss: 0.016063\n",
      "[647/00074] train_loss: 0.022256 kl_loss: 0.204240 normal_loss: 0.016128\n",
      "[649/00024] train_loss: 0.022205 kl_loss: 0.204296 normal_loss: 0.016077\n",
      "[649/00074] MMD 0.005192305892705917\n",
      "[649/00074] TMD 0.06799919903278351\n",
      "[650/00049] train_loss: 0.022180 kl_loss: 0.203593 normal_loss: 0.016072\n",
      "[651/00074] train_loss: 0.022237 kl_loss: 0.204157 normal_loss: 0.016113\n",
      "[653/00024] train_loss: 0.022243 kl_loss: 0.203871 normal_loss: 0.016127\n",
      "[654/00049] train_loss: 0.022104 kl_loss: 0.203224 normal_loss: 0.016007\n",
      "[655/00074] train_loss: 0.022348 kl_loss: 0.204728 normal_loss: 0.016206\n",
      "[657/00024] train_loss: 0.022193 kl_loss: 0.203762 normal_loss: 0.016080\n",
      "[658/00049] train_loss: 0.022295 kl_loss: 0.204075 normal_loss: 0.016173\n",
      "[659/00074] train_loss: 0.022138 kl_loss: 0.203763 normal_loss: 0.016025\n",
      "[661/00024] train_loss: 0.022196 kl_loss: 0.204283 normal_loss: 0.016068\n",
      "[662/00049] train_loss: 0.022143 kl_loss: 0.203469 normal_loss: 0.016039\n",
      "[663/00074] train_loss: 0.022200 kl_loss: 0.203609 normal_loss: 0.016091\n",
      "[665/00024] train_loss: 0.022189 kl_loss: 0.203729 normal_loss: 0.016077\n",
      "[666/00049] train_loss: 0.022091 kl_loss: 0.203541 normal_loss: 0.015984\n",
      "[667/00074] train_loss: 0.022223 kl_loss: 0.203861 normal_loss: 0.016107\n",
      "[669/00024] train_loss: 0.022076 kl_loss: 0.203469 normal_loss: 0.015972\n",
      "[670/00049] train_loss: 0.022268 kl_loss: 0.204141 normal_loss: 0.016144\n",
      "[671/00074] train_loss: 0.022131 kl_loss: 0.203283 normal_loss: 0.016032\n",
      "[673/00024] train_loss: 0.022160 kl_loss: 0.203601 normal_loss: 0.016052\n",
      "[674/00049] train_loss: 0.022089 kl_loss: 0.203109 normal_loss: 0.015996\n",
      "[675/00074] train_loss: 0.022189 kl_loss: 0.203944 normal_loss: 0.016071\n",
      "[677/00024] train_loss: 0.022122 kl_loss: 0.203325 normal_loss: 0.016023\n",
      "[678/00049] train_loss: 0.022180 kl_loss: 0.203514 normal_loss: 0.016074\n",
      "[679/00074] train_loss: 0.022118 kl_loss: 0.203563 normal_loss: 0.016011\n",
      "[681/00024] train_loss: 0.022178 kl_loss: 0.203405 normal_loss: 0.016076\n",
      "[682/00049] train_loss: 0.022078 kl_loss: 0.203112 normal_loss: 0.015984\n",
      "[683/00074] train_loss: 0.022219 kl_loss: 0.203660 normal_loss: 0.016109\n",
      "[685/00024] train_loss: 0.022112 kl_loss: 0.203653 normal_loss: 0.016002\n",
      "[686/00049] train_loss: 0.022047 kl_loss: 0.202901 normal_loss: 0.015960\n",
      "[687/00074] train_loss: 0.022118 kl_loss: 0.203396 normal_loss: 0.016016\n",
      "[689/00024] train_loss: 0.022083 kl_loss: 0.202816 normal_loss: 0.015999\n",
      "[690/00049] train_loss: 0.022148 kl_loss: 0.203847 normal_loss: 0.016032\n",
      "[691/00074] train_loss: 0.022152 kl_loss: 0.203056 normal_loss: 0.016060\n",
      "[693/00024] train_loss: 0.022081 kl_loss: 0.203301 normal_loss: 0.015981\n",
      "[694/00049] train_loss: 0.022200 kl_loss: 0.203393 normal_loss: 0.016099\n",
      "[695/00074] train_loss: 0.022082 kl_loss: 0.202784 normal_loss: 0.015998\n",
      "[697/00024] train_loss: 0.022099 kl_loss: 0.203148 normal_loss: 0.016004\n",
      "[698/00049] train_loss: 0.022122 kl_loss: 0.203207 normal_loss: 0.016026\n",
      "[699/00000] updated kl_weight: 0.03\n",
      "[699/00001] updated kl_weight: 0.03\n",
      "[699/00002] updated kl_weight: 0.03\n",
      "[699/00003] updated kl_weight: 0.03\n",
      "[699/00004] updated kl_weight: 0.03\n",
      "[699/00005] updated kl_weight: 0.03\n",
      "[699/00006] updated kl_weight: 0.03\n",
      "[699/00007] updated kl_weight: 0.03\n",
      "[699/00008] updated kl_weight: 0.03\n",
      "[699/00009] updated kl_weight: 0.03\n",
      "[699/00010] updated kl_weight: 0.03\n",
      "[699/00011] updated kl_weight: 0.03\n",
      "[699/00012] updated kl_weight: 0.03\n",
      "[699/00013] updated kl_weight: 0.03\n",
      "[699/00014] updated kl_weight: 0.03\n",
      "[699/00015] updated kl_weight: 0.03\n",
      "[699/00016] updated kl_weight: 0.03\n",
      "[699/00017] updated kl_weight: 0.03\n",
      "[699/00018] updated kl_weight: 0.03\n",
      "[699/00019] updated kl_weight: 0.03\n",
      "[699/00020] updated kl_weight: 0.03\n",
      "[699/00021] updated kl_weight: 0.03\n",
      "[699/00022] updated kl_weight: 0.03\n",
      "[699/00023] updated kl_weight: 0.03\n",
      "[699/00024] updated kl_weight: 0.03\n",
      "[699/00025] updated kl_weight: 0.03\n",
      "[699/00026] updated kl_weight: 0.03\n",
      "[699/00027] updated kl_weight: 0.03\n",
      "[699/00028] updated kl_weight: 0.03\n",
      "[699/00029] updated kl_weight: 0.03\n",
      "[699/00030] updated kl_weight: 0.03\n",
      "[699/00031] updated kl_weight: 0.03\n",
      "[699/00032] updated kl_weight: 0.03\n",
      "[699/00033] updated kl_weight: 0.03\n",
      "[699/00034] updated kl_weight: 0.03\n",
      "[699/00035] updated kl_weight: 0.03\n",
      "[699/00036] updated kl_weight: 0.03\n",
      "[699/00037] updated kl_weight: 0.03\n",
      "[699/00038] updated kl_weight: 0.03\n",
      "[699/00039] updated kl_weight: 0.03\n",
      "[699/00040] updated kl_weight: 0.03\n",
      "[699/00041] updated kl_weight: 0.03\n",
      "[699/00042] updated kl_weight: 0.03\n",
      "[699/00043] updated kl_weight: 0.03\n",
      "[699/00044] updated kl_weight: 0.03\n",
      "[699/00045] updated kl_weight: 0.03\n",
      "[699/00046] updated kl_weight: 0.03\n",
      "[699/00047] updated kl_weight: 0.03\n",
      "[699/00048] updated kl_weight: 0.03\n",
      "[699/00049] updated kl_weight: 0.03\n",
      "[699/00050] updated kl_weight: 0.03\n",
      "[699/00051] updated kl_weight: 0.03\n",
      "[699/00052] updated kl_weight: 0.03\n",
      "[699/00053] updated kl_weight: 0.03\n",
      "[699/00054] updated kl_weight: 0.03\n",
      "[699/00055] updated kl_weight: 0.03\n",
      "[699/00056] updated kl_weight: 0.03\n",
      "[699/00057] updated kl_weight: 0.03\n",
      "[699/00058] updated kl_weight: 0.03\n",
      "[699/00059] updated kl_weight: 0.03\n",
      "[699/00060] updated kl_weight: 0.03\n",
      "[699/00061] updated kl_weight: 0.03\n",
      "[699/00062] updated kl_weight: 0.03\n",
      "[699/00063] updated kl_weight: 0.03\n",
      "[699/00064] updated kl_weight: 0.03\n",
      "[699/00065] updated kl_weight: 0.03\n",
      "[699/00066] updated kl_weight: 0.03\n",
      "[699/00067] updated kl_weight: 0.03\n",
      "[699/00068] updated kl_weight: 0.03\n",
      "[699/00069] updated kl_weight: 0.03\n",
      "[699/00070] updated kl_weight: 0.03\n",
      "[699/00071] updated kl_weight: 0.03\n",
      "[699/00072] updated kl_weight: 0.03\n",
      "[699/00073] updated kl_weight: 0.03\n",
      "[699/00074] updated kl_weight: 0.03\n",
      "[699/00074] train_loss: 0.022057 kl_loss: 0.202884 normal_loss: 0.015971\n",
      "[699/00074] MMD 0.005001270677894354\n",
      "[699/00074] TMD 0.06325970590114594\n",
      "[701/00024] train_loss: 0.022127 kl_loss: 0.203467 normal_loss: 0.016023\n",
      "[702/00049] train_loss: 0.022055 kl_loss: 0.202846 normal_loss: 0.015969\n",
      "[703/00074] train_loss: 0.022049 kl_loss: 0.202736 normal_loss: 0.015967\n",
      "[705/00024] train_loss: 0.022067 kl_loss: 0.202852 normal_loss: 0.015982\n",
      "[706/00049] train_loss: 0.022095 kl_loss: 0.203414 normal_loss: 0.015993\n",
      "[707/00074] train_loss: 0.022004 kl_loss: 0.202667 normal_loss: 0.015924\n",
      "[709/00024] train_loss: 0.022072 kl_loss: 0.202780 normal_loss: 0.015988\n",
      "[710/00049] train_loss: 0.022031 kl_loss: 0.203042 normal_loss: 0.015940\n",
      "[711/00074] train_loss: 0.022080 kl_loss: 0.202988 normal_loss: 0.015990\n",
      "[713/00024] train_loss: 0.022138 kl_loss: 0.203468 normal_loss: 0.016034\n",
      "[714/00049] train_loss: 0.021956 kl_loss: 0.202012 normal_loss: 0.015895\n",
      "[715/00074] train_loss: 0.022131 kl_loss: 0.203216 normal_loss: 0.016035\n",
      "[717/00024] train_loss: 0.022123 kl_loss: 0.203344 normal_loss: 0.016023\n",
      "[718/00049] train_loss: 0.022004 kl_loss: 0.202620 normal_loss: 0.015925\n",
      "[719/00074] train_loss: 0.022038 kl_loss: 0.202620 normal_loss: 0.015959\n",
      "[721/00024] train_loss: 0.022101 kl_loss: 0.203147 normal_loss: 0.016007\n",
      "[722/00049] train_loss: 0.021973 kl_loss: 0.202135 normal_loss: 0.015909\n",
      "[723/00074] train_loss: 0.022141 kl_loss: 0.203182 normal_loss: 0.016046\n",
      "[725/00024] train_loss: 0.022009 kl_loss: 0.202902 normal_loss: 0.015922\n",
      "[726/00049] train_loss: 0.022068 kl_loss: 0.202685 normal_loss: 0.015988\n",
      "[727/00074] train_loss: 0.022074 kl_loss: 0.202760 normal_loss: 0.015991\n",
      "[729/00024] train_loss: 0.022001 kl_loss: 0.202411 normal_loss: 0.015929\n",
      "[730/00049] train_loss: 0.022086 kl_loss: 0.203169 normal_loss: 0.015991\n",
      "[731/00074] train_loss: 0.022086 kl_loss: 0.202648 normal_loss: 0.016006\n",
      "[733/00024] train_loss: 0.022061 kl_loss: 0.203002 normal_loss: 0.015971\n",
      "[734/00049] train_loss: 0.022074 kl_loss: 0.202743 normal_loss: 0.015992\n",
      "[735/00074] train_loss: 0.021939 kl_loss: 0.202375 normal_loss: 0.015868\n",
      "[737/00024] train_loss: 0.022015 kl_loss: 0.202471 normal_loss: 0.015941\n",
      "[738/00049] train_loss: 0.022035 kl_loss: 0.202680 normal_loss: 0.015955\n",
      "[739/00074] train_loss: 0.022021 kl_loss: 0.202854 normal_loss: 0.015936\n",
      "[741/00024] train_loss: 0.022108 kl_loss: 0.202836 normal_loss: 0.016023\n",
      "[742/00049] train_loss: 0.022020 kl_loss: 0.202810 normal_loss: 0.015935\n",
      "[743/00074] train_loss: 0.021968 kl_loss: 0.202241 normal_loss: 0.015901\n",
      "[745/00024] train_loss: 0.022050 kl_loss: 0.202787 normal_loss: 0.015967\n",
      "[746/00049] train_loss: 0.021949 kl_loss: 0.202075 normal_loss: 0.015887\n",
      "[747/00074] train_loss: 0.022064 kl_loss: 0.202908 normal_loss: 0.015977\n",
      "[749/00024] train_loss: 0.022053 kl_loss: 0.202603 normal_loss: 0.015975\n",
      "[749/00074] MMD 0.004948913585394621\n",
      "[749/00074] TMD 0.05819888412952423\n",
      "[750/00049] train_loss: 0.021989 kl_loss: 0.202695 normal_loss: 0.015908\n",
      "[751/00074] train_loss: 0.022007 kl_loss: 0.202355 normal_loss: 0.015937\n",
      "[753/00024] train_loss: 0.021970 kl_loss: 0.202091 normal_loss: 0.015907\n",
      "[754/00049] train_loss: 0.022007 kl_loss: 0.202887 normal_loss: 0.015921\n",
      "[755/00074] train_loss: 0.022061 kl_loss: 0.202551 normal_loss: 0.015984\n",
      "[757/00024] train_loss: 0.022006 kl_loss: 0.202480 normal_loss: 0.015932\n",
      "[758/00049] train_loss: 0.022066 kl_loss: 0.202428 normal_loss: 0.015993\n",
      "[759/00074] train_loss: 0.022000 kl_loss: 0.202502 normal_loss: 0.015925\n",
      "[761/00024] train_loss: 0.021999 kl_loss: 0.202739 normal_loss: 0.015916\n",
      "[762/00049] train_loss: 0.021986 kl_loss: 0.202241 normal_loss: 0.015919\n",
      "[763/00074] train_loss: 0.021991 kl_loss: 0.202313 normal_loss: 0.015921\n",
      "[765/00024] train_loss: 0.022051 kl_loss: 0.202509 normal_loss: 0.015976\n",
      "[766/00049] train_loss: 0.022025 kl_loss: 0.202517 normal_loss: 0.015950\n",
      "[767/00074] train_loss: 0.021934 kl_loss: 0.202149 normal_loss: 0.015870\n",
      "[769/00024] train_loss: 0.022048 kl_loss: 0.202349 normal_loss: 0.015977\n",
      "[770/00049] train_loss: 0.021901 kl_loss: 0.202047 normal_loss: 0.015840\n",
      "[771/00074] train_loss: 0.021988 kl_loss: 0.202668 normal_loss: 0.015908\n",
      "[773/00024] train_loss: 0.022013 kl_loss: 0.202478 normal_loss: 0.015939\n",
      "[774/00049] train_loss: 0.021952 kl_loss: 0.202321 normal_loss: 0.015882\n",
      "[775/00074] train_loss: 0.022018 kl_loss: 0.202146 normal_loss: 0.015953\n",
      "[777/00024] train_loss: 0.021945 kl_loss: 0.202060 normal_loss: 0.015883\n",
      "[778/00049] train_loss: 0.022059 kl_loss: 0.202393 normal_loss: 0.015987\n",
      "[779/00074] train_loss: 0.022021 kl_loss: 0.202381 normal_loss: 0.015949\n",
      "[781/00024] train_loss: 0.021903 kl_loss: 0.201908 normal_loss: 0.015846\n",
      "[782/00049] train_loss: 0.021987 kl_loss: 0.202345 normal_loss: 0.015916\n",
      "[783/00074] train_loss: 0.021981 kl_loss: 0.202472 normal_loss: 0.015907\n",
      "[785/00024] train_loss: 0.022022 kl_loss: 0.202538 normal_loss: 0.015946\n",
      "[786/00049] train_loss: 0.021880 kl_loss: 0.201999 normal_loss: 0.015820\n",
      "[787/00074] train_loss: 0.021948 kl_loss: 0.202071 normal_loss: 0.015886\n",
      "[789/00024] train_loss: 0.021946 kl_loss: 0.202111 normal_loss: 0.015883\n",
      "[790/00049] train_loss: 0.021921 kl_loss: 0.201821 normal_loss: 0.015866\n",
      "[791/00074] train_loss: 0.022034 kl_loss: 0.202556 normal_loss: 0.015958\n",
      "[793/00024] train_loss: 0.022012 kl_loss: 0.202303 normal_loss: 0.015943\n",
      "[794/00049] train_loss: 0.021940 kl_loss: 0.202059 normal_loss: 0.015878\n",
      "[795/00074] train_loss: 0.021914 kl_loss: 0.202014 normal_loss: 0.015853\n",
      "[797/00024] train_loss: 0.021930 kl_loss: 0.201999 normal_loss: 0.015870\n",
      "[798/00049] train_loss: 0.021970 kl_loss: 0.202173 normal_loss: 0.015904\n",
      "[799/00000] updated kl_weight: 0.03\n",
      "[799/00001] updated kl_weight: 0.03\n",
      "[799/00002] updated kl_weight: 0.03\n",
      "[799/00003] updated kl_weight: 0.03\n",
      "[799/00004] updated kl_weight: 0.03\n",
      "[799/00005] updated kl_weight: 0.03\n",
      "[799/00006] updated kl_weight: 0.03\n",
      "[799/00007] updated kl_weight: 0.03\n",
      "[799/00008] updated kl_weight: 0.03\n",
      "[799/00009] updated kl_weight: 0.03\n",
      "[799/00010] updated kl_weight: 0.03\n",
      "[799/00011] updated kl_weight: 0.03\n",
      "[799/00012] updated kl_weight: 0.03\n",
      "[799/00013] updated kl_weight: 0.03\n",
      "[799/00014] updated kl_weight: 0.03\n",
      "[799/00015] updated kl_weight: 0.03\n",
      "[799/00016] updated kl_weight: 0.03\n",
      "[799/00017] updated kl_weight: 0.03\n",
      "[799/00018] updated kl_weight: 0.03\n",
      "[799/00019] updated kl_weight: 0.03\n",
      "[799/00020] updated kl_weight: 0.03\n",
      "[799/00021] updated kl_weight: 0.03\n",
      "[799/00022] updated kl_weight: 0.03\n",
      "[799/00023] updated kl_weight: 0.03\n",
      "[799/00024] updated kl_weight: 0.03\n",
      "[799/00025] updated kl_weight: 0.03\n",
      "[799/00026] updated kl_weight: 0.03\n",
      "[799/00027] updated kl_weight: 0.03\n",
      "[799/00028] updated kl_weight: 0.03\n",
      "[799/00029] updated kl_weight: 0.03\n",
      "[799/00030] updated kl_weight: 0.03\n",
      "[799/00031] updated kl_weight: 0.03\n",
      "[799/00032] updated kl_weight: 0.03\n",
      "[799/00033] updated kl_weight: 0.03\n",
      "[799/00034] updated kl_weight: 0.03\n",
      "[799/00035] updated kl_weight: 0.03\n",
      "[799/00036] updated kl_weight: 0.03\n",
      "[799/00037] updated kl_weight: 0.03\n",
      "[799/00038] updated kl_weight: 0.03\n",
      "[799/00039] updated kl_weight: 0.03\n",
      "[799/00040] updated kl_weight: 0.03\n",
      "[799/00041] updated kl_weight: 0.03\n",
      "[799/00042] updated kl_weight: 0.03\n",
      "[799/00043] updated kl_weight: 0.03\n",
      "[799/00044] updated kl_weight: 0.03\n",
      "[799/00045] updated kl_weight: 0.03\n",
      "[799/00046] updated kl_weight: 0.03\n",
      "[799/00047] updated kl_weight: 0.03\n",
      "[799/00048] updated kl_weight: 0.03\n",
      "[799/00049] updated kl_weight: 0.03\n",
      "[799/00050] updated kl_weight: 0.03\n",
      "[799/00051] updated kl_weight: 0.03\n",
      "[799/00052] updated kl_weight: 0.03\n",
      "[799/00053] updated kl_weight: 0.03\n",
      "[799/00054] updated kl_weight: 0.03\n",
      "[799/00055] updated kl_weight: 0.03\n",
      "[799/00056] updated kl_weight: 0.03\n",
      "[799/00057] updated kl_weight: 0.03\n",
      "[799/00058] updated kl_weight: 0.03\n",
      "[799/00059] updated kl_weight: 0.03\n",
      "[799/00060] updated kl_weight: 0.03\n",
      "[799/00061] updated kl_weight: 0.03\n",
      "[799/00062] updated kl_weight: 0.03\n",
      "[799/00063] updated kl_weight: 0.03\n",
      "[799/00064] updated kl_weight: 0.03\n",
      "[799/00065] updated kl_weight: 0.03\n",
      "[799/00066] updated kl_weight: 0.03\n",
      "[799/00067] updated kl_weight: 0.03\n",
      "[799/00068] updated kl_weight: 0.03\n",
      "[799/00069] updated kl_weight: 0.03\n",
      "[799/00070] updated kl_weight: 0.03\n",
      "[799/00071] updated kl_weight: 0.03\n",
      "[799/00072] updated kl_weight: 0.03\n",
      "[799/00073] updated kl_weight: 0.03\n",
      "[799/00074] updated kl_weight: 0.03\n",
      "[799/00074] train_loss: 0.021928 kl_loss: 0.202086 normal_loss: 0.015866\n",
      "[799/00074] MMD 0.0052238330245018005\n",
      "[799/00074] TMD 0.06622792780399323\n",
      "[801/00024] train_loss: 0.021970 kl_loss: 0.202201 normal_loss: 0.015904\n",
      "[802/00049] train_loss: 0.021913 kl_loss: 0.201886 normal_loss: 0.015856\n",
      "[803/00074] train_loss: 0.021924 kl_loss: 0.202068 normal_loss: 0.015862\n",
      "[805/00024] train_loss: 0.021951 kl_loss: 0.201995 normal_loss: 0.015891\n",
      "[806/00049] train_loss: 0.021887 kl_loss: 0.201945 normal_loss: 0.015828\n",
      "[807/00074] train_loss: 0.021950 kl_loss: 0.202154 normal_loss: 0.015885\n",
      "[809/00024] train_loss: 0.021902 kl_loss: 0.201985 normal_loss: 0.015842\n",
      "[810/00049] train_loss: 0.022017 kl_loss: 0.202485 normal_loss: 0.015942\n",
      "[811/00074] train_loss: 0.021891 kl_loss: 0.201562 normal_loss: 0.015844\n",
      "[813/00024] train_loss: 0.021887 kl_loss: 0.201796 normal_loss: 0.015833\n",
      "[814/00049] train_loss: 0.022067 kl_loss: 0.202740 normal_loss: 0.015984\n",
      "[815/00074] train_loss: 0.021890 kl_loss: 0.201439 normal_loss: 0.015847\n",
      "[817/00024] train_loss: 0.021879 kl_loss: 0.202061 normal_loss: 0.015817\n",
      "[818/00049] train_loss: 0.021873 kl_loss: 0.201554 normal_loss: 0.015826\n",
      "[819/00074] train_loss: 0.021987 kl_loss: 0.202300 normal_loss: 0.015918\n",
      "[821/00024] train_loss: 0.021986 kl_loss: 0.201943 normal_loss: 0.015928\n",
      "[822/00049] train_loss: 0.021957 kl_loss: 0.202103 normal_loss: 0.015894\n",
      "[823/00074] train_loss: 0.021916 kl_loss: 0.201811 normal_loss: 0.015861\n",
      "[825/00024] train_loss: 0.021961 kl_loss: 0.202127 normal_loss: 0.015897\n",
      "[826/00049] train_loss: 0.021953 kl_loss: 0.202250 normal_loss: 0.015886\n",
      "[827/00074] train_loss: 0.021861 kl_loss: 0.201425 normal_loss: 0.015818\n",
      "[829/00024] train_loss: 0.021878 kl_loss: 0.201779 normal_loss: 0.015825\n",
      "[830/00049] train_loss: 0.022020 kl_loss: 0.202468 normal_loss: 0.015946\n",
      "[831/00074] train_loss: 0.021846 kl_loss: 0.201498 normal_loss: 0.015801\n",
      "[833/00024] train_loss: 0.021977 kl_loss: 0.202385 normal_loss: 0.015906\n",
      "[834/00049] train_loss: 0.021802 kl_loss: 0.201659 normal_loss: 0.015752\n",
      "[835/00074] train_loss: 0.021874 kl_loss: 0.201637 normal_loss: 0.015825\n",
      "[837/00024] train_loss: 0.021875 kl_loss: 0.201597 normal_loss: 0.015827\n",
      "[838/00049] train_loss: 0.021939 kl_loss: 0.202304 normal_loss: 0.015870\n",
      "[839/00074] train_loss: 0.021916 kl_loss: 0.201716 normal_loss: 0.015865\n",
      "[841/00024] train_loss: 0.021913 kl_loss: 0.201639 normal_loss: 0.015864\n",
      "[842/00049] train_loss: 0.021915 kl_loss: 0.201921 normal_loss: 0.015857\n",
      "[843/00074] train_loss: 0.021927 kl_loss: 0.201995 normal_loss: 0.015867\n",
      "[845/00024] train_loss: 0.021888 kl_loss: 0.201575 normal_loss: 0.015840\n",
      "[846/00049] train_loss: 0.021929 kl_loss: 0.202059 normal_loss: 0.015867\n",
      "[847/00074] train_loss: 0.021872 kl_loss: 0.201862 normal_loss: 0.015816\n",
      "[849/00024] train_loss: 0.021898 kl_loss: 0.202158 normal_loss: 0.015834\n",
      "[849/00074] MMD 0.0049630519933998585\n",
      "[849/00074] TMD 0.07152298837900162\n",
      "[850/00049] train_loss: 0.021815 kl_loss: 0.201062 normal_loss: 0.015783\n",
      "[851/00074] train_loss: 0.022000 kl_loss: 0.202211 normal_loss: 0.015933\n",
      "[853/00024] train_loss: 0.021930 kl_loss: 0.202043 normal_loss: 0.015869\n",
      "[854/00049] train_loss: 0.021954 kl_loss: 0.201693 normal_loss: 0.015903\n",
      "[855/00074] train_loss: 0.021883 kl_loss: 0.201637 normal_loss: 0.015834\n",
      "[857/00024] train_loss: 0.021923 kl_loss: 0.201800 normal_loss: 0.015869\n",
      "[858/00049] train_loss: 0.021937 kl_loss: 0.201836 normal_loss: 0.015882\n",
      "[859/00074] train_loss: 0.021867 kl_loss: 0.201683 normal_loss: 0.015816\n",
      "[861/00024] train_loss: 0.021853 kl_loss: 0.201496 normal_loss: 0.015809\n",
      "[862/00049] train_loss: 0.022010 kl_loss: 0.202669 normal_loss: 0.015930\n",
      "[863/00074] train_loss: 0.021797 kl_loss: 0.201096 normal_loss: 0.015764\n",
      "[865/00024] train_loss: 0.021889 kl_loss: 0.201775 normal_loss: 0.015836\n",
      "[866/00049] train_loss: 0.021965 kl_loss: 0.201991 normal_loss: 0.015905\n",
      "[867/00074] train_loss: 0.021822 kl_loss: 0.201433 normal_loss: 0.015779\n",
      "[869/00024] train_loss: 0.021860 kl_loss: 0.201323 normal_loss: 0.015821\n",
      "[870/00049] train_loss: 0.021915 kl_loss: 0.201836 normal_loss: 0.015860\n",
      "[871/00074] train_loss: 0.021927 kl_loss: 0.201980 normal_loss: 0.015867\n",
      "[873/00024] train_loss: 0.021836 kl_loss: 0.201324 normal_loss: 0.015796\n",
      "[874/00049] train_loss: 0.022019 kl_loss: 0.202694 normal_loss: 0.015938\n",
      "[875/00074] train_loss: 0.021856 kl_loss: 0.201063 normal_loss: 0.015824\n",
      "[877/00024] train_loss: 0.021907 kl_loss: 0.201991 normal_loss: 0.015847\n",
      "[878/00049] train_loss: 0.021871 kl_loss: 0.201334 normal_loss: 0.015831\n",
      "[879/00074] train_loss: 0.021895 kl_loss: 0.201696 normal_loss: 0.015844\n",
      "[881/00024] train_loss: 0.021875 kl_loss: 0.201576 normal_loss: 0.015828\n",
      "[882/00049] train_loss: 0.021969 kl_loss: 0.201948 normal_loss: 0.015910\n",
      "[883/00074] train_loss: 0.021863 kl_loss: 0.201439 normal_loss: 0.015820\n",
      "[885/00024] train_loss: 0.021900 kl_loss: 0.201873 normal_loss: 0.015843\n",
      "[886/00049] train_loss: 0.021917 kl_loss: 0.201452 normal_loss: 0.015874\n",
      "[887/00074] train_loss: 0.021844 kl_loss: 0.201578 normal_loss: 0.015796\n",
      "[889/00024] train_loss: 0.021813 kl_loss: 0.201035 normal_loss: 0.015782\n",
      "[890/00049] train_loss: 0.021958 kl_loss: 0.202160 normal_loss: 0.015893\n",
      "[891/00074] train_loss: 0.021853 kl_loss: 0.201650 normal_loss: 0.015803\n",
      "[893/00024] train_loss: 0.021932 kl_loss: 0.201776 normal_loss: 0.015878\n",
      "[894/00049] train_loss: 0.021766 kl_loss: 0.200839 normal_loss: 0.015741\n",
      "[895/00074] train_loss: 0.021994 kl_loss: 0.202170 normal_loss: 0.015929\n",
      "[897/00024] train_loss: 0.021856 kl_loss: 0.201241 normal_loss: 0.015819\n",
      "[898/00049] train_loss: 0.021918 kl_loss: 0.201887 normal_loss: 0.015862\n",
      "[899/00000] updated kl_weight: 0.03\n",
      "[899/00001] updated kl_weight: 0.03\n",
      "[899/00002] updated kl_weight: 0.03\n",
      "[899/00003] updated kl_weight: 0.03\n",
      "[899/00004] updated kl_weight: 0.03\n",
      "[899/00005] updated kl_weight: 0.03\n",
      "[899/00006] updated kl_weight: 0.03\n",
      "[899/00007] updated kl_weight: 0.03\n",
      "[899/00008] updated kl_weight: 0.03\n",
      "[899/00009] updated kl_weight: 0.03\n",
      "[899/00010] updated kl_weight: 0.03\n",
      "[899/00011] updated kl_weight: 0.03\n",
      "[899/00012] updated kl_weight: 0.03\n",
      "[899/00013] updated kl_weight: 0.03\n",
      "[899/00014] updated kl_weight: 0.03\n",
      "[899/00015] updated kl_weight: 0.03\n",
      "[899/00016] updated kl_weight: 0.03\n",
      "[899/00017] updated kl_weight: 0.03\n",
      "[899/00018] updated kl_weight: 0.03\n",
      "[899/00019] updated kl_weight: 0.03\n",
      "[899/00020] updated kl_weight: 0.03\n",
      "[899/00021] updated kl_weight: 0.03\n",
      "[899/00022] updated kl_weight: 0.03\n",
      "[899/00023] updated kl_weight: 0.03\n",
      "[899/00024] updated kl_weight: 0.03\n",
      "[899/00025] updated kl_weight: 0.03\n",
      "[899/00026] updated kl_weight: 0.03\n",
      "[899/00027] updated kl_weight: 0.03\n",
      "[899/00028] updated kl_weight: 0.03\n",
      "[899/00029] updated kl_weight: 0.03\n",
      "[899/00030] updated kl_weight: 0.03\n",
      "[899/00031] updated kl_weight: 0.03\n",
      "[899/00032] updated kl_weight: 0.03\n",
      "[899/00033] updated kl_weight: 0.03\n",
      "[899/00034] updated kl_weight: 0.03\n",
      "[899/00035] updated kl_weight: 0.03\n",
      "[899/00036] updated kl_weight: 0.03\n",
      "[899/00037] updated kl_weight: 0.03\n",
      "[899/00038] updated kl_weight: 0.03\n",
      "[899/00039] updated kl_weight: 0.03\n",
      "[899/00040] updated kl_weight: 0.03\n",
      "[899/00041] updated kl_weight: 0.03\n",
      "[899/00042] updated kl_weight: 0.03\n",
      "[899/00043] updated kl_weight: 0.03\n",
      "[899/00044] updated kl_weight: 0.03\n",
      "[899/00045] updated kl_weight: 0.03\n",
      "[899/00046] updated kl_weight: 0.03\n",
      "[899/00047] updated kl_weight: 0.03\n",
      "[899/00048] updated kl_weight: 0.03\n",
      "[899/00049] updated kl_weight: 0.03\n",
      "[899/00050] updated kl_weight: 0.03\n",
      "[899/00051] updated kl_weight: 0.03\n",
      "[899/00052] updated kl_weight: 0.03\n",
      "[899/00053] updated kl_weight: 0.03\n",
      "[899/00054] updated kl_weight: 0.03\n",
      "[899/00055] updated kl_weight: 0.03\n",
      "[899/00056] updated kl_weight: 0.03\n",
      "[899/00057] updated kl_weight: 0.03\n",
      "[899/00058] updated kl_weight: 0.03\n",
      "[899/00059] updated kl_weight: 0.03\n",
      "[899/00060] updated kl_weight: 0.03\n",
      "[899/00061] updated kl_weight: 0.03\n",
      "[899/00062] updated kl_weight: 0.03\n",
      "[899/00063] updated kl_weight: 0.03\n",
      "[899/00064] updated kl_weight: 0.03\n",
      "[899/00065] updated kl_weight: 0.03\n",
      "[899/00066] updated kl_weight: 0.03\n",
      "[899/00067] updated kl_weight: 0.03\n",
      "[899/00068] updated kl_weight: 0.03\n",
      "[899/00069] updated kl_weight: 0.03\n",
      "[899/00070] updated kl_weight: 0.03\n",
      "[899/00071] updated kl_weight: 0.03\n",
      "[899/00072] updated kl_weight: 0.03\n",
      "[899/00073] updated kl_weight: 0.03\n",
      "[899/00074] updated kl_weight: 0.03\n",
      "[899/00074] train_loss: 0.021926 kl_loss: 0.201600 normal_loss: 0.015878\n",
      "[899/00074] MMD 0.005077994894236326\n",
      "[899/00074] TMD 0.058784738183021545\n",
      "[901/00024] train_loss: 0.021838 kl_loss: 0.201432 normal_loss: 0.015795\n",
      "[902/00049] train_loss: 0.021824 kl_loss: 0.201557 normal_loss: 0.015777\n",
      "[903/00074] train_loss: 0.021897 kl_loss: 0.201690 normal_loss: 0.015846\n",
      "[905/00024] train_loss: 0.021829 kl_loss: 0.201441 normal_loss: 0.015786\n",
      "[906/00049] train_loss: 0.021872 kl_loss: 0.201493 normal_loss: 0.015827\n",
      "[907/00074] train_loss: 0.021924 kl_loss: 0.201717 normal_loss: 0.015873\n",
      "[909/00024] train_loss: 0.021845 kl_loss: 0.201446 normal_loss: 0.015802\n",
      "[910/00049] train_loss: 0.021972 kl_loss: 0.202259 normal_loss: 0.015904\n",
      "[911/00074] train_loss: 0.021798 kl_loss: 0.200916 normal_loss: 0.015771\n",
      "[913/00024] train_loss: 0.021888 kl_loss: 0.201561 normal_loss: 0.015841\n",
      "[914/00049] train_loss: 0.021899 kl_loss: 0.201723 normal_loss: 0.015848\n",
      "[915/00074] train_loss: 0.021820 kl_loss: 0.201308 normal_loss: 0.015781\n",
      "[917/00024] train_loss: 0.021876 kl_loss: 0.201381 normal_loss: 0.015834\n",
      "[918/00049] train_loss: 0.021890 kl_loss: 0.201803 normal_loss: 0.015836\n",
      "[919/00074] train_loss: 0.021899 kl_loss: 0.201378 normal_loss: 0.015858\n",
      "[921/00024] train_loss: 0.021884 kl_loss: 0.201454 normal_loss: 0.015841\n",
      "[922/00049] train_loss: 0.021881 kl_loss: 0.201483 normal_loss: 0.015837\n",
      "[923/00074] train_loss: 0.021843 kl_loss: 0.201598 normal_loss: 0.015795\n",
      "[925/00024] train_loss: 0.021866 kl_loss: 0.201726 normal_loss: 0.015814\n",
      "[926/00049] train_loss: 0.021896 kl_loss: 0.201455 normal_loss: 0.015852\n",
      "[927/00074] train_loss: 0.021817 kl_loss: 0.201324 normal_loss: 0.015777\n",
      "[929/00024] train_loss: 0.021904 kl_loss: 0.201708 normal_loss: 0.015853\n",
      "[930/00049] train_loss: 0.021829 kl_loss: 0.201659 normal_loss: 0.015779\n",
      "[931/00074] train_loss: 0.021793 kl_loss: 0.201108 normal_loss: 0.015760\n",
      "[933/00024] train_loss: 0.021869 kl_loss: 0.201418 normal_loss: 0.015826\n",
      "[934/00049] train_loss: 0.021900 kl_loss: 0.201450 normal_loss: 0.015856\n",
      "[935/00074] train_loss: 0.021886 kl_loss: 0.201576 normal_loss: 0.015839\n",
      "[937/00024] train_loss: 0.021967 kl_loss: 0.201604 normal_loss: 0.015919\n",
      "[938/00049] train_loss: 0.021762 kl_loss: 0.201173 normal_loss: 0.015727\n",
      "[939/00074] train_loss: 0.021919 kl_loss: 0.201639 normal_loss: 0.015870\n",
      "[941/00024] train_loss: 0.021897 kl_loss: 0.202152 normal_loss: 0.015832\n",
      "[942/00049] train_loss: 0.021730 kl_loss: 0.200636 normal_loss: 0.015711\n",
      "[943/00074] train_loss: 0.021901 kl_loss: 0.201597 normal_loss: 0.015853\n",
      "[945/00024] train_loss: 0.021846 kl_loss: 0.201370 normal_loss: 0.015805\n",
      "[946/00049] train_loss: 0.021841 kl_loss: 0.201624 normal_loss: 0.015792\n",
      "[947/00074] train_loss: 0.021832 kl_loss: 0.201363 normal_loss: 0.015791\n",
      "[949/00024] train_loss: 0.021842 kl_loss: 0.201443 normal_loss: 0.015798\n",
      "[949/00074] MMD 0.005082705058157444\n",
      "[949/00074] TMD 0.06526490300893784\n",
      "[950/00049] train_loss: 0.021879 kl_loss: 0.201466 normal_loss: 0.015836\n",
      "[951/00074] train_loss: 0.021870 kl_loss: 0.201417 normal_loss: 0.015828\n",
      "[953/00024] train_loss: 0.021770 kl_loss: 0.200925 normal_loss: 0.015743\n",
      "[954/00049] train_loss: 0.021942 kl_loss: 0.201919 normal_loss: 0.015884\n",
      "[955/00074] train_loss: 0.021909 kl_loss: 0.201452 normal_loss: 0.015865\n",
      "[957/00024] train_loss: 0.021840 kl_loss: 0.201206 normal_loss: 0.015804\n",
      "[958/00049] train_loss: 0.021943 kl_loss: 0.201479 normal_loss: 0.015899\n",
      "[959/00074] train_loss: 0.021869 kl_loss: 0.201582 normal_loss: 0.015821\n",
      "[961/00024] train_loss: 0.021762 kl_loss: 0.201274 normal_loss: 0.015723\n",
      "[962/00049] train_loss: 0.021996 kl_loss: 0.201971 normal_loss: 0.015937\n",
      "[963/00074] train_loss: 0.021798 kl_loss: 0.200994 normal_loss: 0.015769\n",
      "[965/00024] train_loss: 0.021895 kl_loss: 0.201924 normal_loss: 0.015838\n",
      "[966/00049] train_loss: 0.021806 kl_loss: 0.200930 normal_loss: 0.015778\n",
      "[967/00074] train_loss: 0.021864 kl_loss: 0.201355 normal_loss: 0.015823\n",
      "[969/00024] train_loss: 0.021907 kl_loss: 0.201683 normal_loss: 0.015856\n",
      "[970/00049] train_loss: 0.021799 kl_loss: 0.201086 normal_loss: 0.015767\n",
      "[971/00074] train_loss: 0.021851 kl_loss: 0.201412 normal_loss: 0.015809\n",
      "[973/00024] train_loss: 0.021854 kl_loss: 0.201270 normal_loss: 0.015816\n",
      "[974/00049] train_loss: 0.021847 kl_loss: 0.201327 normal_loss: 0.015807\n",
      "[975/00074] train_loss: 0.021862 kl_loss: 0.201554 normal_loss: 0.015816\n",
      "[977/00024] train_loss: 0.021817 kl_loss: 0.201061 normal_loss: 0.015785\n",
      "[978/00049] train_loss: 0.021881 kl_loss: 0.201692 normal_loss: 0.015830\n",
      "[979/00074] train_loss: 0.021861 kl_loss: 0.201369 normal_loss: 0.015820\n",
      "[981/00024] train_loss: 0.021816 kl_loss: 0.201142 normal_loss: 0.015782\n",
      "[982/00049] train_loss: 0.021905 kl_loss: 0.201893 normal_loss: 0.015848\n",
      "[983/00074] train_loss: 0.021840 kl_loss: 0.201058 normal_loss: 0.015808\n",
      "[985/00024] train_loss: 0.021914 kl_loss: 0.201728 normal_loss: 0.015862\n",
      "[986/00049] train_loss: 0.021847 kl_loss: 0.201192 normal_loss: 0.015811\n",
      "[987/00074] train_loss: 0.021816 kl_loss: 0.201143 normal_loss: 0.015781\n",
      "[989/00024] train_loss: 0.021841 kl_loss: 0.201233 normal_loss: 0.015804\n",
      "[990/00049] train_loss: 0.021988 kl_loss: 0.201757 normal_loss: 0.015935\n",
      "[991/00074] train_loss: 0.021793 kl_loss: 0.201045 normal_loss: 0.015762\n",
      "[993/00024] train_loss: 0.021830 kl_loss: 0.201149 normal_loss: 0.015796\n",
      "[994/00049] train_loss: 0.021897 kl_loss: 0.201895 normal_loss: 0.015840\n",
      "[995/00074] train_loss: 0.021830 kl_loss: 0.200962 normal_loss: 0.015802\n",
      "[997/00024] train_loss: 0.021845 kl_loss: 0.201386 normal_loss: 0.015804\n",
      "[998/00049] train_loss: 0.021814 kl_loss: 0.201077 normal_loss: 0.015781\n",
      "[999/00000] updated kl_weight: 0.03\n",
      "[999/00001] updated kl_weight: 0.03\n",
      "[999/00002] updated kl_weight: 0.03\n",
      "[999/00003] updated kl_weight: 0.03\n",
      "[999/00004] updated kl_weight: 0.03\n",
      "[999/00005] updated kl_weight: 0.03\n",
      "[999/00006] updated kl_weight: 0.03\n",
      "[999/00007] updated kl_weight: 0.03\n",
      "[999/00008] updated kl_weight: 0.03\n",
      "[999/00009] updated kl_weight: 0.03\n",
      "[999/00010] updated kl_weight: 0.03\n",
      "[999/00011] updated kl_weight: 0.03\n",
      "[999/00012] updated kl_weight: 0.03\n",
      "[999/00013] updated kl_weight: 0.03\n",
      "[999/00014] updated kl_weight: 0.03\n",
      "[999/00015] updated kl_weight: 0.03\n",
      "[999/00016] updated kl_weight: 0.03\n",
      "[999/00017] updated kl_weight: 0.03\n",
      "[999/00018] updated kl_weight: 0.03\n",
      "[999/00019] updated kl_weight: 0.03\n",
      "[999/00020] updated kl_weight: 0.03\n",
      "[999/00021] updated kl_weight: 0.03\n",
      "[999/00022] updated kl_weight: 0.03\n",
      "[999/00023] updated kl_weight: 0.03\n",
      "[999/00024] updated kl_weight: 0.03\n",
      "[999/00025] updated kl_weight: 0.03\n",
      "[999/00026] updated kl_weight: 0.03\n",
      "[999/00027] updated kl_weight: 0.03\n",
      "[999/00028] updated kl_weight: 0.03\n",
      "[999/00029] updated kl_weight: 0.03\n",
      "[999/00030] updated kl_weight: 0.03\n",
      "[999/00031] updated kl_weight: 0.03\n",
      "[999/00032] updated kl_weight: 0.03\n",
      "[999/00033] updated kl_weight: 0.03\n",
      "[999/00034] updated kl_weight: 0.03\n",
      "[999/00035] updated kl_weight: 0.03\n",
      "[999/00036] updated kl_weight: 0.03\n",
      "[999/00037] updated kl_weight: 0.03\n",
      "[999/00038] updated kl_weight: 0.03\n",
      "[999/00039] updated kl_weight: 0.03\n",
      "[999/00040] updated kl_weight: 0.03\n",
      "[999/00041] updated kl_weight: 0.03\n",
      "[999/00042] updated kl_weight: 0.03\n",
      "[999/00043] updated kl_weight: 0.03\n",
      "[999/00044] updated kl_weight: 0.03\n",
      "[999/00045] updated kl_weight: 0.03\n",
      "[999/00046] updated kl_weight: 0.03\n",
      "[999/00047] updated kl_weight: 0.03\n",
      "[999/00048] updated kl_weight: 0.03\n",
      "[999/00049] updated kl_weight: 0.03\n",
      "[999/00050] updated kl_weight: 0.03\n",
      "[999/00051] updated kl_weight: 0.03\n",
      "[999/00052] updated kl_weight: 0.03\n",
      "[999/00053] updated kl_weight: 0.03\n",
      "[999/00054] updated kl_weight: 0.03\n",
      "[999/00055] updated kl_weight: 0.03\n",
      "[999/00056] updated kl_weight: 0.03\n",
      "[999/00057] updated kl_weight: 0.03\n",
      "[999/00058] updated kl_weight: 0.03\n",
      "[999/00059] updated kl_weight: 0.03\n",
      "[999/00060] updated kl_weight: 0.03\n",
      "[999/00061] updated kl_weight: 0.03\n",
      "[999/00062] updated kl_weight: 0.03\n",
      "[999/00063] updated kl_weight: 0.03\n",
      "[999/00064] updated kl_weight: 0.03\n",
      "[999/00065] updated kl_weight: 0.03\n",
      "[999/00066] updated kl_weight: 0.03\n",
      "[999/00067] updated kl_weight: 0.03\n",
      "[999/00068] updated kl_weight: 0.03\n",
      "[999/00069] updated kl_weight: 0.03\n",
      "[999/00070] updated kl_weight: 0.03\n",
      "[999/00071] updated kl_weight: 0.03\n",
      "[999/00072] updated kl_weight: 0.03\n",
      "[999/00073] updated kl_weight: 0.03\n",
      "[999/00074] updated kl_weight: 0.03\n",
      "[999/00074] train_loss: 0.021876 kl_loss: 0.201513 normal_loss: 0.015830\n",
      "[999/00074] MMD 0.005058863200247288\n",
      "[999/00074] TMD 0.06404420733451843\n"
     ]
    }
   ],
   "source": [
    "# CHAIR VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'mmd_every_epoch': 50,\n",
    "    'tmd_every_epoch': 50,\n",
    "    'iou_every_epoch': 10,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 3\n",
      "[001/00024] train_loss: 0.191461 kl_loss: 0.489047 normal_loss: 0.167009\n",
      "[002/00049] train_loss: 0.140902 kl_loss: 0.431867 normal_loss: 0.119309\n",
      "[003/00074] train_loss: 0.134114 kl_loss: 0.373703 normal_loss: 0.115429\n",
      "[005/00024] train_loss: 0.127796 kl_loss: 0.320461 normal_loss: 0.111773\n",
      "[006/00049] train_loss: 0.126211 kl_loss: 0.293880 normal_loss: 0.111517\n",
      "[007/00074] train_loss: 0.120764 kl_loss: 0.279733 normal_loss: 0.106777\n",
      "[009/00024] train_loss: 0.115604 kl_loss: 0.259998 normal_loss: 0.102604\n",
      "[010/00049] train_loss: 0.110620 kl_loss: 0.244606 normal_loss: 0.098390\n",
      "[011/00074] train_loss: 0.108912 kl_loss: 0.229203 normal_loss: 0.097452\n",
      "[013/00024] train_loss: 0.105591 kl_loss: 0.212820 normal_loss: 0.094950\n",
      "[014/00049] train_loss: 0.105521 kl_loss: 0.200961 normal_loss: 0.095473\n",
      "[015/00074] train_loss: 0.103065 kl_loss: 0.190483 normal_loss: 0.093541\n",
      "[017/00024] train_loss: 0.099156 kl_loss: 0.183402 normal_loss: 0.089985\n",
      "[018/00049] train_loss: 0.096704 kl_loss: 0.180475 normal_loss: 0.087680\n",
      "[019/00074] train_loss: 0.096036 kl_loss: 0.175056 normal_loss: 0.087284\n",
      "[021/00024] train_loss: 0.093384 kl_loss: 0.171327 normal_loss: 0.084818\n",
      "[022/00049] train_loss: 0.090744 kl_loss: 0.167280 normal_loss: 0.082380\n",
      "[023/00074] train_loss: 0.089570 kl_loss: 0.164347 normal_loss: 0.081353\n",
      "[025/00024] train_loss: 0.089292 kl_loss: 0.161032 normal_loss: 0.081240\n",
      "[026/00049] train_loss: 0.087050 kl_loss: 0.156851 normal_loss: 0.079208\n",
      "[027/00074] train_loss: 0.085116 kl_loss: 0.155366 normal_loss: 0.077348\n",
      "[029/00024] train_loss: 0.084744 kl_loss: 0.154589 normal_loss: 0.077015\n",
      "[030/00049] train_loss: 0.083072 kl_loss: 0.153966 normal_loss: 0.075374\n",
      "[031/00074] train_loss: 0.081248 kl_loss: 0.154229 normal_loss: 0.073536\n",
      "[033/00024] train_loss: 0.081332 kl_loss: 0.155411 normal_loss: 0.073561\n",
      "[034/00049] train_loss: 0.079855 kl_loss: 0.155237 normal_loss: 0.072093\n",
      "[035/00074] train_loss: 0.078575 kl_loss: 0.155751 normal_loss: 0.070787\n",
      "[037/00024] train_loss: 0.077851 kl_loss: 0.156746 normal_loss: 0.070014\n",
      "[038/00049] train_loss: 0.076579 kl_loss: 0.156711 normal_loss: 0.068743\n",
      "[039/00074] train_loss: 0.075793 kl_loss: 0.157207 normal_loss: 0.067933\n",
      "[041/00024] train_loss: 0.075226 kl_loss: 0.157686 normal_loss: 0.067342\n",
      "[042/00049] train_loss: 0.073635 kl_loss: 0.158199 normal_loss: 0.065725\n",
      "[043/00074] train_loss: 0.074050 kl_loss: 0.159147 normal_loss: 0.066093\n",
      "[045/00024] train_loss: 0.072659 kl_loss: 0.159545 normal_loss: 0.064682\n",
      "[046/00049] train_loss: 0.072495 kl_loss: 0.161593 normal_loss: 0.064416\n",
      "[047/00074] train_loss: 0.071676 kl_loss: 0.162092 normal_loss: 0.063571\n",
      "[049/00024] train_loss: 0.071598 kl_loss: 0.163504 normal_loss: 0.063423\n",
      "[049/00074] MMD 0.005202087108045816\n",
      "[049/00074] TMD 0.05903511494398117\n",
      "[050/00049] train_loss: 0.070211 kl_loss: 0.163646 normal_loss: 0.062029\n",
      "[051/00074] train_loss: 0.069568 kl_loss: 0.165369 normal_loss: 0.061299\n",
      "[053/00024] train_loss: 0.068898 kl_loss: 0.166524 normal_loss: 0.060571\n",
      "[054/00049] train_loss: 0.068011 kl_loss: 0.165713 normal_loss: 0.059725\n",
      "[055/00074] train_loss: 0.068994 kl_loss: 0.169536 normal_loss: 0.060518\n",
      "[057/00024] train_loss: 0.067160 kl_loss: 0.170067 normal_loss: 0.058657\n",
      "[058/00049] train_loss: 0.067690 kl_loss: 0.171490 normal_loss: 0.059116\n",
      "[059/00074] train_loss: 0.065929 kl_loss: 0.171686 normal_loss: 0.057345\n",
      "[061/00024] train_loss: 0.066051 kl_loss: 0.174247 normal_loss: 0.057338\n",
      "[062/00049] train_loss: 0.065192 kl_loss: 0.173559 normal_loss: 0.056514\n",
      "[063/00074] train_loss: 0.064915 kl_loss: 0.176507 normal_loss: 0.056090\n",
      "[065/00024] train_loss: 0.064503 kl_loss: 0.176727 normal_loss: 0.055667\n",
      "[066/00049] train_loss: 0.063930 kl_loss: 0.178992 normal_loss: 0.054980\n",
      "[067/00074] train_loss: 0.063940 kl_loss: 0.179219 normal_loss: 0.054979\n",
      "[069/00024] train_loss: 0.063102 kl_loss: 0.181194 normal_loss: 0.054042\n",
      "[070/00049] train_loss: 0.063190 kl_loss: 0.182245 normal_loss: 0.054078\n",
      "[071/00074] train_loss: 0.062581 kl_loss: 0.183771 normal_loss: 0.053393\n",
      "[073/00024] train_loss: 0.062588 kl_loss: 0.184462 normal_loss: 0.053365\n",
      "[074/00049] train_loss: 0.060987 kl_loss: 0.186551 normal_loss: 0.051659\n",
      "[075/00074] train_loss: 0.060898 kl_loss: 0.186714 normal_loss: 0.051562\n",
      "[077/00024] train_loss: 0.061310 kl_loss: 0.188202 normal_loss: 0.051900\n",
      "[078/00049] train_loss: 0.060728 kl_loss: 0.189387 normal_loss: 0.051258\n",
      "[079/00074] train_loss: 0.060561 kl_loss: 0.189744 normal_loss: 0.051074\n",
      "[081/00024] train_loss: 0.059486 kl_loss: 0.191240 normal_loss: 0.049924\n",
      "[082/00049] train_loss: 0.059290 kl_loss: 0.192131 normal_loss: 0.049683\n",
      "[083/00074] train_loss: 0.059754 kl_loss: 0.192391 normal_loss: 0.050134\n",
      "[085/00024] train_loss: 0.058511 kl_loss: 0.194336 normal_loss: 0.048794\n",
      "[086/00049] train_loss: 0.058061 kl_loss: 0.194705 normal_loss: 0.048326\n",
      "[087/00074] train_loss: 0.058466 kl_loss: 0.195957 normal_loss: 0.048669\n",
      "[089/00024] train_loss: 0.057879 kl_loss: 0.197062 normal_loss: 0.048026\n",
      "[090/00049] train_loss: 0.057230 kl_loss: 0.196975 normal_loss: 0.047381\n",
      "[091/00074] train_loss: 0.057180 kl_loss: 0.199956 normal_loss: 0.047182\n",
      "[093/00024] train_loss: 0.056349 kl_loss: 0.199624 normal_loss: 0.046368\n",
      "[094/00049] train_loss: 0.056422 kl_loss: 0.200875 normal_loss: 0.046379\n",
      "[095/00074] train_loss: 0.055491 kl_loss: 0.202240 normal_loss: 0.045379\n",
      "[097/00024] train_loss: 0.056190 kl_loss: 0.202541 normal_loss: 0.046063\n",
      "[098/00049] train_loss: 0.055443 kl_loss: 0.203837 normal_loss: 0.045251\n",
      "[099/00000] updated kl_weight: 0.05\n",
      "[099/00001] updated kl_weight: 0.05\n",
      "[099/00002] updated kl_weight: 0.05\n",
      "[099/00003] updated kl_weight: 0.05\n",
      "[099/00004] updated kl_weight: 0.05\n",
      "[099/00005] updated kl_weight: 0.05\n",
      "[099/00006] updated kl_weight: 0.05\n",
      "[099/00007] updated kl_weight: 0.05\n",
      "[099/00008] updated kl_weight: 0.05\n",
      "[099/00009] updated kl_weight: 0.05\n",
      "[099/00010] updated kl_weight: 0.05\n",
      "[099/00011] updated kl_weight: 0.05\n",
      "[099/00012] updated kl_weight: 0.05\n",
      "[099/00013] updated kl_weight: 0.05\n",
      "[099/00014] updated kl_weight: 0.05\n",
      "[099/00015] updated kl_weight: 0.05\n",
      "[099/00016] updated kl_weight: 0.05\n",
      "[099/00017] updated kl_weight: 0.05\n",
      "[099/00018] updated kl_weight: 0.05\n",
      "[099/00019] updated kl_weight: 0.05\n",
      "[099/00020] updated kl_weight: 0.05\n",
      "[099/00021] updated kl_weight: 0.05\n",
      "[099/00022] updated kl_weight: 0.05\n",
      "[099/00023] updated kl_weight: 0.05\n",
      "[099/00024] updated kl_weight: 0.05\n",
      "[099/00025] updated kl_weight: 0.05\n",
      "[099/00026] updated kl_weight: 0.05\n",
      "[099/00027] updated kl_weight: 0.05\n",
      "[099/00028] updated kl_weight: 0.05\n",
      "[099/00029] updated kl_weight: 0.05\n",
      "[099/00030] updated kl_weight: 0.05\n",
      "[099/00031] updated kl_weight: 0.05\n",
      "[099/00032] updated kl_weight: 0.05\n",
      "[099/00033] updated kl_weight: 0.05\n",
      "[099/00034] updated kl_weight: 0.05\n",
      "[099/00035] updated kl_weight: 0.05\n",
      "[099/00036] updated kl_weight: 0.05\n",
      "[099/00037] updated kl_weight: 0.05\n",
      "[099/00038] updated kl_weight: 0.05\n",
      "[099/00039] updated kl_weight: 0.05\n",
      "[099/00040] updated kl_weight: 0.05\n",
      "[099/00041] updated kl_weight: 0.05\n",
      "[099/00042] updated kl_weight: 0.05\n",
      "[099/00043] updated kl_weight: 0.05\n",
      "[099/00044] updated kl_weight: 0.05\n",
      "[099/00045] updated kl_weight: 0.05\n",
      "[099/00046] updated kl_weight: 0.05\n",
      "[099/00047] updated kl_weight: 0.05\n",
      "[099/00048] updated kl_weight: 0.05\n",
      "[099/00049] updated kl_weight: 0.05\n",
      "[099/00050] updated kl_weight: 0.05\n",
      "[099/00051] updated kl_weight: 0.05\n",
      "[099/00052] updated kl_weight: 0.05\n",
      "[099/00053] updated kl_weight: 0.05\n",
      "[099/00054] updated kl_weight: 0.05\n",
      "[099/00055] updated kl_weight: 0.05\n",
      "[099/00056] updated kl_weight: 0.05\n",
      "[099/00057] updated kl_weight: 0.05\n",
      "[099/00058] updated kl_weight: 0.05\n",
      "[099/00059] updated kl_weight: 0.05\n",
      "[099/00060] updated kl_weight: 0.05\n",
      "[099/00061] updated kl_weight: 0.05\n",
      "[099/00062] updated kl_weight: 0.05\n",
      "[099/00063] updated kl_weight: 0.05\n",
      "[099/00064] updated kl_weight: 0.05\n",
      "[099/00065] updated kl_weight: 0.05\n",
      "[099/00066] updated kl_weight: 0.05\n",
      "[099/00067] updated kl_weight: 0.05\n",
      "[099/00068] updated kl_weight: 0.05\n",
      "[099/00069] updated kl_weight: 0.05\n",
      "[099/00070] updated kl_weight: 0.05\n",
      "[099/00071] updated kl_weight: 0.05\n",
      "[099/00072] updated kl_weight: 0.05\n",
      "[099/00073] updated kl_weight: 0.05\n",
      "[099/00074] updated kl_weight: 0.05\n",
      "[099/00074] train_loss: 0.055625 kl_loss: 0.204918 normal_loss: 0.045379\n",
      "[099/00074] MMD 0.005021609365940094\n",
      "[099/00074] TMD 0.06067657098174095\n",
      "[101/00024] train_loss: 0.052218 kl_loss: 0.204885 normal_loss: 0.041974\n",
      "[102/00049] train_loss: 0.051057 kl_loss: 0.203102 normal_loss: 0.040902\n",
      "[103/00074] train_loss: 0.050515 kl_loss: 0.200403 normal_loss: 0.040494\n",
      "[105/00024] train_loss: 0.050257 kl_loss: 0.198618 normal_loss: 0.040326\n",
      "[106/00049] train_loss: 0.050082 kl_loss: 0.198377 normal_loss: 0.040164\n",
      "[107/00074] train_loss: 0.049517 kl_loss: 0.196317 normal_loss: 0.039701\n",
      "[109/00024] train_loss: 0.048866 kl_loss: 0.194975 normal_loss: 0.039118\n",
      "[110/00049] train_loss: 0.048870 kl_loss: 0.194852 normal_loss: 0.039127\n",
      "[111/00074] train_loss: 0.048411 kl_loss: 0.193800 normal_loss: 0.038721\n",
      "[113/00024] train_loss: 0.048453 kl_loss: 0.193099 normal_loss: 0.038798\n",
      "[114/00049] train_loss: 0.047979 kl_loss: 0.192172 normal_loss: 0.038370\n",
      "[115/00074] train_loss: 0.047798 kl_loss: 0.192071 normal_loss: 0.038195\n",
      "[117/00024] train_loss: 0.047822 kl_loss: 0.191987 normal_loss: 0.038222\n",
      "[118/00049] train_loss: 0.047290 kl_loss: 0.190886 normal_loss: 0.037746\n",
      "[119/00074] train_loss: 0.047152 kl_loss: 0.190791 normal_loss: 0.037612\n",
      "[121/00024] train_loss: 0.046859 kl_loss: 0.191199 normal_loss: 0.037299\n",
      "[122/00049] train_loss: 0.046942 kl_loss: 0.190346 normal_loss: 0.037425\n",
      "[123/00074] train_loss: 0.046890 kl_loss: 0.190670 normal_loss: 0.037356\n",
      "[125/00024] train_loss: 0.046498 kl_loss: 0.190668 normal_loss: 0.036965\n",
      "[126/00049] train_loss: 0.046304 kl_loss: 0.189868 normal_loss: 0.036811\n",
      "[127/00074] train_loss: 0.046039 kl_loss: 0.191150 normal_loss: 0.036481\n",
      "[129/00024] train_loss: 0.045567 kl_loss: 0.190774 normal_loss: 0.036028\n",
      "[130/00049] train_loss: 0.045875 kl_loss: 0.191116 normal_loss: 0.036319\n",
      "[131/00074] train_loss: 0.045567 kl_loss: 0.190414 normal_loss: 0.036047\n",
      "[133/00024] train_loss: 0.045299 kl_loss: 0.191766 normal_loss: 0.035710\n",
      "[134/00049] train_loss: 0.044934 kl_loss: 0.190541 normal_loss: 0.035407\n",
      "[135/00074] train_loss: 0.044675 kl_loss: 0.191195 normal_loss: 0.035115\n",
      "[137/00024] train_loss: 0.044536 kl_loss: 0.191256 normal_loss: 0.034974\n",
      "[138/00049] train_loss: 0.044340 kl_loss: 0.191676 normal_loss: 0.034756\n",
      "[139/00074] train_loss: 0.044304 kl_loss: 0.191910 normal_loss: 0.034709\n",
      "[141/00024] train_loss: 0.044235 kl_loss: 0.192023 normal_loss: 0.034633\n",
      "[142/00049] train_loss: 0.044255 kl_loss: 0.192588 normal_loss: 0.034626\n",
      "[143/00074] train_loss: 0.043906 kl_loss: 0.191800 normal_loss: 0.034317\n",
      "[145/00024] train_loss: 0.043836 kl_loss: 0.192020 normal_loss: 0.034235\n",
      "[146/00049] train_loss: 0.043657 kl_loss: 0.192378 normal_loss: 0.034038\n",
      "[147/00074] train_loss: 0.043370 kl_loss: 0.193247 normal_loss: 0.033707\n",
      "[149/00024] train_loss: 0.043197 kl_loss: 0.193363 normal_loss: 0.033528\n",
      "[149/00074] MMD 0.005345713812857866\n",
      "[149/00074] TMD 0.05969274044036865\n",
      "[150/00049] train_loss: 0.043044 kl_loss: 0.192131 normal_loss: 0.033438\n",
      "[151/00074] train_loss: 0.043396 kl_loss: 0.193627 normal_loss: 0.033714\n",
      "[153/00024] train_loss: 0.042725 kl_loss: 0.193863 normal_loss: 0.033032\n",
      "[154/00049] train_loss: 0.042568 kl_loss: 0.193263 normal_loss: 0.032905\n",
      "[155/00074] train_loss: 0.042699 kl_loss: 0.193356 normal_loss: 0.033032\n",
      "[157/00024] train_loss: 0.042267 kl_loss: 0.193834 normal_loss: 0.032575\n",
      "[158/00049] train_loss: 0.042446 kl_loss: 0.193723 normal_loss: 0.032759\n",
      "[159/00074] train_loss: 0.041642 kl_loss: 0.193910 normal_loss: 0.031947\n",
      "[161/00024] train_loss: 0.041982 kl_loss: 0.194401 normal_loss: 0.032262\n",
      "[162/00049] train_loss: 0.041840 kl_loss: 0.194183 normal_loss: 0.032131\n",
      "[163/00074] train_loss: 0.041564 kl_loss: 0.194528 normal_loss: 0.031838\n",
      "[165/00024] train_loss: 0.041366 kl_loss: 0.194504 normal_loss: 0.031640\n",
      "[166/00049] train_loss: 0.041081 kl_loss: 0.194589 normal_loss: 0.031352\n",
      "[167/00074] train_loss: 0.041643 kl_loss: 0.194837 normal_loss: 0.031901\n",
      "[169/00024] train_loss: 0.041255 kl_loss: 0.194764 normal_loss: 0.031516\n",
      "[170/00049] train_loss: 0.041305 kl_loss: 0.195460 normal_loss: 0.031532\n",
      "[171/00074] train_loss: 0.040663 kl_loss: 0.194301 normal_loss: 0.030948\n",
      "[173/00024] train_loss: 0.040641 kl_loss: 0.194792 normal_loss: 0.030901\n",
      "[174/00049] train_loss: 0.040772 kl_loss: 0.195407 normal_loss: 0.031002\n",
      "[175/00074] train_loss: 0.040400 kl_loss: 0.194637 normal_loss: 0.030668\n",
      "[177/00024] train_loss: 0.040434 kl_loss: 0.194756 normal_loss: 0.030696\n",
      "[178/00049] train_loss: 0.040551 kl_loss: 0.195837 normal_loss: 0.030759\n",
      "[179/00074] train_loss: 0.040324 kl_loss: 0.194928 normal_loss: 0.030577\n",
      "[181/00024] train_loss: 0.039536 kl_loss: 0.194606 normal_loss: 0.029806\n",
      "[182/00049] train_loss: 0.039831 kl_loss: 0.195963 normal_loss: 0.030032\n",
      "[183/00074] train_loss: 0.039512 kl_loss: 0.195505 normal_loss: 0.029737\n",
      "[185/00024] train_loss: 0.039896 kl_loss: 0.195449 normal_loss: 0.030124\n",
      "[186/00049] train_loss: 0.039715 kl_loss: 0.195497 normal_loss: 0.029940\n",
      "[187/00074] train_loss: 0.039139 kl_loss: 0.194804 normal_loss: 0.029399\n",
      "[189/00024] train_loss: 0.039562 kl_loss: 0.195158 normal_loss: 0.029804\n",
      "[190/00049] train_loss: 0.039439 kl_loss: 0.195755 normal_loss: 0.029651\n",
      "[191/00074] train_loss: 0.039147 kl_loss: 0.194153 normal_loss: 0.029440\n",
      "[193/00024] train_loss: 0.038850 kl_loss: 0.195147 normal_loss: 0.029093\n",
      "[194/00049] train_loss: 0.038890 kl_loss: 0.194716 normal_loss: 0.029154\n",
      "[195/00074] train_loss: 0.038895 kl_loss: 0.194916 normal_loss: 0.029150\n",
      "[197/00024] train_loss: 0.038694 kl_loss: 0.195755 normal_loss: 0.028906\n",
      "[198/00049] train_loss: 0.038394 kl_loss: 0.194198 normal_loss: 0.028684\n",
      "[199/00000] updated kl_weight: 0.05\n",
      "[199/00001] updated kl_weight: 0.05\n",
      "[199/00002] updated kl_weight: 0.05\n",
      "[199/00003] updated kl_weight: 0.05\n",
      "[199/00004] updated kl_weight: 0.05\n",
      "[199/00005] updated kl_weight: 0.05\n",
      "[199/00006] updated kl_weight: 0.05\n",
      "[199/00007] updated kl_weight: 0.05\n",
      "[199/00008] updated kl_weight: 0.05\n",
      "[199/00009] updated kl_weight: 0.05\n",
      "[199/00010] updated kl_weight: 0.05\n",
      "[199/00011] updated kl_weight: 0.05\n",
      "[199/00012] updated kl_weight: 0.05\n",
      "[199/00013] updated kl_weight: 0.05\n",
      "[199/00014] updated kl_weight: 0.05\n",
      "[199/00015] updated kl_weight: 0.05\n",
      "[199/00016] updated kl_weight: 0.05\n",
      "[199/00017] updated kl_weight: 0.05\n",
      "[199/00018] updated kl_weight: 0.05\n",
      "[199/00019] updated kl_weight: 0.05\n",
      "[199/00020] updated kl_weight: 0.05\n",
      "[199/00021] updated kl_weight: 0.05\n",
      "[199/00022] updated kl_weight: 0.05\n",
      "[199/00023] updated kl_weight: 0.05\n",
      "[199/00024] updated kl_weight: 0.05\n",
      "[199/00025] updated kl_weight: 0.05\n",
      "[199/00026] updated kl_weight: 0.05\n",
      "[199/00027] updated kl_weight: 0.05\n",
      "[199/00028] updated kl_weight: 0.05\n",
      "[199/00029] updated kl_weight: 0.05\n",
      "[199/00030] updated kl_weight: 0.05\n",
      "[199/00031] updated kl_weight: 0.05\n",
      "[199/00032] updated kl_weight: 0.05\n",
      "[199/00033] updated kl_weight: 0.05\n",
      "[199/00034] updated kl_weight: 0.05\n",
      "[199/00035] updated kl_weight: 0.05\n",
      "[199/00036] updated kl_weight: 0.05\n",
      "[199/00037] updated kl_weight: 0.05\n",
      "[199/00038] updated kl_weight: 0.05\n",
      "[199/00039] updated kl_weight: 0.05\n",
      "[199/00040] updated kl_weight: 0.05\n",
      "[199/00041] updated kl_weight: 0.05\n",
      "[199/00042] updated kl_weight: 0.05\n",
      "[199/00043] updated kl_weight: 0.05\n",
      "[199/00044] updated kl_weight: 0.05\n",
      "[199/00045] updated kl_weight: 0.05\n",
      "[199/00046] updated kl_weight: 0.05\n",
      "[199/00047] updated kl_weight: 0.05\n",
      "[199/00048] updated kl_weight: 0.05\n",
      "[199/00049] updated kl_weight: 0.05\n",
      "[199/00050] updated kl_weight: 0.05\n",
      "[199/00051] updated kl_weight: 0.05\n",
      "[199/00052] updated kl_weight: 0.05\n",
      "[199/00053] updated kl_weight: 0.05\n",
      "[199/00054] updated kl_weight: 0.05\n",
      "[199/00055] updated kl_weight: 0.05\n",
      "[199/00056] updated kl_weight: 0.05\n",
      "[199/00057] updated kl_weight: 0.05\n",
      "[199/00058] updated kl_weight: 0.05\n",
      "[199/00059] updated kl_weight: 0.05\n",
      "[199/00060] updated kl_weight: 0.05\n",
      "[199/00061] updated kl_weight: 0.05\n",
      "[199/00062] updated kl_weight: 0.05\n",
      "[199/00063] updated kl_weight: 0.05\n",
      "[199/00064] updated kl_weight: 0.05\n",
      "[199/00065] updated kl_weight: 0.05\n",
      "[199/00066] updated kl_weight: 0.05\n",
      "[199/00067] updated kl_weight: 0.05\n",
      "[199/00068] updated kl_weight: 0.05\n",
      "[199/00069] updated kl_weight: 0.05\n",
      "[199/00070] updated kl_weight: 0.05\n",
      "[199/00071] updated kl_weight: 0.05\n",
      "[199/00072] updated kl_weight: 0.05\n",
      "[199/00073] updated kl_weight: 0.05\n",
      "[199/00074] updated kl_weight: 0.05\n",
      "[199/00074] train_loss: 0.038606 kl_loss: 0.194148 normal_loss: 0.028898\n",
      "[199/00074] MMD 0.005295352078974247\n",
      "[199/00074] TMD 0.05962818115949631\n",
      "[201/00024] train_loss: 0.037051 kl_loss: 0.194026 normal_loss: 0.027349\n",
      "[202/00049] train_loss: 0.036632 kl_loss: 0.194252 normal_loss: 0.026919\n",
      "[203/00074] train_loss: 0.036447 kl_loss: 0.193314 normal_loss: 0.026781\n",
      "[205/00024] train_loss: 0.036091 kl_loss: 0.192326 normal_loss: 0.026475\n",
      "[206/00049] train_loss: 0.036164 kl_loss: 0.192586 normal_loss: 0.026534\n",
      "[207/00074] train_loss: 0.036030 kl_loss: 0.191478 normal_loss: 0.026456\n",
      "[209/00024] train_loss: 0.035953 kl_loss: 0.191740 normal_loss: 0.026366\n",
      "[210/00049] train_loss: 0.035684 kl_loss: 0.190031 normal_loss: 0.026183\n",
      "[211/00074] train_loss: 0.035628 kl_loss: 0.189876 normal_loss: 0.026135\n",
      "[213/00024] train_loss: 0.035602 kl_loss: 0.189755 normal_loss: 0.026114\n",
      "[214/00049] train_loss: 0.035433 kl_loss: 0.188323 normal_loss: 0.026017\n",
      "[215/00074] train_loss: 0.035563 kl_loss: 0.189075 normal_loss: 0.026109\n",
      "[217/00024] train_loss: 0.035345 kl_loss: 0.188096 normal_loss: 0.025940\n",
      "[218/00049] train_loss: 0.035147 kl_loss: 0.187327 normal_loss: 0.025780\n",
      "[219/00074] train_loss: 0.035488 kl_loss: 0.188065 normal_loss: 0.026085\n",
      "[221/00024] train_loss: 0.035016 kl_loss: 0.186608 normal_loss: 0.025686\n",
      "[222/00049] train_loss: 0.035113 kl_loss: 0.186743 normal_loss: 0.025776\n",
      "[223/00074] train_loss: 0.034959 kl_loss: 0.186767 normal_loss: 0.025620\n",
      "[225/00024] train_loss: 0.034978 kl_loss: 0.186357 normal_loss: 0.025660\n",
      "[226/00049] train_loss: 0.034884 kl_loss: 0.185464 normal_loss: 0.025610\n",
      "[227/00074] train_loss: 0.034834 kl_loss: 0.185403 normal_loss: 0.025564\n",
      "[229/00024] train_loss: 0.034865 kl_loss: 0.185318 normal_loss: 0.025599\n",
      "[230/00049] train_loss: 0.034616 kl_loss: 0.184027 normal_loss: 0.025415\n",
      "[231/00074] train_loss: 0.034773 kl_loss: 0.185371 normal_loss: 0.025504\n",
      "[233/00024] train_loss: 0.034484 kl_loss: 0.184379 normal_loss: 0.025265\n",
      "[234/00049] train_loss: 0.034485 kl_loss: 0.184020 normal_loss: 0.025284\n",
      "[235/00074] train_loss: 0.034490 kl_loss: 0.184032 normal_loss: 0.025288\n",
      "[237/00024] train_loss: 0.034269 kl_loss: 0.183444 normal_loss: 0.025097\n",
      "[238/00049] train_loss: 0.034351 kl_loss: 0.183248 normal_loss: 0.025189\n",
      "[239/00074] train_loss: 0.034372 kl_loss: 0.183702 normal_loss: 0.025187\n",
      "[241/00024] train_loss: 0.034231 kl_loss: 0.182487 normal_loss: 0.025107\n",
      "[242/00049] train_loss: 0.034326 kl_loss: 0.183851 normal_loss: 0.025134\n",
      "[243/00074] train_loss: 0.034119 kl_loss: 0.182031 normal_loss: 0.025017\n",
      "[245/00024] train_loss: 0.034121 kl_loss: 0.182349 normal_loss: 0.025004\n",
      "[246/00049] train_loss: 0.034003 kl_loss: 0.181852 normal_loss: 0.024910\n",
      "[247/00074] train_loss: 0.034138 kl_loss: 0.182322 normal_loss: 0.025022\n",
      "[249/00024] train_loss: 0.033791 kl_loss: 0.181767 normal_loss: 0.024703\n",
      "[249/00074] MMD 0.005048416089266539\n",
      "[249/00074] TMD 0.05533239245414734\n",
      "[250/00049] train_loss: 0.033890 kl_loss: 0.181259 normal_loss: 0.024827\n",
      "[251/00074] train_loss: 0.033854 kl_loss: 0.181628 normal_loss: 0.024772\n",
      "[253/00024] train_loss: 0.033661 kl_loss: 0.180974 normal_loss: 0.024613\n",
      "[254/00049] train_loss: 0.033670 kl_loss: 0.181285 normal_loss: 0.024605\n",
      "[255/00074] train_loss: 0.033657 kl_loss: 0.180690 normal_loss: 0.024622\n",
      "[257/00024] train_loss: 0.033710 kl_loss: 0.181037 normal_loss: 0.024658\n",
      "[258/00049] train_loss: 0.033317 kl_loss: 0.180104 normal_loss: 0.024312\n",
      "[259/00074] train_loss: 0.033473 kl_loss: 0.180286 normal_loss: 0.024458\n",
      "[261/00024] train_loss: 0.033606 kl_loss: 0.180743 normal_loss: 0.024569\n",
      "[262/00049] train_loss: 0.033245 kl_loss: 0.180115 normal_loss: 0.024239\n",
      "[263/00074] train_loss: 0.033240 kl_loss: 0.179007 normal_loss: 0.024290\n",
      "[265/00024] train_loss: 0.033327 kl_loss: 0.179114 normal_loss: 0.024372\n",
      "[266/00049] train_loss: 0.033275 kl_loss: 0.180036 normal_loss: 0.024273\n",
      "[267/00074] train_loss: 0.033207 kl_loss: 0.179318 normal_loss: 0.024241\n",
      "[269/00024] train_loss: 0.033044 kl_loss: 0.178961 normal_loss: 0.024096\n",
      "[270/00049] train_loss: 0.033024 kl_loss: 0.179153 normal_loss: 0.024066\n",
      "[271/00074] train_loss: 0.032987 kl_loss: 0.179271 normal_loss: 0.024023\n",
      "[273/00024] train_loss: 0.032984 kl_loss: 0.178899 normal_loss: 0.024039\n",
      "[274/00049] train_loss: 0.033007 kl_loss: 0.178909 normal_loss: 0.024061\n",
      "[275/00074] train_loss: 0.032863 kl_loss: 0.178291 normal_loss: 0.023948\n",
      "[277/00024] train_loss: 0.032855 kl_loss: 0.179121 normal_loss: 0.023899\n",
      "[278/00049] train_loss: 0.032642 kl_loss: 0.177401 normal_loss: 0.023771\n",
      "[279/00074] train_loss: 0.032654 kl_loss: 0.178289 normal_loss: 0.023740\n",
      "[281/00024] train_loss: 0.032708 kl_loss: 0.178146 normal_loss: 0.023801\n",
      "[282/00049] train_loss: 0.032608 kl_loss: 0.177661 normal_loss: 0.023725\n",
      "[283/00074] train_loss: 0.032656 kl_loss: 0.177417 normal_loss: 0.023786\n",
      "[285/00024] train_loss: 0.032575 kl_loss: 0.178099 normal_loss: 0.023670\n",
      "[286/00049] train_loss: 0.032352 kl_loss: 0.177006 normal_loss: 0.023502\n",
      "[287/00074] train_loss: 0.032436 kl_loss: 0.176916 normal_loss: 0.023590\n",
      "[289/00024] train_loss: 0.032398 kl_loss: 0.177508 normal_loss: 0.023522\n",
      "[290/00049] train_loss: 0.032343 kl_loss: 0.176643 normal_loss: 0.023511\n",
      "[291/00074] train_loss: 0.032464 kl_loss: 0.176816 normal_loss: 0.023624\n",
      "[293/00024] train_loss: 0.032176 kl_loss: 0.176682 normal_loss: 0.023342\n",
      "[294/00049] train_loss: 0.032165 kl_loss: 0.176304 normal_loss: 0.023350\n",
      "[295/00074] train_loss: 0.032294 kl_loss: 0.176841 normal_loss: 0.023452\n",
      "[297/00024] train_loss: 0.031973 kl_loss: 0.176103 normal_loss: 0.023168\n",
      "[298/00049] train_loss: 0.032009 kl_loss: 0.176568 normal_loss: 0.023181\n",
      "[299/00000] updated kl_weight: 0.05\n",
      "[299/00001] updated kl_weight: 0.05\n",
      "[299/00002] updated kl_weight: 0.05\n",
      "[299/00003] updated kl_weight: 0.05\n",
      "[299/00004] updated kl_weight: 0.05\n",
      "[299/00005] updated kl_weight: 0.05\n",
      "[299/00006] updated kl_weight: 0.05\n",
      "[299/00007] updated kl_weight: 0.05\n",
      "[299/00008] updated kl_weight: 0.05\n",
      "[299/00009] updated kl_weight: 0.05\n",
      "[299/00010] updated kl_weight: 0.05\n",
      "[299/00011] updated kl_weight: 0.05\n",
      "[299/00012] updated kl_weight: 0.05\n",
      "[299/00013] updated kl_weight: 0.05\n",
      "[299/00014] updated kl_weight: 0.05\n",
      "[299/00015] updated kl_weight: 0.05\n",
      "[299/00016] updated kl_weight: 0.05\n",
      "[299/00017] updated kl_weight: 0.05\n",
      "[299/00018] updated kl_weight: 0.05\n",
      "[299/00019] updated kl_weight: 0.05\n",
      "[299/00020] updated kl_weight: 0.05\n",
      "[299/00021] updated kl_weight: 0.05\n",
      "[299/00022] updated kl_weight: 0.05\n",
      "[299/00023] updated kl_weight: 0.05\n",
      "[299/00024] updated kl_weight: 0.05\n",
      "[299/00025] updated kl_weight: 0.05\n",
      "[299/00026] updated kl_weight: 0.05\n",
      "[299/00027] updated kl_weight: 0.05\n",
      "[299/00028] updated kl_weight: 0.05\n",
      "[299/00029] updated kl_weight: 0.05\n",
      "[299/00030] updated kl_weight: 0.05\n",
      "[299/00031] updated kl_weight: 0.05\n",
      "[299/00032] updated kl_weight: 0.05\n",
      "[299/00033] updated kl_weight: 0.05\n",
      "[299/00034] updated kl_weight: 0.05\n",
      "[299/00035] updated kl_weight: 0.05\n",
      "[299/00036] updated kl_weight: 0.05\n",
      "[299/00037] updated kl_weight: 0.05\n",
      "[299/00038] updated kl_weight: 0.05\n",
      "[299/00039] updated kl_weight: 0.05\n",
      "[299/00040] updated kl_weight: 0.05\n",
      "[299/00041] updated kl_weight: 0.05\n",
      "[299/00042] updated kl_weight: 0.05\n",
      "[299/00043] updated kl_weight: 0.05\n",
      "[299/00044] updated kl_weight: 0.05\n",
      "[299/00045] updated kl_weight: 0.05\n",
      "[299/00046] updated kl_weight: 0.05\n",
      "[299/00047] updated kl_weight: 0.05\n",
      "[299/00048] updated kl_weight: 0.05\n",
      "[299/00049] updated kl_weight: 0.05\n",
      "[299/00050] updated kl_weight: 0.05\n",
      "[299/00051] updated kl_weight: 0.05\n",
      "[299/00052] updated kl_weight: 0.05\n",
      "[299/00053] updated kl_weight: 0.05\n",
      "[299/00054] updated kl_weight: 0.05\n",
      "[299/00055] updated kl_weight: 0.05\n",
      "[299/00056] updated kl_weight: 0.05\n",
      "[299/00057] updated kl_weight: 0.05\n",
      "[299/00058] updated kl_weight: 0.05\n",
      "[299/00059] updated kl_weight: 0.05\n",
      "[299/00060] updated kl_weight: 0.05\n",
      "[299/00061] updated kl_weight: 0.05\n",
      "[299/00062] updated kl_weight: 0.05\n",
      "[299/00063] updated kl_weight: 0.05\n",
      "[299/00064] updated kl_weight: 0.05\n",
      "[299/00065] updated kl_weight: 0.05\n",
      "[299/00066] updated kl_weight: 0.05\n",
      "[299/00067] updated kl_weight: 0.05\n",
      "[299/00068] updated kl_weight: 0.05\n",
      "[299/00069] updated kl_weight: 0.05\n",
      "[299/00070] updated kl_weight: 0.05\n",
      "[299/00071] updated kl_weight: 0.05\n",
      "[299/00072] updated kl_weight: 0.05\n",
      "[299/00073] updated kl_weight: 0.05\n",
      "[299/00074] updated kl_weight: 0.05\n",
      "[299/00074] train_loss: 0.032051 kl_loss: 0.175699 normal_loss: 0.023266\n",
      "[299/00074] MMD 0.00504768081009388\n",
      "[299/00074] TMD 0.053506284952163696\n",
      "[301/00024] train_loss: 0.031461 kl_loss: 0.175726 normal_loss: 0.022675\n",
      "[302/00049] train_loss: 0.031355 kl_loss: 0.176063 normal_loss: 0.022552\n",
      "[303/00074] train_loss: 0.031110 kl_loss: 0.174969 normal_loss: 0.022362\n",
      "[305/00024] train_loss: 0.031008 kl_loss: 0.174541 normal_loss: 0.022281\n",
      "[306/00049] train_loss: 0.031181 kl_loss: 0.175385 normal_loss: 0.022412\n",
      "[307/00074] train_loss: 0.031036 kl_loss: 0.174883 normal_loss: 0.022292\n",
      "[309/00024] train_loss: 0.030887 kl_loss: 0.174468 normal_loss: 0.022164\n",
      "[310/00049] train_loss: 0.030992 kl_loss: 0.174414 normal_loss: 0.022271\n",
      "[311/00074] train_loss: 0.030955 kl_loss: 0.174053 normal_loss: 0.022252\n",
      "[313/00024] train_loss: 0.030929 kl_loss: 0.174019 normal_loss: 0.022228\n",
      "[314/00049] train_loss: 0.030909 kl_loss: 0.173426 normal_loss: 0.022237\n",
      "[315/00074] train_loss: 0.030802 kl_loss: 0.173708 normal_loss: 0.022116\n",
      "[317/00024] train_loss: 0.030789 kl_loss: 0.173552 normal_loss: 0.022112\n",
      "[318/00049] train_loss: 0.030456 kl_loss: 0.172081 normal_loss: 0.021852\n",
      "[319/00074] train_loss: 0.030850 kl_loss: 0.173784 normal_loss: 0.022160\n",
      "[321/00024] train_loss: 0.030489 kl_loss: 0.172784 normal_loss: 0.021849\n",
      "[322/00049] train_loss: 0.030665 kl_loss: 0.172802 normal_loss: 0.022025\n",
      "[323/00074] train_loss: 0.030555 kl_loss: 0.172088 normal_loss: 0.021951\n",
      "[325/00024] train_loss: 0.030581 kl_loss: 0.172463 normal_loss: 0.021958\n",
      "[326/00049] train_loss: 0.030345 kl_loss: 0.171860 normal_loss: 0.021752\n",
      "[327/00074] train_loss: 0.030513 kl_loss: 0.171668 normal_loss: 0.021930\n",
      "[329/00024] train_loss: 0.030483 kl_loss: 0.171590 normal_loss: 0.021904\n",
      "[330/00049] train_loss: 0.030487 kl_loss: 0.171029 normal_loss: 0.021936\n",
      "[331/00074] train_loss: 0.030403 kl_loss: 0.171911 normal_loss: 0.021807\n",
      "[333/00024] train_loss: 0.030521 kl_loss: 0.171564 normal_loss: 0.021943\n",
      "[334/00049] train_loss: 0.030239 kl_loss: 0.170547 normal_loss: 0.021712\n",
      "[335/00074] train_loss: 0.030269 kl_loss: 0.170937 normal_loss: 0.021722\n",
      "[337/00024] train_loss: 0.030253 kl_loss: 0.170615 normal_loss: 0.021723\n",
      "[338/00049] train_loss: 0.030349 kl_loss: 0.170799 normal_loss: 0.021809\n",
      "[339/00074] train_loss: 0.030141 kl_loss: 0.170209 normal_loss: 0.021631\n",
      "[341/00024] train_loss: 0.030184 kl_loss: 0.170121 normal_loss: 0.021678\n",
      "[342/00049] train_loss: 0.030280 kl_loss: 0.170893 normal_loss: 0.021736\n",
      "[343/00074] train_loss: 0.030096 kl_loss: 0.169144 normal_loss: 0.021638\n",
      "[345/00024] train_loss: 0.030091 kl_loss: 0.169970 normal_loss: 0.021593\n",
      "[346/00049] train_loss: 0.029968 kl_loss: 0.169304 normal_loss: 0.021502\n",
      "[347/00074] train_loss: 0.030183 kl_loss: 0.169438 normal_loss: 0.021711\n",
      "[349/00024] train_loss: 0.029933 kl_loss: 0.168879 normal_loss: 0.021489\n",
      "[349/00074] MMD 0.0052010915242135525\n",
      "[349/00074] TMD 0.06301707774400711\n",
      "[350/00049] train_loss: 0.029993 kl_loss: 0.169400 normal_loss: 0.021523\n",
      "[351/00074] train_loss: 0.030126 kl_loss: 0.169167 normal_loss: 0.021668\n",
      "[353/00024] train_loss: 0.030006 kl_loss: 0.169015 normal_loss: 0.021555\n",
      "[354/00049] train_loss: 0.030048 kl_loss: 0.168914 normal_loss: 0.021602\n",
      "[355/00074] train_loss: 0.029830 kl_loss: 0.168457 normal_loss: 0.021407\n",
      "[357/00024] train_loss: 0.029740 kl_loss: 0.168339 normal_loss: 0.021323\n",
      "[358/00049] train_loss: 0.030059 kl_loss: 0.169000 normal_loss: 0.021609\n",
      "[359/00074] train_loss: 0.029813 kl_loss: 0.167868 normal_loss: 0.021420\n",
      "[361/00024] train_loss: 0.029744 kl_loss: 0.168104 normal_loss: 0.021339\n",
      "[362/00049] train_loss: 0.029761 kl_loss: 0.167956 normal_loss: 0.021364\n",
      "[363/00074] train_loss: 0.029782 kl_loss: 0.168035 normal_loss: 0.021380\n",
      "[365/00024] train_loss: 0.029610 kl_loss: 0.167626 normal_loss: 0.021229\n",
      "[366/00049] train_loss: 0.029743 kl_loss: 0.167871 normal_loss: 0.021349\n",
      "[367/00074] train_loss: 0.029749 kl_loss: 0.167507 normal_loss: 0.021374\n",
      "[369/00024] train_loss: 0.029725 kl_loss: 0.167259 normal_loss: 0.021362\n",
      "[370/00049] train_loss: 0.029661 kl_loss: 0.167651 normal_loss: 0.021278\n",
      "[371/00074] train_loss: 0.029610 kl_loss: 0.167058 normal_loss: 0.021257\n",
      "[373/00024] train_loss: 0.029539 kl_loss: 0.167319 normal_loss: 0.021173\n",
      "[374/00049] train_loss: 0.029492 kl_loss: 0.166853 normal_loss: 0.021149\n",
      "[375/00074] train_loss: 0.029631 kl_loss: 0.166766 normal_loss: 0.021293\n",
      "[377/00024] train_loss: 0.029415 kl_loss: 0.166708 normal_loss: 0.021080\n",
      "[378/00049] train_loss: 0.029530 kl_loss: 0.166828 normal_loss: 0.021189\n",
      "[379/00074] train_loss: 0.029347 kl_loss: 0.166382 normal_loss: 0.021028\n",
      "[381/00024] train_loss: 0.029400 kl_loss: 0.166436 normal_loss: 0.021078\n",
      "[382/00049] train_loss: 0.029364 kl_loss: 0.166292 normal_loss: 0.021050\n",
      "[383/00074] train_loss: 0.029439 kl_loss: 0.166162 normal_loss: 0.021131\n",
      "[385/00024] train_loss: 0.029403 kl_loss: 0.166167 normal_loss: 0.021095\n",
      "[386/00049] train_loss: 0.029276 kl_loss: 0.165811 normal_loss: 0.020985\n",
      "[387/00074] train_loss: 0.029307 kl_loss: 0.165926 normal_loss: 0.021011\n",
      "[389/00024] train_loss: 0.029215 kl_loss: 0.165002 normal_loss: 0.020965\n",
      "[390/00049] train_loss: 0.029484 kl_loss: 0.166778 normal_loss: 0.021145\n",
      "[391/00074] train_loss: 0.029202 kl_loss: 0.165202 normal_loss: 0.020942\n",
      "[393/00024] train_loss: 0.029314 kl_loss: 0.165695 normal_loss: 0.021029\n",
      "[394/00049] train_loss: 0.029108 kl_loss: 0.164987 normal_loss: 0.020859\n",
      "[395/00074] train_loss: 0.029210 kl_loss: 0.165377 normal_loss: 0.020941\n",
      "[397/00024] train_loss: 0.029149 kl_loss: 0.165402 normal_loss: 0.020879\n",
      "[398/00049] train_loss: 0.028961 kl_loss: 0.164658 normal_loss: 0.020728\n",
      "[399/00000] updated kl_weight: 0.05\n",
      "[399/00001] updated kl_weight: 0.05\n",
      "[399/00002] updated kl_weight: 0.05\n",
      "[399/00003] updated kl_weight: 0.05\n",
      "[399/00004] updated kl_weight: 0.05\n",
      "[399/00005] updated kl_weight: 0.05\n",
      "[399/00006] updated kl_weight: 0.05\n",
      "[399/00007] updated kl_weight: 0.05\n",
      "[399/00008] updated kl_weight: 0.05\n",
      "[399/00009] updated kl_weight: 0.05\n",
      "[399/00010] updated kl_weight: 0.05\n",
      "[399/00011] updated kl_weight: 0.05\n",
      "[399/00012] updated kl_weight: 0.05\n",
      "[399/00013] updated kl_weight: 0.05\n",
      "[399/00014] updated kl_weight: 0.05\n",
      "[399/00015] updated kl_weight: 0.05\n",
      "[399/00016] updated kl_weight: 0.05\n",
      "[399/00017] updated kl_weight: 0.05\n",
      "[399/00018] updated kl_weight: 0.05\n",
      "[399/00019] updated kl_weight: 0.05\n",
      "[399/00020] updated kl_weight: 0.05\n",
      "[399/00021] updated kl_weight: 0.05\n",
      "[399/00022] updated kl_weight: 0.05\n",
      "[399/00023] updated kl_weight: 0.05\n",
      "[399/00024] updated kl_weight: 0.05\n",
      "[399/00025] updated kl_weight: 0.05\n",
      "[399/00026] updated kl_weight: 0.05\n",
      "[399/00027] updated kl_weight: 0.05\n",
      "[399/00028] updated kl_weight: 0.05\n",
      "[399/00029] updated kl_weight: 0.05\n",
      "[399/00030] updated kl_weight: 0.05\n",
      "[399/00031] updated kl_weight: 0.05\n",
      "[399/00032] updated kl_weight: 0.05\n",
      "[399/00033] updated kl_weight: 0.05\n",
      "[399/00034] updated kl_weight: 0.05\n",
      "[399/00035] updated kl_weight: 0.05\n",
      "[399/00036] updated kl_weight: 0.05\n",
      "[399/00037] updated kl_weight: 0.05\n",
      "[399/00038] updated kl_weight: 0.05\n",
      "[399/00039] updated kl_weight: 0.05\n",
      "[399/00040] updated kl_weight: 0.05\n",
      "[399/00041] updated kl_weight: 0.05\n",
      "[399/00042] updated kl_weight: 0.05\n",
      "[399/00043] updated kl_weight: 0.05\n",
      "[399/00044] updated kl_weight: 0.05\n",
      "[399/00045] updated kl_weight: 0.05\n",
      "[399/00046] updated kl_weight: 0.05\n",
      "[399/00047] updated kl_weight: 0.05\n",
      "[399/00048] updated kl_weight: 0.05\n",
      "[399/00049] updated kl_weight: 0.05\n",
      "[399/00050] updated kl_weight: 0.05\n",
      "[399/00051] updated kl_weight: 0.05\n",
      "[399/00052] updated kl_weight: 0.05\n",
      "[399/00053] updated kl_weight: 0.05\n",
      "[399/00054] updated kl_weight: 0.05\n",
      "[399/00055] updated kl_weight: 0.05\n",
      "[399/00056] updated kl_weight: 0.05\n",
      "[399/00057] updated kl_weight: 0.05\n",
      "[399/00058] updated kl_weight: 0.05\n",
      "[399/00059] updated kl_weight: 0.05\n",
      "[399/00060] updated kl_weight: 0.05\n",
      "[399/00061] updated kl_weight: 0.05\n",
      "[399/00062] updated kl_weight: 0.05\n",
      "[399/00063] updated kl_weight: 0.05\n",
      "[399/00064] updated kl_weight: 0.05\n",
      "[399/00065] updated kl_weight: 0.05\n",
      "[399/00066] updated kl_weight: 0.05\n",
      "[399/00067] updated kl_weight: 0.05\n",
      "[399/00068] updated kl_weight: 0.05\n",
      "[399/00069] updated kl_weight: 0.05\n",
      "[399/00070] updated kl_weight: 0.05\n",
      "[399/00071] updated kl_weight: 0.05\n",
      "[399/00072] updated kl_weight: 0.05\n",
      "[399/00073] updated kl_weight: 0.05\n",
      "[399/00074] updated kl_weight: 0.05\n",
      "[399/00074] train_loss: 0.029163 kl_loss: 0.165070 normal_loss: 0.020909\n",
      "[399/00074] MMD 0.0050043403171002865\n",
      "[399/00074] TMD 0.06578873842954636\n",
      "[401/00024] train_loss: 0.028784 kl_loss: 0.164867 normal_loss: 0.020540\n",
      "[402/00049] train_loss: 0.028837 kl_loss: 0.164454 normal_loss: 0.020614\n",
      "[403/00074] train_loss: 0.028796 kl_loss: 0.164950 normal_loss: 0.020548\n",
      "[405/00024] train_loss: 0.028772 kl_loss: 0.164409 normal_loss: 0.020551\n",
      "[406/00049] train_loss: 0.028766 kl_loss: 0.164929 normal_loss: 0.020520\n",
      "[407/00074] train_loss: 0.028633 kl_loss: 0.164183 normal_loss: 0.020424\n",
      "[409/00024] train_loss: 0.028596 kl_loss: 0.164226 normal_loss: 0.020385\n",
      "[410/00049] train_loss: 0.028665 kl_loss: 0.164521 normal_loss: 0.020439\n",
      "[411/00074] train_loss: 0.028567 kl_loss: 0.164023 normal_loss: 0.020366\n",
      "[413/00024] train_loss: 0.028564 kl_loss: 0.163626 normal_loss: 0.020383\n",
      "[414/00049] train_loss: 0.028537 kl_loss: 0.164193 normal_loss: 0.020327\n",
      "[415/00074] train_loss: 0.028615 kl_loss: 0.164122 normal_loss: 0.020409\n",
      "[417/00024] train_loss: 0.028523 kl_loss: 0.163668 normal_loss: 0.020340\n",
      "[418/00049] train_loss: 0.028602 kl_loss: 0.163836 normal_loss: 0.020410\n",
      "[419/00074] train_loss: 0.028393 kl_loss: 0.163632 normal_loss: 0.020211\n",
      "[421/00024] train_loss: 0.028445 kl_loss: 0.163473 normal_loss: 0.020271\n",
      "[422/00049] train_loss: 0.028496 kl_loss: 0.163286 normal_loss: 0.020332\n",
      "[423/00074] train_loss: 0.028498 kl_loss: 0.163556 normal_loss: 0.020321\n",
      "[425/00024] train_loss: 0.028454 kl_loss: 0.163497 normal_loss: 0.020279\n",
      "[426/00049] train_loss: 0.028490 kl_loss: 0.163077 normal_loss: 0.020336\n",
      "[427/00074] train_loss: 0.028336 kl_loss: 0.162981 normal_loss: 0.020187\n",
      "[429/00024] train_loss: 0.028402 kl_loss: 0.163124 normal_loss: 0.020246\n",
      "[430/00049] train_loss: 0.028355 kl_loss: 0.162834 normal_loss: 0.020213\n",
      "[431/00074] train_loss: 0.028420 kl_loss: 0.162846 normal_loss: 0.020277\n",
      "[433/00024] train_loss: 0.028454 kl_loss: 0.162764 normal_loss: 0.020316\n",
      "[434/00049] train_loss: 0.028191 kl_loss: 0.162750 normal_loss: 0.020054\n",
      "[435/00074] train_loss: 0.028304 kl_loss: 0.162591 normal_loss: 0.020175\n",
      "[437/00024] train_loss: 0.028386 kl_loss: 0.162728 normal_loss: 0.020250\n",
      "[438/00049] train_loss: 0.028251 kl_loss: 0.162578 normal_loss: 0.020122\n",
      "[439/00074] train_loss: 0.028280 kl_loss: 0.162066 normal_loss: 0.020176\n",
      "[441/00024] train_loss: 0.028282 kl_loss: 0.162452 normal_loss: 0.020159\n",
      "[442/00049] train_loss: 0.028249 kl_loss: 0.162155 normal_loss: 0.020142\n",
      "[443/00074] train_loss: 0.028301 kl_loss: 0.162094 normal_loss: 0.020196\n",
      "[445/00024] train_loss: 0.028262 kl_loss: 0.162337 normal_loss: 0.020145\n",
      "[446/00049] train_loss: 0.028304 kl_loss: 0.162302 normal_loss: 0.020188\n",
      "[447/00074] train_loss: 0.028236 kl_loss: 0.161436 normal_loss: 0.020164\n",
      "[449/00024] train_loss: 0.028249 kl_loss: 0.162001 normal_loss: 0.020149\n",
      "[449/00074] MMD 0.005008353851735592\n",
      "[449/00074] TMD 0.05142264813184738\n",
      "[450/00049] train_loss: 0.028105 kl_loss: 0.161785 normal_loss: 0.020016\n",
      "[451/00074] train_loss: 0.028169 kl_loss: 0.161673 normal_loss: 0.020085\n",
      "[453/00024] train_loss: 0.028217 kl_loss: 0.162063 normal_loss: 0.020113\n",
      "[454/00049] train_loss: 0.028136 kl_loss: 0.161226 normal_loss: 0.020074\n",
      "[455/00074] train_loss: 0.028188 kl_loss: 0.161531 normal_loss: 0.020111\n",
      "[457/00024] train_loss: 0.028194 kl_loss: 0.161743 normal_loss: 0.020107\n",
      "[458/00049] train_loss: 0.028049 kl_loss: 0.160875 normal_loss: 0.020005\n",
      "[459/00074] train_loss: 0.028122 kl_loss: 0.161609 normal_loss: 0.020041\n",
      "[461/00024] train_loss: 0.028168 kl_loss: 0.161636 normal_loss: 0.020086\n",
      "[462/00049] train_loss: 0.028117 kl_loss: 0.161366 normal_loss: 0.020048\n",
      "[463/00074] train_loss: 0.028049 kl_loss: 0.160633 normal_loss: 0.020017\n",
      "[465/00024] train_loss: 0.027980 kl_loss: 0.160993 normal_loss: 0.019930\n",
      "[466/00049] train_loss: 0.028108 kl_loss: 0.161242 normal_loss: 0.020046\n",
      "[467/00074] train_loss: 0.028040 kl_loss: 0.160838 normal_loss: 0.019998\n",
      "[469/00024] train_loss: 0.028001 kl_loss: 0.160426 normal_loss: 0.019980\n",
      "[470/00049] train_loss: 0.028069 kl_loss: 0.161392 normal_loss: 0.019999\n",
      "[471/00074] train_loss: 0.027980 kl_loss: 0.160659 normal_loss: 0.019947\n",
      "[473/00024] train_loss: 0.028028 kl_loss: 0.161015 normal_loss: 0.019977\n",
      "[474/00049] train_loss: 0.027884 kl_loss: 0.160474 normal_loss: 0.019860\n",
      "[475/00074] train_loss: 0.027928 kl_loss: 0.160450 normal_loss: 0.019905\n",
      "[477/00024] train_loss: 0.027974 kl_loss: 0.160596 normal_loss: 0.019944\n",
      "[478/00049] train_loss: 0.027850 kl_loss: 0.160624 normal_loss: 0.019819\n",
      "[479/00074] train_loss: 0.027881 kl_loss: 0.160148 normal_loss: 0.019874\n",
      "[481/00024] train_loss: 0.027773 kl_loss: 0.160143 normal_loss: 0.019765\n",
      "[482/00049] train_loss: 0.027911 kl_loss: 0.160251 normal_loss: 0.019898\n",
      "[483/00074] train_loss: 0.028006 kl_loss: 0.160405 normal_loss: 0.019985\n",
      "[485/00024] train_loss: 0.027679 kl_loss: 0.159558 normal_loss: 0.019701\n",
      "[486/00049] train_loss: 0.028024 kl_loss: 0.160716 normal_loss: 0.019988\n",
      "[487/00074] train_loss: 0.027852 kl_loss: 0.159980 normal_loss: 0.019853\n",
      "[489/00024] train_loss: 0.027872 kl_loss: 0.159886 normal_loss: 0.019878\n",
      "[490/00049] train_loss: 0.027796 kl_loss: 0.159889 normal_loss: 0.019802\n",
      "[491/00074] train_loss: 0.027844 kl_loss: 0.160002 normal_loss: 0.019844\n",
      "[493/00024] train_loss: 0.027680 kl_loss: 0.159617 normal_loss: 0.019700\n",
      "[494/00049] train_loss: 0.027793 kl_loss: 0.159795 normal_loss: 0.019803\n",
      "[495/00074] train_loss: 0.027849 kl_loss: 0.159858 normal_loss: 0.019856\n",
      "[497/00024] train_loss: 0.027739 kl_loss: 0.159613 normal_loss: 0.019758\n",
      "[498/00049] train_loss: 0.027762 kl_loss: 0.159802 normal_loss: 0.019772\n",
      "[499/00000] updated kl_weight: 0.05\n",
      "[499/00001] updated kl_weight: 0.05\n",
      "[499/00002] updated kl_weight: 0.05\n",
      "[499/00003] updated kl_weight: 0.05\n",
      "[499/00004] updated kl_weight: 0.05\n",
      "[499/00005] updated kl_weight: 0.05\n",
      "[499/00006] updated kl_weight: 0.05\n",
      "[499/00007] updated kl_weight: 0.05\n",
      "[499/00008] updated kl_weight: 0.05\n",
      "[499/00009] updated kl_weight: 0.05\n",
      "[499/00010] updated kl_weight: 0.05\n",
      "[499/00011] updated kl_weight: 0.05\n",
      "[499/00012] updated kl_weight: 0.05\n",
      "[499/00013] updated kl_weight: 0.05\n",
      "[499/00014] updated kl_weight: 0.05\n",
      "[499/00015] updated kl_weight: 0.05\n",
      "[499/00016] updated kl_weight: 0.05\n",
      "[499/00017] updated kl_weight: 0.05\n",
      "[499/00018] updated kl_weight: 0.05\n",
      "[499/00019] updated kl_weight: 0.05\n",
      "[499/00020] updated kl_weight: 0.05\n",
      "[499/00021] updated kl_weight: 0.05\n",
      "[499/00022] updated kl_weight: 0.05\n",
      "[499/00023] updated kl_weight: 0.05\n",
      "[499/00024] updated kl_weight: 0.05\n",
      "[499/00025] updated kl_weight: 0.05\n",
      "[499/00026] updated kl_weight: 0.05\n",
      "[499/00027] updated kl_weight: 0.05\n",
      "[499/00028] updated kl_weight: 0.05\n",
      "[499/00029] updated kl_weight: 0.05\n",
      "[499/00030] updated kl_weight: 0.05\n",
      "[499/00031] updated kl_weight: 0.05\n",
      "[499/00032] updated kl_weight: 0.05\n",
      "[499/00033] updated kl_weight: 0.05\n",
      "[499/00034] updated kl_weight: 0.05\n",
      "[499/00035] updated kl_weight: 0.05\n",
      "[499/00036] updated kl_weight: 0.05\n",
      "[499/00037] updated kl_weight: 0.05\n",
      "[499/00038] updated kl_weight: 0.05\n",
      "[499/00039] updated kl_weight: 0.05\n",
      "[499/00040] updated kl_weight: 0.05\n",
      "[499/00041] updated kl_weight: 0.05\n",
      "[499/00042] updated kl_weight: 0.05\n",
      "[499/00043] updated kl_weight: 0.05\n",
      "[499/00044] updated kl_weight: 0.05\n",
      "[499/00045] updated kl_weight: 0.05\n",
      "[499/00046] updated kl_weight: 0.05\n",
      "[499/00047] updated kl_weight: 0.05\n",
      "[499/00048] updated kl_weight: 0.05\n",
      "[499/00049] updated kl_weight: 0.05\n",
      "[499/00050] updated kl_weight: 0.05\n",
      "[499/00051] updated kl_weight: 0.05\n",
      "[499/00052] updated kl_weight: 0.05\n",
      "[499/00053] updated kl_weight: 0.05\n",
      "[499/00054] updated kl_weight: 0.05\n",
      "[499/00055] updated kl_weight: 0.05\n",
      "[499/00056] updated kl_weight: 0.05\n",
      "[499/00057] updated kl_weight: 0.05\n",
      "[499/00058] updated kl_weight: 0.05\n",
      "[499/00059] updated kl_weight: 0.05\n",
      "[499/00060] updated kl_weight: 0.05\n",
      "[499/00061] updated kl_weight: 0.05\n",
      "[499/00062] updated kl_weight: 0.05\n",
      "[499/00063] updated kl_weight: 0.05\n",
      "[499/00064] updated kl_weight: 0.05\n",
      "[499/00065] updated kl_weight: 0.05\n",
      "[499/00066] updated kl_weight: 0.05\n",
      "[499/00067] updated kl_weight: 0.05\n",
      "[499/00068] updated kl_weight: 0.05\n",
      "[499/00069] updated kl_weight: 0.05\n",
      "[499/00070] updated kl_weight: 0.05\n",
      "[499/00071] updated kl_weight: 0.05\n",
      "[499/00072] updated kl_weight: 0.05\n",
      "[499/00073] updated kl_weight: 0.05\n",
      "[499/00074] updated kl_weight: 0.05\n",
      "[499/00074] train_loss: 0.027723 kl_loss: 0.159388 normal_loss: 0.019754\n",
      "[499/00074] MMD 0.0048997290432453156\n",
      "[499/00074] TMD 0.058455757796764374\n",
      "[501/00024] train_loss: 0.027730 kl_loss: 0.159951 normal_loss: 0.019733\n",
      "[502/00049] train_loss: 0.027486 kl_loss: 0.159157 normal_loss: 0.019528\n",
      "[503/00074] train_loss: 0.027550 kl_loss: 0.159270 normal_loss: 0.019587\n",
      "[505/00024] train_loss: 0.027516 kl_loss: 0.159329 normal_loss: 0.019549\n",
      "[506/00049] train_loss: 0.027461 kl_loss: 0.158707 normal_loss: 0.019525\n",
      "[507/00074] train_loss: 0.027654 kl_loss: 0.159993 normal_loss: 0.019654\n",
      "[509/00024] train_loss: 0.027530 kl_loss: 0.159003 normal_loss: 0.019579\n",
      "[510/00049] train_loss: 0.027525 kl_loss: 0.159765 normal_loss: 0.019537\n",
      "[511/00074] train_loss: 0.027426 kl_loss: 0.158912 normal_loss: 0.019481\n",
      "[513/00024] train_loss: 0.027432 kl_loss: 0.158847 normal_loss: 0.019490\n",
      "[514/00049] train_loss: 0.027611 kl_loss: 0.159720 normal_loss: 0.019625\n",
      "[515/00074] train_loss: 0.027444 kl_loss: 0.158740 normal_loss: 0.019507\n",
      "[517/00024] train_loss: 0.027510 kl_loss: 0.158681 normal_loss: 0.019576\n",
      "[518/00049] train_loss: 0.027502 kl_loss: 0.159040 normal_loss: 0.019550\n",
      "[519/00074] train_loss: 0.027445 kl_loss: 0.159241 normal_loss: 0.019483\n",
      "[521/00024] train_loss: 0.027425 kl_loss: 0.159230 normal_loss: 0.019464\n",
      "[522/00049] train_loss: 0.027465 kl_loss: 0.158762 normal_loss: 0.019527\n",
      "[523/00074] train_loss: 0.027409 kl_loss: 0.158606 normal_loss: 0.019478\n",
      "[525/00024] train_loss: 0.027347 kl_loss: 0.158677 normal_loss: 0.019413\n",
      "[526/00049] train_loss: 0.027497 kl_loss: 0.159229 normal_loss: 0.019536\n",
      "[527/00074] train_loss: 0.027412 kl_loss: 0.158351 normal_loss: 0.019494\n",
      "[529/00024] train_loss: 0.027335 kl_loss: 0.158419 normal_loss: 0.019414\n",
      "[530/00049] train_loss: 0.027445 kl_loss: 0.158940 normal_loss: 0.019498\n",
      "[531/00074] train_loss: 0.027350 kl_loss: 0.158554 normal_loss: 0.019423\n",
      "[533/00024] train_loss: 0.027424 kl_loss: 0.158326 normal_loss: 0.019507\n",
      "[534/00049] train_loss: 0.027393 kl_loss: 0.158609 normal_loss: 0.019462\n",
      "[535/00074] train_loss: 0.027317 kl_loss: 0.158639 normal_loss: 0.019385\n",
      "[537/00024] train_loss: 0.027449 kl_loss: 0.158804 normal_loss: 0.019509\n",
      "[538/00049] train_loss: 0.027143 kl_loss: 0.157405 normal_loss: 0.019273\n",
      "[539/00074] train_loss: 0.027553 kl_loss: 0.158998 normal_loss: 0.019603\n",
      "[541/00024] train_loss: 0.027296 kl_loss: 0.158153 normal_loss: 0.019388\n",
      "[542/00049] train_loss: 0.027453 kl_loss: 0.158580 normal_loss: 0.019524\n",
      "[543/00074] train_loss: 0.027362 kl_loss: 0.158156 normal_loss: 0.019454\n",
      "[545/00024] train_loss: 0.027385 kl_loss: 0.158273 normal_loss: 0.019471\n",
      "[546/00049] train_loss: 0.027237 kl_loss: 0.157858 normal_loss: 0.019344\n",
      "[547/00074] train_loss: 0.027250 kl_loss: 0.158438 normal_loss: 0.019328\n",
      "[549/00024] train_loss: 0.027323 kl_loss: 0.158016 normal_loss: 0.019423\n",
      "[549/00074] MMD 0.0049570705741643906\n",
      "[549/00074] TMD 0.05959930643439293\n",
      "[550/00049] train_loss: 0.027272 kl_loss: 0.158343 normal_loss: 0.019355\n",
      "[551/00074] train_loss: 0.027322 kl_loss: 0.157857 normal_loss: 0.019429\n",
      "[553/00024] train_loss: 0.027302 kl_loss: 0.158302 normal_loss: 0.019387\n",
      "[554/00049] train_loss: 0.027269 kl_loss: 0.157728 normal_loss: 0.019383\n",
      "[555/00074] train_loss: 0.027259 kl_loss: 0.157877 normal_loss: 0.019365\n",
      "[557/00024] train_loss: 0.027319 kl_loss: 0.157722 normal_loss: 0.019433\n",
      "[558/00049] train_loss: 0.027211 kl_loss: 0.158353 normal_loss: 0.019294\n",
      "[559/00074] train_loss: 0.027186 kl_loss: 0.157509 normal_loss: 0.019311\n",
      "[561/00024] train_loss: 0.027218 kl_loss: 0.158092 normal_loss: 0.019313\n",
      "[562/00049] train_loss: 0.027190 kl_loss: 0.157421 normal_loss: 0.019319\n",
      "[563/00074] train_loss: 0.027306 kl_loss: 0.157739 normal_loss: 0.019419\n",
      "[565/00024] train_loss: 0.027188 kl_loss: 0.157571 normal_loss: 0.019309\n",
      "[566/00049] train_loss: 0.027186 kl_loss: 0.157661 normal_loss: 0.019303\n",
      "[567/00074] train_loss: 0.027232 kl_loss: 0.157712 normal_loss: 0.019347\n",
      "[569/00024] train_loss: 0.027220 kl_loss: 0.157598 normal_loss: 0.019341\n",
      "[570/00049] train_loss: 0.027252 kl_loss: 0.157787 normal_loss: 0.019363\n",
      "[571/00074] train_loss: 0.027174 kl_loss: 0.157242 normal_loss: 0.019312\n",
      "[573/00024] train_loss: 0.027286 kl_loss: 0.157835 normal_loss: 0.019395\n",
      "[574/00049] train_loss: 0.027078 kl_loss: 0.156958 normal_loss: 0.019230\n",
      "[575/00074] train_loss: 0.027142 kl_loss: 0.157522 normal_loss: 0.019265\n",
      "[577/00024] train_loss: 0.027114 kl_loss: 0.157485 normal_loss: 0.019240\n",
      "[578/00049] train_loss: 0.027115 kl_loss: 0.157269 normal_loss: 0.019251\n",
      "[579/00074] train_loss: 0.027103 kl_loss: 0.157230 normal_loss: 0.019242\n",
      "[581/00024] train_loss: 0.027277 kl_loss: 0.157594 normal_loss: 0.019398\n",
      "[582/00049] train_loss: 0.027025 kl_loss: 0.156704 normal_loss: 0.019190\n",
      "[583/00074] train_loss: 0.027226 kl_loss: 0.157388 normal_loss: 0.019357\n",
      "[585/00024] train_loss: 0.027096 kl_loss: 0.157196 normal_loss: 0.019236\n",
      "[586/00049] train_loss: 0.027082 kl_loss: 0.156908 normal_loss: 0.019237\n",
      "[587/00074] train_loss: 0.027077 kl_loss: 0.157285 normal_loss: 0.019213\n",
      "[589/00024] train_loss: 0.027116 kl_loss: 0.157379 normal_loss: 0.019247\n",
      "[590/00049] train_loss: 0.027057 kl_loss: 0.156788 normal_loss: 0.019218\n",
      "[591/00074] train_loss: 0.027123 kl_loss: 0.156902 normal_loss: 0.019278\n",
      "[593/00024] train_loss: 0.027064 kl_loss: 0.156971 normal_loss: 0.019216\n",
      "[594/00049] train_loss: 0.027030 kl_loss: 0.156735 normal_loss: 0.019193\n",
      "[595/00074] train_loss: 0.027024 kl_loss: 0.157076 normal_loss: 0.019171\n",
      "[597/00024] train_loss: 0.027134 kl_loss: 0.157071 normal_loss: 0.019280\n",
      "[598/00049] train_loss: 0.027052 kl_loss: 0.156452 normal_loss: 0.019230\n",
      "[599/00000] updated kl_weight: 0.05\n",
      "[599/00001] updated kl_weight: 0.05\n",
      "[599/00002] updated kl_weight: 0.05\n",
      "[599/00003] updated kl_weight: 0.05\n",
      "[599/00004] updated kl_weight: 0.05\n",
      "[599/00005] updated kl_weight: 0.05\n",
      "[599/00006] updated kl_weight: 0.05\n",
      "[599/00007] updated kl_weight: 0.05\n",
      "[599/00008] updated kl_weight: 0.05\n",
      "[599/00009] updated kl_weight: 0.05\n",
      "[599/00010] updated kl_weight: 0.05\n",
      "[599/00011] updated kl_weight: 0.05\n",
      "[599/00012] updated kl_weight: 0.05\n",
      "[599/00013] updated kl_weight: 0.05\n",
      "[599/00014] updated kl_weight: 0.05\n",
      "[599/00015] updated kl_weight: 0.05\n",
      "[599/00016] updated kl_weight: 0.05\n",
      "[599/00017] updated kl_weight: 0.05\n",
      "[599/00018] updated kl_weight: 0.05\n",
      "[599/00019] updated kl_weight: 0.05\n",
      "[599/00020] updated kl_weight: 0.05\n",
      "[599/00021] updated kl_weight: 0.05\n",
      "[599/00022] updated kl_weight: 0.05\n",
      "[599/00023] updated kl_weight: 0.05\n",
      "[599/00024] updated kl_weight: 0.05\n",
      "[599/00025] updated kl_weight: 0.05\n",
      "[599/00026] updated kl_weight: 0.05\n",
      "[599/00027] updated kl_weight: 0.05\n",
      "[599/00028] updated kl_weight: 0.05\n",
      "[599/00029] updated kl_weight: 0.05\n",
      "[599/00030] updated kl_weight: 0.05\n",
      "[599/00031] updated kl_weight: 0.05\n",
      "[599/00032] updated kl_weight: 0.05\n",
      "[599/00033] updated kl_weight: 0.05\n",
      "[599/00034] updated kl_weight: 0.05\n",
      "[599/00035] updated kl_weight: 0.05\n",
      "[599/00036] updated kl_weight: 0.05\n",
      "[599/00037] updated kl_weight: 0.05\n",
      "[599/00038] updated kl_weight: 0.05\n",
      "[599/00039] updated kl_weight: 0.05\n",
      "[599/00040] updated kl_weight: 0.05\n",
      "[599/00041] updated kl_weight: 0.05\n",
      "[599/00042] updated kl_weight: 0.05\n",
      "[599/00043] updated kl_weight: 0.05\n",
      "[599/00044] updated kl_weight: 0.05\n",
      "[599/00045] updated kl_weight: 0.05\n",
      "[599/00046] updated kl_weight: 0.05\n",
      "[599/00047] updated kl_weight: 0.05\n",
      "[599/00048] updated kl_weight: 0.05\n",
      "[599/00049] updated kl_weight: 0.05\n",
      "[599/00050] updated kl_weight: 0.05\n",
      "[599/00051] updated kl_weight: 0.05\n",
      "[599/00052] updated kl_weight: 0.05\n",
      "[599/00053] updated kl_weight: 0.05\n",
      "[599/00054] updated kl_weight: 0.05\n",
      "[599/00055] updated kl_weight: 0.05\n",
      "[599/00056] updated kl_weight: 0.05\n",
      "[599/00057] updated kl_weight: 0.05\n",
      "[599/00058] updated kl_weight: 0.05\n",
      "[599/00059] updated kl_weight: 0.05\n",
      "[599/00060] updated kl_weight: 0.05\n",
      "[599/00061] updated kl_weight: 0.05\n",
      "[599/00062] updated kl_weight: 0.05\n",
      "[599/00063] updated kl_weight: 0.05\n",
      "[599/00064] updated kl_weight: 0.05\n",
      "[599/00065] updated kl_weight: 0.05\n",
      "[599/00066] updated kl_weight: 0.05\n",
      "[599/00067] updated kl_weight: 0.05\n",
      "[599/00068] updated kl_weight: 0.05\n",
      "[599/00069] updated kl_weight: 0.05\n",
      "[599/00070] updated kl_weight: 0.05\n",
      "[599/00071] updated kl_weight: 0.05\n",
      "[599/00072] updated kl_weight: 0.05\n",
      "[599/00073] updated kl_weight: 0.05\n",
      "[599/00074] updated kl_weight: 0.05\n",
      "[599/00074] train_loss: 0.027076 kl_loss: 0.156973 normal_loss: 0.019228\n",
      "[599/00074] MMD 0.005250429268926382\n",
      "[599/00074] TMD 0.060155920684337616\n",
      "[601/00024] train_loss: 0.026896 kl_loss: 0.156291 normal_loss: 0.019082\n",
      "[602/00049] train_loss: 0.027186 kl_loss: 0.157590 normal_loss: 0.019306\n",
      "[603/00074] train_loss: 0.026867 kl_loss: 0.156379 normal_loss: 0.019048\n",
      "[605/00024] train_loss: 0.026968 kl_loss: 0.156563 normal_loss: 0.019140\n",
      "[606/00049] train_loss: 0.026906 kl_loss: 0.156965 normal_loss: 0.019058\n",
      "[607/00074] train_loss: 0.026902 kl_loss: 0.156553 normal_loss: 0.019074\n",
      "[609/00024] train_loss: 0.026986 kl_loss: 0.156822 normal_loss: 0.019145\n",
      "[610/00049] train_loss: 0.026896 kl_loss: 0.156664 normal_loss: 0.019063\n",
      "[611/00074] train_loss: 0.026933 kl_loss: 0.156412 normal_loss: 0.019112\n",
      "[613/00024] train_loss: 0.026858 kl_loss: 0.156393 normal_loss: 0.019039\n",
      "[614/00049] train_loss: 0.026959 kl_loss: 0.156758 normal_loss: 0.019122\n",
      "[615/00074] train_loss: 0.026966 kl_loss: 0.156566 normal_loss: 0.019138\n",
      "[617/00024] train_loss: 0.026973 kl_loss: 0.156549 normal_loss: 0.019146\n",
      "[618/00049] train_loss: 0.026828 kl_loss: 0.156184 normal_loss: 0.019019\n",
      "[619/00074] train_loss: 0.026965 kl_loss: 0.156819 normal_loss: 0.019124\n",
      "[621/00024] train_loss: 0.026843 kl_loss: 0.156275 normal_loss: 0.019029\n",
      "[622/00049] train_loss: 0.027008 kl_loss: 0.156776 normal_loss: 0.019169\n",
      "[623/00074] train_loss: 0.026863 kl_loss: 0.156336 normal_loss: 0.019046\n",
      "[625/00024] train_loss: 0.026907 kl_loss: 0.156348 normal_loss: 0.019090\n",
      "[626/00049] train_loss: 0.026857 kl_loss: 0.156365 normal_loss: 0.019039\n",
      "[627/00074] train_loss: 0.026869 kl_loss: 0.156510 normal_loss: 0.019044\n",
      "[629/00024] train_loss: 0.026889 kl_loss: 0.156795 normal_loss: 0.019050\n",
      "[630/00049] train_loss: 0.026882 kl_loss: 0.156187 normal_loss: 0.019073\n",
      "[631/00074] train_loss: 0.026906 kl_loss: 0.156056 normal_loss: 0.019103\n",
      "[633/00024] train_loss: 0.026858 kl_loss: 0.156233 normal_loss: 0.019046\n",
      "[634/00049] train_loss: 0.026849 kl_loss: 0.156028 normal_loss: 0.019048\n",
      "[635/00074] train_loss: 0.026925 kl_loss: 0.156609 normal_loss: 0.019094\n",
      "[637/00024] train_loss: 0.026955 kl_loss: 0.156443 normal_loss: 0.019132\n",
      "[638/00049] train_loss: 0.026808 kl_loss: 0.155965 normal_loss: 0.019010\n",
      "[639/00074] train_loss: 0.026880 kl_loss: 0.156295 normal_loss: 0.019066\n",
      "[641/00024] train_loss: 0.026910 kl_loss: 0.156393 normal_loss: 0.019091\n",
      "[642/00049] train_loss: 0.026694 kl_loss: 0.155883 normal_loss: 0.018900\n",
      "[643/00074] train_loss: 0.026928 kl_loss: 0.156262 normal_loss: 0.019115\n",
      "[645/00024] train_loss: 0.026863 kl_loss: 0.156235 normal_loss: 0.019052\n",
      "[646/00049] train_loss: 0.026920 kl_loss: 0.156452 normal_loss: 0.019098\n",
      "[647/00074] train_loss: 0.026782 kl_loss: 0.155674 normal_loss: 0.018999\n",
      "[649/00024] train_loss: 0.026756 kl_loss: 0.155618 normal_loss: 0.018975\n",
      "[649/00074] MMD 0.004930701572448015\n",
      "[649/00074] TMD 0.0652347058057785\n",
      "[650/00049] train_loss: 0.026916 kl_loss: 0.156639 normal_loss: 0.019084\n",
      "[651/00074] train_loss: 0.026858 kl_loss: 0.155947 normal_loss: 0.019061\n",
      "[653/00024] train_loss: 0.026818 kl_loss: 0.155542 normal_loss: 0.019040\n",
      "[654/00049] train_loss: 0.026877 kl_loss: 0.156547 normal_loss: 0.019050\n",
      "[655/00074] train_loss: 0.026783 kl_loss: 0.155951 normal_loss: 0.018985\n",
      "[657/00024] train_loss: 0.026781 kl_loss: 0.155767 normal_loss: 0.018993\n",
      "[658/00049] train_loss: 0.026725 kl_loss: 0.155717 normal_loss: 0.018939\n",
      "[659/00074] train_loss: 0.026799 kl_loss: 0.156381 normal_loss: 0.018980\n",
      "[661/00024] train_loss: 0.026753 kl_loss: 0.155773 normal_loss: 0.018964\n",
      "[662/00049] train_loss: 0.026851 kl_loss: 0.156415 normal_loss: 0.019030\n",
      "[663/00074] train_loss: 0.026785 kl_loss: 0.155487 normal_loss: 0.019011\n",
      "[665/00024] train_loss: 0.026819 kl_loss: 0.155970 normal_loss: 0.019020\n",
      "[666/00049] train_loss: 0.026689 kl_loss: 0.155602 normal_loss: 0.018908\n",
      "[667/00074] train_loss: 0.026833 kl_loss: 0.155946 normal_loss: 0.019036\n",
      "[669/00024] train_loss: 0.026844 kl_loss: 0.156078 normal_loss: 0.019040\n",
      "[670/00049] train_loss: 0.026758 kl_loss: 0.155358 normal_loss: 0.018990\n",
      "[671/00074] train_loss: 0.026823 kl_loss: 0.155937 normal_loss: 0.019026\n",
      "[673/00024] train_loss: 0.026799 kl_loss: 0.155907 normal_loss: 0.019004\n",
      "[674/00049] train_loss: 0.026770 kl_loss: 0.155896 normal_loss: 0.018975\n",
      "[675/00074] train_loss: 0.026790 kl_loss: 0.155411 normal_loss: 0.019019\n",
      "[677/00024] train_loss: 0.026760 kl_loss: 0.155610 normal_loss: 0.018980\n",
      "[678/00049] train_loss: 0.026689 kl_loss: 0.155731 normal_loss: 0.018902\n",
      "[679/00074] train_loss: 0.026797 kl_loss: 0.155720 normal_loss: 0.019011\n",
      "[681/00024] train_loss: 0.026686 kl_loss: 0.155624 normal_loss: 0.018904\n",
      "[682/00049] train_loss: 0.026824 kl_loss: 0.155625 normal_loss: 0.019042\n",
      "[683/00074] train_loss: 0.026826 kl_loss: 0.155650 normal_loss: 0.019043\n",
      "[685/00024] train_loss: 0.026728 kl_loss: 0.155600 normal_loss: 0.018948\n",
      "[686/00049] train_loss: 0.026787 kl_loss: 0.155487 normal_loss: 0.019013\n",
      "[687/00074] train_loss: 0.026754 kl_loss: 0.155667 normal_loss: 0.018971\n",
      "[689/00024] train_loss: 0.026639 kl_loss: 0.155496 normal_loss: 0.018864\n",
      "[690/00049] train_loss: 0.026771 kl_loss: 0.155400 normal_loss: 0.019001\n",
      "[691/00074] train_loss: 0.026778 kl_loss: 0.155710 normal_loss: 0.018993\n",
      "[693/00024] train_loss: 0.026724 kl_loss: 0.155484 normal_loss: 0.018950\n",
      "[694/00049] train_loss: 0.026595 kl_loss: 0.155069 normal_loss: 0.018841\n",
      "[695/00074] train_loss: 0.026842 kl_loss: 0.155902 normal_loss: 0.019047\n",
      "[697/00024] train_loss: 0.026757 kl_loss: 0.155468 normal_loss: 0.018984\n",
      "[698/00049] train_loss: 0.026602 kl_loss: 0.155540 normal_loss: 0.018825\n",
      "[699/00000] updated kl_weight: 0.05\n",
      "[699/00001] updated kl_weight: 0.05\n",
      "[699/00002] updated kl_weight: 0.05\n",
      "[699/00003] updated kl_weight: 0.05\n",
      "[699/00004] updated kl_weight: 0.05\n",
      "[699/00005] updated kl_weight: 0.05\n",
      "[699/00006] updated kl_weight: 0.05\n",
      "[699/00007] updated kl_weight: 0.05\n",
      "[699/00008] updated kl_weight: 0.05\n",
      "[699/00009] updated kl_weight: 0.05\n",
      "[699/00010] updated kl_weight: 0.05\n",
      "[699/00011] updated kl_weight: 0.05\n",
      "[699/00012] updated kl_weight: 0.05\n",
      "[699/00013] updated kl_weight: 0.05\n",
      "[699/00014] updated kl_weight: 0.05\n",
      "[699/00015] updated kl_weight: 0.05\n",
      "[699/00016] updated kl_weight: 0.05\n",
      "[699/00017] updated kl_weight: 0.05\n",
      "[699/00018] updated kl_weight: 0.05\n",
      "[699/00019] updated kl_weight: 0.05\n",
      "[699/00020] updated kl_weight: 0.05\n",
      "[699/00021] updated kl_weight: 0.05\n",
      "[699/00022] updated kl_weight: 0.05\n",
      "[699/00023] updated kl_weight: 0.05\n",
      "[699/00024] updated kl_weight: 0.05\n",
      "[699/00025] updated kl_weight: 0.05\n",
      "[699/00026] updated kl_weight: 0.05\n",
      "[699/00027] updated kl_weight: 0.05\n",
      "[699/00028] updated kl_weight: 0.05\n",
      "[699/00029] updated kl_weight: 0.05\n",
      "[699/00030] updated kl_weight: 0.05\n",
      "[699/00031] updated kl_weight: 0.05\n",
      "[699/00032] updated kl_weight: 0.05\n",
      "[699/00033] updated kl_weight: 0.05\n",
      "[699/00034] updated kl_weight: 0.05\n",
      "[699/00035] updated kl_weight: 0.05\n",
      "[699/00036] updated kl_weight: 0.05\n",
      "[699/00037] updated kl_weight: 0.05\n",
      "[699/00038] updated kl_weight: 0.05\n",
      "[699/00039] updated kl_weight: 0.05\n",
      "[699/00040] updated kl_weight: 0.05\n",
      "[699/00041] updated kl_weight: 0.05\n",
      "[699/00042] updated kl_weight: 0.05\n",
      "[699/00043] updated kl_weight: 0.05\n",
      "[699/00044] updated kl_weight: 0.05\n",
      "[699/00045] updated kl_weight: 0.05\n",
      "[699/00046] updated kl_weight: 0.05\n",
      "[699/00047] updated kl_weight: 0.05\n",
      "[699/00048] updated kl_weight: 0.05\n",
      "[699/00049] updated kl_weight: 0.05\n",
      "[699/00050] updated kl_weight: 0.05\n",
      "[699/00051] updated kl_weight: 0.05\n",
      "[699/00052] updated kl_weight: 0.05\n",
      "[699/00053] updated kl_weight: 0.05\n",
      "[699/00054] updated kl_weight: 0.05\n",
      "[699/00055] updated kl_weight: 0.05\n",
      "[699/00056] updated kl_weight: 0.05\n",
      "[699/00057] updated kl_weight: 0.05\n",
      "[699/00058] updated kl_weight: 0.05\n",
      "[699/00059] updated kl_weight: 0.05\n",
      "[699/00060] updated kl_weight: 0.05\n",
      "[699/00061] updated kl_weight: 0.05\n",
      "[699/00062] updated kl_weight: 0.05\n",
      "[699/00063] updated kl_weight: 0.05\n",
      "[699/00064] updated kl_weight: 0.05\n",
      "[699/00065] updated kl_weight: 0.05\n",
      "[699/00066] updated kl_weight: 0.05\n",
      "[699/00067] updated kl_weight: 0.05\n",
      "[699/00068] updated kl_weight: 0.05\n",
      "[699/00069] updated kl_weight: 0.05\n",
      "[699/00070] updated kl_weight: 0.05\n",
      "[699/00071] updated kl_weight: 0.05\n",
      "[699/00072] updated kl_weight: 0.05\n",
      "[699/00073] updated kl_weight: 0.05\n",
      "[699/00074] updated kl_weight: 0.05\n",
      "[699/00074] train_loss: 0.026763 kl_loss: 0.155295 normal_loss: 0.018998\n",
      "[699/00074] MMD 0.005130420438945293\n",
      "[699/00074] TMD 0.053835660219192505\n",
      "[701/00024] train_loss: 0.026696 kl_loss: 0.155284 normal_loss: 0.018932\n",
      "[702/00049] train_loss: 0.026933 kl_loss: 0.156171 normal_loss: 0.019125\n",
      "[703/00074] train_loss: 0.026584 kl_loss: 0.154730 normal_loss: 0.018848\n",
      "[705/00024] train_loss: 0.026692 kl_loss: 0.155396 normal_loss: 0.018922\n",
      "[706/00049] train_loss: 0.026550 kl_loss: 0.154877 normal_loss: 0.018807\n",
      "[707/00074] train_loss: 0.026717 kl_loss: 0.155836 normal_loss: 0.018925\n",
      "[709/00024] train_loss: 0.026664 kl_loss: 0.155283 normal_loss: 0.018900\n",
      "[710/00049] train_loss: 0.026594 kl_loss: 0.155195 normal_loss: 0.018834\n",
      "[711/00074] train_loss: 0.026661 kl_loss: 0.155544 normal_loss: 0.018884\n",
      "[713/00024] train_loss: 0.026701 kl_loss: 0.155193 normal_loss: 0.018941\n",
      "[714/00049] train_loss: 0.026694 kl_loss: 0.155555 normal_loss: 0.018916\n",
      "[715/00074] train_loss: 0.026709 kl_loss: 0.155191 normal_loss: 0.018950\n",
      "[717/00024] train_loss: 0.026600 kl_loss: 0.155123 normal_loss: 0.018844\n",
      "[718/00049] train_loss: 0.026764 kl_loss: 0.155792 normal_loss: 0.018974\n",
      "[719/00074] train_loss: 0.026628 kl_loss: 0.154948 normal_loss: 0.018881\n",
      "[721/00024] train_loss: 0.026713 kl_loss: 0.155643 normal_loss: 0.018931\n",
      "[722/00049] train_loss: 0.026626 kl_loss: 0.154810 normal_loss: 0.018885\n",
      "[723/00074] train_loss: 0.026622 kl_loss: 0.155338 normal_loss: 0.018856\n",
      "[725/00024] train_loss: 0.026536 kl_loss: 0.154934 normal_loss: 0.018789\n",
      "[726/00049] train_loss: 0.026732 kl_loss: 0.155730 normal_loss: 0.018946\n",
      "[727/00074] train_loss: 0.026599 kl_loss: 0.155044 normal_loss: 0.018847\n",
      "[729/00024] train_loss: 0.026755 kl_loss: 0.155710 normal_loss: 0.018969\n",
      "[730/00049] train_loss: 0.026616 kl_loss: 0.155014 normal_loss: 0.018865\n",
      "[731/00074] train_loss: 0.026573 kl_loss: 0.154900 normal_loss: 0.018828\n",
      "[733/00024] train_loss: 0.026636 kl_loss: 0.155379 normal_loss: 0.018867\n",
      "[734/00049] train_loss: 0.026561 kl_loss: 0.154714 normal_loss: 0.018826\n",
      "[735/00074] train_loss: 0.026643 kl_loss: 0.155456 normal_loss: 0.018870\n",
      "[737/00024] train_loss: 0.026578 kl_loss: 0.154915 normal_loss: 0.018832\n",
      "[738/00049] train_loss: 0.026600 kl_loss: 0.155350 normal_loss: 0.018833\n",
      "[739/00074] train_loss: 0.026630 kl_loss: 0.155200 normal_loss: 0.018870\n",
      "[741/00024] train_loss: 0.026627 kl_loss: 0.155249 normal_loss: 0.018864\n",
      "[742/00049] train_loss: 0.026625 kl_loss: 0.154861 normal_loss: 0.018882\n",
      "[743/00074] train_loss: 0.026680 kl_loss: 0.155274 normal_loss: 0.018916\n",
      "[745/00024] train_loss: 0.026601 kl_loss: 0.155080 normal_loss: 0.018847\n",
      "[746/00049] train_loss: 0.026648 kl_loss: 0.154937 normal_loss: 0.018901\n",
      "[747/00074] train_loss: 0.026591 kl_loss: 0.155296 normal_loss: 0.018826\n",
      "[749/00024] train_loss: 0.026537 kl_loss: 0.155151 normal_loss: 0.018780\n",
      "[749/00074] MMD 0.005028392653912306\n",
      "[749/00074] TMD 0.05772531405091286\n",
      "[750/00049] train_loss: 0.026545 kl_loss: 0.155037 normal_loss: 0.018793\n",
      "[751/00074] train_loss: 0.026643 kl_loss: 0.155042 normal_loss: 0.018891\n",
      "[753/00024] train_loss: 0.026579 kl_loss: 0.155066 normal_loss: 0.018826\n",
      "[754/00049] train_loss: 0.026560 kl_loss: 0.155039 normal_loss: 0.018808\n",
      "[755/00074] train_loss: 0.026567 kl_loss: 0.155044 normal_loss: 0.018815\n",
      "[757/00024] train_loss: 0.026613 kl_loss: 0.155081 normal_loss: 0.018859\n",
      "[758/00049] train_loss: 0.026549 kl_loss: 0.154411 normal_loss: 0.018829\n",
      "[759/00074] train_loss: 0.026642 kl_loss: 0.155575 normal_loss: 0.018864\n",
      "[761/00024] train_loss: 0.026586 kl_loss: 0.154960 normal_loss: 0.018838\n",
      "[762/00049] train_loss: 0.026646 kl_loss: 0.155099 normal_loss: 0.018891\n",
      "[763/00074] train_loss: 0.026579 kl_loss: 0.154931 normal_loss: 0.018833\n",
      "[765/00024] train_loss: 0.026577 kl_loss: 0.155135 normal_loss: 0.018821\n",
      "[766/00049] train_loss: 0.026616 kl_loss: 0.155243 normal_loss: 0.018854\n",
      "[767/00074] train_loss: 0.026439 kl_loss: 0.154531 normal_loss: 0.018713\n",
      "[769/00024] train_loss: 0.026689 kl_loss: 0.155275 normal_loss: 0.018925\n",
      "[770/00049] train_loss: 0.026610 kl_loss: 0.154637 normal_loss: 0.018878\n",
      "[771/00074] train_loss: 0.026571 kl_loss: 0.154916 normal_loss: 0.018825\n",
      "[773/00024] train_loss: 0.026532 kl_loss: 0.154799 normal_loss: 0.018793\n",
      "[774/00049] train_loss: 0.026578 kl_loss: 0.155202 normal_loss: 0.018818\n",
      "[775/00074] train_loss: 0.026586 kl_loss: 0.154753 normal_loss: 0.018849\n",
      "[777/00024] train_loss: 0.026537 kl_loss: 0.154997 normal_loss: 0.018787\n",
      "[778/00049] train_loss: 0.026642 kl_loss: 0.155015 normal_loss: 0.018892\n",
      "[779/00074] train_loss: 0.026479 kl_loss: 0.154657 normal_loss: 0.018746\n",
      "[781/00024] train_loss: 0.026554 kl_loss: 0.154659 normal_loss: 0.018821\n",
      "[782/00049] train_loss: 0.026650 kl_loss: 0.155257 normal_loss: 0.018887\n",
      "[783/00074] train_loss: 0.026522 kl_loss: 0.154671 normal_loss: 0.018789\n",
      "[785/00024] train_loss: 0.026657 kl_loss: 0.154678 normal_loss: 0.018923\n",
      "[786/00049] train_loss: 0.026640 kl_loss: 0.155238 normal_loss: 0.018878\n",
      "[787/00074] train_loss: 0.026545 kl_loss: 0.154596 normal_loss: 0.018815\n",
      "[789/00024] train_loss: 0.026555 kl_loss: 0.155012 normal_loss: 0.018805\n",
      "[790/00049] train_loss: 0.026576 kl_loss: 0.154668 normal_loss: 0.018843\n",
      "[791/00074] train_loss: 0.026533 kl_loss: 0.154758 normal_loss: 0.018795\n",
      "[793/00024] train_loss: 0.026616 kl_loss: 0.155142 normal_loss: 0.018859\n",
      "[794/00049] train_loss: 0.026465 kl_loss: 0.154862 normal_loss: 0.018721\n",
      "[795/00074] train_loss: 0.026403 kl_loss: 0.154356 normal_loss: 0.018685\n",
      "[797/00024] train_loss: 0.026546 kl_loss: 0.154701 normal_loss: 0.018811\n",
      "[798/00049] train_loss: 0.026565 kl_loss: 0.155073 normal_loss: 0.018811\n",
      "[799/00000] updated kl_weight: 0.05\n",
      "[799/00001] updated kl_weight: 0.05\n",
      "[799/00002] updated kl_weight: 0.05\n",
      "[799/00003] updated kl_weight: 0.05\n",
      "[799/00004] updated kl_weight: 0.05\n",
      "[799/00005] updated kl_weight: 0.05\n",
      "[799/00006] updated kl_weight: 0.05\n",
      "[799/00007] updated kl_weight: 0.05\n",
      "[799/00008] updated kl_weight: 0.05\n",
      "[799/00009] updated kl_weight: 0.05\n",
      "[799/00010] updated kl_weight: 0.05\n",
      "[799/00011] updated kl_weight: 0.05\n",
      "[799/00012] updated kl_weight: 0.05\n",
      "[799/00013] updated kl_weight: 0.05\n",
      "[799/00014] updated kl_weight: 0.05\n",
      "[799/00015] updated kl_weight: 0.05\n",
      "[799/00016] updated kl_weight: 0.05\n",
      "[799/00017] updated kl_weight: 0.05\n",
      "[799/00018] updated kl_weight: 0.05\n",
      "[799/00019] updated kl_weight: 0.05\n",
      "[799/00020] updated kl_weight: 0.05\n",
      "[799/00021] updated kl_weight: 0.05\n",
      "[799/00022] updated kl_weight: 0.05\n",
      "[799/00023] updated kl_weight: 0.05\n",
      "[799/00024] updated kl_weight: 0.05\n",
      "[799/00025] updated kl_weight: 0.05\n",
      "[799/00026] updated kl_weight: 0.05\n",
      "[799/00027] updated kl_weight: 0.05\n",
      "[799/00028] updated kl_weight: 0.05\n",
      "[799/00029] updated kl_weight: 0.05\n",
      "[799/00030] updated kl_weight: 0.05\n",
      "[799/00031] updated kl_weight: 0.05\n",
      "[799/00032] updated kl_weight: 0.05\n",
      "[799/00033] updated kl_weight: 0.05\n",
      "[799/00034] updated kl_weight: 0.05\n",
      "[799/00035] updated kl_weight: 0.05\n",
      "[799/00036] updated kl_weight: 0.05\n",
      "[799/00037] updated kl_weight: 0.05\n",
      "[799/00038] updated kl_weight: 0.05\n",
      "[799/00039] updated kl_weight: 0.05\n",
      "[799/00040] updated kl_weight: 0.05\n",
      "[799/00041] updated kl_weight: 0.05\n",
      "[799/00042] updated kl_weight: 0.05\n",
      "[799/00043] updated kl_weight: 0.05\n",
      "[799/00044] updated kl_weight: 0.05\n",
      "[799/00045] updated kl_weight: 0.05\n",
      "[799/00046] updated kl_weight: 0.05\n",
      "[799/00047] updated kl_weight: 0.05\n",
      "[799/00048] updated kl_weight: 0.05\n",
      "[799/00049] updated kl_weight: 0.05\n",
      "[799/00050] updated kl_weight: 0.05\n",
      "[799/00051] updated kl_weight: 0.05\n",
      "[799/00052] updated kl_weight: 0.05\n",
      "[799/00053] updated kl_weight: 0.05\n",
      "[799/00054] updated kl_weight: 0.05\n",
      "[799/00055] updated kl_weight: 0.05\n",
      "[799/00056] updated kl_weight: 0.05\n",
      "[799/00057] updated kl_weight: 0.05\n",
      "[799/00058] updated kl_weight: 0.05\n",
      "[799/00059] updated kl_weight: 0.05\n",
      "[799/00060] updated kl_weight: 0.05\n",
      "[799/00061] updated kl_weight: 0.05\n",
      "[799/00062] updated kl_weight: 0.05\n",
      "[799/00063] updated kl_weight: 0.05\n",
      "[799/00064] updated kl_weight: 0.05\n",
      "[799/00065] updated kl_weight: 0.05\n",
      "[799/00066] updated kl_weight: 0.05\n",
      "[799/00067] updated kl_weight: 0.05\n",
      "[799/00068] updated kl_weight: 0.05\n",
      "[799/00069] updated kl_weight: 0.05\n",
      "[799/00070] updated kl_weight: 0.05\n",
      "[799/00071] updated kl_weight: 0.05\n",
      "[799/00072] updated kl_weight: 0.05\n",
      "[799/00073] updated kl_weight: 0.05\n",
      "[799/00074] updated kl_weight: 0.05\n",
      "[799/00074] train_loss: 0.026511 kl_loss: 0.154495 normal_loss: 0.018787\n",
      "[799/00074] MMD 0.0049097612500190735\n",
      "[799/00074] TMD 0.0697641670703888\n",
      "[801/00024] train_loss: 0.026566 kl_loss: 0.154697 normal_loss: 0.018831\n",
      "[802/00049] train_loss: 0.026536 kl_loss: 0.154706 normal_loss: 0.018801\n",
      "[803/00074] train_loss: 0.026584 kl_loss: 0.154800 normal_loss: 0.018844\n",
      "[805/00024] train_loss: 0.026443 kl_loss: 0.154435 normal_loss: 0.018721\n",
      "[806/00049] train_loss: 0.026557 kl_loss: 0.154881 normal_loss: 0.018813\n",
      "[807/00074] train_loss: 0.026574 kl_loss: 0.154847 normal_loss: 0.018832\n",
      "[809/00024] train_loss: 0.026435 kl_loss: 0.154378 normal_loss: 0.018716\n",
      "[810/00049] train_loss: 0.026599 kl_loss: 0.155399 normal_loss: 0.018829\n",
      "[811/00074] train_loss: 0.026513 kl_loss: 0.154347 normal_loss: 0.018796\n",
      "[813/00024] train_loss: 0.026502 kl_loss: 0.154662 normal_loss: 0.018769\n",
      "[814/00049] train_loss: 0.026475 kl_loss: 0.154877 normal_loss: 0.018731\n",
      "[815/00074] train_loss: 0.026481 kl_loss: 0.154544 normal_loss: 0.018754\n",
      "[817/00024] train_loss: 0.026449 kl_loss: 0.154636 normal_loss: 0.018717\n",
      "[818/00049] train_loss: 0.026593 kl_loss: 0.155497 normal_loss: 0.018818\n",
      "[819/00074] train_loss: 0.026406 kl_loss: 0.153907 normal_loss: 0.018711\n",
      "[821/00024] train_loss: 0.026558 kl_loss: 0.154693 normal_loss: 0.018823\n",
      "[822/00049] train_loss: 0.026392 kl_loss: 0.154423 normal_loss: 0.018671\n",
      "[823/00074] train_loss: 0.026548 kl_loss: 0.154881 normal_loss: 0.018804\n",
      "[825/00024] train_loss: 0.026515 kl_loss: 0.154834 normal_loss: 0.018773\n",
      "[826/00049] train_loss: 0.026455 kl_loss: 0.154367 normal_loss: 0.018736\n",
      "[827/00074] train_loss: 0.026539 kl_loss: 0.154752 normal_loss: 0.018802\n",
      "[829/00024] train_loss: 0.026483 kl_loss: 0.154568 normal_loss: 0.018755\n",
      "[830/00049] train_loss: 0.026443 kl_loss: 0.154374 normal_loss: 0.018724\n",
      "[831/00074] train_loss: 0.026512 kl_loss: 0.154971 normal_loss: 0.018763\n",
      "[833/00024] train_loss: 0.026519 kl_loss: 0.154710 normal_loss: 0.018784\n",
      "[834/00049] train_loss: 0.026400 kl_loss: 0.154589 normal_loss: 0.018671\n",
      "[835/00074] train_loss: 0.026469 kl_loss: 0.154573 normal_loss: 0.018740\n",
      "[837/00024] train_loss: 0.026549 kl_loss: 0.154717 normal_loss: 0.018814\n",
      "[838/00049] train_loss: 0.026612 kl_loss: 0.154888 normal_loss: 0.018867\n",
      "[839/00074] train_loss: 0.026350 kl_loss: 0.154225 normal_loss: 0.018639\n",
      "[841/00024] train_loss: 0.026524 kl_loss: 0.154892 normal_loss: 0.018780\n",
      "[842/00049] train_loss: 0.026472 kl_loss: 0.154173 normal_loss: 0.018763\n",
      "[843/00074] train_loss: 0.026488 kl_loss: 0.154724 normal_loss: 0.018752\n",
      "[845/00024] train_loss: 0.026427 kl_loss: 0.154405 normal_loss: 0.018706\n",
      "[846/00049] train_loss: 0.026595 kl_loss: 0.155307 normal_loss: 0.018830\n",
      "[847/00074] train_loss: 0.026418 kl_loss: 0.154035 normal_loss: 0.018716\n",
      "[849/00024] train_loss: 0.026437 kl_loss: 0.154667 normal_loss: 0.018704\n",
      "[849/00074] MMD 0.0050461022183299065\n",
      "[849/00074] TMD 0.08243072777986526\n",
      "[850/00049] train_loss: 0.026578 kl_loss: 0.154579 normal_loss: 0.018849\n",
      "[851/00074] train_loss: 0.026406 kl_loss: 0.154457 normal_loss: 0.018683\n",
      "[853/00024] train_loss: 0.026474 kl_loss: 0.154403 normal_loss: 0.018754\n",
      "[854/00049] train_loss: 0.026524 kl_loss: 0.154668 normal_loss: 0.018791\n",
      "[855/00074] train_loss: 0.026490 kl_loss: 0.154592 normal_loss: 0.018761\n",
      "[857/00024] train_loss: 0.026460 kl_loss: 0.154654 normal_loss: 0.018727\n",
      "[858/00049] train_loss: 0.026332 kl_loss: 0.154003 normal_loss: 0.018632\n",
      "[859/00074] train_loss: 0.026521 kl_loss: 0.154963 normal_loss: 0.018772\n",
      "[861/00024] train_loss: 0.026499 kl_loss: 0.154678 normal_loss: 0.018765\n",
      "[862/00049] train_loss: 0.026434 kl_loss: 0.154587 normal_loss: 0.018704\n",
      "[863/00074] train_loss: 0.026473 kl_loss: 0.154309 normal_loss: 0.018758\n",
      "[865/00024] train_loss: 0.026409 kl_loss: 0.154225 normal_loss: 0.018698\n",
      "[866/00049] train_loss: 0.026625 kl_loss: 0.155159 normal_loss: 0.018867\n",
      "[867/00074] train_loss: 0.026282 kl_loss: 0.154148 normal_loss: 0.018574\n",
      "[869/00024] train_loss: 0.026370 kl_loss: 0.154223 normal_loss: 0.018659\n",
      "[870/00049] train_loss: 0.026588 kl_loss: 0.155090 normal_loss: 0.018833\n",
      "[871/00074] train_loss: 0.026381 kl_loss: 0.154173 normal_loss: 0.018672\n",
      "[873/00024] train_loss: 0.026502 kl_loss: 0.154587 normal_loss: 0.018773\n",
      "[874/00049] train_loss: 0.026478 kl_loss: 0.154532 normal_loss: 0.018751\n",
      "[875/00074] train_loss: 0.026543 kl_loss: 0.154324 normal_loss: 0.018827\n",
      "[877/00024] train_loss: 0.026511 kl_loss: 0.154822 normal_loss: 0.018770\n",
      "[878/00049] train_loss: 0.026447 kl_loss: 0.154257 normal_loss: 0.018734\n",
      "[879/00074] train_loss: 0.026429 kl_loss: 0.154324 normal_loss: 0.018713\n",
      "[881/00024] train_loss: 0.026402 kl_loss: 0.154257 normal_loss: 0.018689\n",
      "[882/00049] train_loss: 0.026591 kl_loss: 0.154758 normal_loss: 0.018853\n",
      "[883/00074] train_loss: 0.026387 kl_loss: 0.154348 normal_loss: 0.018669\n",
      "[885/00024] train_loss: 0.026419 kl_loss: 0.154383 normal_loss: 0.018700\n",
      "[886/00049] train_loss: 0.026583 kl_loss: 0.154715 normal_loss: 0.018847\n",
      "[887/00074] train_loss: 0.026298 kl_loss: 0.154222 normal_loss: 0.018587\n",
      "[889/00024] train_loss: 0.026364 kl_loss: 0.154313 normal_loss: 0.018648\n",
      "[890/00049] train_loss: 0.026496 kl_loss: 0.154556 normal_loss: 0.018768\n",
      "[891/00074] train_loss: 0.026500 kl_loss: 0.154406 normal_loss: 0.018779\n",
      "[893/00024] train_loss: 0.026379 kl_loss: 0.154143 normal_loss: 0.018672\n",
      "[894/00049] train_loss: 0.026476 kl_loss: 0.154379 normal_loss: 0.018757\n",
      "[895/00074] train_loss: 0.026499 kl_loss: 0.154714 normal_loss: 0.018764\n",
      "[897/00024] train_loss: 0.026485 kl_loss: 0.154512 normal_loss: 0.018759\n",
      "[898/00049] train_loss: 0.026379 kl_loss: 0.154269 normal_loss: 0.018666\n",
      "[899/00000] updated kl_weight: 0.05\n",
      "[899/00001] updated kl_weight: 0.05\n",
      "[899/00002] updated kl_weight: 0.05\n",
      "[899/00003] updated kl_weight: 0.05\n",
      "[899/00004] updated kl_weight: 0.05\n",
      "[899/00005] updated kl_weight: 0.05\n",
      "[899/00006] updated kl_weight: 0.05\n",
      "[899/00007] updated kl_weight: 0.05\n",
      "[899/00008] updated kl_weight: 0.05\n",
      "[899/00009] updated kl_weight: 0.05\n",
      "[899/00010] updated kl_weight: 0.05\n",
      "[899/00011] updated kl_weight: 0.05\n",
      "[899/00012] updated kl_weight: 0.05\n",
      "[899/00013] updated kl_weight: 0.05\n",
      "[899/00014] updated kl_weight: 0.05\n",
      "[899/00015] updated kl_weight: 0.05\n",
      "[899/00016] updated kl_weight: 0.05\n",
      "[899/00017] updated kl_weight: 0.05\n",
      "[899/00018] updated kl_weight: 0.05\n",
      "[899/00019] updated kl_weight: 0.05\n",
      "[899/00020] updated kl_weight: 0.05\n",
      "[899/00021] updated kl_weight: 0.05\n",
      "[899/00022] updated kl_weight: 0.05\n",
      "[899/00023] updated kl_weight: 0.05\n",
      "[899/00024] updated kl_weight: 0.05\n",
      "[899/00025] updated kl_weight: 0.05\n",
      "[899/00026] updated kl_weight: 0.05\n",
      "[899/00027] updated kl_weight: 0.05\n",
      "[899/00028] updated kl_weight: 0.05\n",
      "[899/00029] updated kl_weight: 0.05\n",
      "[899/00030] updated kl_weight: 0.05\n",
      "[899/00031] updated kl_weight: 0.05\n",
      "[899/00032] updated kl_weight: 0.05\n",
      "[899/00033] updated kl_weight: 0.05\n",
      "[899/00034] updated kl_weight: 0.05\n",
      "[899/00035] updated kl_weight: 0.05\n",
      "[899/00036] updated kl_weight: 0.05\n",
      "[899/00037] updated kl_weight: 0.05\n",
      "[899/00038] updated kl_weight: 0.05\n",
      "[899/00039] updated kl_weight: 0.05\n",
      "[899/00040] updated kl_weight: 0.05\n",
      "[899/00041] updated kl_weight: 0.05\n",
      "[899/00042] updated kl_weight: 0.05\n",
      "[899/00043] updated kl_weight: 0.05\n",
      "[899/00044] updated kl_weight: 0.05\n",
      "[899/00045] updated kl_weight: 0.05\n",
      "[899/00046] updated kl_weight: 0.05\n",
      "[899/00047] updated kl_weight: 0.05\n",
      "[899/00048] updated kl_weight: 0.05\n",
      "[899/00049] updated kl_weight: 0.05\n",
      "[899/00050] updated kl_weight: 0.05\n",
      "[899/00051] updated kl_weight: 0.05\n",
      "[899/00052] updated kl_weight: 0.05\n",
      "[899/00053] updated kl_weight: 0.05\n",
      "[899/00054] updated kl_weight: 0.05\n",
      "[899/00055] updated kl_weight: 0.05\n",
      "[899/00056] updated kl_weight: 0.05\n",
      "[899/00057] updated kl_weight: 0.05\n",
      "[899/00058] updated kl_weight: 0.05\n",
      "[899/00059] updated kl_weight: 0.05\n",
      "[899/00060] updated kl_weight: 0.05\n",
      "[899/00061] updated kl_weight: 0.05\n",
      "[899/00062] updated kl_weight: 0.05\n",
      "[899/00063] updated kl_weight: 0.05\n",
      "[899/00064] updated kl_weight: 0.05\n",
      "[899/00065] updated kl_weight: 0.05\n",
      "[899/00066] updated kl_weight: 0.05\n",
      "[899/00067] updated kl_weight: 0.05\n",
      "[899/00068] updated kl_weight: 0.05\n",
      "[899/00069] updated kl_weight: 0.05\n",
      "[899/00070] updated kl_weight: 0.05\n",
      "[899/00071] updated kl_weight: 0.05\n",
      "[899/00072] updated kl_weight: 0.05\n",
      "[899/00073] updated kl_weight: 0.05\n",
      "[899/00074] updated kl_weight: 0.05\n",
      "[899/00074] train_loss: 0.026410 kl_loss: 0.154416 normal_loss: 0.018689\n",
      "[899/00074] MMD 0.0050826589576900005\n",
      "[899/00074] TMD 0.06414811313152313\n",
      "[901/00024] train_loss: 0.026485 kl_loss: 0.154630 normal_loss: 0.018754\n",
      "[902/00049] train_loss: 0.026508 kl_loss: 0.154387 normal_loss: 0.018789\n",
      "[903/00074] train_loss: 0.026423 kl_loss: 0.154147 normal_loss: 0.018716\n",
      "[905/00024] train_loss: 0.026496 kl_loss: 0.154627 normal_loss: 0.018765\n",
      "[906/00049] train_loss: 0.026477 kl_loss: 0.154130 normal_loss: 0.018770\n",
      "[907/00074] train_loss: 0.026420 kl_loss: 0.154385 normal_loss: 0.018701\n",
      "[909/00024] train_loss: 0.026362 kl_loss: 0.154056 normal_loss: 0.018659\n",
      "[910/00049] train_loss: 0.026522 kl_loss: 0.154609 normal_loss: 0.018792\n",
      "[911/00074] train_loss: 0.026427 kl_loss: 0.154455 normal_loss: 0.018705\n",
      "[913/00024] train_loss: 0.026428 kl_loss: 0.154399 normal_loss: 0.018708\n",
      "[914/00049] train_loss: 0.026337 kl_loss: 0.153901 normal_loss: 0.018641\n",
      "[915/00074] train_loss: 0.026449 kl_loss: 0.154800 normal_loss: 0.018709\n",
      "[917/00024] train_loss: 0.026486 kl_loss: 0.154514 normal_loss: 0.018761\n",
      "[918/00049] train_loss: 0.026424 kl_loss: 0.154512 normal_loss: 0.018698\n",
      "[919/00074] train_loss: 0.026396 kl_loss: 0.154051 normal_loss: 0.018693\n",
      "[921/00024] train_loss: 0.026415 kl_loss: 0.154255 normal_loss: 0.018703\n",
      "[922/00049] train_loss: 0.026508 kl_loss: 0.154795 normal_loss: 0.018769\n",
      "[923/00074] train_loss: 0.026350 kl_loss: 0.154006 normal_loss: 0.018650\n",
      "[925/00024] train_loss: 0.026440 kl_loss: 0.154013 normal_loss: 0.018739\n",
      "[926/00049] train_loss: 0.026418 kl_loss: 0.154585 normal_loss: 0.018689\n",
      "[927/00074] train_loss: 0.026495 kl_loss: 0.154439 normal_loss: 0.018773\n",
      "[929/00024] train_loss: 0.026352 kl_loss: 0.154227 normal_loss: 0.018641\n",
      "[930/00049] train_loss: 0.026351 kl_loss: 0.154138 normal_loss: 0.018644\n",
      "[931/00074] train_loss: 0.026454 kl_loss: 0.154651 normal_loss: 0.018722\n",
      "[933/00024] train_loss: 0.026469 kl_loss: 0.154446 normal_loss: 0.018746\n",
      "[934/00049] train_loss: 0.026459 kl_loss: 0.154399 normal_loss: 0.018739\n",
      "[935/00074] train_loss: 0.026389 kl_loss: 0.154148 normal_loss: 0.018682\n",
      "[937/00024] train_loss: 0.026489 kl_loss: 0.154408 normal_loss: 0.018768\n",
      "[938/00049] train_loss: 0.026324 kl_loss: 0.154308 normal_loss: 0.018609\n",
      "[939/00074] train_loss: 0.026492 kl_loss: 0.154256 normal_loss: 0.018779\n",
      "[941/00024] train_loss: 0.026502 kl_loss: 0.154661 normal_loss: 0.018769\n",
      "[942/00049] train_loss: 0.026350 kl_loss: 0.154040 normal_loss: 0.018648\n",
      "[943/00074] train_loss: 0.026423 kl_loss: 0.154251 normal_loss: 0.018711\n",
      "[945/00024] train_loss: 0.026295 kl_loss: 0.153642 normal_loss: 0.018613\n",
      "[946/00049] train_loss: 0.026576 kl_loss: 0.155330 normal_loss: 0.018809\n",
      "[947/00074] train_loss: 0.026345 kl_loss: 0.153958 normal_loss: 0.018647\n",
      "[949/00024] train_loss: 0.026463 kl_loss: 0.154506 normal_loss: 0.018737\n",
      "[949/00074] MMD 0.004882458131760359\n",
      "[949/00074] TMD 0.054297689348459244\n",
      "[950/00049] train_loss: 0.026377 kl_loss: 0.154328 normal_loss: 0.018660\n",
      "[951/00074] train_loss: 0.026395 kl_loss: 0.154074 normal_loss: 0.018692\n",
      "[953/00024] train_loss: 0.026325 kl_loss: 0.153789 normal_loss: 0.018635\n",
      "[954/00049] train_loss: 0.026539 kl_loss: 0.154883 normal_loss: 0.018795\n",
      "[955/00074] train_loss: 0.026370 kl_loss: 0.154214 normal_loss: 0.018659\n",
      "[957/00024] train_loss: 0.026457 kl_loss: 0.154304 normal_loss: 0.018742\n",
      "[958/00049] train_loss: 0.026403 kl_loss: 0.154482 normal_loss: 0.018679\n",
      "[959/00074] train_loss: 0.026384 kl_loss: 0.154078 normal_loss: 0.018680\n",
      "[961/00024] train_loss: 0.026415 kl_loss: 0.154374 normal_loss: 0.018696\n",
      "[962/00049] train_loss: 0.026414 kl_loss: 0.154054 normal_loss: 0.018711\n",
      "[963/00074] train_loss: 0.026364 kl_loss: 0.154415 normal_loss: 0.018644\n",
      "[965/00024] train_loss: 0.026396 kl_loss: 0.153982 normal_loss: 0.018696\n",
      "[966/00049] train_loss: 0.026487 kl_loss: 0.154841 normal_loss: 0.018745\n",
      "[967/00074] train_loss: 0.026402 kl_loss: 0.153999 normal_loss: 0.018702\n",
      "[969/00024] train_loss: 0.026487 kl_loss: 0.154297 normal_loss: 0.018772\n",
      "[970/00049] train_loss: 0.026302 kl_loss: 0.154419 normal_loss: 0.018581\n",
      "[971/00074] train_loss: 0.026382 kl_loss: 0.154085 normal_loss: 0.018678\n",
      "[973/00024] train_loss: 0.026375 kl_loss: 0.154383 normal_loss: 0.018656\n",
      "[974/00049] train_loss: 0.026408 kl_loss: 0.153983 normal_loss: 0.018709\n",
      "[975/00074] train_loss: 0.026428 kl_loss: 0.154413 normal_loss: 0.018707\n",
      "[977/00024] train_loss: 0.026304 kl_loss: 0.154266 normal_loss: 0.018591\n",
      "[978/00049] train_loss: 0.026407 kl_loss: 0.154198 normal_loss: 0.018697\n",
      "[979/00074] train_loss: 0.026505 kl_loss: 0.154292 normal_loss: 0.018790\n",
      "[981/00024] train_loss: 0.026372 kl_loss: 0.153970 normal_loss: 0.018674\n",
      "[982/00049] train_loss: 0.026423 kl_loss: 0.154172 normal_loss: 0.018714\n",
      "[983/00074] train_loss: 0.026508 kl_loss: 0.154594 normal_loss: 0.018779\n",
      "[985/00024] train_loss: 0.026508 kl_loss: 0.154513 normal_loss: 0.018782\n",
      "[986/00049] train_loss: 0.026375 kl_loss: 0.154059 normal_loss: 0.018672\n",
      "[987/00074] train_loss: 0.026452 kl_loss: 0.154147 normal_loss: 0.018744\n",
      "[989/00024] train_loss: 0.026270 kl_loss: 0.154077 normal_loss: 0.018566\n",
      "[990/00049] train_loss: 0.026445 kl_loss: 0.154228 normal_loss: 0.018734\n",
      "[991/00074] train_loss: 0.026448 kl_loss: 0.154394 normal_loss: 0.018729\n",
      "[993/00024] train_loss: 0.026491 kl_loss: 0.154264 normal_loss: 0.018778\n",
      "[994/00049] train_loss: 0.026335 kl_loss: 0.153886 normal_loss: 0.018640\n",
      "[995/00074] train_loss: 0.026508 kl_loss: 0.154529 normal_loss: 0.018782\n",
      "[997/00024] train_loss: 0.026496 kl_loss: 0.154563 normal_loss: 0.018768\n",
      "[998/00049] train_loss: 0.026347 kl_loss: 0.153962 normal_loss: 0.018648\n",
      "[999/00000] updated kl_weight: 0.05\n",
      "[999/00001] updated kl_weight: 0.05\n",
      "[999/00002] updated kl_weight: 0.05\n",
      "[999/00003] updated kl_weight: 0.05\n",
      "[999/00004] updated kl_weight: 0.05\n",
      "[999/00005] updated kl_weight: 0.05\n",
      "[999/00006] updated kl_weight: 0.05\n",
      "[999/00007] updated kl_weight: 0.05\n",
      "[999/00008] updated kl_weight: 0.05\n",
      "[999/00009] updated kl_weight: 0.05\n",
      "[999/00010] updated kl_weight: 0.05\n",
      "[999/00011] updated kl_weight: 0.05\n",
      "[999/00012] updated kl_weight: 0.05\n",
      "[999/00013] updated kl_weight: 0.05\n",
      "[999/00014] updated kl_weight: 0.05\n",
      "[999/00015] updated kl_weight: 0.05\n",
      "[999/00016] updated kl_weight: 0.05\n",
      "[999/00017] updated kl_weight: 0.05\n",
      "[999/00018] updated kl_weight: 0.05\n",
      "[999/00019] updated kl_weight: 0.05\n",
      "[999/00020] updated kl_weight: 0.05\n",
      "[999/00021] updated kl_weight: 0.05\n",
      "[999/00022] updated kl_weight: 0.05\n",
      "[999/00023] updated kl_weight: 0.05\n",
      "[999/00024] updated kl_weight: 0.05\n",
      "[999/00025] updated kl_weight: 0.05\n",
      "[999/00026] updated kl_weight: 0.05\n",
      "[999/00027] updated kl_weight: 0.05\n",
      "[999/00028] updated kl_weight: 0.05\n",
      "[999/00029] updated kl_weight: 0.05\n",
      "[999/00030] updated kl_weight: 0.05\n",
      "[999/00031] updated kl_weight: 0.05\n",
      "[999/00032] updated kl_weight: 0.05\n",
      "[999/00033] updated kl_weight: 0.05\n",
      "[999/00034] updated kl_weight: 0.05\n",
      "[999/00035] updated kl_weight: 0.05\n",
      "[999/00036] updated kl_weight: 0.05\n",
      "[999/00037] updated kl_weight: 0.05\n",
      "[999/00038] updated kl_weight: 0.05\n",
      "[999/00039] updated kl_weight: 0.05\n",
      "[999/00040] updated kl_weight: 0.05\n",
      "[999/00041] updated kl_weight: 0.05\n",
      "[999/00042] updated kl_weight: 0.05\n",
      "[999/00043] updated kl_weight: 0.05\n",
      "[999/00044] updated kl_weight: 0.05\n",
      "[999/00045] updated kl_weight: 0.05\n",
      "[999/00046] updated kl_weight: 0.05\n",
      "[999/00047] updated kl_weight: 0.05\n",
      "[999/00048] updated kl_weight: 0.05\n",
      "[999/00049] updated kl_weight: 0.05\n",
      "[999/00050] updated kl_weight: 0.05\n",
      "[999/00051] updated kl_weight: 0.05\n",
      "[999/00052] updated kl_weight: 0.05\n",
      "[999/00053] updated kl_weight: 0.05\n",
      "[999/00054] updated kl_weight: 0.05\n",
      "[999/00055] updated kl_weight: 0.05\n",
      "[999/00056] updated kl_weight: 0.05\n",
      "[999/00057] updated kl_weight: 0.05\n",
      "[999/00058] updated kl_weight: 0.05\n",
      "[999/00059] updated kl_weight: 0.05\n",
      "[999/00060] updated kl_weight: 0.05\n",
      "[999/00061] updated kl_weight: 0.05\n",
      "[999/00062] updated kl_weight: 0.05\n",
      "[999/00063] updated kl_weight: 0.05\n",
      "[999/00064] updated kl_weight: 0.05\n",
      "[999/00065] updated kl_weight: 0.05\n",
      "[999/00066] updated kl_weight: 0.05\n",
      "[999/00067] updated kl_weight: 0.05\n",
      "[999/00068] updated kl_weight: 0.05\n",
      "[999/00069] updated kl_weight: 0.05\n",
      "[999/00070] updated kl_weight: 0.05\n",
      "[999/00071] updated kl_weight: 0.05\n",
      "[999/00072] updated kl_weight: 0.05\n",
      "[999/00073] updated kl_weight: 0.05\n",
      "[999/00074] updated kl_weight: 0.05\n",
      "[999/00074] train_loss: 0.026423 kl_loss: 0.154134 normal_loss: 0.018717\n",
      "[999/00074] MMD 0.004902235697954893\n",
      "[999/00074] TMD 0.062358465045690536\n"
     ]
    }
   ],
   "source": [
    "# CHAIR VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_vad_0.05kl',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.05,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'mmd_every_epoch': 50,\n",
    "    'tmd_every_epoch': 50,\n",
    "    'iou_every_epoch': 10,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'car_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'mmd_every_epoch': 150,\n",
    "    'tmd_every_epoch': 150,\n",
    "    'iou_every_epoch': 10,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#                   #\n",
    "#    VISUALIZING    #\n",
    "#                   #\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.visualize import visualize_dataset_sample, visualize_ad, visualize_vad, visualize_vad_norm, visualize_vad_norm, visualize_interpolation_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1275 950\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f861b2566c54447e80efb8aca4728416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ed0301f5b074286bcf6ccfab8f69be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bfffdde69b421dafc9c34689f152c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "experiment = \"chair_ad\"\n",
    "experiment2 = \"sofa_ad\"\n",
    "# experiment2 = \"sofa_ad\"\n",
    "filter_class = \"chair\"\n",
    "index = 4123\n",
    "index1 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "index2 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "# index1 = 1200\n",
    "# index2 = 81\n",
    "a1 = 0.5\n",
    "a2 = 1 - a1\n",
    "#-------\n",
    "# visualize_ad(\"airplane_ad\", index1)\n",
    "#-------\n",
    "# visualize_vad_norm(\"chair_vad_0.05kl\")\n",
    "# visualize_vad_norm(\"table_vad\")\n",
    "# visualize_vad_norm(experiment)\n",
    "# visualize_ad(experiment, index1)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_ad(experiment, index)\n",
    "#-------\n",
    "# visualize_vad_norm(experiment)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_dataset_sample(filter_class, index)\n",
    "#-------\n",
    "print(index1, index2)\n",
    "visualize_interpolation_ad(experiment, index1, index2, a1, a2)\n",
    "visualize_ad(experiment, index1)\n",
    "visualize_ad(experiment, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#                  #\n",
    "#    EVALUATION    #\n",
    "#                  #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate import generate_samples, convert_df_to_point_cloud, chamfer_distance, MMD, convert_set_to_point_cloud, visualize_point_cloud, _mmd, TMD, IOU, min_sample, ONE_NN\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set samples: 600\n",
      "Sample 0 done. 0.00788299273699522\n",
      "Sample 1 done. 0.010865248739719391\n",
      "Sample 2 done. 0.006288700737059116\n",
      "Sample 3 done. 0.007191748823970556\n",
      "Sample 4 done. 0.006720454897731543\n",
      "Sample 5 done. 0.011375686153769493\n",
      "Sample 6 done. 0.005120906047523022\n",
      "Sample 7 done. 0.011266939342021942\n",
      "Sample 8 done. 0.005326530896127224\n",
      "Sample 9 done. 0.003879130817949772\n",
      "Sample 10 done. 0.005525801330804825\n",
      "Sample 11 done. 0.005089952144771814\n",
      "Sample 12 done. 0.0051506925374269485\n",
      "Sample 13 done. 0.004655638709664345\n",
      "Sample 14 done. 0.008662420324981213\n",
      "Sample 15 done. 0.009089370258152485\n",
      "Sample 16 done. 0.011698471382260323\n",
      "Sample 17 done. 0.005696102976799011\n",
      "Sample 18 done. 0.0044777351431548595\n",
      "Sample 19 done. 0.0042466819286346436\n",
      "Sample 20 done. 0.011195540428161621\n",
      "Sample 21 done. 0.008172894828021526\n",
      "Sample 22 done. 0.006144986487925053\n",
      "Sample 23 done. 0.004533893894404173\n",
      "Sample 24 done. 0.005230196751654148\n",
      "Sample 25 done. 0.011141072027385235\n",
      "Sample 26 done. 0.011557936668395996\n",
      "Sample 27 done. 0.01057981513440609\n",
      "Sample 28 done. 0.006592028774321079\n",
      "Sample 29 done. 0.011077500879764557\n",
      "Sample 30 done. 0.013576705008745193\n",
      "Sample 31 done. 0.006293215788900852\n",
      "Sample 32 done. 0.010057471692562103\n",
      "Sample 33 done. 0.002941083163022995\n",
      "Sample 34 done. 0.01070442982017994\n",
      "Sample 35 done. 0.011735215783119202\n",
      "Sample 36 done. 0.004119315184652805\n",
      "Sample 37 done. 0.003544846083968878\n",
      "Sample 38 done. 0.0032512014731764793\n",
      "Sample 39 done. 0.009939341805875301\n",
      "Sample 40 done. 0.008229423314332962\n",
      "Sample 41 done. 0.0026369893457740545\n",
      "Sample 42 done. 0.00802244246006012\n",
      "Sample 43 done. 0.0065695070661604404\n",
      "Sample 44 done. 0.0041236006654798985\n",
      "Sample 45 done. 0.00493467366322875\n",
      "Sample 46 done. 0.007666589692234993\n",
      "Sample 47 done. 0.005020875483751297\n",
      "Sample 48 done. 0.004090350121259689\n",
      "Sample 49 done. 0.0041126529686152935\n",
      "Sample 50 done. 0.004418589174747467\n",
      "Sample 51 done. 0.005245427135378122\n",
      "Sample 52 done. 0.004330482333898544\n",
      "Sample 53 done. 0.011004995554685593\n",
      "Sample 54 done. 0.007309822365641594\n",
      "Sample 55 done. 0.011169401928782463\n",
      "Sample 56 done. 0.004273011814802885\n",
      "Sample 57 done. 0.009132854640483856\n",
      "Sample 58 done. 0.008550042286515236\n",
      "Sample 59 done. 0.0149356909096241\n",
      "Sample 60 done. 0.014678934589028358\n",
      "Sample 61 done. 0.007960776798427105\n",
      "Sample 62 done. 0.006815211847424507\n",
      "Sample 63 done. 0.004591177683323622\n",
      "Sample 64 done. 0.011826371774077415\n",
      "Sample 65 done. 0.0031424311455339193\n",
      "Sample 66 done. 0.004404858686029911\n",
      "Sample 67 done. 0.01234382949769497\n",
      "Sample 68 done. 0.007424020208418369\n",
      "Sample 69 done. 0.010718758217990398\n",
      "Sample 70 done. 0.00661772396415472\n",
      "Sample 71 done. 0.004917270969599485\n",
      "Sample 72 done. 0.0163273923099041\n",
      "Sample 73 done. 0.011783141642808914\n",
      "Sample 74 done. 0.005868712440133095\n",
      "Sample 75 done. 0.006440362893044949\n",
      "Sample 76 done. 0.01141370739787817\n",
      "Sample 77 done. 0.005249318666756153\n",
      "Sample 78 done. 0.004814130254089832\n",
      "Sample 79 done. 0.004662801511585712\n",
      "Sample 80 done. 0.006267040967941284\n",
      "Sample 81 done. 0.012432127259671688\n",
      "Sample 82 done. 0.006716917268931866\n",
      "Sample 83 done. 0.008938288316130638\n",
      "Sample 84 done. 0.008192593231797218\n",
      "Sample 85 done. 0.010156942531466484\n",
      "Sample 86 done. 0.011661261320114136\n",
      "Sample 87 done. 0.005067247431725264\n",
      "Sample 88 done. 0.00476611964404583\n",
      "Sample 89 done. 0.005607879254966974\n",
      "Sample 90 done. 0.012384969741106033\n",
      "Sample 91 done. 0.010050972923636436\n",
      "Sample 92 done. 0.01390006858855486\n",
      "Sample 93 done. 0.004936658311635256\n",
      "Sample 94 done. 0.010744408704340458\n",
      "Sample 95 done. 0.01267765462398529\n",
      "Sample 96 done. 0.005744077730923891\n",
      "Sample 97 done. 0.004019571468234062\n",
      "Sample 98 done. 0.0038947202265262604\n",
      "Sample 99 done. 0.008263365365564823\n",
      "Sample 100 done. 0.013102799654006958\n",
      "Sample 101 done. 0.004044370725750923\n",
      "Sample 102 done. 0.006171351298689842\n",
      "Sample 103 done. 0.012782908976078033\n",
      "Sample 104 done. 0.004197160713374615\n",
      "Sample 105 done. 0.007316253148019314\n",
      "Sample 106 done. 0.005687875207513571\n",
      "Sample 107 done. 0.00940993893891573\n",
      "Sample 108 done. 0.008240011520683765\n",
      "Sample 109 done. 0.0037912558764219284\n",
      "Sample 110 done. 0.017616674304008484\n",
      "Sample 111 done. 0.007968390360474586\n",
      "Sample 112 done. 0.0077697886154055595\n",
      "Sample 113 done. 0.011396790854632854\n",
      "Sample 114 done. 0.00748037314042449\n",
      "Sample 115 done. 0.007920880801975727\n",
      "Sample 116 done. 0.006567046046257019\n",
      "Sample 117 done. 0.00912921130657196\n",
      "Sample 118 done. 0.006126119755208492\n",
      "Sample 119 done. 0.008819248527288437\n",
      "Sample 120 done. 0.0043207742273807526\n",
      "Sample 121 done. 0.0046597435139119625\n",
      "Sample 122 done. 0.004728684201836586\n",
      "Sample 123 done. 0.010864535346627235\n",
      "Sample 124 done. 0.003661101683974266\n",
      "Sample 125 done. 0.009623517282307148\n",
      "Sample 126 done. 0.007316530682146549\n",
      "Sample 127 done. 0.006820655427873135\n",
      "Sample 128 done. 0.007482346147298813\n",
      "Sample 129 done. 0.0035078609362244606\n",
      "Sample 130 done. 0.010998556390404701\n",
      "Sample 131 done. 0.009401648305356503\n",
      "Sample 132 done. 0.003494876902550459\n",
      "Sample 133 done. 0.0037675616331398487\n",
      "Sample 134 done. 0.00971653126180172\n",
      "Sample 135 done. 0.005911385640501976\n",
      "Sample 136 done. 0.005189830437302589\n",
      "Sample 137 done. 0.007406765595078468\n",
      "Sample 138 done. 0.003963783383369446\n",
      "Sample 139 done. 0.004699463956058025\n",
      "Sample 140 done. 0.00866609811782837\n",
      "Sample 141 done. 0.0036177963484078646\n",
      "Sample 142 done. 0.007082040887326002\n",
      "Sample 143 done. 0.007609497755765915\n",
      "Sample 144 done. 0.008625887334346771\n",
      "Sample 145 done. 0.007730322889983654\n",
      "Sample 146 done. 0.008144784718751907\n",
      "Sample 147 done. 0.006150912493467331\n",
      "Sample 148 done. 0.004378719255328178\n",
      "Sample 149 done. 0.005127232521772385\n",
      "Sample 150 done. 0.006004630587995052\n",
      "Sample 151 done. 0.014139293693006039\n",
      "Sample 152 done. 0.010445427149534225\n",
      "Sample 153 done. 0.0032683098688721657\n",
      "Sample 154 done. 0.0033488962799310684\n",
      "Sample 155 done. 0.016284741461277008\n",
      "Sample 156 done. 0.008698736317455769\n",
      "Sample 157 done. 0.006028855685144663\n",
      "Sample 158 done. 0.0047768703661859035\n",
      "Sample 159 done. 0.00324448524042964\n",
      "Sample 160 done. 0.004415733274072409\n",
      "Sample 161 done. 0.010382645763456821\n",
      "Sample 162 done. 0.0067228940315544605\n",
      "Sample 163 done. 0.00919216126203537\n",
      "Sample 164 done. 0.010765635408461094\n",
      "Sample 165 done. 0.0035644471645355225\n",
      "Sample 166 done. 0.003669159719720483\n",
      "Sample 167 done. 0.012338824570178986\n",
      "Sample 168 done. 0.006465597078204155\n",
      "Sample 169 done. 0.004179863259196281\n",
      "Sample 170 done. 0.008428862318396568\n",
      "Sample 171 done. 0.006178469862788916\n",
      "Sample 172 done. 0.004959871061146259\n",
      "Sample 173 done. 0.010993446223437786\n",
      "Sample 174 done. 0.004050225485116243\n",
      "Sample 175 done. 0.008361471816897392\n",
      "Sample 176 done. 0.006329511757940054\n",
      "Sample 177 done. 0.007768245413899422\n",
      "Sample 178 done. 0.012662732042372227\n",
      "Sample 179 done. 0.006811624858528376\n",
      "Sample 180 done. 0.009992395527660847\n",
      "Sample 181 done. 0.007252579554915428\n",
      "Sample 182 done. 0.0075235916301608086\n",
      "Sample 183 done. 0.004304779693484306\n",
      "Sample 184 done. 0.007682606112211943\n",
      "Sample 185 done. 0.008853128179907799\n",
      "Sample 186 done. 0.010762060061097145\n",
      "Sample 187 done. 0.0064112721011042595\n",
      "Sample 188 done. 0.005586428567767143\n",
      "Sample 189 done. 0.00853778701275587\n",
      "Sample 190 done. 0.008508335798978806\n",
      "Sample 191 done. 0.007265130057930946\n",
      "Sample 192 done. 0.005598747171461582\n",
      "Sample 193 done. 0.01058142725378275\n",
      "Sample 194 done. 0.007955674082040787\n",
      "Sample 195 done. 0.004745734855532646\n",
      "Sample 196 done. 0.013132940046489239\n",
      "Sample 197 done. 0.007694372907280922\n",
      "Sample 198 done. 0.00862685777246952\n",
      "Sample 199 done. 0.008750047534704208\n",
      "Sample 200 done. 0.008642816916108131\n",
      "Sample 201 done. 0.014525501057505608\n",
      "Sample 202 done. 0.014740517362952232\n",
      "Sample 203 done. 0.007589472457766533\n",
      "Sample 204 done. 0.009235510602593422\n",
      "Sample 205 done. 0.005761838983744383\n",
      "Sample 206 done. 0.010162785649299622\n",
      "Sample 207 done. 0.005431903526186943\n",
      "Sample 208 done. 0.004622063599526882\n",
      "Sample 209 done. 0.004401694051921368\n",
      "Sample 210 done. 0.011512046679854393\n",
      "Sample 211 done. 0.009881850332021713\n",
      "Sample 212 done. 0.004978214856237173\n",
      "Sample 213 done. 0.003856734139844775\n",
      "Sample 214 done. 0.010970191098749638\n",
      "Sample 215 done. 0.013318379409611225\n",
      "Sample 216 done. 0.0049603707157075405\n",
      "Sample 217 done. 0.010751564055681229\n",
      "Sample 218 done. 0.007861889898777008\n",
      "Sample 219 done. 0.005116491578519344\n",
      "Sample 220 done. 0.0038137887604534626\n",
      "Sample 221 done. 0.005530460737645626\n",
      "Sample 222 done. 0.012652628123760223\n",
      "Sample 223 done. 0.0048945206217467785\n",
      "Sample 224 done. 0.006406991742551327\n",
      "Sample 225 done. 0.00813787616789341\n",
      "Sample 226 done. 0.003953292965888977\n",
      "Sample 227 done. 0.007461491506546736\n",
      "Sample 228 done. 0.008505087345838547\n",
      "Sample 229 done. 0.003690243698656559\n",
      "Sample 230 done. 0.011114436201751232\n",
      "Sample 231 done. 0.008283079601824284\n",
      "Sample 232 done. 0.004558561835438013\n",
      "Sample 233 done. 0.004915486089885235\n",
      "Sample 234 done. 0.008764877915382385\n",
      "Sample 235 done. 0.004094818606972694\n",
      "Sample 236 done. 0.005969887599349022\n",
      "Sample 237 done. 0.004324633628129959\n",
      "Sample 238 done. 0.015312793664634228\n",
      "Sample 239 done. 0.009752911515533924\n",
      "Sample 240 done. 0.006891394965350628\n",
      "Sample 241 done. 0.010250255465507507\n",
      "Sample 242 done. 0.006556420587003231\n",
      "Sample 243 done. 0.008711349219083786\n",
      "Sample 244 done. 0.00463238637894392\n",
      "Sample 245 done. 0.010092096403241158\n",
      "Sample 246 done. 0.014654826372861862\n",
      "Sample 247 done. 0.0034135833848267794\n",
      "Sample 248 done. 0.003007293213158846\n",
      "Sample 249 done. 0.004891845863312483\n",
      "Sample 250 done. 0.0053854286670684814\n",
      "Sample 251 done. 0.009170571342110634\n",
      "Sample 252 done. 0.00552640575915575\n",
      "Sample 253 done. 0.002918828744441271\n",
      "Sample 254 done. 0.006527582183480263\n",
      "Sample 255 done. 0.005042986944317818\n",
      "Sample 256 done. 0.008661126717925072\n",
      "Sample 257 done. 0.009177230298519135\n",
      "Sample 258 done. 0.009627167135477066\n",
      "Sample 259 done. 0.012281779199838638\n",
      "Sample 260 done. 0.004316645674407482\n",
      "Sample 261 done. 0.009281915612518787\n",
      "Sample 262 done. 0.009434303268790245\n",
      "Sample 263 done. 0.009021857753396034\n",
      "Sample 264 done. 0.003513151314109564\n",
      "Sample 265 done. 0.006899801082909107\n",
      "Sample 266 done. 0.00988547969609499\n",
      "Sample 267 done. 0.007909337058663368\n",
      "Sample 268 done. 0.007545173168182373\n",
      "Sample 269 done. 0.010884037241339684\n",
      "Sample 270 done. 0.005755813326686621\n",
      "Sample 271 done. 0.01074756309390068\n",
      "Sample 272 done. 0.0068620480597019196\n",
      "Sample 273 done. 0.006991349160671234\n",
      "Sample 274 done. 0.005122067406773567\n",
      "Sample 275 done. 0.005496816709637642\n",
      "Sample 276 done. 0.0041006156243383884\n",
      "Sample 277 done. 0.0084939394146204\n",
      "Sample 278 done. 0.009105352684855461\n",
      "Sample 279 done. 0.006112934090197086\n",
      "Sample 280 done. 0.005814601667225361\n",
      "Sample 281 done. 0.008211496286094189\n",
      "Sample 282 done. 0.009875386022031307\n",
      "Sample 283 done. 0.006529371254146099\n",
      "Sample 284 done. 0.004292747937142849\n",
      "Sample 285 done. 0.004534844774752855\n",
      "Sample 286 done. 0.022822953760623932\n",
      "Sample 287 done. 0.007613420486450195\n",
      "Sample 288 done. 0.006366692949086428\n",
      "Sample 289 done. 0.009770452976226807\n",
      "Sample 290 done. 0.008828651160001755\n",
      "Sample 291 done. 0.008820830844342709\n",
      "Sample 292 done. 0.005840848200023174\n",
      "Sample 293 done. 0.005394033156335354\n",
      "Sample 294 done. 0.013398481532931328\n",
      "Sample 295 done. 0.007902322337031364\n",
      "Sample 296 done. 0.003002405632287264\n",
      "Sample 297 done. 0.008546996861696243\n",
      "Sample 298 done. 0.005874373950064182\n",
      "Sample 299 done. 0.008910702541470528\n",
      "Sample 300 done. 0.009256145916879177\n",
      "Sample 301 done. 0.009184935130178928\n",
      "Sample 302 done. 0.004509524442255497\n",
      "Sample 303 done. 0.008488645777106285\n",
      "Sample 304 done. 0.004399431869387627\n",
      "Sample 305 done. 0.00905981007963419\n",
      "Sample 306 done. 0.004503593780100346\n",
      "Sample 307 done. 0.009234897792339325\n",
      "Sample 308 done. 0.005311253946274519\n",
      "Sample 309 done. 0.005406050011515617\n",
      "Sample 310 done. 0.009449897333979607\n",
      "Sample 311 done. 0.009617339819669724\n",
      "Sample 312 done. 0.005654817447066307\n",
      "Sample 313 done. 0.009863458573818207\n",
      "Sample 314 done. 0.0040812063962221146\n",
      "Sample 315 done. 0.005669964477419853\n",
      "Sample 316 done. 0.0029584693256765604\n",
      "Sample 317 done. 0.014556417241692543\n",
      "Sample 318 done. 0.00540507584810257\n",
      "Sample 319 done. 0.006659946404397488\n",
      "Sample 320 done. 0.011966423131525517\n",
      "Sample 321 done. 0.009859740734100342\n",
      "Sample 322 done. 0.005334073677659035\n",
      "Sample 323 done. 0.005242793820798397\n",
      "Sample 324 done. 0.011676879599690437\n",
      "Sample 325 done. 0.008181843906641006\n",
      "Sample 326 done. 0.0049322741106152534\n",
      "Sample 327 done. 0.006909252144396305\n",
      "Sample 328 done. 0.01126270554959774\n",
      "Sample 329 done. 0.01352002378553152\n",
      "Sample 330 done. 0.003584406105801463\n",
      "Sample 331 done. 0.004909242037683725\n",
      "Sample 332 done. 0.005625390447676182\n",
      "Sample 333 done. 0.004646974615752697\n",
      "Sample 334 done. 0.015828749164938927\n",
      "Sample 335 done. 0.00509800435975194\n",
      "Sample 336 done. 0.006602679379284382\n",
      "Sample 337 done. 0.0049088504165410995\n",
      "Sample 338 done. 0.010138556361198425\n",
      "Sample 339 done. 0.008344669826328754\n",
      "Sample 340 done. 0.005687827710062265\n",
      "Sample 341 done. 0.010500242933630943\n",
      "Sample 342 done. 0.004077691584825516\n",
      "Sample 343 done. 0.006590490695089102\n",
      "Sample 344 done. 0.015243738889694214\n",
      "Sample 345 done. 0.0042944010347127914\n",
      "Sample 346 done. 0.007394547574222088\n",
      "Sample 347 done. 0.005419001914560795\n",
      "Sample 348 done. 0.007932089269161224\n",
      "Sample 349 done. 0.00363333523273468\n",
      "Sample 350 done. 0.011908041313290596\n",
      "Sample 351 done. 0.005096067674458027\n",
      "Sample 352 done. 0.009047570638358593\n",
      "Sample 353 done. 0.005549030844122171\n",
      "Sample 354 done. 0.006295492872595787\n",
      "Sample 355 done. 0.0050017028115689754\n",
      "Sample 356 done. 0.015307219699025154\n",
      "Sample 357 done. 0.0043950919061899185\n",
      "Sample 358 done. 0.003977101296186447\n",
      "Sample 359 done. 0.010174097493290901\n",
      "Sample 360 done. 0.005892164073884487\n",
      "Sample 361 done. 0.0044994838535785675\n",
      "Sample 362 done. 0.01732890121638775\n",
      "Sample 363 done. 0.005828069057315588\n",
      "Sample 364 done. 0.003945185802876949\n",
      "Sample 365 done. 0.004458339419215918\n",
      "Sample 366 done. 0.005714543629437685\n",
      "Sample 367 done. 0.004503186792135239\n",
      "Sample 368 done. 0.00940584484487772\n",
      "Sample 369 done. 0.004469756502658129\n",
      "Sample 370 done. 0.0066340407356619835\n",
      "Sample 371 done. 0.010384876281023026\n",
      "Sample 372 done. 0.004158049821853638\n",
      "Sample 373 done. 0.006149004213511944\n",
      "Sample 374 done. 0.008048346266150475\n",
      "Sample 375 done. 0.0044034505262970924\n",
      "Sample 376 done. 0.007764562498778105\n",
      "Sample 377 done. 0.0038030727300792933\n",
      "Sample 378 done. 0.004802183248102665\n",
      "Sample 379 done. 0.006432476919144392\n",
      "Sample 380 done. 0.007601241581141949\n",
      "Sample 381 done. 0.010330043733119965\n",
      "Sample 382 done. 0.007339547388255596\n",
      "Sample 383 done. 0.011559529229998589\n",
      "Sample 384 done. 0.02075255662202835\n",
      "Sample 385 done. 0.007630046457052231\n",
      "Sample 386 done. 0.014627860859036446\n",
      "Sample 387 done. 0.006693252827972174\n",
      "Sample 388 done. 0.0059247324243187904\n",
      "Sample 389 done. 0.006254144944250584\n",
      "Sample 390 done. 0.005933917127549648\n",
      "Sample 391 done. 0.005392350722104311\n",
      "Sample 392 done. 0.005399206653237343\n",
      "Sample 393 done. 0.00587324146181345\n",
      "Sample 394 done. 0.014816858805716038\n",
      "Sample 395 done. 0.008323969319462776\n",
      "Sample 396 done. 0.009989959187805653\n",
      "Sample 397 done. 0.010302235372364521\n",
      "Sample 398 done. 0.0078058186918497086\n",
      "Sample 399 done. 0.00619898084551096\n",
      "Sample 400 done. 0.007328976411372423\n",
      "Sample 401 done. 0.0061533208936452866\n",
      "Sample 402 done. 0.008212734013795853\n",
      "Sample 403 done. 0.008263982832431793\n",
      "Sample 404 done. 0.00577158760279417\n",
      "Sample 405 done. 0.0038529278244823217\n",
      "Sample 406 done. 0.013667038641870022\n",
      "Sample 407 done. 0.006942997686564922\n",
      "Sample 408 done. 0.0077396538108587265\n",
      "Sample 409 done. 0.007648656144738197\n",
      "Sample 410 done. 0.008266840130090714\n",
      "Sample 411 done. 0.010505132377147675\n",
      "Sample 412 done. 0.0035970276221632957\n",
      "Sample 413 done. 0.004683457314968109\n",
      "Sample 414 done. 0.007669978309422731\n",
      "Sample 415 done. 0.01432446576654911\n",
      "Sample 416 done. 0.010694400407373905\n",
      "Sample 417 done. 0.01314167957752943\n",
      "Sample 418 done. 0.008637500926852226\n",
      "Sample 419 done. 0.008793490007519722\n",
      "Sample 420 done. 0.003791768103837967\n",
      "Sample 421 done. 0.012612703256309032\n",
      "Sample 422 done. 0.013679862022399902\n",
      "Sample 423 done. 0.010003622621297836\n",
      "Sample 424 done. 0.006047848612070084\n",
      "Sample 425 done. 0.005820317193865776\n",
      "Sample 426 done. 0.005847057327628136\n",
      "Sample 427 done. 0.005011432338505983\n",
      "Sample 428 done. 0.004368505906313658\n",
      "Sample 429 done. 0.010844780132174492\n",
      "Sample 430 done. 0.009386925026774406\n",
      "Sample 431 done. 0.006782383657991886\n",
      "Sample 432 done. 0.013936882838606834\n",
      "Sample 433 done. 0.00476977601647377\n",
      "Sample 434 done. 0.010293937288224697\n",
      "Sample 435 done. 0.010959746316075325\n",
      "Sample 436 done. 0.005797266960144043\n",
      "Sample 437 done. 0.009050235152244568\n",
      "Sample 438 done. 0.007854724302887917\n",
      "Sample 439 done. 0.00788701232522726\n",
      "Sample 440 done. 0.007808739319443703\n",
      "Sample 441 done. 0.012807752937078476\n",
      "Sample 442 done. 0.013914678245782852\n",
      "Sample 443 done. 0.00617092102766037\n",
      "Sample 444 done. 0.01316146831959486\n",
      "Sample 445 done. 0.0057539185509085655\n",
      "Sample 446 done. 0.008914148434996605\n",
      "Sample 447 done. 0.011563371866941452\n",
      "Sample 448 done. 0.005913460627198219\n",
      "Sample 449 done. 0.005664811935275793\n",
      "Sample 450 done. 0.007914584130048752\n",
      "Sample 451 done. 0.0030906128231436014\n",
      "Sample 452 done. 0.009371934458613396\n",
      "Sample 453 done. 0.004375622607767582\n",
      "Sample 454 done. 0.004483317490667105\n",
      "Sample 455 done. 0.004123374819755554\n",
      "Sample 456 done. 0.004814642481505871\n",
      "Sample 457 done. 0.004240208771079779\n",
      "Sample 458 done. 0.004276696592569351\n",
      "Sample 459 done. 0.0111458208411932\n",
      "Sample 460 done. 0.00866939127445221\n",
      "Sample 461 done. 0.007187179289758205\n",
      "Sample 462 done. 0.007248425856232643\n",
      "Sample 463 done. 0.007262836210429668\n",
      "Sample 464 done. 0.005637005437165499\n",
      "Sample 465 done. 0.00917288102209568\n",
      "Sample 466 done. 0.003941875416785479\n",
      "Sample 467 done. 0.004681160673499107\n",
      "Sample 468 done. 0.005153199657797813\n",
      "Sample 469 done. 0.008664356544613838\n",
      "Sample 470 done. 0.010375235229730606\n",
      "Sample 471 done. 0.006924767978489399\n",
      "Sample 472 done. 0.005148681811988354\n",
      "Sample 473 done. 0.011943129822611809\n",
      "Sample 474 done. 0.008592560887336731\n",
      "Sample 475 done. 0.004675829783082008\n",
      "Sample 476 done. 0.009894965216517448\n",
      "Sample 477 done. 0.014519116841256618\n",
      "Sample 478 done. 0.008072692900896072\n",
      "Sample 479 done. 0.007193227298557758\n",
      "Sample 480 done. 0.00843174010515213\n",
      "Sample 481 done. 0.010522278025746346\n",
      "Sample 482 done. 0.005552057642489672\n",
      "Sample 483 done. 0.007825195789337158\n",
      "Sample 484 done. 0.007240009494125843\n",
      "Sample 485 done. 0.007635811809450388\n",
      "Sample 486 done. 0.005358872935175896\n",
      "Sample 487 done. 0.01078038290143013\n",
      "Sample 488 done. 0.004512744955718517\n",
      "Sample 489 done. 0.013147145509719849\n",
      "Sample 490 done. 0.008498629555106163\n",
      "Sample 491 done. 0.011319384910166264\n",
      "Sample 492 done. 0.0037129030097275972\n",
      "Sample 493 done. 0.0033611557446420193\n",
      "Sample 494 done. 0.007266235537827015\n",
      "Sample 495 done. 0.00966115016490221\n",
      "Sample 496 done. 0.008328025229275227\n",
      "Sample 497 done. 0.0054836636409163475\n",
      "Sample 498 done. 0.007516404613852501\n",
      "Sample 499 done. 0.004156186245381832\n",
      "Sample 500 done. 0.008636382408440113\n",
      "Sample 501 done. 0.006362416781485081\n",
      "Sample 502 done. 0.006962789222598076\n",
      "Sample 503 done. 0.011525807902216911\n",
      "Sample 504 done. 0.013767454773187637\n",
      "Sample 505 done. 0.010574723593890667\n",
      "Sample 506 done. 0.0039805094711482525\n",
      "Sample 507 done. 0.00578063540160656\n",
      "Sample 508 done. 0.0037134578451514244\n",
      "Sample 509 done. 0.0076906876638531685\n",
      "Sample 510 done. 0.007892805151641369\n",
      "Sample 511 done. 0.012471086345613003\n",
      "Sample 512 done. 0.006986078340560198\n",
      "Sample 513 done. 0.007081045303493738\n",
      "Sample 514 done. 0.008972188457846642\n",
      "Sample 515 done. 0.006332069635391235\n",
      "Sample 516 done. 0.008657531812787056\n",
      "Sample 517 done. 0.007472208701074123\n",
      "Sample 518 done. 0.010630553588271141\n",
      "Sample 519 done. 0.012172255665063858\n",
      "Sample 520 done. 0.005628863349556923\n",
      "Sample 521 done. 0.011527614668011665\n",
      "Sample 522 done. 0.006174606271088123\n",
      "Sample 523 done. 0.005107036791741848\n",
      "Sample 524 done. 0.006487325299531221\n",
      "Sample 525 done. 0.0033779609948396683\n",
      "Sample 526 done. 0.004768491722643375\n",
      "Sample 527 done. 0.009205766022205353\n",
      "Sample 528 done. 0.007187323644757271\n",
      "Sample 529 done. 0.005701588466763496\n",
      "Sample 530 done. 0.008451152592897415\n",
      "Sample 531 done. 0.017206210643053055\n",
      "Sample 532 done. 0.0035977219231426716\n",
      "Sample 533 done. 0.004813182633370161\n",
      "Sample 534 done. 0.005265587940812111\n",
      "Sample 535 done. 0.012077603489160538\n",
      "Sample 536 done. 0.0034512688871473074\n",
      "Sample 537 done. 0.004323220811784267\n",
      "Sample 538 done. 0.009232662618160248\n",
      "Sample 539 done. 0.012531919404864311\n",
      "Sample 540 done. 0.011833365075290203\n",
      "Sample 541 done. 0.007881784811615944\n",
      "Sample 542 done. 0.008015371859073639\n",
      "Sample 543 done. 0.004749970510601997\n",
      "Sample 544 done. 0.009334176778793335\n",
      "Sample 545 done. 0.003874062094837427\n",
      "Sample 546 done. 0.002902404870837927\n",
      "Sample 547 done. 0.007282125297933817\n",
      "Sample 548 done. 0.0062184883281588554\n",
      "Sample 549 done. 0.009530477225780487\n",
      "Sample 550 done. 0.0051329233683645725\n",
      "Sample 551 done. 0.014314224943518639\n",
      "Sample 552 done. 0.011146984994411469\n",
      "Sample 553 done. 0.012471146881580353\n",
      "Sample 554 done. 0.005778937600553036\n",
      "Sample 555 done. 0.0048409514129161835\n",
      "Sample 556 done. 0.0029367010574787855\n",
      "Sample 557 done. 0.009477097541093826\n",
      "Sample 558 done. 0.011680481024086475\n",
      "Sample 559 done. 0.0038484809920191765\n",
      "Sample 560 done. 0.009965561330318451\n",
      "Sample 561 done. 0.009124422445893288\n",
      "Sample 562 done. 0.003111131489276886\n",
      "Sample 563 done. 0.010345883667469025\n",
      "Sample 564 done. 0.006919293198734522\n",
      "Sample 565 done. 0.004140160977840424\n",
      "Sample 566 done. 0.004281673580408096\n",
      "Sample 567 done. 0.005656068213284016\n",
      "Sample 568 done. 0.010604888200759888\n",
      "Sample 569 done. 0.005869691260159016\n",
      "Sample 570 done. 0.008341846987605095\n",
      "Sample 571 done. 0.011168107390403748\n",
      "Sample 572 done. 0.013732202351093292\n",
      "Sample 573 done. 0.0042745089158415794\n",
      "Sample 574 done. 0.010223142802715302\n",
      "Sample 575 done. 0.006499394308775663\n",
      "Sample 576 done. 0.0034682629629969597\n",
      "Sample 577 done. 0.00532393716275692\n",
      "Sample 578 done. 0.004567405674606562\n",
      "Sample 579 done. 0.006831953767687082\n",
      "Sample 580 done. 0.008728822693228722\n",
      "Sample 581 done. 0.01337769441306591\n",
      "Sample 582 done. 0.007212964817881584\n",
      "Sample 583 done. 0.008089974522590637\n",
      "Sample 584 done. 0.014623073861002922\n",
      "Sample 585 done. 0.004763320088386536\n",
      "Sample 586 done. 0.010697034187614918\n",
      "Sample 587 done. 0.004124603699892759\n",
      "Sample 588 done. 0.014240359887480736\n",
      "Sample 589 done. 0.0037489342503249645\n",
      "Sample 590 done. 0.0033310512080788612\n",
      "Sample 591 done. 0.008927175775170326\n",
      "Sample 592 done. 0.005482623353600502\n",
      "Sample 593 done. 0.009209325537085533\n",
      "Sample 594 done. 0.008376051671802998\n",
      "Sample 595 done. 0.0074095819145441055\n",
      "Sample 596 done. 0.005009033717215061\n",
      "Sample 597 done. 0.007644327357411385\n",
      "Sample 598 done. 0.011726503260433674\n",
      "Sample 599 done. 0.009627963416278362\n",
      "Sample (val) 0 done. 0.00855357013642788\n",
      "Sample (val) 1 done. 0.006490052677690983\n",
      "Sample (val) 2 done. 0.010511737316846848\n",
      "Sample (val) 3 done. 0.013280106708407402\n",
      "Sample (val) 4 done. 0.011110318824648857\n",
      "Sample (val) 5 done. 0.00913647748529911\n",
      "Sample (val) 6 done. 0.003926332574337721\n",
      "Sample (val) 7 done. 0.004992000758647919\n",
      "Sample (val) 8 done. 0.0062515465542674065\n",
      "Sample (val) 9 done. 0.006369087845087051\n",
      "Sample (val) 10 done. 0.00547937024384737\n",
      "Sample (val) 11 done. 0.002538279164582491\n",
      "Sample (val) 12 done. 0.004469090607017279\n",
      "Sample (val) 13 done. 0.00966775231063366\n",
      "Sample (val) 14 done. 0.0044008842669427395\n",
      "Sample (val) 15 done. 0.006611417047679424\n",
      "Sample (val) 16 done. 0.00953243300318718\n",
      "Sample (val) 17 done. 0.014659196138381958\n",
      "Sample (val) 18 done. 0.007050148211419582\n",
      "Sample (val) 19 done. 0.004373728297650814\n",
      "Sample (val) 20 done. 0.010685835033655167\n",
      "Sample (val) 21 done. 0.006552151869982481\n",
      "Sample (val) 22 done. 0.010053947567939758\n",
      "Sample (val) 23 done. 0.009521832689642906\n",
      "Sample (val) 24 done. 0.0030786725692451\n",
      "Sample (val) 25 done. 0.007379212882369757\n",
      "Sample (val) 26 done. 0.004614789970219135\n",
      "Sample (val) 27 done. 0.004577142186462879\n",
      "Sample (val) 28 done. 0.008945980109274387\n",
      "Sample (val) 29 done. 0.007580159232020378\n",
      "Sample (val) 30 done. 0.0024953726679086685\n",
      "Sample (val) 31 done. 0.007875896990299225\n",
      "Sample (val) 32 done. 0.004102662671357393\n",
      "Sample (val) 33 done. 0.007403913419693708\n",
      "Sample (val) 34 done. 0.010272974148392677\n",
      "Sample (val) 35 done. 0.004364142194390297\n",
      "Sample (val) 36 done. 0.004841097630560398\n",
      "Sample (val) 37 done. 0.0092141292989254\n",
      "Sample (val) 38 done. 0.008772864937782288\n",
      "Sample (val) 39 done. 0.004007937386631966\n",
      "Sample (val) 40 done. 0.011267968453466892\n",
      "Sample (val) 41 done. 0.007885950617492199\n",
      "Sample (val) 42 done. 0.003432551631703973\n",
      "Sample (val) 43 done. 0.013988818041980267\n",
      "Sample (val) 44 done. 0.0031239474192261696\n",
      "Sample (val) 45 done. 0.01208227127790451\n",
      "Sample (val) 46 done. 0.011640982702374458\n",
      "Sample (val) 47 done. 0.009432448074221611\n",
      "Sample (val) 48 done. 0.006006956100463867\n",
      "Sample (val) 49 done. 0.01507879514247179\n",
      "Sample (val) 50 done. 0.013096879236400127\n",
      "Sample (val) 51 done. 0.005698598921298981\n",
      "Sample (val) 52 done. 0.005812561605125666\n",
      "Sample (val) 53 done. 0.013391053304076195\n",
      "Sample (val) 54 done. 0.004694376140832901\n",
      "Sample (val) 55 done. 0.007617391180247068\n",
      "Sample (val) 56 done. 0.00519274128600955\n",
      "Sample (val) 57 done. 0.004426967352628708\n",
      "Sample (val) 58 done. 0.012348853051662445\n",
      "Sample (val) 59 done. 0.010527152568101883\n",
      "Sample (val) 60 done. 0.004374437965452671\n",
      "Sample (val) 61 done. 0.007799162063747644\n",
      "Sample (val) 62 done. 0.004161554854363203\n",
      "Sample (val) 63 done. 0.009104782715439796\n",
      "Sample (val) 64 done. 0.0036967522464692593\n",
      "Sample (val) 65 done. 0.00519796134904027\n",
      "Sample (val) 66 done. 0.005091408267617226\n",
      "Sample (val) 67 done. 0.005265441723167896\n",
      "Sample (val) 68 done. 0.005042760167270899\n",
      "Sample (val) 69 done. 0.006240717135369778\n",
      "Sample (val) 70 done. 0.010620122775435448\n",
      "Sample (val) 71 done. 0.010687654837965965\n",
      "Sample (val) 72 done. 0.011087286286056042\n",
      "Sample (val) 73 done. 0.011470363475382328\n",
      "Sample (val) 74 done. 0.005239836871623993\n",
      "Sample (val) 75 done. 0.006009526550769806\n",
      "Sample (val) 76 done. 0.00551735982298851\n",
      "Sample (val) 77 done. 0.006835521198809147\n",
      "Sample (val) 78 done. 0.003764690365642309\n",
      "Sample (val) 79 done. 0.003853111993521452\n",
      "Sample (val) 80 done. 0.008749276399612427\n",
      "Sample (val) 81 done. 0.008321540430188179\n",
      "Sample (val) 82 done. 0.0038865827955305576\n",
      "Sample (val) 83 done. 0.009134754538536072\n",
      "Sample (val) 84 done. 0.004102919716387987\n",
      "Sample (val) 85 done. 0.004078580066561699\n",
      "Sample (val) 86 done. 0.011633574962615967\n",
      "Sample (val) 87 done. 0.009810992516577244\n",
      "Sample (val) 88 done. 0.0036766084376722574\n",
      "Sample (val) 89 done. 0.008158224634826183\n",
      "Sample (val) 90 done. 0.0040519991889595985\n",
      "Sample (val) 91 done. 0.005495102144777775\n",
      "Sample (val) 92 done. 0.012792441993951797\n",
      "Sample (val) 93 done. 0.006466497667133808\n",
      "Sample (val) 94 done. 0.0077833132818341255\n",
      "Sample (val) 95 done. 0.005304522812366486\n",
      "Sample (val) 96 done. 0.019562136381864548\n",
      "Sample (val) 97 done. 0.00297112250700593\n",
      "Sample (val) 98 done. 0.003542743157595396\n",
      "Sample (val) 99 done. 0.00910148024559021\n",
      "Sample (val) 100 done. 0.00653750728815794\n",
      "Sample (val) 101 done. 0.008093887008726597\n",
      "Sample (val) 102 done. 0.004191190470010042\n",
      "Sample (val) 103 done. 0.012838681228458881\n",
      "Sample (val) 104 done. 0.0043857768177986145\n",
      "Sample (val) 105 done. 0.00952303409576416\n",
      "Sample (val) 106 done. 0.0065781353041529655\n",
      "Sample (val) 107 done. 0.005563930608332157\n",
      "Sample (val) 108 done. 0.007591684348881245\n",
      "Sample (val) 109 done. 0.007077942136675119\n",
      "Sample (val) 110 done. 0.012661655433475971\n",
      "Sample (val) 111 done. 0.00441876333206892\n",
      "Sample (val) 112 done. 0.01266535371541977\n",
      "Sample (val) 113 done. 0.010200412943959236\n",
      "Sample (val) 114 done. 0.006717679090797901\n",
      "Sample (val) 115 done. 0.008585749194025993\n",
      "Sample (val) 116 done. 0.00973171554505825\n",
      "Sample (val) 117 done. 0.00578923849388957\n",
      "Sample (val) 118 done. 0.004436681978404522\n",
      "Sample (val) 119 done. 0.009089735336601734\n",
      "Sample (val) 120 done. 0.00619994942098856\n",
      "Sample (val) 121 done. 0.004197242669761181\n",
      "Sample (val) 122 done. 0.0029005552642047405\n",
      "Sample (val) 123 done. 0.013341117650270462\n",
      "Sample (val) 124 done. 0.01188122108578682\n",
      "Sample (val) 125 done. 0.005412844009697437\n",
      "Sample (val) 126 done. 0.00699481600895524\n",
      "Sample (val) 127 done. 0.00432151323184371\n",
      "Sample (val) 128 done. 0.005165057256817818\n",
      "Sample (val) 129 done. 0.008548165671527386\n",
      "Sample (val) 130 done. 0.008781573735177517\n",
      "Sample (val) 131 done. 0.007587009109556675\n",
      "Sample (val) 132 done. 0.02908201888203621\n",
      "Sample (val) 133 done. 0.009100519120693207\n",
      "Sample (val) 134 done. 0.011219428852200508\n",
      "Sample (val) 135 done. 0.008144896477460861\n",
      "Sample (val) 136 done. 0.007044599391520023\n",
      "Sample (val) 137 done. 0.01085146889090538\n",
      "Sample (val) 138 done. 0.008605124428868294\n",
      "Sample (val) 139 done. 0.009661318734288216\n",
      "Sample (val) 140 done. 0.00553702749311924\n",
      "Sample (val) 141 done. 0.010199778713285923\n",
      "Sample (val) 142 done. 0.00566439051181078\n",
      "Sample (val) 143 done. 0.004239215049892664\n",
      "Sample (val) 144 done. 0.012825234793126583\n",
      "Sample (val) 145 done. 0.007189235650002956\n",
      "Sample (val) 146 done. 0.004872820805758238\n",
      "Sample (val) 147 done. 0.005925786215811968\n",
      "Sample (val) 148 done. 0.007083915639668703\n",
      "Sample (val) 149 done. 0.011022558435797691\n",
      "Sample (val) 150 done. 0.010887179523706436\n",
      "Sample (val) 151 done. 0.007549697067588568\n",
      "Sample (val) 152 done. 0.009624801576137543\n",
      "Sample (val) 153 done. 0.00825455505400896\n",
      "Sample (val) 154 done. 0.008679034188389778\n",
      "Sample (val) 155 done. 0.005557870492339134\n",
      "Sample (val) 156 done. 0.005576740019023418\n",
      "Sample (val) 157 done. 0.007964110001921654\n",
      "Sample (val) 158 done. 0.012492554262280464\n",
      "Sample (val) 159 done. 0.012303434312343597\n",
      "Sample (val) 160 done. 0.007899938151240349\n",
      "Sample (val) 161 done. 0.009343136101961136\n",
      "Sample (val) 162 done. 0.004752808250486851\n",
      "Sample (val) 163 done. 0.012543722055852413\n",
      "Sample (val) 164 done. 0.010186847299337387\n",
      "Sample (val) 165 done. 0.007251085713505745\n",
      "Sample (val) 166 done. 0.0041904086247086525\n",
      "Sample (val) 167 done. 0.007300522178411484\n",
      "Sample (val) 168 done. 0.005466195289045572\n",
      "Sample (val) 169 done. 0.009374969638884068\n",
      "Sample (val) 170 done. 0.005257383920252323\n",
      "Sample (val) 171 done. 0.006498104892671108\n",
      "Sample (val) 172 done. 0.004974376875907183\n",
      "Sample (val) 173 done. 0.00987841747701168\n",
      "Sample (val) 174 done. 0.02090124785900116\n",
      "Sample (val) 175 done. 0.007775723002851009\n",
      "Sample (val) 176 done. 0.01065356656908989\n",
      "Sample (val) 177 done. 0.009656522423028946\n",
      "Sample (val) 178 done. 0.00812467373907566\n",
      "Sample (val) 179 done. 0.005071462597697973\n",
      "Sample (val) 180 done. 0.014477688819169998\n",
      "Sample (val) 181 done. 0.004857887513935566\n",
      "Sample (val) 182 done. 0.00676244730129838\n",
      "Sample (val) 183 done. 0.00904957763850689\n",
      "Sample (val) 184 done. 0.003373337909579277\n",
      "Sample (val) 185 done. 0.007094711065292358\n",
      "Sample (val) 186 done. 0.0026843538507819176\n",
      "Sample (val) 187 done. 0.004491528030484915\n",
      "Sample (val) 188 done. 0.006075993645936251\n",
      "Sample (val) 189 done. 0.0041213566437363625\n",
      "Sample (val) 190 done. 0.006812982261180878\n",
      "Sample (val) 191 done. 0.002581024542450905\n",
      "Sample (val) 192 done. 0.008111716248095036\n",
      "Sample (val) 193 done. 0.0059073069132864475\n",
      "Sample (val) 194 done. 0.0055124638602137566\n",
      "Sample (val) 195 done. 0.005888891406357288\n",
      "Sample (val) 196 done. 0.012272175401449203\n",
      "Sample (val) 197 done. 0.004737446550279856\n",
      "Sample (val) 198 done. 0.011174944229424\n",
      "Sample (val) 199 done. 0.0061842696741223335\n",
      "Sample (val) 200 done. 0.009770222008228302\n",
      "Sample (val) 201 done. 0.010246517136693\n",
      "Sample (val) 202 done. 0.009376711212098598\n",
      "Sample (val) 203 done. 0.004508278798311949\n",
      "Sample (val) 204 done. 0.007961048744618893\n",
      "Sample (val) 205 done. 0.0055486783385276794\n",
      "Sample (val) 206 done. 0.015968870371580124\n",
      "Sample (val) 207 done. 0.007606098428368568\n",
      "Sample (val) 208 done. 0.006257506553083658\n",
      "Sample (val) 209 done. 0.0031132432632148266\n",
      "Sample (val) 210 done. 0.004983927123248577\n",
      "Sample (val) 211 done. 0.004007153213024139\n",
      "Sample (val) 212 done. 0.005599925294518471\n",
      "Sample (val) 213 done. 0.0035167874302715063\n",
      "Sample (val) 214 done. 0.008595343679189682\n",
      "Sample (val) 215 done. 0.013092833571135998\n",
      "Sample (val) 216 done. 0.005525813903659582\n",
      "Sample (val) 217 done. 0.007320439908653498\n",
      "Sample (val) 218 done. 0.003896999405696988\n",
      "Sample (val) 219 done. 0.008515852503478527\n",
      "Sample (val) 220 done. 0.011989672668278217\n",
      "Sample (val) 221 done. 0.0038112341426312923\n",
      "Sample (val) 222 done. 0.008073280565440655\n",
      "Sample (val) 223 done. 0.007848204113543034\n",
      "Sample (val) 224 done. 0.004087896551936865\n",
      "Sample (val) 225 done. 0.004154377616941929\n",
      "Sample (val) 226 done. 0.007789905183017254\n",
      "Sample (val) 227 done. 0.004623348359018564\n",
      "Sample (val) 228 done. 0.008781814947724342\n",
      "Sample (val) 229 done. 0.004809377249330282\n",
      "Sample (val) 230 done. 0.0059700715355575085\n",
      "Sample (val) 231 done. 0.012298070825636387\n",
      "Sample (val) 232 done. 0.008491618558764458\n",
      "Sample (val) 233 done. 0.00966801680624485\n",
      "Sample (val) 234 done. 0.011490285396575928\n",
      "Sample (val) 235 done. 0.010540099814534187\n",
      "Sample (val) 236 done. 0.010129575617611408\n",
      "Sample (val) 237 done. 0.007039885967969894\n",
      "Sample (val) 238 done. 0.01033563818782568\n",
      "Sample (val) 239 done. 0.010273121297359467\n",
      "Sample (val) 240 done. 0.008466662839055061\n",
      "Sample (val) 241 done. 0.006339461542665958\n",
      "Sample (val) 242 done. 0.005614597350358963\n",
      "Sample (val) 243 done. 0.011369794607162476\n",
      "Sample (val) 244 done. 0.0032803965732455254\n",
      "Sample (val) 245 done. 0.005229094997048378\n",
      "Sample (val) 246 done. 0.005199382081627846\n",
      "Sample (val) 247 done. 0.006477619521319866\n",
      "Sample (val) 248 done. 0.011527241207659245\n",
      "Sample (val) 249 done. 0.009173830039799213\n",
      "Sample (val) 250 done. 0.008460698649287224\n",
      "Sample (val) 251 done. 0.009301183745265007\n",
      "Sample (val) 252 done. 0.009591617621481419\n",
      "Sample (val) 253 done. 0.006609666161239147\n",
      "Sample (val) 254 done. 0.00711005087941885\n",
      "Sample (val) 255 done. 0.010845266282558441\n",
      "Sample (val) 256 done. 0.006453929468989372\n",
      "Sample (val) 257 done. 0.01203910168260336\n",
      "Sample (val) 258 done. 0.006900967098772526\n",
      "Sample (val) 259 done. 0.007807694375514984\n",
      "Sample (val) 260 done. 0.01031125895678997\n",
      "Sample (val) 261 done. 0.009682541713118553\n",
      "Sample (val) 262 done. 0.012119255028665066\n",
      "Sample (val) 263 done. 0.009383982047438622\n",
      "Sample (val) 264 done. 0.008372430689632893\n",
      "Sample (val) 265 done. 0.0056339045986533165\n",
      "Sample (val) 266 done. 0.0039560627192258835\n",
      "Sample (val) 267 done. 0.005929350852966309\n",
      "Sample (val) 268 done. 0.010749688372015953\n",
      "Sample (val) 269 done. 0.010007986798882484\n",
      "Sample (val) 270 done. 0.006251179613173008\n",
      "Sample (val) 271 done. 0.007494528777897358\n",
      "Sample (val) 272 done. 0.01486985757946968\n",
      "Sample (val) 273 done. 0.003771889954805374\n",
      "Sample (val) 274 done. 0.009364649653434753\n",
      "Sample (val) 275 done. 0.009167918935418129\n",
      "Sample (val) 276 done. 0.008430379442870617\n",
      "Sample (val) 277 done. 0.008394163101911545\n",
      "Sample (val) 278 done. 0.005693433340638876\n",
      "Sample (val) 279 done. 0.008178075775504112\n",
      "Sample (val) 280 done. 0.010512041859328747\n",
      "Sample (val) 281 done. 0.010692091658711433\n",
      "Sample (val) 282 done. 0.013131210580468178\n",
      "Sample (val) 283 done. 0.0085068941116333\n",
      "Sample (val) 284 done. 0.003924350254237652\n",
      "Sample (val) 285 done. 0.00518842414021492\n",
      "Sample (val) 286 done. 0.0032669203355908394\n",
      "Sample (val) 287 done. 0.005247903987765312\n",
      "Sample (val) 288 done. 0.008741922676563263\n",
      "Sample (val) 289 done. 0.004862047731876373\n",
      "Sample (val) 290 done. 0.0076436856761574745\n",
      "Sample (val) 291 done. 0.012778613716363907\n",
      "Sample (val) 292 done. 0.006322312168776989\n",
      "Sample (val) 293 done. 0.015025964006781578\n",
      "Sample (val) 294 done. 0.0042645689100027084\n",
      "Sample (val) 295 done. 0.01207796297967434\n",
      "Sample (val) 296 done. 0.012729359790682793\n",
      "Sample (val) 297 done. 0.01069226861000061\n",
      "Sample (val) 298 done. 0.011452678591012955\n",
      "Sample (val) 299 done. 0.010686718858778477\n",
      "Sample (val) 300 done. 0.0023569941986352205\n",
      "Sample (val) 301 done. 0.011425122618675232\n",
      "Sample (val) 302 done. 0.006383049301803112\n",
      "Sample (val) 303 done. 0.008329097181558609\n",
      "Sample (val) 304 done. 0.003463130444288254\n",
      "Sample (val) 305 done. 0.009584199637174606\n",
      "Sample (val) 306 done. 0.006525624543428421\n",
      "Sample (val) 307 done. 0.005748054012656212\n",
      "Sample (val) 308 done. 0.005554450675845146\n",
      "Sample (val) 309 done. 0.0024230671115219593\n",
      "Sample (val) 310 done. 0.007935313507914543\n",
      "Sample (val) 311 done. 0.009878866374492645\n",
      "Sample (val) 312 done. 0.00815361738204956\n",
      "Sample (val) 313 done. 0.00635280879214406\n",
      "Sample (val) 314 done. 0.005431123077869415\n",
      "Sample (val) 315 done. 0.00596411619335413\n",
      "Sample (val) 316 done. 0.002087461296468973\n",
      "Sample (val) 317 done. 0.0069679394364356995\n",
      "Sample (val) 318 done. 0.002611416857689619\n",
      "Sample (val) 319 done. 0.00822812132537365\n",
      "Sample (val) 320 done. 0.004980576224625111\n",
      "Sample (val) 321 done. 0.006061544641852379\n",
      "Sample (val) 322 done. 0.007550958544015884\n",
      "Sample (val) 323 done. 0.01037275418639183\n",
      "Sample (val) 324 done. 0.011221176013350487\n",
      "Sample (val) 325 done. 0.0072004906833171844\n",
      "Sample (val) 326 done. 0.010848421603441238\n",
      "Sample (val) 327 done. 0.012106613256037235\n",
      "Sample (val) 328 done. 0.006755807902663946\n",
      "Sample (val) 329 done. 0.011514903046190739\n",
      "Sample (val) 330 done. 0.01750222221016884\n",
      "Sample (val) 331 done. 0.007772592827677727\n",
      "Sample (val) 332 done. 0.006065382622182369\n",
      "Sample (val) 333 done. 0.0042000641115009785\n",
      "Sample (val) 334 done. 0.005306145176291466\n",
      "Sample (val) 335 done. 0.008459709584712982\n",
      "Sample (val) 336 done. 0.00960824079811573\n",
      "Sample (val) 337 done. 0.0126827796921134\n",
      "Sample (val) 338 done. 0.008182580582797527\n",
      "Sample (val) 339 done. 0.011540714651346207\n",
      "Sample (val) 340 done. 0.016495676711201668\n",
      "Sample (val) 341 done. 0.009872950613498688\n",
      "Sample (val) 342 done. 0.008325628936290741\n",
      "Sample (val) 343 done. 0.009427757933735847\n",
      "Sample (val) 344 done. 0.0072249360382556915\n",
      "Sample (val) 345 done. 0.007219119928777218\n",
      "Sample (val) 346 done. 0.008153697475790977\n",
      "Sample (val) 347 done. 0.012862855568528175\n",
      "Sample (val) 348 done. 0.007254256866872311\n",
      "Sample (val) 349 done. 0.004517113324254751\n",
      "Sample (val) 350 done. 0.00482057873159647\n",
      "Sample (val) 351 done. 0.004379719495773315\n",
      "Sample (val) 352 done. 0.008172468282282352\n",
      "Sample (val) 353 done. 0.011217605322599411\n",
      "Sample (val) 354 done. 0.007349464576691389\n",
      "Sample (val) 355 done. 0.0058828177861869335\n",
      "Sample (val) 356 done. 0.003599241143092513\n",
      "Sample (val) 357 done. 0.00905604287981987\n",
      "Sample (val) 358 done. 0.012872230261564255\n",
      "Sample (val) 359 done. 0.00546953035518527\n",
      "Sample (val) 360 done. 0.013482947833836079\n",
      "Sample (val) 361 done. 0.0058336518704891205\n",
      "Sample (val) 362 done. 0.010111670009791851\n",
      "Sample (val) 363 done. 0.01207171194255352\n",
      "Sample (val) 364 done. 0.012155579403042793\n",
      "Sample (val) 365 done. 0.010461803525686264\n",
      "Sample (val) 366 done. 0.010333683341741562\n",
      "Sample (val) 367 done. 0.010032940655946732\n",
      "Sample (val) 368 done. 0.011162079870700836\n",
      "Sample (val) 369 done. 0.015845416113734245\n",
      "Sample (val) 370 done. 0.004487368743866682\n",
      "Sample (val) 371 done. 0.008148719556629658\n",
      "Sample (val) 372 done. 0.01814592257142067\n",
      "Sample (val) 373 done. 0.006313066463917494\n",
      "Sample (val) 374 done. 0.008351339027285576\n",
      "Sample (val) 375 done. 0.0038885471876710653\n",
      "Sample (val) 376 done. 0.012967458926141262\n",
      "Sample (val) 377 done. 0.0031224428676068783\n",
      "Sample (val) 378 done. 0.005147398449480534\n",
      "Sample (val) 379 done. 0.0096967164427042\n",
      "Sample (val) 380 done. 0.009234722703695297\n",
      "Sample (val) 381 done. 0.009604975581169128\n",
      "Sample (val) 382 done. 0.011701760813593864\n",
      "Sample (val) 383 done. 0.010001556016504765\n",
      "Sample (val) 384 done. 0.010451234877109528\n",
      "Sample (val) 385 done. 0.003228407585993409\n",
      "Sample (val) 386 done. 0.0057567511685192585\n",
      "Sample (val) 387 done. 0.009904040023684502\n",
      "Sample (val) 388 done. 0.004984857980161905\n",
      "Sample (val) 389 done. 0.0050184521824121475\n",
      "Sample (val) 390 done. 0.006628836505115032\n",
      "Sample (val) 391 done. 0.0031917025335133076\n",
      "Sample (val) 392 done. 0.011983463540673256\n",
      "Sample (val) 393 done. 0.009559313766658306\n",
      "Sample (val) 394 done. 0.009130206890404224\n",
      "Sample (val) 395 done. 0.008630653843283653\n",
      "Sample (val) 396 done. 0.008629085496068\n",
      "Sample (val) 397 done. 0.004776867106556892\n",
      "Sample (val) 398 done. 0.003406230825930834\n",
      "Sample (val) 399 done. 0.003980603534728289\n",
      "Sample (val) 400 done. 0.008954719640314579\n",
      "Sample (val) 401 done. 0.00900543387979269\n",
      "Sample (val) 402 done. 0.006063943728804588\n",
      "Sample (val) 403 done. 0.009278382174670696\n",
      "Sample (val) 404 done. 0.006255886517465115\n",
      "Sample (val) 405 done. 0.004798315931111574\n",
      "Sample (val) 406 done. 0.01163102313876152\n",
      "Sample (val) 407 done. 0.0067657665349543095\n",
      "Sample (val) 408 done. 0.004485939629375935\n",
      "Sample (val) 409 done. 0.007142048794776201\n",
      "Sample (val) 410 done. 0.0128936180844903\n",
      "Sample (val) 411 done. 0.0032647582702338696\n",
      "Sample (val) 412 done. 0.005641750991344452\n",
      "Sample (val) 413 done. 0.011758491396903992\n",
      "Sample (val) 414 done. 0.01238960586488247\n",
      "Sample (val) 415 done. 0.01139550469815731\n",
      "Sample (val) 416 done. 0.0030344410333782434\n",
      "Sample (val) 417 done. 0.005175112746655941\n",
      "Sample (val) 418 done. 0.011613079346716404\n",
      "Sample (val) 419 done. 0.008419279009103775\n",
      "Sample (val) 420 done. 0.007281898520886898\n",
      "Sample (val) 421 done. 0.011277821846306324\n",
      "Sample (val) 422 done. 0.0053827520459890366\n",
      "Sample (val) 423 done. 0.014902008697390556\n",
      "Sample (val) 424 done. 0.009940728545188904\n",
      "Sample (val) 425 done. 0.00817584153264761\n",
      "Sample (val) 426 done. 0.010105481371283531\n",
      "Sample (val) 427 done. 0.010577188804745674\n",
      "Sample (val) 428 done. 0.008668918162584305\n",
      "Sample (val) 429 done. 0.018432587385177612\n",
      "Sample (val) 430 done. 0.0045382678508758545\n",
      "Sample (val) 431 done. 0.01130320318043232\n",
      "Sample (val) 432 done. 0.007219933904707432\n",
      "Sample (val) 433 done. 0.010214980691671371\n",
      "Sample (val) 434 done. 0.007634708657860756\n",
      "Sample (val) 435 done. 0.005570996552705765\n",
      "Sample (val) 436 done. 0.007092542480677366\n",
      "Sample (val) 437 done. 0.008182508870959282\n",
      "Sample (val) 438 done. 0.0048346808180212975\n",
      "Sample (val) 439 done. 0.00918024592101574\n",
      "Sample (val) 440 done. 0.008525406941771507\n",
      "Sample (val) 441 done. 0.0052133603021502495\n",
      "Sample (val) 442 done. 0.005614034365862608\n",
      "Sample (val) 443 done. 0.004388649947941303\n",
      "Sample (val) 444 done. 0.002366339322179556\n",
      "Sample (val) 445 done. 0.010785426944494247\n",
      "Sample (val) 446 done. 0.0049398913979530334\n",
      "Sample (val) 447 done. 0.010503359138965607\n",
      "Sample (val) 448 done. 0.004978841636329889\n",
      "Sample (val) 449 done. 0.01479273196309805\n",
      "Sample (val) 450 done. 0.009732676669955254\n",
      "Sample (val) 451 done. 0.010391264222562313\n",
      "Sample (val) 452 done. 0.006916198879480362\n",
      "Sample (val) 453 done. 0.00845315121114254\n",
      "Sample (val) 454 done. 0.005488940514624119\n",
      "Sample (val) 455 done. 0.0056403037160634995\n",
      "Sample (val) 456 done. 0.004942231811583042\n",
      "Sample (val) 457 done. 0.01313762366771698\n",
      "Sample (val) 458 done. 0.010923784226179123\n",
      "Sample (val) 459 done. 0.004263173323124647\n",
      "Sample (val) 460 done. 0.002364175161346793\n",
      "Sample (val) 461 done. 0.01031566970050335\n",
      "Sample (val) 462 done. 0.006546875927597284\n",
      "Sample (val) 463 done. 0.008569329977035522\n",
      "Sample (val) 464 done. 0.024731868878006935\n",
      "Sample (val) 465 done. 0.004783179610967636\n",
      "Sample (val) 466 done. 0.009676073677837849\n",
      "Sample (val) 467 done. 0.003953746519982815\n",
      "Sample (val) 468 done. 0.00725509924814105\n",
      "Sample (val) 469 done. 0.008752569556236267\n",
      "Sample (val) 470 done. 0.016311174258589745\n",
      "Sample (val) 471 done. 0.009973425418138504\n",
      "Sample (val) 472 done. 0.00522684957832098\n",
      "Sample (val) 473 done. 0.014261783100664616\n",
      "Sample (val) 474 done. 0.008192544803023338\n",
      "Sample (val) 475 done. 0.008873053826391697\n",
      "Sample (val) 476 done. 0.00821303017437458\n",
      "Sample (val) 477 done. 0.004758189432322979\n",
      "Sample (val) 478 done. 0.006601376459002495\n",
      "Sample (val) 479 done. 0.00384656828828156\n",
      "Sample (val) 480 done. 0.013472659513354301\n",
      "Sample (val) 481 done. 0.010013531893491745\n",
      "Sample (val) 482 done. 0.0047405436635017395\n",
      "Sample (val) 483 done. 0.004153050947934389\n",
      "Sample (val) 484 done. 0.007323870435357094\n",
      "Sample (val) 485 done. 0.008995536714792252\n",
      "Sample (val) 486 done. 0.00950261577963829\n",
      "Sample (val) 487 done. 0.004595241509377956\n",
      "Sample (val) 488 done. 0.008094850927591324\n",
      "Sample (val) 489 done. 0.011432632803916931\n",
      "Sample (val) 490 done. 0.011590199545025826\n",
      "Sample (val) 491 done. 0.016522148624062538\n",
      "Sample (val) 492 done. 0.012914536520838737\n",
      "Sample (val) 493 done. 0.004557016771286726\n",
      "Sample (val) 494 done. 0.006098500918596983\n",
      "Sample (val) 495 done. 0.007010120432823896\n",
      "Sample (val) 496 done. 0.00999340321868658\n",
      "Sample (val) 497 done. 0.006715856026858091\n",
      "Sample (val) 498 done. 0.0060319118201732635\n",
      "Sample (val) 499 done. 0.01120375469326973\n",
      "Sample (val) 500 done. 0.005119276233017445\n",
      "Sample (val) 501 done. 0.006500255782157183\n",
      "Sample (val) 502 done. 0.008647965267300606\n",
      "Sample (val) 503 done. 0.0045852079056203365\n",
      "Sample (val) 504 done. 0.01525285467505455\n",
      "Sample (val) 505 done. 0.00724981352686882\n",
      "Sample (val) 506 done. 0.02137344516813755\n",
      "Sample (val) 507 done. 0.008261509239673615\n",
      "Sample (val) 508 done. 0.0063981288112699986\n",
      "Sample (val) 509 done. 0.004406455904245377\n",
      "Sample (val) 510 done. 0.00380910187959671\n",
      "Sample (val) 511 done. 0.006987299304455519\n",
      "Sample (val) 512 done. 0.014672784134745598\n",
      "Sample (val) 513 done. 0.013861194252967834\n",
      "Sample (val) 514 done. 0.004431314766407013\n",
      "Sample (val) 515 done. 0.010280901566147804\n",
      "Sample (val) 516 done. 0.011188101954758167\n",
      "Sample (val) 517 done. 0.009667163714766502\n",
      "Sample (val) 518 done. 0.010419318452477455\n",
      "Sample (val) 519 done. 0.002142487559467554\n",
      "Sample (val) 520 done. 0.006545840296894312\n",
      "Sample (val) 521 done. 0.009045465849339962\n",
      "Sample (val) 522 done. 0.012677645310759544\n",
      "Sample (val) 523 done. 0.009597951546311378\n",
      "Sample (val) 524 done. 0.004841262008994818\n",
      "Sample (val) 525 done. 0.008993267081677914\n",
      "Sample (val) 526 done. 0.008375884965062141\n",
      "Sample (val) 527 done. 0.006073454394936562\n",
      "Sample (val) 528 done. 0.011286371387541294\n",
      "Sample (val) 529 done. 0.003418350126594305\n",
      "Sample (val) 530 done. 0.006571804638952017\n",
      "Sample (val) 531 done. 0.004062213934957981\n",
      "Sample (val) 532 done. 0.003343381453305483\n",
      "Sample (val) 533 done. 0.008142994716763496\n",
      "Sample (val) 534 done. 0.005424621514976025\n",
      "Sample (val) 535 done. 0.004101261496543884\n",
      "Sample (val) 536 done. 0.011481890454888344\n",
      "Sample (val) 537 done. 0.0033471437636762857\n",
      "Sample (val) 538 done. 0.010687870904803276\n",
      "Sample (val) 539 done. 0.006397380493581295\n",
      "Sample (val) 540 done. 0.009883427061140537\n",
      "Sample (val) 541 done. 0.0072250524535775185\n",
      "Sample (val) 542 done. 0.0035168128088116646\n",
      "Sample (val) 543 done. 0.011174440383911133\n",
      "Sample (val) 544 done. 0.005258721299469471\n",
      "Sample (val) 545 done. 0.005798808299005032\n",
      "Sample (val) 546 done. 0.0074323127046227455\n",
      "Sample (val) 547 done. 0.011834481731057167\n",
      "Sample (val) 548 done. 0.013860529288649559\n",
      "Sample (val) 549 done. 0.007964780554175377\n",
      "Sample (val) 550 done. 0.006373198702931404\n",
      "Sample (val) 551 done. 0.0072938124649226665\n",
      "Sample (val) 552 done. 0.007817834615707397\n",
      "Sample (val) 553 done. 0.00872044451534748\n",
      "Sample (val) 554 done. 0.005067712161689997\n",
      "Sample (val) 555 done. 0.01017738040536642\n",
      "Sample (val) 556 done. 0.008759420365095139\n",
      "Sample (val) 557 done. 0.010225935839116573\n",
      "Sample (val) 558 done. 0.005634157452732325\n",
      "Sample (val) 559 done. 0.011062584817409515\n",
      "Sample (val) 560 done. 0.010340843349695206\n",
      "Sample (val) 561 done. 0.013088775798678398\n",
      "Sample (val) 562 done. 0.008094124495983124\n",
      "Sample (val) 563 done. 0.0041864593513309956\n",
      "Sample (val) 564 done. 0.01545962318778038\n",
      "Sample (val) 565 done. 0.01461777277290821\n",
      "Sample (val) 566 done. 0.003106948919594288\n",
      "Sample (val) 567 done. 0.004282759036868811\n",
      "Sample (val) 568 done. 0.011763254180550575\n",
      "Sample (val) 569 done. 0.011969550512731075\n",
      "Sample (val) 570 done. 0.009554751217365265\n",
      "Sample (val) 571 done. 0.004320050589740276\n",
      "Sample (val) 572 done. 0.014485499821603298\n",
      "Sample (val) 573 done. 0.005725268740206957\n",
      "Sample (val) 574 done. 0.006890386343002319\n",
      "Sample (val) 575 done. 0.011002497747540474\n",
      "Sample (val) 576 done. 0.00311510032042861\n",
      "Sample (val) 577 done. 0.007844505831599236\n",
      "Sample (val) 578 done. 0.01141129806637764\n",
      "Sample (val) 579 done. 0.009698089212179184\n",
      "Sample (val) 580 done. 0.015154169872403145\n",
      "Sample (val) 581 done. 0.012941312976181507\n",
      "Sample (val) 582 done. 0.0039267754182219505\n",
      "Sample (val) 583 done. 0.010213103145360947\n",
      "Sample (val) 584 done. 0.003574708942323923\n",
      "Sample (val) 585 done. 0.007044898811727762\n",
      "Sample (val) 586 done. 0.009299065917730331\n",
      "Sample (val) 587 done. 0.007749848533421755\n",
      "Sample (val) 588 done. 0.006464152596890926\n",
      "Sample (val) 589 done. 0.008499613031744957\n",
      "Sample (val) 590 done. 0.01125330850481987\n",
      "Sample (val) 591 done. 0.004788021557033062\n",
      "Sample (val) 592 done. 0.007299284916371107\n",
      "Sample (val) 593 done. 0.007872389629483223\n",
      "Sample (val) 594 done. 0.005393221043050289\n",
      "Sample (val) 595 done. 0.004623495042324066\n",
      "Sample (val) 596 done. 0.006262878887355328\n",
      "Sample (val) 597 done. 0.012567583471536636\n",
      "Sample (val) 598 done. 0.010793008841574192\n",
      "Sample (val) 599 done. 0.011160383000969887\n",
      "chair 1-NN: 0.685\n"
     ]
    }
   ],
   "source": [
    "# 1-NN\n",
    "# one_nn = ONE_NN('airplane_vad', 'val', 'airplane', device=torch.device('cuda:0'))\n",
    "# print(f'airplane 1-NN: {one_nn}')\n",
    "one_nn = ONE_NN('chair_vad_0.05kl', 'val', 'chair', device=torch.device('cuda:0'))\n",
    "print(f'chair 1-NN: {one_nn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane mmd: 0.004007788375020027\n",
      "chair mmd: 0.005049305967986584\n",
      "table mmd: 0.0055902316235005856\n"
     ]
    }
   ],
   "source": [
    "# MMD\n",
    "mmd, mmds = MMD('airplane_vad', 'val', 'airplane', n_samples=10, device=torch.device('cuda:0'))\n",
    "print(f'airplane mmd: {mmd}')\n",
    "mmd, mmds = MMD('chair_vad_0.05kl', 'val', 'chair', n_samples=10, device=torch.device('cuda:0'))\n",
    "print(f'chair mmd: {mmd}')\n",
    "mmd, mmds = MMD('table_vad', 'val', 'table', n_samples=10, device=torch.device('cuda:0'))\n",
    "print(f'table mmd: {mmd}')\n",
    "# avg: 4.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane mmd: 0.03189944103360176\n",
      "chair mmd: 0.06351591646671295\n",
      "table mmd: 0.05973496288061142\n"
     ]
    }
   ],
   "source": [
    "# TMD\n",
    "tmd, samples = TMD('airplane_vad', n_samples=10, device=torch.device('cuda:0'))\n",
    "tmd\n",
    "print(f'airplane mmd: {tmd}')\n",
    "tmd, samples = TMD('chair_vad_0.05kl', n_samples=10, device=torch.device('cuda:0'))\n",
    "tmd\n",
    "print(f'chair mmd: {tmd}')\n",
    "tmd, samples = TMD('table_vad', n_samples=10, device=torch.device('cuda:0'))\n",
    "tmd\n",
    "print(f'table mmd: {tmd}')\n",
    "# avg: 5.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#                  #\n",
    "#    PLAYGROUND    #\n",
    "#                  #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN SAMPLE\n",
    "# generate n new samples\n",
    "n = 10\n",
    "samples = generate_samples('table_vad', n)\n",
    "samples = samples.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c4a32758d242d5b57fae80793004b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890a89827726478eb1eb2bd939159592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get min cf distance to val\n",
    "index = 0\n",
    "sample, cf_distance = min_sample('val', 'table', samples[index], device=torch.device('cuda:0'))\n",
    "# visualize\n",
    "input_mesh = marching_cubes(sample.cpu().detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "input_mesh = marching_cubes(samples[index].cpu().detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0036, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0018)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc = convert_df_to_point_cloud(sample)\n",
    "chamfer_distance(convert_df_to_point_cloud(sample), convert_df_to_point_cloud(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.unsqueeze(0).cuda().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0019, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from chamferdist  import ChamferDistance\n",
    "chamfer_dist = ChamferDistance()\n",
    "dist1 = chamfer_dist(convert_df_to_point_cloud(sample).unsqueeze(0).cuda(), convert_df_to_point_cloud(sample).unsqueeze(0).cuda())\n",
    "print(dist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = []\n",
    "val_dataset = ShapeNet('val', filter_class='airplane')\n",
    "for data_dict in val_dataset:\n",
    "    target_df = torch.from_numpy(data_dict['target_df']).float()\n",
    "    val.append(target_df)\n",
    "val = torch.stack(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs = convert_set_to_point_cloud(val[:1])\n",
    "pcs2 = convert_set_to_point_cloud(val[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0016])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mmd(pcs, pcs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c07b06cf854b4e888d79e6b8f2c53ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 5\n",
    "input_mesh = marching_cubes(samples[index].cpu().detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n new samples\n",
    "n = 10\n",
    "samples = generate_samples('table_vad', n)\n",
    "samples = samples.squeeze(1)\n",
    "# convert_set_to_point_cloud(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd33410c3d30460da76090db6fd40007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 8\n",
    "input_mesh = marching_cubes(samples[index].detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8edcb304c74d7de69396267fdd221ef1d3cdc7db9124f1020d58ca6af5038c14"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
