{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 197376  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 131584  \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | decoder1     | Sequential       | 8389376 \n",
      "6  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "7  | decoder1.1   | BatchNorm3d      | 512     \n",
      "8  | decoder1.2   | ReLU             | 0       \n",
      "9  | decoder2     | Sequential       | 2097536 \n",
      "10 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "11 | decoder2.1   | BatchNorm3d      | 256     \n",
      "12 | decoder2.2   | ReLU             | 0       \n",
      "13 | decoder3     | Sequential       | 524480  \n",
      "14 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "15 | decoder3.1   | BatchNorm3d      | 128     \n",
      "16 | decoder3.2   | ReLU             | 0       \n",
      "17 | decoder4     | Sequential       | 4097    \n",
      "18 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "19 | TOTAL        | ThreeDEPNDecoder | 11212865\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 3173\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class = 'sofa')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a4ad4a7ccd485bbb555824fb40f2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#                #\n",
    "#    TRAINING    #\n",
    "#                #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 3\n",
      "[001/00004] train_loss: 0.010115 kl_loss: 0.007572 normal_loss: 0.002543\n",
      "[001/00004] train_loss: 0.010374 kl_loss: 0.007749 normal_loss: 0.002626\n",
      "[002/00004] train_loss: 0.008716 kl_loss: 0.007542 normal_loss: 0.001174\n",
      "[002/00004] train_loss: 0.008393 kl_loss: 0.007422 normal_loss: 0.000971\n",
      "[003/00004] train_loss: 0.008175 kl_loss: 0.007302 normal_loss: 0.000872\n",
      "[003/00004] train_loss: 0.008076 kl_loss: 0.007325 normal_loss: 0.000750\n",
      "[004/00004] train_loss: 0.007750 kl_loss: 0.007125 normal_loss: 0.000625\n",
      "[004/00004] train_loss: 0.007689 kl_loss: 0.007174 normal_loss: 0.000515\n"
     ]
    }
   ],
   "source": [
    "# OVERFIT\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 4,\n",
    "    'print_every_n': 1,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 1,\n",
    "    'resume_ckpt': None\n",
    "\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4045\n",
      "Training params: 3\n",
      "[001/00035] train_loss: 0.114128 kl_loss: 0.161506 normal_loss: 0.109283\n",
      "[003/00007] train_loss: 0.052188 kl_loss: 0.132147 normal_loss: 0.048223\n",
      "[004/00043] train_loss: 0.044743 kl_loss: 0.095125 normal_loss: 0.041889\n",
      "[006/00015] train_loss: 0.039508 kl_loss: 0.071801 normal_loss: 0.037354\n",
      "[007/00051] train_loss: 0.034843 kl_loss: 0.058168 normal_loss: 0.033097\n",
      "[009/00023] train_loss: 0.033226 kl_loss: 0.053595 normal_loss: 0.031618\n",
      "[010/00059] train_loss: 0.030942 kl_loss: 0.050560 normal_loss: 0.029425\n",
      "[012/00031] train_loss: 0.031112 kl_loss: 0.045498 normal_loss: 0.029747\n",
      "[014/00003] train_loss: 0.030366 kl_loss: 0.041296 normal_loss: 0.029128\n",
      "[015/00039] train_loss: 0.030400 kl_loss: 0.038173 normal_loss: 0.029255\n",
      "[017/00011] train_loss: 0.029179 kl_loss: 0.037097 normal_loss: 0.028066\n",
      "[018/00047] train_loss: 0.028428 kl_loss: 0.037124 normal_loss: 0.027314\n",
      "[020/00019] train_loss: 0.027122 kl_loss: 0.038336 normal_loss: 0.025972\n",
      "[021/00055] train_loss: 0.022507 kl_loss: 0.038033 normal_loss: 0.021366\n",
      "[023/00027] train_loss: 0.022860 kl_loss: 0.035561 normal_loss: 0.021793\n",
      "[024/00063] train_loss: 0.022813 kl_loss: 0.034200 normal_loss: 0.021787\n",
      "[026/00035] train_loss: 0.022576 kl_loss: 0.033834 normal_loss: 0.021561\n",
      "[028/00007] train_loss: 0.022373 kl_loss: 0.034005 normal_loss: 0.021353\n",
      "[029/00043] train_loss: 0.022007 kl_loss: 0.034684 normal_loss: 0.020966\n",
      "[031/00015] train_loss: 0.022097 kl_loss: 0.035406 normal_loss: 0.021035\n",
      "[032/00051] train_loss: 0.022083 kl_loss: 0.036272 normal_loss: 0.020995\n",
      "[034/00023] train_loss: 0.021719 kl_loss: 0.037126 normal_loss: 0.020606\n",
      "[035/00059] train_loss: 0.021652 kl_loss: 0.038146 normal_loss: 0.020508\n",
      "[037/00031] train_loss: 0.020807 kl_loss: 0.039508 normal_loss: 0.019621\n",
      "[039/00003] train_loss: 0.021356 kl_loss: 0.040066 normal_loss: 0.020154\n",
      "[040/00039] train_loss: 0.020410 kl_loss: 0.041546 normal_loss: 0.019163\n",
      "[042/00011] train_loss: 0.018381 kl_loss: 0.041271 normal_loss: 0.017143\n",
      "[043/00047] train_loss: 0.018571 kl_loss: 0.041053 normal_loss: 0.017339\n",
      "[045/00019] train_loss: 0.018562 kl_loss: 0.040616 normal_loss: 0.017344\n",
      "[046/00055] train_loss: 0.018861 kl_loss: 0.040391 normal_loss: 0.017650\n",
      "[048/00027] train_loss: 0.018602 kl_loss: 0.040199 normal_loss: 0.017396\n",
      "[049/00063] train_loss: 0.018444 kl_loss: 0.040322 normal_loss: 0.017234\n",
      "[051/00035] train_loss: 0.018332 kl_loss: 0.040329 normal_loss: 0.017123\n",
      "[053/00007] train_loss: 0.018469 kl_loss: 0.040217 normal_loss: 0.017262\n",
      "[054/00043] train_loss: 0.018330 kl_loss: 0.040312 normal_loss: 0.017121\n",
      "[056/00015] train_loss: 0.018353 kl_loss: 0.040351 normal_loss: 0.017143\n",
      "[057/00051] train_loss: 0.018360 kl_loss: 0.040360 normal_loss: 0.017149\n",
      "[059/00023] train_loss: 0.018123 kl_loss: 0.040516 normal_loss: 0.016908\n",
      "[060/00059] train_loss: 0.017537 kl_loss: 0.040724 normal_loss: 0.016316\n",
      "[062/00031] train_loss: 0.016990 kl_loss: 0.040639 normal_loss: 0.015771\n",
      "[064/00003] train_loss: 0.016986 kl_loss: 0.040742 normal_loss: 0.015763\n",
      "[065/00039] train_loss: 0.016956 kl_loss: 0.040560 normal_loss: 0.015739\n",
      "[067/00011] train_loss: 0.016928 kl_loss: 0.040312 normal_loss: 0.015719\n",
      "[068/00047] train_loss: 0.016988 kl_loss: 0.040446 normal_loss: 0.015775\n",
      "[070/00019] train_loss: 0.016955 kl_loss: 0.040214 normal_loss: 0.015748\n",
      "[071/00055] train_loss: 0.016725 kl_loss: 0.040288 normal_loss: 0.015516\n",
      "[073/00027] train_loss: 0.016875 kl_loss: 0.040523 normal_loss: 0.015659\n",
      "[074/00063] train_loss: 0.016866 kl_loss: 0.040179 normal_loss: 0.015661\n",
      "[076/00035] train_loss: 0.016761 kl_loss: 0.040100 normal_loss: 0.015558\n",
      "[078/00007] train_loss: 0.017030 kl_loss: 0.040471 normal_loss: 0.015816\n",
      "[079/00043] train_loss: 0.016726 kl_loss: 0.039987 normal_loss: 0.015527\n",
      "[081/00015] train_loss: 0.016252 kl_loss: 0.040563 normal_loss: 0.015035\n",
      "[082/00051] train_loss: 0.016407 kl_loss: 0.040330 normal_loss: 0.015197\n",
      "[084/00023] train_loss: 0.016264 kl_loss: 0.040411 normal_loss: 0.015052\n",
      "[085/00059] train_loss: 0.016272 kl_loss: 0.040411 normal_loss: 0.015060\n",
      "[087/00031] train_loss: 0.016312 kl_loss: 0.040318 normal_loss: 0.015102\n",
      "[089/00003] train_loss: 0.016214 kl_loss: 0.040058 normal_loss: 0.015012\n",
      "[090/00039] train_loss: 0.016280 kl_loss: 0.040371 normal_loss: 0.015069\n",
      "[092/00011] train_loss: 0.016169 kl_loss: 0.040361 normal_loss: 0.014958\n",
      "[093/00047] train_loss: 0.016253 kl_loss: 0.040388 normal_loss: 0.015041\n",
      "[095/00019] train_loss: 0.016106 kl_loss: 0.040092 normal_loss: 0.014903\n",
      "[096/00055] train_loss: 0.016208 kl_loss: 0.040225 normal_loss: 0.015001\n",
      "[098/00027] train_loss: 0.016159 kl_loss: 0.040019 normal_loss: 0.014958\n",
      "[099/00063] train_loss: 0.016027 kl_loss: 0.040364 normal_loss: 0.014816\n",
      "[101/00035] train_loss: 0.016010 kl_loss: 0.040316 normal_loss: 0.014800\n",
      "[103/00007] train_loss: 0.015901 kl_loss: 0.040170 normal_loss: 0.014696\n",
      "[104/00043] train_loss: 0.016068 kl_loss: 0.040397 normal_loss: 0.014857\n",
      "[106/00015] train_loss: 0.015826 kl_loss: 0.040003 normal_loss: 0.014626\n",
      "[107/00051] train_loss: 0.015927 kl_loss: 0.040437 normal_loss: 0.014714\n",
      "[109/00023] train_loss: 0.015870 kl_loss: 0.040209 normal_loss: 0.014664\n",
      "[110/00059] train_loss: 0.015887 kl_loss: 0.040166 normal_loss: 0.014682\n",
      "[112/00031] train_loss: 0.015856 kl_loss: 0.040380 normal_loss: 0.014645\n",
      "[114/00003] train_loss: 0.015920 kl_loss: 0.040104 normal_loss: 0.014717\n",
      "[115/00039] train_loss: 0.015917 kl_loss: 0.040315 normal_loss: 0.014708\n",
      "[117/00011] train_loss: 0.015790 kl_loss: 0.040199 normal_loss: 0.014584\n",
      "[118/00047] train_loss: 0.015912 kl_loss: 0.040149 normal_loss: 0.014707\n",
      "[120/00019] train_loss: 0.015845 kl_loss: 0.040342 normal_loss: 0.014635\n",
      "[121/00055] train_loss: 0.015766 kl_loss: 0.040428 normal_loss: 0.014553\n",
      "[123/00027] train_loss: 0.015695 kl_loss: 0.040138 normal_loss: 0.014491\n",
      "[124/00063] train_loss: 0.015786 kl_loss: 0.040350 normal_loss: 0.014575\n",
      "[126/00035] train_loss: 0.015736 kl_loss: 0.040148 normal_loss: 0.014531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mairplane_vad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000020?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=66'>67</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=67'>68</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mmax_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=68'>69</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(train_dataloader):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=69'>70</a>\u001b[0m         \u001b[39m# Move batch to device, set optimizer gradients to zero, perform forward pass\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=70'>71</a>\u001b[0m         ShapeNet\u001b[39m.\u001b[39mmove_batch_to_device(batch, device)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=71'>72</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=365'>366</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=366'>367</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=367'>368</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:314\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=311'>312</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=312'>313</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=313'>314</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:927\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=919'>920</a>\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=920'>921</a>\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=921'>922</a>\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=922'>923</a>\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=923'>924</a>\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=924'>925</a>\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=925'>926</a>\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=926'>927</a>\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=927'>928</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=928'>929</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=117'>118</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=118'>119</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=119'>120</a>\u001b[0m _cleanup()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=122'>123</a>\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=123'>124</a>\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=221'>222</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=222'>223</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=223'>224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=323'>324</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=324'>325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=325'>326</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=326'>327</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=91'>92</a>\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=92'>93</a>\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=93'>94</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=94'>95</a>\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=58'>59</a>\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=59'>60</a>\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AIRPLANE VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4045\n",
      "Training params: 2\n",
      "[001/00035] train_loss: 0.100487 kl_loss: 0.000000 normal_loss: 0.100487\n",
      "[003/00007] train_loss: 0.045317 kl_loss: 0.000000 normal_loss: 0.045317\n",
      "[004/00043] train_loss: 0.037515 kl_loss: 0.000000 normal_loss: 0.037515\n",
      "[006/00015] train_loss: 0.032982 kl_loss: 0.000000 normal_loss: 0.032982\n",
      "[007/00051] train_loss: 0.031851 kl_loss: 0.000000 normal_loss: 0.031851\n",
      "[009/00023] train_loss: 0.031178 kl_loss: 0.000000 normal_loss: 0.031178\n",
      "[010/00059] train_loss: 0.029498 kl_loss: 0.000000 normal_loss: 0.029498\n",
      "[012/00031] train_loss: 0.028000 kl_loss: 0.000000 normal_loss: 0.028000\n",
      "[014/00003] train_loss: 0.026500 kl_loss: 0.000000 normal_loss: 0.026500\n",
      "[015/00039] train_loss: 0.025869 kl_loss: 0.000000 normal_loss: 0.025869\n",
      "[017/00011] train_loss: 0.024928 kl_loss: 0.000000 normal_loss: 0.024928\n",
      "[018/00047] train_loss: 0.024444 kl_loss: 0.000000 normal_loss: 0.024444\n",
      "[020/00019] train_loss: 0.023165 kl_loss: 0.000000 normal_loss: 0.023165\n",
      "[021/00055] train_loss: 0.018484 kl_loss: 0.000000 normal_loss: 0.018484\n",
      "[023/00027] train_loss: 0.018993 kl_loss: 0.000000 normal_loss: 0.018993\n",
      "[024/00063] train_loss: 0.018475 kl_loss: 0.000000 normal_loss: 0.018475\n",
      "[026/00035] train_loss: 0.018003 kl_loss: 0.000000 normal_loss: 0.018003\n",
      "[028/00007] train_loss: 0.017932 kl_loss: 0.000000 normal_loss: 0.017932\n",
      "[029/00043] train_loss: 0.017762 kl_loss: 0.000000 normal_loss: 0.017762\n",
      "[031/00015] train_loss: 0.017184 kl_loss: 0.000000 normal_loss: 0.017184\n",
      "[032/00051] train_loss: 0.016959 kl_loss: 0.000000 normal_loss: 0.016959\n",
      "[034/00023] train_loss: 0.016815 kl_loss: 0.000000 normal_loss: 0.016815\n",
      "[035/00059] train_loss: 0.016776 kl_loss: 0.000000 normal_loss: 0.016776\n",
      "[037/00031] train_loss: 0.016224 kl_loss: 0.000000 normal_loss: 0.016224\n",
      "[039/00003] train_loss: 0.016114 kl_loss: 0.000000 normal_loss: 0.016114\n",
      "[040/00039] train_loss: 0.015268 kl_loss: 0.000000 normal_loss: 0.015268\n",
      "[042/00011] train_loss: 0.013257 kl_loss: 0.000000 normal_loss: 0.013257\n",
      "[043/00047] train_loss: 0.013239 kl_loss: 0.000000 normal_loss: 0.013239\n",
      "[045/00019] train_loss: 0.012938 kl_loss: 0.000000 normal_loss: 0.012938\n",
      "[046/00055] train_loss: 0.013109 kl_loss: 0.000000 normal_loss: 0.013109\n",
      "[048/00027] train_loss: 0.013147 kl_loss: 0.000000 normal_loss: 0.013147\n",
      "[049/00063] train_loss: 0.013316 kl_loss: 0.000000 normal_loss: 0.013316\n",
      "[051/00035] train_loss: 0.013135 kl_loss: 0.000000 normal_loss: 0.013135\n",
      "[053/00007] train_loss: 0.013154 kl_loss: 0.000000 normal_loss: 0.013154\n",
      "[054/00043] train_loss: 0.012913 kl_loss: 0.000000 normal_loss: 0.012913\n",
      "[056/00015] train_loss: 0.012610 kl_loss: 0.000000 normal_loss: 0.012610\n",
      "[057/00051] train_loss: 0.012786 kl_loss: 0.000000 normal_loss: 0.012786\n",
      "[059/00023] train_loss: 0.012564 kl_loss: 0.000000 normal_loss: 0.012564\n",
      "[060/00059] train_loss: 0.011882 kl_loss: 0.000000 normal_loss: 0.011882\n",
      "[062/00031] train_loss: 0.011356 kl_loss: 0.000000 normal_loss: 0.011356\n",
      "[064/00003] train_loss: 0.011342 kl_loss: 0.000000 normal_loss: 0.011342\n",
      "[065/00039] train_loss: 0.011359 kl_loss: 0.000000 normal_loss: 0.011359\n",
      "[067/00011] train_loss: 0.011205 kl_loss: 0.000000 normal_loss: 0.011205\n",
      "[068/00047] train_loss: 0.011212 kl_loss: 0.000000 normal_loss: 0.011212\n",
      "[070/00019] train_loss: 0.011222 kl_loss: 0.000000 normal_loss: 0.011222\n",
      "[071/00055] train_loss: 0.011329 kl_loss: 0.000000 normal_loss: 0.011329\n",
      "[073/00027] train_loss: 0.011024 kl_loss: 0.000000 normal_loss: 0.011024\n",
      "[074/00063] train_loss: 0.011194 kl_loss: 0.000000 normal_loss: 0.011194\n",
      "[076/00035] train_loss: 0.011126 kl_loss: 0.000000 normal_loss: 0.011126\n",
      "[078/00007] train_loss: 0.010896 kl_loss: 0.000000 normal_loss: 0.010896\n",
      "[079/00043] train_loss: 0.010860 kl_loss: 0.000000 normal_loss: 0.010860\n",
      "[081/00015] train_loss: 0.010644 kl_loss: 0.000000 normal_loss: 0.010644\n",
      "[082/00051] train_loss: 0.010540 kl_loss: 0.000000 normal_loss: 0.010540\n",
      "[084/00023] train_loss: 0.010574 kl_loss: 0.000000 normal_loss: 0.010574\n",
      "[085/00059] train_loss: 0.010323 kl_loss: 0.000000 normal_loss: 0.010323\n",
      "[087/00031] train_loss: 0.010495 kl_loss: 0.000000 normal_loss: 0.010495\n",
      "[089/00003] train_loss: 0.010310 kl_loss: 0.000000 normal_loss: 0.010310\n",
      "[090/00039] train_loss: 0.010391 kl_loss: 0.000000 normal_loss: 0.010391\n",
      "[092/00011] train_loss: 0.010340 kl_loss: 0.000000 normal_loss: 0.010340\n",
      "[093/00047] train_loss: 0.010228 kl_loss: 0.000000 normal_loss: 0.010228\n",
      "[095/00019] train_loss: 0.010141 kl_loss: 0.000000 normal_loss: 0.010141\n",
      "[096/00055] train_loss: 0.010193 kl_loss: 0.000000 normal_loss: 0.010193\n",
      "[098/00027] train_loss: 0.010363 kl_loss: 0.000000 normal_loss: 0.010363\n",
      "[099/00063] train_loss: 0.010065 kl_loss: 0.000000 normal_loss: 0.010065\n",
      "[101/00035] train_loss: 0.010003 kl_loss: 0.000000 normal_loss: 0.010003\n",
      "[103/00007] train_loss: 0.009961 kl_loss: 0.000000 normal_loss: 0.009961\n",
      "[104/00043] train_loss: 0.009882 kl_loss: 0.000000 normal_loss: 0.009882\n",
      "[106/00015] train_loss: 0.009877 kl_loss: 0.000000 normal_loss: 0.009877\n",
      "[107/00051] train_loss: 0.009966 kl_loss: 0.000000 normal_loss: 0.009966\n",
      "[109/00023] train_loss: 0.009858 kl_loss: 0.000000 normal_loss: 0.009858\n",
      "[110/00059] train_loss: 0.009913 kl_loss: 0.000000 normal_loss: 0.009913\n",
      "[112/00031] train_loss: 0.009904 kl_loss: 0.000000 normal_loss: 0.009904\n",
      "[114/00003] train_loss: 0.009803 kl_loss: 0.000000 normal_loss: 0.009803\n",
      "[115/00039] train_loss: 0.009801 kl_loss: 0.000000 normal_loss: 0.009801\n",
      "[117/00011] train_loss: 0.009906 kl_loss: 0.000000 normal_loss: 0.009906\n",
      "[118/00047] train_loss: 0.009652 kl_loss: 0.000000 normal_loss: 0.009652\n",
      "[120/00019] train_loss: 0.009822 kl_loss: 0.000000 normal_loss: 0.009822\n",
      "[121/00055] train_loss: 0.009641 kl_loss: 0.000000 normal_loss: 0.009641\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mairplane_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=66'>67</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=67'>68</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mmax_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=68'>69</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(train_dataloader):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=69'>70</a>\u001b[0m         \u001b[39m# Move batch to device, set optimizer gradients to zero, perform forward pass\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=70'>71</a>\u001b[0m         ShapeNet\u001b[39m.\u001b[39mmove_batch_to_device(batch, device)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=71'>72</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=365'>366</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=366'>367</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=367'>368</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:314\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=311'>312</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=312'>313</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=313'>314</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:927\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=919'>920</a>\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=920'>921</a>\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=921'>922</a>\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=922'>923</a>\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=923'>924</a>\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=924'>925</a>\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=925'>926</a>\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=926'>927</a>\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=927'>928</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=928'>929</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=117'>118</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=118'>119</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=119'>120</a>\u001b[0m _cleanup()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=122'>123</a>\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=123'>124</a>\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=221'>222</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=222'>223</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=223'>224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=323'>324</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=324'>325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=325'>326</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=326'>327</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=91'>92</a>\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=92'>93</a>\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=93'>94</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=94'>95</a>\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=58'>59</a>\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=59'>60</a>\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AIRPLANE AD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 6000\n",
      "Training params: 2\n",
      "[001/00005] train_loss: 0.170789 kl_loss: 0.000000 normal_loss: 0.170789\n",
      "[002/00011] train_loss: 0.119310 kl_loss: 0.000000 normal_loss: 0.119310\n",
      "[003/00017] train_loss: 0.112543 kl_loss: 0.000000 normal_loss: 0.112543\n",
      "[004/00023] train_loss: 0.106655 kl_loss: 0.000000 normal_loss: 0.106655\n",
      "[005/00029] train_loss: 0.104214 kl_loss: 0.000000 normal_loss: 0.104214\n",
      "[006/00035] train_loss: 0.098748 kl_loss: 0.000000 normal_loss: 0.098748\n",
      "[007/00041] train_loss: 0.095269 kl_loss: 0.000000 normal_loss: 0.095269\n",
      "[008/00047] train_loss: 0.090359 kl_loss: 0.000000 normal_loss: 0.090359\n",
      "[009/00053] train_loss: 0.089151 kl_loss: 0.000000 normal_loss: 0.089151\n",
      "[010/00059] train_loss: 0.084609 kl_loss: 0.000000 normal_loss: 0.084609\n",
      "[011/00065] train_loss: 0.083053 kl_loss: 0.000000 normal_loss: 0.083053\n",
      "[012/00071] train_loss: 0.081139 kl_loss: 0.000000 normal_loss: 0.081139\n",
      "[013/00077] train_loss: 0.077102 kl_loss: 0.000000 normal_loss: 0.077102\n",
      "[014/00083] train_loss: 0.074768 kl_loss: 0.000000 normal_loss: 0.074768\n",
      "[015/00089] train_loss: 0.073191 kl_loss: 0.000000 normal_loss: 0.073191\n",
      "[017/00001] train_loss: 0.069703 kl_loss: 0.000000 normal_loss: 0.069703\n",
      "[018/00007] train_loss: 0.067230 kl_loss: 0.000000 normal_loss: 0.067230\n",
      "[019/00013] train_loss: 0.064515 kl_loss: 0.000000 normal_loss: 0.064515\n",
      "[020/00019] train_loss: 0.061605 kl_loss: 0.000000 normal_loss: 0.061605\n",
      "[021/00025] train_loss: 0.059996 kl_loss: 0.000000 normal_loss: 0.059996\n",
      "[022/00031] train_loss: 0.058021 kl_loss: 0.000000 normal_loss: 0.058021\n",
      "[023/00037] train_loss: 0.056354 kl_loss: 0.000000 normal_loss: 0.056354\n",
      "[024/00043] train_loss: 0.054989 kl_loss: 0.000000 normal_loss: 0.054989\n",
      "[025/00049] train_loss: 0.054142 kl_loss: 0.000000 normal_loss: 0.054142\n",
      "[026/00055] train_loss: 0.051831 kl_loss: 0.000000 normal_loss: 0.051831\n",
      "[027/00061] train_loss: 0.049995 kl_loss: 0.000000 normal_loss: 0.049995\n",
      "[028/00067] train_loss: 0.049668 kl_loss: 0.000000 normal_loss: 0.049668\n",
      "[029/00073] train_loss: 0.047884 kl_loss: 0.000000 normal_loss: 0.047884\n",
      "[030/00079] train_loss: 0.047094 kl_loss: 0.000000 normal_loss: 0.047094\n",
      "[031/00085] train_loss: 0.044595 kl_loss: 0.000000 normal_loss: 0.044595\n",
      "[032/00091] train_loss: 0.045461 kl_loss: 0.000000 normal_loss: 0.045461\n",
      "[034/00003] train_loss: 0.043912 kl_loss: 0.000000 normal_loss: 0.043912\n",
      "[035/00009] train_loss: 0.042033 kl_loss: 0.000000 normal_loss: 0.042033\n",
      "[036/00015] train_loss: 0.040447 kl_loss: 0.000000 normal_loss: 0.040447\n",
      "[037/00021] train_loss: 0.040736 kl_loss: 0.000000 normal_loss: 0.040736\n",
      "[038/00027] train_loss: 0.038222 kl_loss: 0.000000 normal_loss: 0.038222\n",
      "[039/00033] train_loss: 0.038747 kl_loss: 0.000000 normal_loss: 0.038747\n",
      "[040/00039] train_loss: 0.037341 kl_loss: 0.000000 normal_loss: 0.037341\n",
      "[041/00045] train_loss: 0.036072 kl_loss: 0.000000 normal_loss: 0.036072\n",
      "[042/00051] train_loss: 0.035633 kl_loss: 0.000000 normal_loss: 0.035633\n",
      "[043/00057] train_loss: 0.036622 kl_loss: 0.000000 normal_loss: 0.036622\n",
      "[044/00063] train_loss: 0.035593 kl_loss: 0.000000 normal_loss: 0.035593\n",
      "[045/00069] train_loss: 0.034966 kl_loss: 0.000000 normal_loss: 0.034966\n",
      "[046/00075] train_loss: 0.033288 kl_loss: 0.000000 normal_loss: 0.033288\n",
      "[047/00081] train_loss: 0.033041 kl_loss: 0.000000 normal_loss: 0.033041\n",
      "[048/00087] train_loss: 0.032261 kl_loss: 0.000000 normal_loss: 0.032261\n",
      "[049/00093] train_loss: 0.032458 kl_loss: 0.000000 normal_loss: 0.032458\n",
      "[051/00005] train_loss: 0.031604 kl_loss: 0.000000 normal_loss: 0.031604\n",
      "[052/00011] train_loss: 0.030685 kl_loss: 0.000000 normal_loss: 0.030685\n",
      "[053/00017] train_loss: 0.029773 kl_loss: 0.000000 normal_loss: 0.029773\n",
      "[054/00023] train_loss: 0.029984 kl_loss: 0.000000 normal_loss: 0.029984\n",
      "[055/00029] train_loss: 0.029556 kl_loss: 0.000000 normal_loss: 0.029556\n",
      "[056/00035] train_loss: 0.029127 kl_loss: 0.000000 normal_loss: 0.029127\n",
      "[057/00041] train_loss: 0.028202 kl_loss: 0.000000 normal_loss: 0.028202\n",
      "[058/00047] train_loss: 0.028775 kl_loss: 0.000000 normal_loss: 0.028775\n",
      "[059/00053] train_loss: 0.027824 kl_loss: 0.000000 normal_loss: 0.027824\n",
      "[060/00059] train_loss: 0.026386 kl_loss: 0.000000 normal_loss: 0.026386\n",
      "[061/00065] train_loss: 0.027572 kl_loss: 0.000000 normal_loss: 0.027572\n",
      "[062/00071] train_loss: 0.027168 kl_loss: 0.000000 normal_loss: 0.027168\n",
      "[063/00077] train_loss: 0.026140 kl_loss: 0.000000 normal_loss: 0.026140\n",
      "[064/00083] train_loss: 0.026373 kl_loss: 0.000000 normal_loss: 0.026373\n",
      "[065/00089] train_loss: 0.025501 kl_loss: 0.000000 normal_loss: 0.025501\n",
      "[067/00001] train_loss: 0.026152 kl_loss: 0.000000 normal_loss: 0.026152\n",
      "[068/00007] train_loss: 0.025439 kl_loss: 0.000000 normal_loss: 0.025439\n",
      "[069/00013] train_loss: 0.025331 kl_loss: 0.000000 normal_loss: 0.025331\n",
      "[070/00019] train_loss: 0.024891 kl_loss: 0.000000 normal_loss: 0.024891\n",
      "[071/00025] train_loss: 0.023769 kl_loss: 0.000000 normal_loss: 0.023769\n",
      "[072/00031] train_loss: 0.023991 kl_loss: 0.000000 normal_loss: 0.023991\n",
      "[073/00037] train_loss: 0.023924 kl_loss: 0.000000 normal_loss: 0.023924\n",
      "[074/00043] train_loss: 0.024434 kl_loss: 0.000000 normal_loss: 0.024434\n",
      "[075/00049] train_loss: 0.023806 kl_loss: 0.000000 normal_loss: 0.023806\n",
      "[076/00055] train_loss: 0.022933 kl_loss: 0.000000 normal_loss: 0.022933\n",
      "[077/00061] train_loss: 0.023120 kl_loss: 0.000000 normal_loss: 0.023120\n",
      "[078/00067] train_loss: 0.023447 kl_loss: 0.000000 normal_loss: 0.023447\n",
      "[079/00073] train_loss: 0.023398 kl_loss: 0.000000 normal_loss: 0.023398\n",
      "[080/00079] train_loss: 0.023096 kl_loss: 0.000000 normal_loss: 0.023096\n",
      "[081/00085] train_loss: 0.022661 kl_loss: 0.000000 normal_loss: 0.022661\n",
      "[082/00091] train_loss: 0.022616 kl_loss: 0.000000 normal_loss: 0.022616\n",
      "[084/00003] train_loss: 0.022916 kl_loss: 0.000000 normal_loss: 0.022916\n",
      "[085/00009] train_loss: 0.022678 kl_loss: 0.000000 normal_loss: 0.022678\n",
      "[086/00015] train_loss: 0.022548 kl_loss: 0.000000 normal_loss: 0.022548\n",
      "[087/00021] train_loss: 0.022197 kl_loss: 0.000000 normal_loss: 0.022197\n",
      "[088/00027] train_loss: 0.022057 kl_loss: 0.000000 normal_loss: 0.022057\n",
      "[089/00033] train_loss: 0.021671 kl_loss: 0.000000 normal_loss: 0.021671\n",
      "[090/00039] train_loss: 0.021144 kl_loss: 0.000000 normal_loss: 0.021144\n",
      "[091/00045] train_loss: 0.021607 kl_loss: 0.000000 normal_loss: 0.021607\n",
      "[092/00051] train_loss: 0.021018 kl_loss: 0.000000 normal_loss: 0.021018\n",
      "[093/00057] train_loss: 0.021011 kl_loss: 0.000000 normal_loss: 0.021011\n",
      "[094/00063] train_loss: 0.020048 kl_loss: 0.000000 normal_loss: 0.020048\n",
      "[095/00069] train_loss: 0.020947 kl_loss: 0.000000 normal_loss: 0.020947\n",
      "[096/00075] train_loss: 0.021325 kl_loss: 0.000000 normal_loss: 0.021325\n",
      "[097/00081] train_loss: 0.020538 kl_loss: 0.000000 normal_loss: 0.020538\n",
      "[098/00087] train_loss: 0.021223 kl_loss: 0.000000 normal_loss: 0.021223\n",
      "[099/00093] train_loss: 0.020470 kl_loss: 0.000000 normal_loss: 0.020470\n",
      "[101/00005] train_loss: 0.017555 kl_loss: 0.000000 normal_loss: 0.017555\n",
      "[102/00011] train_loss: 0.015651 kl_loss: 0.000000 normal_loss: 0.015651\n",
      "[103/00017] train_loss: 0.015574 kl_loss: 0.000000 normal_loss: 0.015574\n",
      "[104/00023] train_loss: 0.015257 kl_loss: 0.000000 normal_loss: 0.015257\n",
      "[105/00029] train_loss: 0.015437 kl_loss: 0.000000 normal_loss: 0.015437\n",
      "[106/00035] train_loss: 0.015329 kl_loss: 0.000000 normal_loss: 0.015329\n",
      "[107/00041] train_loss: 0.015530 kl_loss: 0.000000 normal_loss: 0.015530\n",
      "[108/00047] train_loss: 0.015547 kl_loss: 0.000000 normal_loss: 0.015547\n",
      "[109/00053] train_loss: 0.015737 kl_loss: 0.000000 normal_loss: 0.015737\n",
      "[110/00059] train_loss: 0.015648 kl_loss: 0.000000 normal_loss: 0.015648\n",
      "[111/00065] train_loss: 0.015295 kl_loss: 0.000000 normal_loss: 0.015295\n",
      "[112/00071] train_loss: 0.015117 kl_loss: 0.000000 normal_loss: 0.015117\n",
      "[113/00077] train_loss: 0.015775 kl_loss: 0.000000 normal_loss: 0.015775\n",
      "[114/00083] train_loss: 0.016034 kl_loss: 0.000000 normal_loss: 0.016034\n",
      "[115/00089] train_loss: 0.015433 kl_loss: 0.000000 normal_loss: 0.015433\n",
      "[117/00001] train_loss: 0.015185 kl_loss: 0.000000 normal_loss: 0.015185\n",
      "[118/00007] train_loss: 0.015829 kl_loss: 0.000000 normal_loss: 0.015829\n",
      "[119/00013] train_loss: 0.015666 kl_loss: 0.000000 normal_loss: 0.015666\n",
      "[120/00019] train_loss: 0.015286 kl_loss: 0.000000 normal_loss: 0.015286\n",
      "[121/00025] train_loss: 0.015482 kl_loss: 0.000000 normal_loss: 0.015482\n",
      "[122/00031] train_loss: 0.015904 kl_loss: 0.000000 normal_loss: 0.015904\n",
      "[123/00037] train_loss: 0.015639 kl_loss: 0.000000 normal_loss: 0.015639\n",
      "[124/00043] train_loss: 0.015177 kl_loss: 0.000000 normal_loss: 0.015177\n",
      "[125/00049] train_loss: 0.014996 kl_loss: 0.000000 normal_loss: 0.014996\n",
      "[126/00055] train_loss: 0.014901 kl_loss: 0.000000 normal_loss: 0.014901\n",
      "[127/00061] train_loss: 0.015679 kl_loss: 0.000000 normal_loss: 0.015679\n",
      "[128/00067] train_loss: 0.015258 kl_loss: 0.000000 normal_loss: 0.015258\n",
      "[129/00073] train_loss: 0.015340 kl_loss: 0.000000 normal_loss: 0.015340\n",
      "[130/00079] train_loss: 0.015420 kl_loss: 0.000000 normal_loss: 0.015420\n",
      "[131/00085] train_loss: 0.015211 kl_loss: 0.000000 normal_loss: 0.015211\n",
      "[132/00091] train_loss: 0.014825 kl_loss: 0.000000 normal_loss: 0.014825\n",
      "[134/00003] train_loss: 0.014890 kl_loss: 0.000000 normal_loss: 0.014890\n",
      "[135/00009] train_loss: 0.015089 kl_loss: 0.000000 normal_loss: 0.015089\n",
      "[136/00015] train_loss: 0.015091 kl_loss: 0.000000 normal_loss: 0.015091\n",
      "[137/00021] train_loss: 0.015065 kl_loss: 0.000000 normal_loss: 0.015065\n",
      "[138/00027] train_loss: 0.014805 kl_loss: 0.000000 normal_loss: 0.014805\n",
      "[139/00033] train_loss: 0.014967 kl_loss: 0.000000 normal_loss: 0.014967\n",
      "[140/00039] train_loss: 0.014646 kl_loss: 0.000000 normal_loss: 0.014646\n",
      "[141/00045] train_loss: 0.014927 kl_loss: 0.000000 normal_loss: 0.014927\n",
      "[142/00051] train_loss: 0.014857 kl_loss: 0.000000 normal_loss: 0.014857\n",
      "[143/00057] train_loss: 0.014817 kl_loss: 0.000000 normal_loss: 0.014817\n",
      "[144/00063] train_loss: 0.014645 kl_loss: 0.000000 normal_loss: 0.014645\n",
      "[145/00069] train_loss: 0.014525 kl_loss: 0.000000 normal_loss: 0.014525\n",
      "[146/00075] train_loss: 0.014808 kl_loss: 0.000000 normal_loss: 0.014808\n",
      "[147/00081] train_loss: 0.014537 kl_loss: 0.000000 normal_loss: 0.014537\n",
      "[148/00087] train_loss: 0.014322 kl_loss: 0.000000 normal_loss: 0.014322\n",
      "[149/00093] train_loss: 0.014082 kl_loss: 0.000000 normal_loss: 0.014082\n",
      "[151/00005] train_loss: 0.014684 kl_loss: 0.000000 normal_loss: 0.014684\n",
      "[152/00011] train_loss: 0.014507 kl_loss: 0.000000 normal_loss: 0.014507\n",
      "[153/00017] train_loss: 0.015022 kl_loss: 0.000000 normal_loss: 0.015022\n",
      "[154/00023] train_loss: 0.014567 kl_loss: 0.000000 normal_loss: 0.014567\n",
      "[155/00029] train_loss: 0.014520 kl_loss: 0.000000 normal_loss: 0.014520\n",
      "[156/00035] train_loss: 0.014692 kl_loss: 0.000000 normal_loss: 0.014692\n",
      "[157/00041] train_loss: 0.014294 kl_loss: 0.000000 normal_loss: 0.014294\n",
      "[158/00047] train_loss: 0.014516 kl_loss: 0.000000 normal_loss: 0.014516\n",
      "[159/00053] train_loss: 0.013960 kl_loss: 0.000000 normal_loss: 0.013960\n",
      "[160/00059] train_loss: 0.013956 kl_loss: 0.000000 normal_loss: 0.013956\n",
      "[161/00065] train_loss: 0.014307 kl_loss: 0.000000 normal_loss: 0.014307\n",
      "[162/00071] train_loss: 0.014611 kl_loss: 0.000000 normal_loss: 0.014611\n",
      "[163/00077] train_loss: 0.014380 kl_loss: 0.000000 normal_loss: 0.014380\n",
      "[164/00083] train_loss: 0.014190 kl_loss: 0.000000 normal_loss: 0.014190\n",
      "[165/00089] train_loss: 0.014364 kl_loss: 0.000000 normal_loss: 0.014364\n",
      "[167/00001] train_loss: 0.014217 kl_loss: 0.000000 normal_loss: 0.014217\n",
      "[168/00007] train_loss: 0.013905 kl_loss: 0.000000 normal_loss: 0.013905\n",
      "[169/00013] train_loss: 0.013691 kl_loss: 0.000000 normal_loss: 0.013691\n",
      "[170/00019] train_loss: 0.013563 kl_loss: 0.000000 normal_loss: 0.013563\n",
      "[171/00025] train_loss: 0.014289 kl_loss: 0.000000 normal_loss: 0.014289\n",
      "[172/00031] train_loss: 0.014284 kl_loss: 0.000000 normal_loss: 0.014284\n",
      "[173/00037] train_loss: 0.014072 kl_loss: 0.000000 normal_loss: 0.014072\n",
      "[174/00043] train_loss: 0.014386 kl_loss: 0.000000 normal_loss: 0.014386\n",
      "[175/00049] train_loss: 0.014073 kl_loss: 0.000000 normal_loss: 0.014073\n",
      "[176/00055] train_loss: 0.013806 kl_loss: 0.000000 normal_loss: 0.013806\n",
      "[177/00061] train_loss: 0.013736 kl_loss: 0.000000 normal_loss: 0.013736\n",
      "[178/00067] train_loss: 0.014115 kl_loss: 0.000000 normal_loss: 0.014115\n",
      "[179/00073] train_loss: 0.013644 kl_loss: 0.000000 normal_loss: 0.013644\n",
      "[180/00079] train_loss: 0.013642 kl_loss: 0.000000 normal_loss: 0.013642\n",
      "[181/00085] train_loss: 0.013909 kl_loss: 0.000000 normal_loss: 0.013909\n",
      "[182/00091] train_loss: 0.013913 kl_loss: 0.000000 normal_loss: 0.013913\n",
      "[184/00003] train_loss: 0.014096 kl_loss: 0.000000 normal_loss: 0.014096\n",
      "[185/00009] train_loss: 0.013900 kl_loss: 0.000000 normal_loss: 0.013900\n",
      "[186/00015] train_loss: 0.013933 kl_loss: 0.000000 normal_loss: 0.013933\n",
      "[187/00021] train_loss: 0.013852 kl_loss: 0.000000 normal_loss: 0.013852\n",
      "[188/00027] train_loss: 0.013193 kl_loss: 0.000000 normal_loss: 0.013193\n",
      "[189/00033] train_loss: 0.013133 kl_loss: 0.000000 normal_loss: 0.013133\n",
      "[190/00039] train_loss: 0.013644 kl_loss: 0.000000 normal_loss: 0.013644\n",
      "[191/00045] train_loss: 0.013735 kl_loss: 0.000000 normal_loss: 0.013735\n",
      "[192/00051] train_loss: 0.014015 kl_loss: 0.000000 normal_loss: 0.014015\n",
      "[193/00057] train_loss: 0.013924 kl_loss: 0.000000 normal_loss: 0.013924\n",
      "[194/00063] train_loss: 0.013562 kl_loss: 0.000000 normal_loss: 0.013562\n",
      "[195/00069] train_loss: 0.013730 kl_loss: 0.000000 normal_loss: 0.013730\n",
      "[196/00075] train_loss: 0.013645 kl_loss: 0.000000 normal_loss: 0.013645\n",
      "[197/00081] train_loss: 0.013245 kl_loss: 0.000000 normal_loss: 0.013245\n",
      "[198/00087] train_loss: 0.013792 kl_loss: 0.000000 normal_loss: 0.013792\n",
      "[199/00093] train_loss: 0.013519 kl_loss: 0.000000 normal_loss: 0.013519\n",
      "[201/00005] train_loss: 0.012326 kl_loss: 0.000000 normal_loss: 0.012326\n",
      "[202/00011] train_loss: 0.011625 kl_loss: 0.000000 normal_loss: 0.011625\n",
      "[203/00017] train_loss: 0.011559 kl_loss: 0.000000 normal_loss: 0.011559\n",
      "[204/00023] train_loss: 0.011415 kl_loss: 0.000000 normal_loss: 0.011415\n",
      "[205/00029] train_loss: 0.011501 kl_loss: 0.000000 normal_loss: 0.011501\n",
      "[206/00035] train_loss: 0.011453 kl_loss: 0.000000 normal_loss: 0.011453\n",
      "[207/00041] train_loss: 0.011392 kl_loss: 0.000000 normal_loss: 0.011392\n",
      "[208/00047] train_loss: 0.011547 kl_loss: 0.000000 normal_loss: 0.011547\n",
      "[209/00053] train_loss: 0.011568 kl_loss: 0.000000 normal_loss: 0.011568\n",
      "[210/00059] train_loss: 0.011670 kl_loss: 0.000000 normal_loss: 0.011670\n",
      "[211/00065] train_loss: 0.011558 kl_loss: 0.000000 normal_loss: 0.011558\n",
      "[212/00071] train_loss: 0.011680 kl_loss: 0.000000 normal_loss: 0.011680\n",
      "[213/00077] train_loss: 0.011607 kl_loss: 0.000000 normal_loss: 0.011607\n",
      "[214/00083] train_loss: 0.011376 kl_loss: 0.000000 normal_loss: 0.011376\n",
      "[215/00089] train_loss: 0.011502 kl_loss: 0.000000 normal_loss: 0.011502\n",
      "[217/00001] train_loss: 0.011622 kl_loss: 0.000000 normal_loss: 0.011622\n",
      "[218/00007] train_loss: 0.011672 kl_loss: 0.000000 normal_loss: 0.011672\n",
      "[219/00013] train_loss: 0.011692 kl_loss: 0.000000 normal_loss: 0.011692\n",
      "[220/00019] train_loss: 0.011661 kl_loss: 0.000000 normal_loss: 0.011661\n",
      "[221/00025] train_loss: 0.011621 kl_loss: 0.000000 normal_loss: 0.011621\n",
      "[222/00031] train_loss: 0.011576 kl_loss: 0.000000 normal_loss: 0.011576\n",
      "[223/00037] train_loss: 0.011488 kl_loss: 0.000000 normal_loss: 0.011488\n",
      "[224/00043] train_loss: 0.011545 kl_loss: 0.000000 normal_loss: 0.011545\n",
      "[225/00049] train_loss: 0.011521 kl_loss: 0.000000 normal_loss: 0.011521\n",
      "[226/00055] train_loss: 0.011663 kl_loss: 0.000000 normal_loss: 0.011663\n",
      "[227/00061] train_loss: 0.011432 kl_loss: 0.000000 normal_loss: 0.011432\n",
      "[228/00067] train_loss: 0.011706 kl_loss: 0.000000 normal_loss: 0.011706\n",
      "[229/00073] train_loss: 0.011519 kl_loss: 0.000000 normal_loss: 0.011519\n",
      "[230/00079] train_loss: 0.011489 kl_loss: 0.000000 normal_loss: 0.011489\n",
      "[231/00085] train_loss: 0.011500 kl_loss: 0.000000 normal_loss: 0.011500\n",
      "[232/00091] train_loss: 0.011477 kl_loss: 0.000000 normal_loss: 0.011477\n",
      "[234/00003] train_loss: 0.011595 kl_loss: 0.000000 normal_loss: 0.011595\n",
      "[235/00009] train_loss: 0.011436 kl_loss: 0.000000 normal_loss: 0.011436\n",
      "[236/00015] train_loss: 0.011719 kl_loss: 0.000000 normal_loss: 0.011719\n",
      "[237/00021] train_loss: 0.011364 kl_loss: 0.000000 normal_loss: 0.011364\n",
      "[238/00027] train_loss: 0.011530 kl_loss: 0.000000 normal_loss: 0.011530\n",
      "[239/00033] train_loss: 0.011472 kl_loss: 0.000000 normal_loss: 0.011472\n",
      "[240/00039] train_loss: 0.011302 kl_loss: 0.000000 normal_loss: 0.011302\n",
      "[241/00045] train_loss: 0.011359 kl_loss: 0.000000 normal_loss: 0.011359\n",
      "[242/00051] train_loss: 0.011315 kl_loss: 0.000000 normal_loss: 0.011315\n",
      "[243/00057] train_loss: 0.011296 kl_loss: 0.000000 normal_loss: 0.011296\n",
      "[244/00063] train_loss: 0.011389 kl_loss: 0.000000 normal_loss: 0.011389\n",
      "[245/00069] train_loss: 0.011350 kl_loss: 0.000000 normal_loss: 0.011350\n",
      "[246/00075] train_loss: 0.011162 kl_loss: 0.000000 normal_loss: 0.011162\n",
      "[247/00081] train_loss: 0.011681 kl_loss: 0.000000 normal_loss: 0.011681\n",
      "[248/00087] train_loss: 0.011401 kl_loss: 0.000000 normal_loss: 0.011401\n",
      "[249/00093] train_loss: 0.011451 kl_loss: 0.000000 normal_loss: 0.011451\n",
      "[251/00005] train_loss: 0.011298 kl_loss: 0.000000 normal_loss: 0.011298\n",
      "[252/00011] train_loss: 0.011140 kl_loss: 0.000000 normal_loss: 0.011140\n",
      "[253/00017] train_loss: 0.011190 kl_loss: 0.000000 normal_loss: 0.011190\n",
      "[254/00023] train_loss: 0.011457 kl_loss: 0.000000 normal_loss: 0.011457\n",
      "[255/00029] train_loss: 0.011560 kl_loss: 0.000000 normal_loss: 0.011560\n",
      "[256/00035] train_loss: 0.011217 kl_loss: 0.000000 normal_loss: 0.011217\n",
      "[257/00041] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[258/00047] train_loss: 0.011403 kl_loss: 0.000000 normal_loss: 0.011403\n",
      "[259/00053] train_loss: 0.011368 kl_loss: 0.000000 normal_loss: 0.011368\n",
      "[260/00059] train_loss: 0.011352 kl_loss: 0.000000 normal_loss: 0.011352\n",
      "[261/00065] train_loss: 0.011223 kl_loss: 0.000000 normal_loss: 0.011223\n",
      "[262/00071] train_loss: 0.011281 kl_loss: 0.000000 normal_loss: 0.011281\n",
      "[263/00077] train_loss: 0.011121 kl_loss: 0.000000 normal_loss: 0.011121\n",
      "[264/00083] train_loss: 0.011101 kl_loss: 0.000000 normal_loss: 0.011101\n",
      "[265/00089] train_loss: 0.011221 kl_loss: 0.000000 normal_loss: 0.011221\n",
      "[267/00001] train_loss: 0.011442 kl_loss: 0.000000 normal_loss: 0.011442\n",
      "[268/00007] train_loss: 0.011258 kl_loss: 0.000000 normal_loss: 0.011258\n",
      "[269/00013] train_loss: 0.011185 kl_loss: 0.000000 normal_loss: 0.011185\n",
      "[270/00019] train_loss: 0.011169 kl_loss: 0.000000 normal_loss: 0.011169\n",
      "[271/00025] train_loss: 0.011144 kl_loss: 0.000000 normal_loss: 0.011144\n",
      "[272/00031] train_loss: 0.011069 kl_loss: 0.000000 normal_loss: 0.011069\n",
      "[273/00037] train_loss: 0.011306 kl_loss: 0.000000 normal_loss: 0.011306\n",
      "[274/00043] train_loss: 0.011143 kl_loss: 0.000000 normal_loss: 0.011143\n",
      "[275/00049] train_loss: 0.011317 kl_loss: 0.000000 normal_loss: 0.011317\n",
      "[276/00055] train_loss: 0.011050 kl_loss: 0.000000 normal_loss: 0.011050\n",
      "[277/00061] train_loss: 0.011095 kl_loss: 0.000000 normal_loss: 0.011095\n",
      "[278/00067] train_loss: 0.011292 kl_loss: 0.000000 normal_loss: 0.011292\n",
      "[279/00073] train_loss: 0.011090 kl_loss: 0.000000 normal_loss: 0.011090\n",
      "[280/00079] train_loss: 0.011195 kl_loss: 0.000000 normal_loss: 0.011195\n",
      "[281/00085] train_loss: 0.011025 kl_loss: 0.000000 normal_loss: 0.011025\n",
      "[282/00091] train_loss: 0.011012 kl_loss: 0.000000 normal_loss: 0.011012\n",
      "[284/00003] train_loss: 0.010984 kl_loss: 0.000000 normal_loss: 0.010984\n",
      "[285/00009] train_loss: 0.011148 kl_loss: 0.000000 normal_loss: 0.011148\n",
      "[286/00015] train_loss: 0.011265 kl_loss: 0.000000 normal_loss: 0.011265\n",
      "[287/00021] train_loss: 0.011245 kl_loss: 0.000000 normal_loss: 0.011245\n",
      "[288/00027] train_loss: 0.010894 kl_loss: 0.000000 normal_loss: 0.010894\n",
      "[289/00033] train_loss: 0.010918 kl_loss: 0.000000 normal_loss: 0.010918\n",
      "[290/00039] train_loss: 0.011049 kl_loss: 0.000000 normal_loss: 0.011049\n",
      "[291/00045] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[292/00051] train_loss: 0.011161 kl_loss: 0.000000 normal_loss: 0.011161\n",
      "[293/00057] train_loss: 0.011162 kl_loss: 0.000000 normal_loss: 0.011162\n",
      "[294/00063] train_loss: 0.011098 kl_loss: 0.000000 normal_loss: 0.011098\n",
      "[295/00069] train_loss: 0.010842 kl_loss: 0.000000 normal_loss: 0.010842\n",
      "[296/00075] train_loss: 0.011159 kl_loss: 0.000000 normal_loss: 0.011159\n",
      "[297/00081] train_loss: 0.010983 kl_loss: 0.000000 normal_loss: 0.010983\n",
      "[298/00087] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[299/00093] train_loss: 0.011041 kl_loss: 0.000000 normal_loss: 0.011041\n",
      "[301/00005] train_loss: 0.010517 kl_loss: 0.000000 normal_loss: 0.010517\n",
      "[302/00011] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[303/00017] train_loss: 0.010262 kl_loss: 0.000000 normal_loss: 0.010262\n",
      "[304/00023] train_loss: 0.010249 kl_loss: 0.000000 normal_loss: 0.010249\n",
      "[305/00029] train_loss: 0.010271 kl_loss: 0.000000 normal_loss: 0.010271\n",
      "[306/00035] train_loss: 0.010117 kl_loss: 0.000000 normal_loss: 0.010117\n",
      "[307/00041] train_loss: 0.010226 kl_loss: 0.000000 normal_loss: 0.010226\n",
      "[308/00047] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[309/00053] train_loss: 0.010148 kl_loss: 0.000000 normal_loss: 0.010148\n",
      "[310/00059] train_loss: 0.010184 kl_loss: 0.000000 normal_loss: 0.010184\n",
      "[311/00065] train_loss: 0.010260 kl_loss: 0.000000 normal_loss: 0.010260\n",
      "[312/00071] train_loss: 0.010252 kl_loss: 0.000000 normal_loss: 0.010252\n",
      "[313/00077] train_loss: 0.010283 kl_loss: 0.000000 normal_loss: 0.010283\n",
      "[314/00083] train_loss: 0.010220 kl_loss: 0.000000 normal_loss: 0.010220\n",
      "[315/00089] train_loss: 0.010255 kl_loss: 0.000000 normal_loss: 0.010255\n",
      "[317/00001] train_loss: 0.010203 kl_loss: 0.000000 normal_loss: 0.010203\n",
      "[318/00007] train_loss: 0.010226 kl_loss: 0.000000 normal_loss: 0.010226\n",
      "[319/00013] train_loss: 0.010222 kl_loss: 0.000000 normal_loss: 0.010222\n",
      "[320/00019] train_loss: 0.010265 kl_loss: 0.000000 normal_loss: 0.010265\n",
      "[321/00025] train_loss: 0.010266 kl_loss: 0.000000 normal_loss: 0.010266\n",
      "[322/00031] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[323/00037] train_loss: 0.010218 kl_loss: 0.000000 normal_loss: 0.010218\n",
      "[324/00043] train_loss: 0.010209 kl_loss: 0.000000 normal_loss: 0.010209\n",
      "[325/00049] train_loss: 0.010175 kl_loss: 0.000000 normal_loss: 0.010175\n",
      "[326/00055] train_loss: 0.010127 kl_loss: 0.000000 normal_loss: 0.010127\n",
      "[327/00061] train_loss: 0.010312 kl_loss: 0.000000 normal_loss: 0.010312\n",
      "[328/00067] train_loss: 0.010165 kl_loss: 0.000000 normal_loss: 0.010165\n",
      "[329/00073] train_loss: 0.010248 kl_loss: 0.000000 normal_loss: 0.010248\n",
      "[330/00079] train_loss: 0.010207 kl_loss: 0.000000 normal_loss: 0.010207\n",
      "[331/00085] train_loss: 0.010198 kl_loss: 0.000000 normal_loss: 0.010198\n",
      "[332/00091] train_loss: 0.010193 kl_loss: 0.000000 normal_loss: 0.010193\n",
      "[334/00003] train_loss: 0.010178 kl_loss: 0.000000 normal_loss: 0.010178\n",
      "[335/00009] train_loss: 0.010130 kl_loss: 0.000000 normal_loss: 0.010130\n",
      "[336/00015] train_loss: 0.010189 kl_loss: 0.000000 normal_loss: 0.010189\n",
      "[337/00021] train_loss: 0.010203 kl_loss: 0.000000 normal_loss: 0.010203\n",
      "[338/00027] train_loss: 0.010221 kl_loss: 0.000000 normal_loss: 0.010221\n",
      "[339/00033] train_loss: 0.010256 kl_loss: 0.000000 normal_loss: 0.010256\n",
      "[340/00039] train_loss: 0.010155 kl_loss: 0.000000 normal_loss: 0.010155\n",
      "[341/00045] train_loss: 0.010119 kl_loss: 0.000000 normal_loss: 0.010119\n",
      "[342/00051] train_loss: 0.010171 kl_loss: 0.000000 normal_loss: 0.010171\n",
      "[343/00057] train_loss: 0.010198 kl_loss: 0.000000 normal_loss: 0.010198\n",
      "[344/00063] train_loss: 0.010181 kl_loss: 0.000000 normal_loss: 0.010181\n",
      "[345/00069] train_loss: 0.010173 kl_loss: 0.000000 normal_loss: 0.010173\n",
      "[346/00075] train_loss: 0.010180 kl_loss: 0.000000 normal_loss: 0.010180\n",
      "[347/00081] train_loss: 0.010072 kl_loss: 0.000000 normal_loss: 0.010072\n",
      "[348/00087] train_loss: 0.010118 kl_loss: 0.000000 normal_loss: 0.010118\n",
      "[349/00093] train_loss: 0.010134 kl_loss: 0.000000 normal_loss: 0.010134\n",
      "[351/00005] train_loss: 0.010090 kl_loss: 0.000000 normal_loss: 0.010090\n",
      "[352/00011] train_loss: 0.010110 kl_loss: 0.000000 normal_loss: 0.010110\n",
      "[353/00017] train_loss: 0.010092 kl_loss: 0.000000 normal_loss: 0.010092\n",
      "[354/00023] train_loss: 0.010162 kl_loss: 0.000000 normal_loss: 0.010162\n",
      "[355/00029] train_loss: 0.010098 kl_loss: 0.000000 normal_loss: 0.010098\n",
      "[356/00035] train_loss: 0.010046 kl_loss: 0.000000 normal_loss: 0.010046\n",
      "[357/00041] train_loss: 0.010071 kl_loss: 0.000000 normal_loss: 0.010071\n",
      "[358/00047] train_loss: 0.010113 kl_loss: 0.000000 normal_loss: 0.010113\n",
      "[359/00053] train_loss: 0.010060 kl_loss: 0.000000 normal_loss: 0.010060\n",
      "[360/00059] train_loss: 0.010085 kl_loss: 0.000000 normal_loss: 0.010085\n",
      "[361/00065] train_loss: 0.010169 kl_loss: 0.000000 normal_loss: 0.010169\n",
      "[362/00071] train_loss: 0.010050 kl_loss: 0.000000 normal_loss: 0.010050\n",
      "[363/00077] train_loss: 0.010029 kl_loss: 0.000000 normal_loss: 0.010029\n",
      "[364/00083] train_loss: 0.010070 kl_loss: 0.000000 normal_loss: 0.010070\n",
      "[365/00089] train_loss: 0.010169 kl_loss: 0.000000 normal_loss: 0.010169\n",
      "[367/00001] train_loss: 0.010005 kl_loss: 0.000000 normal_loss: 0.010005\n",
      "[368/00007] train_loss: 0.010060 kl_loss: 0.000000 normal_loss: 0.010060\n",
      "[369/00013] train_loss: 0.009988 kl_loss: 0.000000 normal_loss: 0.009988\n",
      "[370/00019] train_loss: 0.010120 kl_loss: 0.000000 normal_loss: 0.010120\n",
      "[371/00025] train_loss: 0.010066 kl_loss: 0.000000 normal_loss: 0.010066\n",
      "[372/00031] train_loss: 0.009933 kl_loss: 0.000000 normal_loss: 0.009933\n",
      "[373/00037] train_loss: 0.010093 kl_loss: 0.000000 normal_loss: 0.010093\n",
      "[374/00043] train_loss: 0.009972 kl_loss: 0.000000 normal_loss: 0.009972\n",
      "[375/00049] train_loss: 0.010037 kl_loss: 0.000000 normal_loss: 0.010037\n",
      "[376/00055] train_loss: 0.009991 kl_loss: 0.000000 normal_loss: 0.009991\n",
      "[377/00061] train_loss: 0.010022 kl_loss: 0.000000 normal_loss: 0.010022\n",
      "[378/00067] train_loss: 0.010008 kl_loss: 0.000000 normal_loss: 0.010008\n",
      "[379/00073] train_loss: 0.009970 kl_loss: 0.000000 normal_loss: 0.009970\n",
      "[380/00079] train_loss: 0.010011 kl_loss: 0.000000 normal_loss: 0.010011\n",
      "[381/00085] train_loss: 0.009967 kl_loss: 0.000000 normal_loss: 0.009967\n",
      "[382/00091] train_loss: 0.009960 kl_loss: 0.000000 normal_loss: 0.009960\n",
      "[384/00003] train_loss: 0.010048 kl_loss: 0.000000 normal_loss: 0.010048\n",
      "[385/00009] train_loss: 0.009959 kl_loss: 0.000000 normal_loss: 0.009959\n",
      "[386/00015] train_loss: 0.010017 kl_loss: 0.000000 normal_loss: 0.010017\n",
      "[387/00021] train_loss: 0.010008 kl_loss: 0.000000 normal_loss: 0.010008\n",
      "[388/00027] train_loss: 0.009994 kl_loss: 0.000000 normal_loss: 0.009994\n",
      "[389/00033] train_loss: 0.010066 kl_loss: 0.000000 normal_loss: 0.010066\n",
      "[390/00039] train_loss: 0.010017 kl_loss: 0.000000 normal_loss: 0.010017\n",
      "[391/00045] train_loss: 0.009861 kl_loss: 0.000000 normal_loss: 0.009861\n",
      "[392/00051] train_loss: 0.010109 kl_loss: 0.000000 normal_loss: 0.010109\n",
      "[393/00057] train_loss: 0.009989 kl_loss: 0.000000 normal_loss: 0.009989\n",
      "[394/00063] train_loss: 0.009996 kl_loss: 0.000000 normal_loss: 0.009996\n",
      "[395/00069] train_loss: 0.009870 kl_loss: 0.000000 normal_loss: 0.009870\n",
      "[396/00075] train_loss: 0.009969 kl_loss: 0.000000 normal_loss: 0.009969\n",
      "[397/00081] train_loss: 0.009880 kl_loss: 0.000000 normal_loss: 0.009880\n",
      "[398/00087] train_loss: 0.009947 kl_loss: 0.000000 normal_loss: 0.009947\n",
      "[399/00093] train_loss: 0.009983 kl_loss: 0.000000 normal_loss: 0.009983\n",
      "[401/00005] train_loss: 0.009788 kl_loss: 0.000000 normal_loss: 0.009788\n",
      "[402/00011] train_loss: 0.009675 kl_loss: 0.000000 normal_loss: 0.009675\n",
      "[403/00017] train_loss: 0.009665 kl_loss: 0.000000 normal_loss: 0.009665\n",
      "[404/00023] train_loss: 0.009673 kl_loss: 0.000000 normal_loss: 0.009673\n",
      "[405/00029] train_loss: 0.009681 kl_loss: 0.000000 normal_loss: 0.009681\n",
      "[406/00035] train_loss: 0.009675 kl_loss: 0.000000 normal_loss: 0.009675\n",
      "[407/00041] train_loss: 0.009686 kl_loss: 0.000000 normal_loss: 0.009686\n",
      "[408/00047] train_loss: 0.009626 kl_loss: 0.000000 normal_loss: 0.009626\n",
      "[409/00053] train_loss: 0.009652 kl_loss: 0.000000 normal_loss: 0.009652\n",
      "[410/00059] train_loss: 0.009662 kl_loss: 0.000000 normal_loss: 0.009662\n",
      "[411/00065] train_loss: 0.009678 kl_loss: 0.000000 normal_loss: 0.009678\n",
      "[412/00071] train_loss: 0.009712 kl_loss: 0.000000 normal_loss: 0.009712\n",
      "[413/00077] train_loss: 0.009683 kl_loss: 0.000000 normal_loss: 0.009683\n",
      "[414/00083] train_loss: 0.009658 kl_loss: 0.000000 normal_loss: 0.009658\n",
      "[415/00089] train_loss: 0.009679 kl_loss: 0.000000 normal_loss: 0.009679\n",
      "[417/00001] train_loss: 0.009674 kl_loss: 0.000000 normal_loss: 0.009674\n",
      "[418/00007] train_loss: 0.009655 kl_loss: 0.000000 normal_loss: 0.009655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mchair_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CHAIR\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 9173\n",
      "Training params: 3\n",
      "[000/00099] train_loss: 0.178056 kl_loss: 0.500034 normal_loss: 0.163055\n",
      "[001/00055] train_loss: 0.132347 kl_loss: 0.470443 normal_loss: 0.118233\n",
      "[002/00011] train_loss: 0.124186 kl_loss: 0.438878 normal_loss: 0.111019\n",
      "[002/00111] train_loss: 0.121336 kl_loss: 0.385560 normal_loss: 0.109769\n",
      "[003/00067] train_loss: 0.119753 kl_loss: 0.350740 normal_loss: 0.109230\n",
      "[004/00023] train_loss: 0.117066 kl_loss: 0.325709 normal_loss: 0.107294\n",
      "[004/00123] train_loss: 0.113976 kl_loss: 0.298345 normal_loss: 0.105026\n",
      "[005/00079] train_loss: 0.110803 kl_loss: 0.291768 normal_loss: 0.102050\n",
      "[006/00035] train_loss: 0.109214 kl_loss: 0.287032 normal_loss: 0.100603\n",
      "[006/00135] train_loss: 0.108182 kl_loss: 0.279403 normal_loss: 0.099800\n",
      "[007/00091] train_loss: 0.104936 kl_loss: 0.269548 normal_loss: 0.096849\n",
      "[008/00047] train_loss: 0.101741 kl_loss: 0.267557 normal_loss: 0.093714\n",
      "[009/00003] train_loss: 0.100989 kl_loss: 0.270261 normal_loss: 0.092881\n",
      "[009/00103] train_loss: 0.096803 kl_loss: 0.271671 normal_loss: 0.088653\n",
      "[010/00059] train_loss: 0.093735 kl_loss: 0.271919 normal_loss: 0.085577\n",
      "[011/00015] train_loss: 0.092613 kl_loss: 0.271636 normal_loss: 0.084464\n",
      "[011/00115] train_loss: 0.091549 kl_loss: 0.270807 normal_loss: 0.083425\n",
      "[012/00071] train_loss: 0.089025 kl_loss: 0.267663 normal_loss: 0.080995\n",
      "[013/00027] train_loss: 0.087582 kl_loss: 0.267876 normal_loss: 0.079546\n",
      "[013/00127] train_loss: 0.086233 kl_loss: 0.267362 normal_loss: 0.078213\n",
      "[014/00083] train_loss: 0.084727 kl_loss: 0.266303 normal_loss: 0.076738\n",
      "[015/00039] train_loss: 0.083200 kl_loss: 0.265377 normal_loss: 0.075239\n",
      "[015/00139] train_loss: 0.082058 kl_loss: 0.264324 normal_loss: 0.074128\n",
      "[016/00095] train_loss: 0.080509 kl_loss: 0.262058 normal_loss: 0.072647\n",
      "[017/00051] train_loss: 0.081169 kl_loss: 0.262728 normal_loss: 0.073287\n",
      "[018/00007] train_loss: 0.078990 kl_loss: 0.260191 normal_loss: 0.071185\n",
      "[018/00107] train_loss: 0.078766 kl_loss: 0.258532 normal_loss: 0.071010\n",
      "[019/00063] train_loss: 0.076132 kl_loss: 0.256643 normal_loss: 0.068433\n",
      "[020/00019] train_loss: 0.075786 kl_loss: 0.255598 normal_loss: 0.068118\n",
      "[020/00119] train_loss: 0.076643 kl_loss: 0.254097 normal_loss: 0.069021\n",
      "[021/00075] train_loss: 0.074903 kl_loss: 0.251185 normal_loss: 0.067367\n",
      "[022/00031] train_loss: 0.073803 kl_loss: 0.251858 normal_loss: 0.066247\n",
      "[022/00131] train_loss: 0.073523 kl_loss: 0.251622 normal_loss: 0.065975\n",
      "[023/00087] train_loss: 0.072428 kl_loss: 0.250056 normal_loss: 0.064926\n",
      "[024/00043] train_loss: 0.070849 kl_loss: 0.247852 normal_loss: 0.063413\n",
      "[024/00143] train_loss: 0.071675 kl_loss: 0.249177 normal_loss: 0.064200\n",
      "[025/00099] train_loss: 0.070496 kl_loss: 0.247242 normal_loss: 0.063079\n",
      "[026/00055] train_loss: 0.070750 kl_loss: 0.246987 normal_loss: 0.063340\n",
      "[027/00011] train_loss: 0.069423 kl_loss: 0.245468 normal_loss: 0.062059\n",
      "[027/00111] train_loss: 0.068992 kl_loss: 0.247674 normal_loss: 0.061561\n",
      "[028/00067] train_loss: 0.067448 kl_loss: 0.247611 normal_loss: 0.060020\n",
      "[029/00023] train_loss: 0.066939 kl_loss: 0.244323 normal_loss: 0.059610\n",
      "[029/00123] train_loss: 0.067726 kl_loss: 0.246355 normal_loss: 0.060335\n",
      "[030/00079] train_loss: 0.065808 kl_loss: 0.245076 normal_loss: 0.058455\n",
      "[031/00035] train_loss: 0.065672 kl_loss: 0.246059 normal_loss: 0.058290\n",
      "[031/00135] train_loss: 0.065236 kl_loss: 0.245531 normal_loss: 0.057870\n",
      "[032/00091] train_loss: 0.064714 kl_loss: 0.246709 normal_loss: 0.057312\n",
      "[033/00047] train_loss: 0.063874 kl_loss: 0.245579 normal_loss: 0.056507\n",
      "[034/00003] train_loss: 0.063412 kl_loss: 0.246808 normal_loss: 0.056008\n",
      "[034/00103] train_loss: 0.062436 kl_loss: 0.246970 normal_loss: 0.055027\n",
      "[035/00059] train_loss: 0.063666 kl_loss: 0.246149 normal_loss: 0.056281\n",
      "[036/00015] train_loss: 0.063026 kl_loss: 0.248889 normal_loss: 0.055559\n",
      "[036/00115] train_loss: 0.062002 kl_loss: 0.247659 normal_loss: 0.054572\n",
      "[037/00071] train_loss: 0.061698 kl_loss: 0.248861 normal_loss: 0.054232\n",
      "[038/00027] train_loss: 0.061683 kl_loss: 0.248673 normal_loss: 0.054223\n",
      "[038/00127] train_loss: 0.060715 kl_loss: 0.247957 normal_loss: 0.053276\n",
      "[039/00083] train_loss: 0.061106 kl_loss: 0.248824 normal_loss: 0.053641\n",
      "[040/00039] train_loss: 0.059661 kl_loss: 0.249238 normal_loss: 0.052184\n",
      "[040/00139] train_loss: 0.059992 kl_loss: 0.249576 normal_loss: 0.052505\n",
      "[041/00095] train_loss: 0.059233 kl_loss: 0.249153 normal_loss: 0.051759\n",
      "[042/00051] train_loss: 0.060006 kl_loss: 0.252168 normal_loss: 0.052441\n",
      "[043/00007] train_loss: 0.059109 kl_loss: 0.249892 normal_loss: 0.051612\n",
      "[043/00107] train_loss: 0.058655 kl_loss: 0.251770 normal_loss: 0.051102\n",
      "[044/00063] train_loss: 0.058663 kl_loss: 0.254003 normal_loss: 0.051043\n",
      "[045/00019] train_loss: 0.057070 kl_loss: 0.249459 normal_loss: 0.049586\n",
      "[045/00119] train_loss: 0.057266 kl_loss: 0.253435 normal_loss: 0.049663\n",
      "[046/00075] train_loss: 0.057590 kl_loss: 0.254977 normal_loss: 0.049940\n",
      "[047/00031] train_loss: 0.057299 kl_loss: 0.253367 normal_loss: 0.049698\n",
      "[047/00131] train_loss: 0.056911 kl_loss: 0.253495 normal_loss: 0.049306\n",
      "[048/00087] train_loss: 0.056909 kl_loss: 0.254600 normal_loss: 0.049271\n",
      "[049/00043] train_loss: 0.056479 kl_loss: 0.256457 normal_loss: 0.048786\n",
      "[049/00143] train_loss: 0.056401 kl_loss: 0.255242 normal_loss: 0.048744\n",
      "[050/00099] train_loss: 0.055483 kl_loss: 0.258583 normal_loss: 0.047726\n",
      "[051/00055] train_loss: 0.054811 kl_loss: 0.255326 normal_loss: 0.047151\n",
      "[052/00011] train_loss: 0.055771 kl_loss: 0.260833 normal_loss: 0.047946\n",
      "[052/00111] train_loss: 0.054686 kl_loss: 0.259473 normal_loss: 0.046902\n",
      "[053/00067] train_loss: 0.055177 kl_loss: 0.259532 normal_loss: 0.047392\n",
      "[054/00023] train_loss: 0.054129 kl_loss: 0.260401 normal_loss: 0.046317\n",
      "[054/00123] train_loss: 0.054392 kl_loss: 0.261862 normal_loss: 0.046536\n",
      "[055/00079] train_loss: 0.053618 kl_loss: 0.260896 normal_loss: 0.045791\n",
      "[056/00035] train_loss: 0.054311 kl_loss: 0.266210 normal_loss: 0.046325\n",
      "[056/00135] train_loss: 0.053341 kl_loss: 0.263181 normal_loss: 0.045445\n",
      "[057/00091] train_loss: 0.053781 kl_loss: 0.264619 normal_loss: 0.045843\n",
      "[058/00047] train_loss: 0.052426 kl_loss: 0.265049 normal_loss: 0.044475\n",
      "[059/00003] train_loss: 0.052965 kl_loss: 0.266419 normal_loss: 0.044972\n",
      "[059/00103] train_loss: 0.052348 kl_loss: 0.267315 normal_loss: 0.044328\n",
      "[060/00059] train_loss: 0.052523 kl_loss: 0.267306 normal_loss: 0.044504\n",
      "[061/00015] train_loss: 0.051938 kl_loss: 0.267502 normal_loss: 0.043913\n",
      "[061/00115] train_loss: 0.052264 kl_loss: 0.268840 normal_loss: 0.044198\n",
      "[062/00071] train_loss: 0.051734 kl_loss: 0.270296 normal_loss: 0.043625\n",
      "[063/00027] train_loss: 0.051298 kl_loss: 0.269993 normal_loss: 0.043198\n",
      "[063/00127] train_loss: 0.051311 kl_loss: 0.270659 normal_loss: 0.043192\n",
      "[064/00083] train_loss: 0.050953 kl_loss: 0.270751 normal_loss: 0.042831\n",
      "[065/00039] train_loss: 0.051084 kl_loss: 0.272073 normal_loss: 0.042922\n",
      "[065/00139] train_loss: 0.050978 kl_loss: 0.272515 normal_loss: 0.042803\n",
      "[066/00095] train_loss: 0.050564 kl_loss: 0.273932 normal_loss: 0.042346\n",
      "[067/00051] train_loss: 0.049713 kl_loss: 0.273193 normal_loss: 0.041517\n",
      "[068/00007] train_loss: 0.050326 kl_loss: 0.272818 normal_loss: 0.042141\n",
      "[068/00107] train_loss: 0.049486 kl_loss: 0.274378 normal_loss: 0.041254\n",
      "[069/00063] train_loss: 0.049896 kl_loss: 0.276724 normal_loss: 0.041594\n",
      "[070/00019] train_loss: 0.049585 kl_loss: 0.275923 normal_loss: 0.041307\n",
      "[070/00119] train_loss: 0.049243 kl_loss: 0.275239 normal_loss: 0.040986\n",
      "[071/00075] train_loss: 0.048726 kl_loss: 0.275780 normal_loss: 0.040452\n",
      "[072/00031] train_loss: 0.048544 kl_loss: 0.278686 normal_loss: 0.040184\n",
      "[072/00131] train_loss: 0.048849 kl_loss: 0.276979 normal_loss: 0.040539\n",
      "[073/00087] train_loss: 0.048719 kl_loss: 0.277878 normal_loss: 0.040383\n",
      "[074/00043] train_loss: 0.048200 kl_loss: 0.278985 normal_loss: 0.039830\n",
      "[074/00143] train_loss: 0.048600 kl_loss: 0.279265 normal_loss: 0.040222\n",
      "[075/00099] train_loss: 0.048193 kl_loss: 0.280765 normal_loss: 0.039770\n",
      "[076/00055] train_loss: 0.047750 kl_loss: 0.279599 normal_loss: 0.039362\n",
      "[077/00011] train_loss: 0.047409 kl_loss: 0.281790 normal_loss: 0.038955\n",
      "[077/00111] train_loss: 0.047186 kl_loss: 0.281502 normal_loss: 0.038741\n",
      "[078/00067] train_loss: 0.047231 kl_loss: 0.281031 normal_loss: 0.038801\n",
      "[079/00023] train_loss: 0.047635 kl_loss: 0.284626 normal_loss: 0.039097\n",
      "[079/00123] train_loss: 0.046417 kl_loss: 0.281763 normal_loss: 0.037964\n",
      "[080/00079] train_loss: 0.046869 kl_loss: 0.284804 normal_loss: 0.038325\n",
      "[081/00035] train_loss: 0.046306 kl_loss: 0.284909 normal_loss: 0.037759\n",
      "[081/00135] train_loss: 0.046672 kl_loss: 0.283368 normal_loss: 0.038171\n",
      "[082/00091] train_loss: 0.046300 kl_loss: 0.285188 normal_loss: 0.037744\n",
      "[083/00047] train_loss: 0.046318 kl_loss: 0.286340 normal_loss: 0.037728\n",
      "[084/00003] train_loss: 0.045584 kl_loss: 0.284953 normal_loss: 0.037036\n",
      "[084/00103] train_loss: 0.045361 kl_loss: 0.287632 normal_loss: 0.036733\n",
      "[085/00059] train_loss: 0.045094 kl_loss: 0.284641 normal_loss: 0.036555\n",
      "[086/00015] train_loss: 0.045904 kl_loss: 0.288443 normal_loss: 0.037251\n",
      "[086/00115] train_loss: 0.045288 kl_loss: 0.287585 normal_loss: 0.036661\n",
      "[087/00071] train_loss: 0.045460 kl_loss: 0.288669 normal_loss: 0.036800\n",
      "[088/00027] train_loss: 0.045100 kl_loss: 0.286324 normal_loss: 0.036511\n",
      "[088/00127] train_loss: 0.044886 kl_loss: 0.290115 normal_loss: 0.036183\n",
      "[089/00083] train_loss: 0.044756 kl_loss: 0.289549 normal_loss: 0.036069\n",
      "[090/00039] train_loss: 0.044304 kl_loss: 0.288877 normal_loss: 0.035638\n",
      "[090/00139] train_loss: 0.044159 kl_loss: 0.288700 normal_loss: 0.035498\n",
      "[091/00095] train_loss: 0.044557 kl_loss: 0.290656 normal_loss: 0.035838\n",
      "[092/00051] train_loss: 0.044263 kl_loss: 0.290524 normal_loss: 0.035548\n",
      "[093/00007] train_loss: 0.044261 kl_loss: 0.289907 normal_loss: 0.035564\n",
      "[093/00107] train_loss: 0.043844 kl_loss: 0.291300 normal_loss: 0.035105\n",
      "[094/00063] train_loss: 0.043386 kl_loss: 0.290729 normal_loss: 0.034664\n",
      "[095/00019] train_loss: 0.043397 kl_loss: 0.291993 normal_loss: 0.034637\n",
      "[095/00119] train_loss: 0.043411 kl_loss: 0.291043 normal_loss: 0.034680\n",
      "[096/00075] train_loss: 0.043530 kl_loss: 0.294686 normal_loss: 0.034689\n",
      "[097/00031] train_loss: 0.043204 kl_loss: 0.292399 normal_loss: 0.034432\n",
      "[097/00131] train_loss: 0.042960 kl_loss: 0.292194 normal_loss: 0.034194\n",
      "[098/00087] train_loss: 0.043005 kl_loss: 0.292429 normal_loss: 0.034232\n",
      "[099/00043] train_loss: 0.042556 kl_loss: 0.293014 normal_loss: 0.033765\n",
      "[099/00143] train_loss: 0.042899 kl_loss: 0.292936 normal_loss: 0.034111\n",
      "[100/00099] train_loss: 0.040152 kl_loss: 0.291792 normal_loss: 0.031398\n",
      "[101/00055] train_loss: 0.039739 kl_loss: 0.291357 normal_loss: 0.030998\n",
      "[102/00011] train_loss: 0.039314 kl_loss: 0.290390 normal_loss: 0.030602\n",
      "[102/00111] train_loss: 0.038563 kl_loss: 0.286350 normal_loss: 0.029973\n",
      "[103/00067] train_loss: 0.038410 kl_loss: 0.285762 normal_loss: 0.029837\n",
      "[104/00023] train_loss: 0.038414 kl_loss: 0.283668 normal_loss: 0.029904\n",
      "[104/00123] train_loss: 0.038022 kl_loss: 0.280938 normal_loss: 0.029594\n",
      "[105/00079] train_loss: 0.038328 kl_loss: 0.281367 normal_loss: 0.029887\n",
      "[106/00035] train_loss: 0.037501 kl_loss: 0.276584 normal_loss: 0.029204\n",
      "[106/00135] train_loss: 0.037915 kl_loss: 0.277469 normal_loss: 0.029591\n",
      "[107/00091] train_loss: 0.037531 kl_loss: 0.273937 normal_loss: 0.029312\n",
      "[108/00047] train_loss: 0.037414 kl_loss: 0.274603 normal_loss: 0.029176\n",
      "[109/00003] train_loss: 0.037234 kl_loss: 0.272516 normal_loss: 0.029058\n",
      "[109/00103] train_loss: 0.037240 kl_loss: 0.271107 normal_loss: 0.029107\n",
      "[110/00059] train_loss: 0.036917 kl_loss: 0.268448 normal_loss: 0.028863\n",
      "[111/00015] train_loss: 0.037223 kl_loss: 0.271296 normal_loss: 0.029084\n",
      "[111/00115] train_loss: 0.036726 kl_loss: 0.266582 normal_loss: 0.028729\n",
      "[112/00071] train_loss: 0.036604 kl_loss: 0.265738 normal_loss: 0.028632\n",
      "[113/00027] train_loss: 0.037043 kl_loss: 0.268455 normal_loss: 0.028989\n",
      "[113/00127] train_loss: 0.036721 kl_loss: 0.264765 normal_loss: 0.028778\n",
      "[114/00083] train_loss: 0.036638 kl_loss: 0.262726 normal_loss: 0.028756\n",
      "[115/00039] train_loss: 0.036525 kl_loss: 0.264790 normal_loss: 0.028582\n",
      "[115/00139] train_loss: 0.036238 kl_loss: 0.261317 normal_loss: 0.028399\n",
      "[116/00095] train_loss: 0.036525 kl_loss: 0.263339 normal_loss: 0.028625\n",
      "[117/00051] train_loss: 0.036052 kl_loss: 0.259873 normal_loss: 0.028255\n",
      "[118/00007] train_loss: 0.036026 kl_loss: 0.260458 normal_loss: 0.028212\n",
      "[118/00107] train_loss: 0.036026 kl_loss: 0.261037 normal_loss: 0.028194\n",
      "[119/00063] train_loss: 0.036064 kl_loss: 0.259948 normal_loss: 0.028266\n",
      "[120/00019] train_loss: 0.035867 kl_loss: 0.260190 normal_loss: 0.028061\n",
      "[120/00119] train_loss: 0.035833 kl_loss: 0.258816 normal_loss: 0.028068\n",
      "[121/00075] train_loss: 0.035450 kl_loss: 0.258133 normal_loss: 0.027706\n",
      "[122/00031] train_loss: 0.035474 kl_loss: 0.257314 normal_loss: 0.027754\n",
      "[122/00131] train_loss: 0.035753 kl_loss: 0.257771 normal_loss: 0.028020\n",
      "[123/00087] train_loss: 0.035855 kl_loss: 0.258949 normal_loss: 0.028087\n",
      "[124/00043] train_loss: 0.035387 kl_loss: 0.255994 normal_loss: 0.027708\n",
      "[124/00143] train_loss: 0.035420 kl_loss: 0.255885 normal_loss: 0.027743\n",
      "[125/00099] train_loss: 0.035161 kl_loss: 0.255795 normal_loss: 0.027487\n",
      "[126/00055] train_loss: 0.034944 kl_loss: 0.255526 normal_loss: 0.027278\n",
      "[127/00011] train_loss: 0.035407 kl_loss: 0.257131 normal_loss: 0.027693\n",
      "[127/00111] train_loss: 0.035076 kl_loss: 0.255190 normal_loss: 0.027420\n",
      "[128/00067] train_loss: 0.034981 kl_loss: 0.255304 normal_loss: 0.027322\n",
      "[129/00023] train_loss: 0.035123 kl_loss: 0.254925 normal_loss: 0.027475\n",
      "[129/00123] train_loss: 0.034909 kl_loss: 0.254930 normal_loss: 0.027261\n",
      "[130/00079] train_loss: 0.035058 kl_loss: 0.254832 normal_loss: 0.027413\n",
      "[131/00035] train_loss: 0.034656 kl_loss: 0.255165 normal_loss: 0.027001\n",
      "[131/00135] train_loss: 0.034711 kl_loss: 0.254287 normal_loss: 0.027083\n",
      "[132/00091] train_loss: 0.034718 kl_loss: 0.254123 normal_loss: 0.027095\n",
      "[133/00047] train_loss: 0.034638 kl_loss: 0.254734 normal_loss: 0.026996\n",
      "[134/00003] train_loss: 0.034474 kl_loss: 0.252737 normal_loss: 0.026892\n",
      "[134/00103] train_loss: 0.034226 kl_loss: 0.254319 normal_loss: 0.026596\n",
      "[135/00059] train_loss: 0.034466 kl_loss: 0.251787 normal_loss: 0.026912\n",
      "[136/00015] train_loss: 0.034527 kl_loss: 0.255032 normal_loss: 0.026876\n",
      "[136/00115] train_loss: 0.034264 kl_loss: 0.253268 normal_loss: 0.026666\n",
      "[137/00071] train_loss: 0.033911 kl_loss: 0.252803 normal_loss: 0.026327\n",
      "[138/00027] train_loss: 0.034124 kl_loss: 0.252411 normal_loss: 0.026552\n",
      "[138/00127] train_loss: 0.034020 kl_loss: 0.252920 normal_loss: 0.026433\n",
      "[139/00083] train_loss: 0.034053 kl_loss: 0.253203 normal_loss: 0.026457\n",
      "[140/00039] train_loss: 0.034056 kl_loss: 0.252114 normal_loss: 0.026493\n",
      "[140/00139] train_loss: 0.033965 kl_loss: 0.252020 normal_loss: 0.026404\n",
      "[141/00095] train_loss: 0.033779 kl_loss: 0.252567 normal_loss: 0.026202\n",
      "[142/00051] train_loss: 0.034255 kl_loss: 0.254233 normal_loss: 0.026628\n",
      "[143/00007] train_loss: 0.033544 kl_loss: 0.250503 normal_loss: 0.026029\n",
      "[143/00107] train_loss: 0.033522 kl_loss: 0.251886 normal_loss: 0.025965\n",
      "[144/00063] train_loss: 0.034000 kl_loss: 0.253323 normal_loss: 0.026400\n",
      "[145/00019] train_loss: 0.033472 kl_loss: 0.251121 normal_loss: 0.025939\n",
      "[145/00119] train_loss: 0.033557 kl_loss: 0.251895 normal_loss: 0.026000\n",
      "[146/00075] train_loss: 0.033173 kl_loss: 0.251709 normal_loss: 0.025622\n",
      "[147/00031] train_loss: 0.033586 kl_loss: 0.252297 normal_loss: 0.026017\n",
      "[147/00131] train_loss: 0.033554 kl_loss: 0.252183 normal_loss: 0.025988\n",
      "[148/00087] train_loss: 0.033209 kl_loss: 0.251545 normal_loss: 0.025663\n",
      "[149/00043] train_loss: 0.033103 kl_loss: 0.251473 normal_loss: 0.025559\n",
      "[149/00143] train_loss: 0.032779 kl_loss: 0.251278 normal_loss: 0.025241\n",
      "[150/00099] train_loss: 0.033400 kl_loss: 0.251151 normal_loss: 0.025866\n",
      "[151/00055] train_loss: 0.032951 kl_loss: 0.250573 normal_loss: 0.025434\n",
      "[152/00011] train_loss: 0.033452 kl_loss: 0.252995 normal_loss: 0.025863\n",
      "[152/00111] train_loss: 0.033063 kl_loss: 0.250283 normal_loss: 0.025555\n",
      "[153/00067] train_loss: 0.033183 kl_loss: 0.252036 normal_loss: 0.025622\n",
      "[154/00023] train_loss: 0.032774 kl_loss: 0.250273 normal_loss: 0.025266\n",
      "[154/00123] train_loss: 0.032730 kl_loss: 0.250082 normal_loss: 0.025227\n",
      "[155/00079] train_loss: 0.032855 kl_loss: 0.249799 normal_loss: 0.025361\n",
      "[156/00035] train_loss: 0.033080 kl_loss: 0.252530 normal_loss: 0.025504\n",
      "[156/00135] train_loss: 0.032622 kl_loss: 0.249968 normal_loss: 0.025123\n",
      "[157/00091] train_loss: 0.032697 kl_loss: 0.249844 normal_loss: 0.025202\n",
      "[158/00047] train_loss: 0.032626 kl_loss: 0.250858 normal_loss: 0.025100\n",
      "[159/00003] train_loss: 0.032543 kl_loss: 0.249370 normal_loss: 0.025062\n",
      "[159/00103] train_loss: 0.032484 kl_loss: 0.250889 normal_loss: 0.024958\n",
      "[160/00059] train_loss: 0.032367 kl_loss: 0.247865 normal_loss: 0.024931\n",
      "[161/00015] train_loss: 0.032243 kl_loss: 0.251793 normal_loss: 0.024689\n",
      "[161/00115] train_loss: 0.032393 kl_loss: 0.249277 normal_loss: 0.024915\n",
      "[162/00071] train_loss: 0.032469 kl_loss: 0.248493 normal_loss: 0.025014\n",
      "[163/00027] train_loss: 0.032412 kl_loss: 0.250827 normal_loss: 0.024887\n",
      "[163/00127] train_loss: 0.032307 kl_loss: 0.248919 normal_loss: 0.024839\n",
      "[164/00083] train_loss: 0.032360 kl_loss: 0.250649 normal_loss: 0.024841\n",
      "[165/00039] train_loss: 0.032119 kl_loss: 0.248557 normal_loss: 0.024662\n",
      "[165/00139] train_loss: 0.032173 kl_loss: 0.249034 normal_loss: 0.024702\n",
      "[166/00095] train_loss: 0.031945 kl_loss: 0.249156 normal_loss: 0.024470\n",
      "[167/00051] train_loss: 0.032121 kl_loss: 0.248106 normal_loss: 0.024678\n",
      "[168/00007] train_loss: 0.032026 kl_loss: 0.249562 normal_loss: 0.024539\n",
      "[168/00107] train_loss: 0.031893 kl_loss: 0.249234 normal_loss: 0.024416\n",
      "[169/00063] train_loss: 0.031878 kl_loss: 0.247390 normal_loss: 0.024456\n",
      "[170/00019] train_loss: 0.031807 kl_loss: 0.248701 normal_loss: 0.024346\n",
      "[170/00119] train_loss: 0.031619 kl_loss: 0.246683 normal_loss: 0.024219\n",
      "[171/00075] train_loss: 0.031845 kl_loss: 0.249977 normal_loss: 0.024346\n",
      "[172/00031] train_loss: 0.031450 kl_loss: 0.247379 normal_loss: 0.024029\n",
      "[172/00131] train_loss: 0.031364 kl_loss: 0.248192 normal_loss: 0.023919\n",
      "[173/00087] train_loss: 0.031657 kl_loss: 0.246939 normal_loss: 0.024248\n",
      "[174/00043] train_loss: 0.031581 kl_loss: 0.247121 normal_loss: 0.024167\n",
      "[174/00143] train_loss: 0.031893 kl_loss: 0.248433 normal_loss: 0.024440\n",
      "[175/00099] train_loss: 0.031280 kl_loss: 0.246741 normal_loss: 0.023877\n",
      "[176/00055] train_loss: 0.031332 kl_loss: 0.246883 normal_loss: 0.023925\n",
      "[177/00011] train_loss: 0.031662 kl_loss: 0.247943 normal_loss: 0.024224\n",
      "[177/00111] train_loss: 0.031403 kl_loss: 0.247128 normal_loss: 0.023989\n",
      "[178/00067] train_loss: 0.030975 kl_loss: 0.245763 normal_loss: 0.023602\n",
      "[179/00023] train_loss: 0.031549 kl_loss: 0.247069 normal_loss: 0.024137\n",
      "[179/00123] train_loss: 0.031120 kl_loss: 0.246204 normal_loss: 0.023734\n",
      "[180/00079] train_loss: 0.031156 kl_loss: 0.246429 normal_loss: 0.023763\n",
      "[181/00035] train_loss: 0.031279 kl_loss: 0.246046 normal_loss: 0.023898\n",
      "[181/00135] train_loss: 0.031043 kl_loss: 0.245660 normal_loss: 0.023673\n",
      "[182/00091] train_loss: 0.031122 kl_loss: 0.246993 normal_loss: 0.023712\n",
      "[183/00047] train_loss: 0.030896 kl_loss: 0.244677 normal_loss: 0.023555\n",
      "[184/00003] train_loss: 0.031237 kl_loss: 0.246721 normal_loss: 0.023835\n",
      "[184/00103] train_loss: 0.031001 kl_loss: 0.244801 normal_loss: 0.023657\n",
      "[185/00059] train_loss: 0.030800 kl_loss: 0.245076 normal_loss: 0.023448\n",
      "[186/00015] train_loss: 0.030624 kl_loss: 0.245247 normal_loss: 0.023266\n",
      "[186/00115] train_loss: 0.030967 kl_loss: 0.245333 normal_loss: 0.023607\n",
      "[187/00071] train_loss: 0.030816 kl_loss: 0.243699 normal_loss: 0.023505\n",
      "[188/00027] train_loss: 0.030659 kl_loss: 0.246207 normal_loss: 0.023273\n",
      "[188/00127] train_loss: 0.030629 kl_loss: 0.244396 normal_loss: 0.023297\n",
      "[189/00083] train_loss: 0.030643 kl_loss: 0.243868 normal_loss: 0.023327\n",
      "[190/00039] train_loss: 0.030732 kl_loss: 0.245113 normal_loss: 0.023379\n",
      "[190/00139] train_loss: 0.030742 kl_loss: 0.243691 normal_loss: 0.023431\n",
      "[191/00095] train_loss: 0.030332 kl_loss: 0.243170 normal_loss: 0.023037\n",
      "[192/00051] train_loss: 0.030616 kl_loss: 0.244095 normal_loss: 0.023293\n",
      "[193/00007] train_loss: 0.030639 kl_loss: 0.244479 normal_loss: 0.023304\n",
      "[193/00107] train_loss: 0.030426 kl_loss: 0.242879 normal_loss: 0.023140\n",
      "[194/00063] train_loss: 0.030319 kl_loss: 0.243214 normal_loss: 0.023022\n",
      "[195/00019] train_loss: 0.030255 kl_loss: 0.243085 normal_loss: 0.022962\n",
      "[195/00119] train_loss: 0.030457 kl_loss: 0.243556 normal_loss: 0.023150\n",
      "[196/00075] train_loss: 0.030312 kl_loss: 0.242279 normal_loss: 0.023043\n",
      "[197/00031] train_loss: 0.030293 kl_loss: 0.241905 normal_loss: 0.023036\n",
      "[197/00131] train_loss: 0.030356 kl_loss: 0.243044 normal_loss: 0.023064\n",
      "[198/00087] train_loss: 0.030215 kl_loss: 0.242264 normal_loss: 0.022947\n",
      "[199/00043] train_loss: 0.030282 kl_loss: 0.242322 normal_loss: 0.023012\n",
      "[199/00143] train_loss: 0.030035 kl_loss: 0.242103 normal_loss: 0.022772\n",
      "[200/00099] train_loss: 0.029288 kl_loss: 0.242344 normal_loss: 0.022018\n",
      "[201/00055] train_loss: 0.028698 kl_loss: 0.240796 normal_loss: 0.021474\n",
      "[202/00011] train_loss: 0.028705 kl_loss: 0.240676 normal_loss: 0.021484\n",
      "[202/00111] train_loss: 0.028508 kl_loss: 0.239821 normal_loss: 0.021313\n",
      "[203/00067] train_loss: 0.028212 kl_loss: 0.238860 normal_loss: 0.021046\n",
      "[204/00023] train_loss: 0.028536 kl_loss: 0.240080 normal_loss: 0.021334\n",
      "[204/00123] train_loss: 0.028309 kl_loss: 0.238343 normal_loss: 0.021158\n",
      "[205/00079] train_loss: 0.028081 kl_loss: 0.236193 normal_loss: 0.020995\n",
      "[206/00035] train_loss: 0.028257 kl_loss: 0.237430 normal_loss: 0.021134\n",
      "[206/00135] train_loss: 0.028246 kl_loss: 0.237272 normal_loss: 0.021128\n",
      "[207/00091] train_loss: 0.028027 kl_loss: 0.235573 normal_loss: 0.020960\n",
      "[208/00047] train_loss: 0.028163 kl_loss: 0.236331 normal_loss: 0.021073\n",
      "[209/00003] train_loss: 0.028048 kl_loss: 0.234455 normal_loss: 0.021015\n",
      "[209/00103] train_loss: 0.027991 kl_loss: 0.234416 normal_loss: 0.020959\n",
      "[210/00059] train_loss: 0.027876 kl_loss: 0.234823 normal_loss: 0.020832\n",
      "[211/00015] train_loss: 0.028004 kl_loss: 0.232953 normal_loss: 0.021016\n",
      "[211/00115] train_loss: 0.027883 kl_loss: 0.232852 normal_loss: 0.020898\n",
      "[212/00071] train_loss: 0.027923 kl_loss: 0.233213 normal_loss: 0.020927\n",
      "[213/00027] train_loss: 0.027739 kl_loss: 0.232021 normal_loss: 0.020779\n",
      "[213/00127] train_loss: 0.027800 kl_loss: 0.231535 normal_loss: 0.020854\n",
      "[214/00083] train_loss: 0.027718 kl_loss: 0.231741 normal_loss: 0.020766\n",
      "[215/00039] train_loss: 0.027568 kl_loss: 0.229301 normal_loss: 0.020689\n",
      "[215/00139] train_loss: 0.027729 kl_loss: 0.231548 normal_loss: 0.020782\n",
      "[216/00095] train_loss: 0.027612 kl_loss: 0.228903 normal_loss: 0.020745\n",
      "[217/00051] train_loss: 0.027538 kl_loss: 0.230413 normal_loss: 0.020626\n",
      "[218/00007] train_loss: 0.027581 kl_loss: 0.228978 normal_loss: 0.020712\n",
      "[218/00107] train_loss: 0.027503 kl_loss: 0.229156 normal_loss: 0.020628\n",
      "[219/00063] train_loss: 0.027708 kl_loss: 0.227942 normal_loss: 0.020869\n",
      "[220/00019] train_loss: 0.027487 kl_loss: 0.228904 normal_loss: 0.020620\n",
      "[220/00119] train_loss: 0.027640 kl_loss: 0.228008 normal_loss: 0.020800\n",
      "[221/00075] train_loss: 0.027558 kl_loss: 0.228408 normal_loss: 0.020706\n",
      "[222/00031] train_loss: 0.027222 kl_loss: 0.224916 normal_loss: 0.020475\n",
      "[222/00131] train_loss: 0.027316 kl_loss: 0.226603 normal_loss: 0.020518\n",
      "[223/00087] train_loss: 0.027285 kl_loss: 0.226650 normal_loss: 0.020485\n",
      "[224/00043] train_loss: 0.027442 kl_loss: 0.226283 normal_loss: 0.020653\n",
      "[224/00143] train_loss: 0.027295 kl_loss: 0.225965 normal_loss: 0.020517\n",
      "[225/00099] train_loss: 0.027124 kl_loss: 0.224906 normal_loss: 0.020377\n",
      "[226/00055] train_loss: 0.027125 kl_loss: 0.224743 normal_loss: 0.020383\n",
      "[227/00011] train_loss: 0.027145 kl_loss: 0.225426 normal_loss: 0.020382\n",
      "[227/00111] train_loss: 0.027252 kl_loss: 0.224806 normal_loss: 0.020508\n",
      "[228/00067] train_loss: 0.027152 kl_loss: 0.222968 normal_loss: 0.020462\n",
      "[229/00023] train_loss: 0.027010 kl_loss: 0.224914 normal_loss: 0.020262\n",
      "[229/00123] train_loss: 0.027239 kl_loss: 0.223178 normal_loss: 0.020544\n",
      "[230/00079] train_loss: 0.027138 kl_loss: 0.223873 normal_loss: 0.020422\n",
      "[231/00035] train_loss: 0.027124 kl_loss: 0.223551 normal_loss: 0.020417\n",
      "[231/00135] train_loss: 0.026916 kl_loss: 0.222339 normal_loss: 0.020246\n",
      "[232/00091] train_loss: 0.027117 kl_loss: 0.223412 normal_loss: 0.020414\n",
      "[233/00047] train_loss: 0.026950 kl_loss: 0.220747 normal_loss: 0.020327\n",
      "[234/00003] train_loss: 0.027094 kl_loss: 0.223118 normal_loss: 0.020400\n",
      "[234/00103] train_loss: 0.026962 kl_loss: 0.222060 normal_loss: 0.020300\n",
      "[235/00059] train_loss: 0.026854 kl_loss: 0.220826 normal_loss: 0.020229\n",
      "[236/00015] train_loss: 0.026877 kl_loss: 0.222270 normal_loss: 0.020209\n",
      "[236/00115] train_loss: 0.026736 kl_loss: 0.220291 normal_loss: 0.020127\n",
      "[237/00071] train_loss: 0.026818 kl_loss: 0.221457 normal_loss: 0.020174\n",
      "[238/00027] train_loss: 0.026837 kl_loss: 0.221054 normal_loss: 0.020205\n",
      "[238/00127] train_loss: 0.026831 kl_loss: 0.220130 normal_loss: 0.020227\n",
      "[239/00083] train_loss: 0.026689 kl_loss: 0.219438 normal_loss: 0.020106\n",
      "[240/00039] train_loss: 0.026696 kl_loss: 0.220478 normal_loss: 0.020082\n",
      "[240/00139] train_loss: 0.026769 kl_loss: 0.220614 normal_loss: 0.020151\n",
      "[241/00095] train_loss: 0.026803 kl_loss: 0.220701 normal_loss: 0.020182\n",
      "[242/00051] train_loss: 0.026677 kl_loss: 0.218704 normal_loss: 0.020116\n",
      "[243/00007] train_loss: 0.026822 kl_loss: 0.219365 normal_loss: 0.020241\n",
      "[243/00107] train_loss: 0.026597 kl_loss: 0.219505 normal_loss: 0.020012\n",
      "[244/00063] train_loss: 0.026312 kl_loss: 0.218016 normal_loss: 0.019772\n",
      "[245/00019] train_loss: 0.026782 kl_loss: 0.219673 normal_loss: 0.020191\n",
      "[245/00119] train_loss: 0.026498 kl_loss: 0.218205 normal_loss: 0.019952\n",
      "[246/00075] train_loss: 0.026459 kl_loss: 0.217374 normal_loss: 0.019937\n",
      "[247/00031] train_loss: 0.026666 kl_loss: 0.219726 normal_loss: 0.020075\n",
      "[247/00131] train_loss: 0.026485 kl_loss: 0.218714 normal_loss: 0.019923\n",
      "[248/00087] train_loss: 0.026288 kl_loss: 0.216772 normal_loss: 0.019785\n",
      "[249/00043] train_loss: 0.026312 kl_loss: 0.216997 normal_loss: 0.019802\n",
      "[249/00143] train_loss: 0.026700 kl_loss: 0.218559 normal_loss: 0.020143\n",
      "[250/00099] train_loss: 0.026352 kl_loss: 0.216679 normal_loss: 0.019851\n",
      "[251/00055] train_loss: 0.026737 kl_loss: 0.218578 normal_loss: 0.020180\n",
      "[252/00011] train_loss: 0.026256 kl_loss: 0.215714 normal_loss: 0.019784\n",
      "[252/00111] train_loss: 0.026396 kl_loss: 0.217727 normal_loss: 0.019864\n",
      "[253/00067] train_loss: 0.026182 kl_loss: 0.216371 normal_loss: 0.019690\n",
      "[254/00023] train_loss: 0.026419 kl_loss: 0.216635 normal_loss: 0.019920\n",
      "[254/00123] train_loss: 0.026417 kl_loss: 0.216018 normal_loss: 0.019936\n",
      "[255/00079] train_loss: 0.026317 kl_loss: 0.217185 normal_loss: 0.019801\n",
      "[256/00035] train_loss: 0.026311 kl_loss: 0.215742 normal_loss: 0.019839\n",
      "[256/00135] train_loss: 0.026125 kl_loss: 0.215462 normal_loss: 0.019661\n",
      "[257/00091] train_loss: 0.026185 kl_loss: 0.216582 normal_loss: 0.019688\n",
      "[258/00047] train_loss: 0.026145 kl_loss: 0.215236 normal_loss: 0.019688\n",
      "[259/00003] train_loss: 0.026283 kl_loss: 0.215270 normal_loss: 0.019825\n",
      "[259/00103] train_loss: 0.026170 kl_loss: 0.214840 normal_loss: 0.019725\n",
      "[260/00059] train_loss: 0.026371 kl_loss: 0.216147 normal_loss: 0.019887\n",
      "[261/00015] train_loss: 0.026070 kl_loss: 0.214582 normal_loss: 0.019632\n",
      "[261/00115] train_loss: 0.026037 kl_loss: 0.214077 normal_loss: 0.019614\n",
      "[262/00071] train_loss: 0.026327 kl_loss: 0.215833 normal_loss: 0.019852\n",
      "[263/00027] train_loss: 0.026043 kl_loss: 0.213437 normal_loss: 0.019640\n",
      "[263/00127] train_loss: 0.026037 kl_loss: 0.214940 normal_loss: 0.019589\n",
      "[264/00083] train_loss: 0.025833 kl_loss: 0.213220 normal_loss: 0.019437\n",
      "[265/00039] train_loss: 0.026056 kl_loss: 0.214317 normal_loss: 0.019627\n",
      "[265/00139] train_loss: 0.026018 kl_loss: 0.214894 normal_loss: 0.019571\n",
      "[266/00095] train_loss: 0.026024 kl_loss: 0.214393 normal_loss: 0.019592\n",
      "[267/00051] train_loss: 0.025813 kl_loss: 0.213123 normal_loss: 0.019420\n",
      "[268/00007] train_loss: 0.025857 kl_loss: 0.213527 normal_loss: 0.019451\n",
      "[268/00107] train_loss: 0.025950 kl_loss: 0.213046 normal_loss: 0.019559\n",
      "[269/00063] train_loss: 0.025989 kl_loss: 0.213294 normal_loss: 0.019590\n",
      "[270/00019] train_loss: 0.026022 kl_loss: 0.213816 normal_loss: 0.019607\n",
      "[270/00119] train_loss: 0.025817 kl_loss: 0.213166 normal_loss: 0.019422\n",
      "[271/00075] train_loss: 0.025793 kl_loss: 0.213255 normal_loss: 0.019395\n",
      "[272/00031] train_loss: 0.025679 kl_loss: 0.212383 normal_loss: 0.019307\n",
      "[272/00131] train_loss: 0.025695 kl_loss: 0.212398 normal_loss: 0.019323\n",
      "[273/00087] train_loss: 0.025682 kl_loss: 0.211609 normal_loss: 0.019334\n",
      "[274/00043] train_loss: 0.025936 kl_loss: 0.213033 normal_loss: 0.019545\n",
      "[274/00143] train_loss: 0.025779 kl_loss: 0.211938 normal_loss: 0.019421\n",
      "[275/00099] train_loss: 0.025921 kl_loss: 0.212455 normal_loss: 0.019547\n",
      "[276/00055] train_loss: 0.025570 kl_loss: 0.211528 normal_loss: 0.019224\n",
      "[277/00011] train_loss: 0.025465 kl_loss: 0.211365 normal_loss: 0.019124\n",
      "[277/00111] train_loss: 0.025719 kl_loss: 0.211575 normal_loss: 0.019372\n",
      "[278/00067] train_loss: 0.025701 kl_loss: 0.211355 normal_loss: 0.019360\n",
      "[279/00023] train_loss: 0.025619 kl_loss: 0.211121 normal_loss: 0.019285\n",
      "[279/00123] train_loss: 0.025725 kl_loss: 0.212143 normal_loss: 0.019360\n",
      "[280/00079] train_loss: 0.025491 kl_loss: 0.210050 normal_loss: 0.019190\n",
      "[281/00035] train_loss: 0.025674 kl_loss: 0.210907 normal_loss: 0.019346\n",
      "[281/00135] train_loss: 0.025693 kl_loss: 0.210748 normal_loss: 0.019371\n",
      "[282/00091] train_loss: 0.025397 kl_loss: 0.210223 normal_loss: 0.019091\n",
      "[283/00047] train_loss: 0.025529 kl_loss: 0.210483 normal_loss: 0.019214\n",
      "[284/00003] train_loss: 0.025500 kl_loss: 0.210785 normal_loss: 0.019176\n",
      "[284/00103] train_loss: 0.025352 kl_loss: 0.210282 normal_loss: 0.019044\n",
      "[285/00059] train_loss: 0.025480 kl_loss: 0.209688 normal_loss: 0.019189\n",
      "[286/00015] train_loss: 0.025345 kl_loss: 0.209926 normal_loss: 0.019047\n",
      "[286/00115] train_loss: 0.025461 kl_loss: 0.209807 normal_loss: 0.019167\n",
      "[287/00071] train_loss: 0.025390 kl_loss: 0.209240 normal_loss: 0.019112\n",
      "[288/00027] train_loss: 0.025395 kl_loss: 0.210024 normal_loss: 0.019094\n",
      "[288/00127] train_loss: 0.025328 kl_loss: 0.209076 normal_loss: 0.019055\n",
      "[289/00083] train_loss: 0.025157 kl_loss: 0.208153 normal_loss: 0.018913\n",
      "[290/00039] train_loss: 0.025452 kl_loss: 0.209910 normal_loss: 0.019154\n",
      "[290/00139] train_loss: 0.025233 kl_loss: 0.209086 normal_loss: 0.018961\n",
      "[291/00095] train_loss: 0.025239 kl_loss: 0.208507 normal_loss: 0.018984\n",
      "[292/00051] train_loss: 0.025429 kl_loss: 0.209521 normal_loss: 0.019143\n",
      "[293/00007] train_loss: 0.025212 kl_loss: 0.208566 normal_loss: 0.018955\n",
      "[293/00107] train_loss: 0.025275 kl_loss: 0.208542 normal_loss: 0.019018\n",
      "[294/00063] train_loss: 0.025044 kl_loss: 0.207210 normal_loss: 0.018828\n",
      "[295/00019] train_loss: 0.025098 kl_loss: 0.208498 normal_loss: 0.018843\n",
      "[295/00119] train_loss: 0.025269 kl_loss: 0.208821 normal_loss: 0.019005\n",
      "[296/00075] train_loss: 0.025042 kl_loss: 0.207735 normal_loss: 0.018810\n",
      "[297/00031] train_loss: 0.025130 kl_loss: 0.206585 normal_loss: 0.018933\n",
      "[297/00131] train_loss: 0.025187 kl_loss: 0.207851 normal_loss: 0.018952\n",
      "[298/00087] train_loss: 0.025074 kl_loss: 0.207049 normal_loss: 0.018863\n",
      "[299/00043] train_loss: 0.025032 kl_loss: 0.207510 normal_loss: 0.018807\n",
      "[299/00143] train_loss: 0.024966 kl_loss: 0.207479 normal_loss: 0.018742\n",
      "[300/00099] train_loss: 0.024610 kl_loss: 0.207288 normal_loss: 0.018391\n",
      "[301/00055] train_loss: 0.024567 kl_loss: 0.206546 normal_loss: 0.018371\n",
      "[302/00011] train_loss: 0.024358 kl_loss: 0.206038 normal_loss: 0.018177\n",
      "[302/00111] train_loss: 0.024447 kl_loss: 0.206630 normal_loss: 0.018248\n",
      "[303/00067] train_loss: 0.024325 kl_loss: 0.205880 normal_loss: 0.018148\n",
      "[304/00023] train_loss: 0.024469 kl_loss: 0.206932 normal_loss: 0.018261\n",
      "[304/00123] train_loss: 0.024344 kl_loss: 0.206088 normal_loss: 0.018162\n",
      "[305/00079] train_loss: 0.024311 kl_loss: 0.205331 normal_loss: 0.018152\n",
      "[306/00035] train_loss: 0.024137 kl_loss: 0.205040 normal_loss: 0.017986\n",
      "[306/00135] train_loss: 0.024351 kl_loss: 0.205517 normal_loss: 0.018186\n",
      "[307/00091] train_loss: 0.024238 kl_loss: 0.205546 normal_loss: 0.018071\n",
      "[308/00047] train_loss: 0.024131 kl_loss: 0.204488 normal_loss: 0.017996\n",
      "[309/00003] train_loss: 0.024260 kl_loss: 0.204729 normal_loss: 0.018118\n",
      "[309/00103] train_loss: 0.024236 kl_loss: 0.204717 normal_loss: 0.018094\n",
      "[310/00059] train_loss: 0.024052 kl_loss: 0.204471 normal_loss: 0.017918\n",
      "[311/00015] train_loss: 0.024198 kl_loss: 0.203852 normal_loss: 0.018082\n",
      "[311/00115] train_loss: 0.024165 kl_loss: 0.204844 normal_loss: 0.018020\n",
      "[312/00071] train_loss: 0.024068 kl_loss: 0.203208 normal_loss: 0.017972\n",
      "[313/00027] train_loss: 0.024122 kl_loss: 0.203639 normal_loss: 0.018013\n",
      "[313/00127] train_loss: 0.024133 kl_loss: 0.204061 normal_loss: 0.018011\n",
      "[314/00083] train_loss: 0.023964 kl_loss: 0.202497 normal_loss: 0.017889\n",
      "[315/00039] train_loss: 0.024048 kl_loss: 0.202890 normal_loss: 0.017962\n",
      "[315/00139] train_loss: 0.024055 kl_loss: 0.203390 normal_loss: 0.017953\n",
      "[316/00095] train_loss: 0.024048 kl_loss: 0.202913 normal_loss: 0.017961\n",
      "[317/00051] train_loss: 0.023952 kl_loss: 0.202159 normal_loss: 0.017887\n",
      "[318/00007] train_loss: 0.024009 kl_loss: 0.202384 normal_loss: 0.017937\n",
      "[318/00107] train_loss: 0.023865 kl_loss: 0.202125 normal_loss: 0.017801\n",
      "[319/00063] train_loss: 0.023932 kl_loss: 0.202375 normal_loss: 0.017861\n",
      "[320/00019] train_loss: 0.023918 kl_loss: 0.201905 normal_loss: 0.017861\n",
      "[320/00119] train_loss: 0.023985 kl_loss: 0.201652 normal_loss: 0.017936\n",
      "[321/00075] train_loss: 0.024136 kl_loss: 0.202836 normal_loss: 0.018051\n",
      "[322/00031] train_loss: 0.023868 kl_loss: 0.200945 normal_loss: 0.017840\n",
      "[322/00131] train_loss: 0.023912 kl_loss: 0.201451 normal_loss: 0.017868\n",
      "[323/00087] train_loss: 0.023763 kl_loss: 0.200885 normal_loss: 0.017736\n",
      "[324/00043] train_loss: 0.023901 kl_loss: 0.201426 normal_loss: 0.017859\n",
      "[324/00143] train_loss: 0.023778 kl_loss: 0.200452 normal_loss: 0.017765\n",
      "[325/00099] train_loss: 0.023840 kl_loss: 0.201088 normal_loss: 0.017807\n",
      "[326/00055] train_loss: 0.023759 kl_loss: 0.200460 normal_loss: 0.017745\n",
      "[327/00011] train_loss: 0.023823 kl_loss: 0.200382 normal_loss: 0.017812\n",
      "[327/00111] train_loss: 0.023845 kl_loss: 0.200859 normal_loss: 0.017819\n",
      "[328/00067] train_loss: 0.023715 kl_loss: 0.198784 normal_loss: 0.017752\n",
      "[329/00023] train_loss: 0.023913 kl_loss: 0.201224 normal_loss: 0.017876\n",
      "[329/00123] train_loss: 0.023685 kl_loss: 0.198820 normal_loss: 0.017720\n",
      "[330/00079] train_loss: 0.023949 kl_loss: 0.200912 normal_loss: 0.017921\n",
      "[331/00035] train_loss: 0.023687 kl_loss: 0.199157 normal_loss: 0.017712\n",
      "[331/00135] train_loss: 0.023719 kl_loss: 0.199262 normal_loss: 0.017742\n",
      "[332/00091] train_loss: 0.023693 kl_loss: 0.199409 normal_loss: 0.017711\n",
      "[333/00047] train_loss: 0.023729 kl_loss: 0.200064 normal_loss: 0.017727\n",
      "[334/00003] train_loss: 0.023766 kl_loss: 0.198744 normal_loss: 0.017804\n",
      "[334/00103] train_loss: 0.023586 kl_loss: 0.198320 normal_loss: 0.017636\n",
      "[335/00059] train_loss: 0.023721 kl_loss: 0.199216 normal_loss: 0.017744\n",
      "[336/00015] train_loss: 0.023676 kl_loss: 0.199335 normal_loss: 0.017696\n",
      "[336/00115] train_loss: 0.023552 kl_loss: 0.197910 normal_loss: 0.017615\n",
      "[337/00071] train_loss: 0.023541 kl_loss: 0.198334 normal_loss: 0.017591\n",
      "[338/00027] train_loss: 0.023888 kl_loss: 0.199764 normal_loss: 0.017895\n",
      "[338/00127] train_loss: 0.023579 kl_loss: 0.197801 normal_loss: 0.017645\n",
      "[339/00083] train_loss: 0.023405 kl_loss: 0.197211 normal_loss: 0.017489\n",
      "[340/00039] train_loss: 0.023548 kl_loss: 0.198507 normal_loss: 0.017592\n",
      "[340/00139] train_loss: 0.023699 kl_loss: 0.198389 normal_loss: 0.017748\n",
      "[341/00095] train_loss: 0.023634 kl_loss: 0.197862 normal_loss: 0.017698\n",
      "[342/00051] train_loss: 0.023493 kl_loss: 0.196946 normal_loss: 0.017585\n",
      "[343/00007] train_loss: 0.023618 kl_loss: 0.198411 normal_loss: 0.017666\n",
      "[343/00107] train_loss: 0.023577 kl_loss: 0.197472 normal_loss: 0.017653\n",
      "[344/00063] train_loss: 0.023545 kl_loss: 0.197557 normal_loss: 0.017618\n",
      "[345/00019] train_loss: 0.023567 kl_loss: 0.197877 normal_loss: 0.017631\n",
      "[345/00119] train_loss: 0.023429 kl_loss: 0.195947 normal_loss: 0.017551\n",
      "[346/00075] train_loss: 0.023508 kl_loss: 0.197179 normal_loss: 0.017592\n",
      "[347/00031] train_loss: 0.023629 kl_loss: 0.197706 normal_loss: 0.017698\n",
      "[347/00131] train_loss: 0.023494 kl_loss: 0.196893 normal_loss: 0.017587\n",
      "[348/00087] train_loss: 0.023222 kl_loss: 0.196164 normal_loss: 0.017337\n",
      "[349/00043] train_loss: 0.023421 kl_loss: 0.196220 normal_loss: 0.017534\n",
      "[349/00143] train_loss: 0.023565 kl_loss: 0.197328 normal_loss: 0.017645\n",
      "[350/00099] train_loss: 0.023481 kl_loss: 0.196853 normal_loss: 0.017575\n",
      "[351/00055] train_loss: 0.023330 kl_loss: 0.195445 normal_loss: 0.017467\n",
      "[352/00011] train_loss: 0.023658 kl_loss: 0.197283 normal_loss: 0.017739\n",
      "[352/00111] train_loss: 0.023399 kl_loss: 0.196094 normal_loss: 0.017517\n",
      "[353/00067] train_loss: 0.023294 kl_loss: 0.195462 normal_loss: 0.017431\n",
      "[354/00023] train_loss: 0.023416 kl_loss: 0.195724 normal_loss: 0.017544\n",
      "[354/00123] train_loss: 0.023328 kl_loss: 0.195663 normal_loss: 0.017458\n",
      "[355/00079] train_loss: 0.023285 kl_loss: 0.196136 normal_loss: 0.017401\n",
      "[356/00035] train_loss: 0.023321 kl_loss: 0.195617 normal_loss: 0.017452\n",
      "[356/00135] train_loss: 0.023325 kl_loss: 0.195808 normal_loss: 0.017451\n",
      "[357/00091] train_loss: 0.023297 kl_loss: 0.195258 normal_loss: 0.017439\n",
      "[358/00047] train_loss: 0.023330 kl_loss: 0.195501 normal_loss: 0.017465\n",
      "[359/00003] train_loss: 0.023253 kl_loss: 0.194951 normal_loss: 0.017405\n",
      "[359/00103] train_loss: 0.023419 kl_loss: 0.196083 normal_loss: 0.017536\n",
      "[360/00059] train_loss: 0.023051 kl_loss: 0.193883 normal_loss: 0.017234\n",
      "[361/00015] train_loss: 0.023425 kl_loss: 0.195308 normal_loss: 0.017566\n",
      "[361/00115] train_loss: 0.023231 kl_loss: 0.194961 normal_loss: 0.017382\n",
      "[362/00071] train_loss: 0.023222 kl_loss: 0.193704 normal_loss: 0.017410\n",
      "[363/00027] train_loss: 0.023347 kl_loss: 0.195136 normal_loss: 0.017493\n",
      "[363/00127] train_loss: 0.023265 kl_loss: 0.195020 normal_loss: 0.017414\n",
      "[364/00083] train_loss: 0.023159 kl_loss: 0.193931 normal_loss: 0.017341\n",
      "[365/00039] train_loss: 0.023293 kl_loss: 0.194702 normal_loss: 0.017452\n",
      "[365/00139] train_loss: 0.023216 kl_loss: 0.194446 normal_loss: 0.017383\n",
      "[366/00095] train_loss: 0.023076 kl_loss: 0.193655 normal_loss: 0.017266\n",
      "[367/00051] train_loss: 0.023278 kl_loss: 0.195438 normal_loss: 0.017415\n",
      "[368/00007] train_loss: 0.023156 kl_loss: 0.193450 normal_loss: 0.017352\n",
      "[368/00107] train_loss: 0.023244 kl_loss: 0.194582 normal_loss: 0.017406\n",
      "[369/00063] train_loss: 0.023104 kl_loss: 0.193907 normal_loss: 0.017287\n",
      "[370/00019] train_loss: 0.023023 kl_loss: 0.193275 normal_loss: 0.017225\n",
      "[370/00119] train_loss: 0.023214 kl_loss: 0.194238 normal_loss: 0.017387\n",
      "[371/00075] train_loss: 0.023061 kl_loss: 0.193546 normal_loss: 0.017255\n",
      "[372/00031] train_loss: 0.023192 kl_loss: 0.193838 normal_loss: 0.017377\n",
      "[372/00131] train_loss: 0.023012 kl_loss: 0.192917 normal_loss: 0.017224\n",
      "[373/00087] train_loss: 0.023143 kl_loss: 0.193714 normal_loss: 0.017332\n",
      "[374/00043] train_loss: 0.023115 kl_loss: 0.193455 normal_loss: 0.017311\n",
      "[374/00143] train_loss: 0.023052 kl_loss: 0.192903 normal_loss: 0.017265\n",
      "[375/00099] train_loss: 0.023113 kl_loss: 0.193644 normal_loss: 0.017304\n",
      "[376/00055] train_loss: 0.022992 kl_loss: 0.192444 normal_loss: 0.017219\n",
      "[377/00011] train_loss: 0.023061 kl_loss: 0.192967 normal_loss: 0.017272\n",
      "[377/00111] train_loss: 0.023069 kl_loss: 0.193089 normal_loss: 0.017276\n",
      "[378/00067] train_loss: 0.022987 kl_loss: 0.192952 normal_loss: 0.017198\n",
      "[379/00023] train_loss: 0.022954 kl_loss: 0.192106 normal_loss: 0.017191\n",
      "[379/00123] train_loss: 0.023006 kl_loss: 0.192995 normal_loss: 0.017216\n",
      "[380/00079] train_loss: 0.022822 kl_loss: 0.191314 normal_loss: 0.017082\n",
      "[381/00035] train_loss: 0.023122 kl_loss: 0.194082 normal_loss: 0.017299\n",
      "[381/00135] train_loss: 0.022952 kl_loss: 0.191663 normal_loss: 0.017202\n",
      "[382/00091] train_loss: 0.023028 kl_loss: 0.192409 normal_loss: 0.017256\n",
      "[383/00047] train_loss: 0.022879 kl_loss: 0.191905 normal_loss: 0.017122\n",
      "[384/00003] train_loss: 0.022903 kl_loss: 0.192419 normal_loss: 0.017130\n",
      "[384/00103] train_loss: 0.023010 kl_loss: 0.191870 normal_loss: 0.017254\n",
      "[385/00059] train_loss: 0.022780 kl_loss: 0.191082 normal_loss: 0.017048\n",
      "[386/00015] train_loss: 0.023072 kl_loss: 0.193117 normal_loss: 0.017278\n",
      "[386/00115] train_loss: 0.022933 kl_loss: 0.192125 normal_loss: 0.017170\n",
      "[387/00071] train_loss: 0.022850 kl_loss: 0.191522 normal_loss: 0.017105\n",
      "[388/00027] train_loss: 0.022966 kl_loss: 0.191281 normal_loss: 0.017228\n",
      "[388/00127] train_loss: 0.022900 kl_loss: 0.191856 normal_loss: 0.017144\n",
      "[389/00083] train_loss: 0.022934 kl_loss: 0.191053 normal_loss: 0.017202\n",
      "[390/00039] train_loss: 0.022759 kl_loss: 0.191818 normal_loss: 0.017005\n",
      "[390/00139] train_loss: 0.022839 kl_loss: 0.191163 normal_loss: 0.017104\n",
      "[391/00095] train_loss: 0.022744 kl_loss: 0.190619 normal_loss: 0.017025\n",
      "[392/00051] train_loss: 0.022901 kl_loss: 0.191961 normal_loss: 0.017142\n",
      "[393/00007] train_loss: 0.022788 kl_loss: 0.190457 normal_loss: 0.017075\n",
      "[393/00107] train_loss: 0.022814 kl_loss: 0.191419 normal_loss: 0.017072\n",
      "[394/00063] train_loss: 0.022862 kl_loss: 0.190974 normal_loss: 0.017133\n",
      "[395/00019] train_loss: 0.022748 kl_loss: 0.190363 normal_loss: 0.017037\n",
      "[395/00119] train_loss: 0.022788 kl_loss: 0.190707 normal_loss: 0.017067\n",
      "[396/00075] train_loss: 0.022797 kl_loss: 0.190515 normal_loss: 0.017082\n",
      "[397/00031] train_loss: 0.022768 kl_loss: 0.190447 normal_loss: 0.017054\n",
      "[397/00131] train_loss: 0.022722 kl_loss: 0.190531 normal_loss: 0.017007\n",
      "[398/00087] train_loss: 0.022763 kl_loss: 0.190554 normal_loss: 0.017046\n",
      "[399/00043] train_loss: 0.022662 kl_loss: 0.190057 normal_loss: 0.016960\n",
      "[399/00143] train_loss: 0.022722 kl_loss: 0.190911 normal_loss: 0.016995\n",
      "[400/00099] train_loss: 0.022561 kl_loss: 0.190097 normal_loss: 0.016858\n",
      "[401/00055] train_loss: 0.022505 kl_loss: 0.190792 normal_loss: 0.016781\n",
      "[402/00011] train_loss: 0.022392 kl_loss: 0.189382 normal_loss: 0.016710\n",
      "[402/00111] train_loss: 0.022491 kl_loss: 0.189716 normal_loss: 0.016799\n",
      "[403/00067] train_loss: 0.022474 kl_loss: 0.190060 normal_loss: 0.016772\n",
      "[404/00023] train_loss: 0.022464 kl_loss: 0.189802 normal_loss: 0.016770\n",
      "[404/00123] train_loss: 0.022410 kl_loss: 0.189954 normal_loss: 0.016711\n",
      "[405/00079] train_loss: 0.022578 kl_loss: 0.190434 normal_loss: 0.016865\n",
      "[406/00035] train_loss: 0.022320 kl_loss: 0.189081 normal_loss: 0.016647\n",
      "[406/00135] train_loss: 0.022404 kl_loss: 0.189412 normal_loss: 0.016722\n",
      "[407/00091] train_loss: 0.022400 kl_loss: 0.189280 normal_loss: 0.016721\n",
      "[408/00047] train_loss: 0.022494 kl_loss: 0.189588 normal_loss: 0.016806\n",
      "[409/00003] train_loss: 0.022339 kl_loss: 0.189578 normal_loss: 0.016651\n",
      "[409/00103] train_loss: 0.022335 kl_loss: 0.189351 normal_loss: 0.016654\n",
      "[410/00059] train_loss: 0.022323 kl_loss: 0.189422 normal_loss: 0.016641\n",
      "[411/00015] train_loss: 0.022273 kl_loss: 0.188585 normal_loss: 0.016615\n",
      "[411/00115] train_loss: 0.022324 kl_loss: 0.188983 normal_loss: 0.016654\n",
      "[412/00071] train_loss: 0.022373 kl_loss: 0.189524 normal_loss: 0.016687\n",
      "[413/00027] train_loss: 0.022359 kl_loss: 0.189217 normal_loss: 0.016683\n",
      "[413/00127] train_loss: 0.022279 kl_loss: 0.188477 normal_loss: 0.016625\n",
      "[414/00083] train_loss: 0.022327 kl_loss: 0.189474 normal_loss: 0.016643\n",
      "[415/00039] train_loss: 0.022309 kl_loss: 0.188325 normal_loss: 0.016660\n",
      "[415/00139] train_loss: 0.022338 kl_loss: 0.188862 normal_loss: 0.016672\n",
      "[416/00095] train_loss: 0.022258 kl_loss: 0.188331 normal_loss: 0.016608\n",
      "[417/00051] train_loss: 0.022304 kl_loss: 0.188763 normal_loss: 0.016641\n",
      "[418/00007] train_loss: 0.022271 kl_loss: 0.188720 normal_loss: 0.016609\n",
      "[418/00107] train_loss: 0.022337 kl_loss: 0.188901 normal_loss: 0.016670\n",
      "[419/00063] train_loss: 0.022280 kl_loss: 0.188336 normal_loss: 0.016630\n",
      "[420/00019] train_loss: 0.022135 kl_loss: 0.187387 normal_loss: 0.016513\n",
      "[420/00119] train_loss: 0.022232 kl_loss: 0.188145 normal_loss: 0.016588\n",
      "[421/00075] train_loss: 0.022266 kl_loss: 0.187994 normal_loss: 0.016626\n",
      "[422/00031] train_loss: 0.022303 kl_loss: 0.188796 normal_loss: 0.016639\n",
      "[422/00131] train_loss: 0.022333 kl_loss: 0.188736 normal_loss: 0.016671\n",
      "[423/00087] train_loss: 0.022106 kl_loss: 0.187116 normal_loss: 0.016493\n",
      "[424/00043] train_loss: 0.022145 kl_loss: 0.187878 normal_loss: 0.016508\n",
      "[424/00143] train_loss: 0.022292 kl_loss: 0.188544 normal_loss: 0.016635\n",
      "[425/00099] train_loss: 0.022300 kl_loss: 0.188459 normal_loss: 0.016646\n",
      "[426/00055] train_loss: 0.022178 kl_loss: 0.187420 normal_loss: 0.016556\n",
      "[427/00011] train_loss: 0.022176 kl_loss: 0.187702 normal_loss: 0.016545\n",
      "[427/00111] train_loss: 0.022152 kl_loss: 0.187054 normal_loss: 0.016541\n",
      "[428/00067] train_loss: 0.022231 kl_loss: 0.188269 normal_loss: 0.016583\n",
      "[429/00023] train_loss: 0.022189 kl_loss: 0.187843 normal_loss: 0.016554\n",
      "[429/00123] train_loss: 0.022228 kl_loss: 0.187243 normal_loss: 0.016611\n",
      "[430/00079] train_loss: 0.022188 kl_loss: 0.188092 normal_loss: 0.016546\n",
      "[431/00035] train_loss: 0.022029 kl_loss: 0.186430 normal_loss: 0.016436\n",
      "[431/00135] train_loss: 0.022190 kl_loss: 0.187648 normal_loss: 0.016561\n",
      "[432/00091] train_loss: 0.022190 kl_loss: 0.187926 normal_loss: 0.016552\n",
      "[433/00047] train_loss: 0.022086 kl_loss: 0.186318 normal_loss: 0.016497\n",
      "[434/00003] train_loss: 0.022212 kl_loss: 0.188116 normal_loss: 0.016569\n",
      "[434/00103] train_loss: 0.022086 kl_loss: 0.186788 normal_loss: 0.016483\n",
      "[435/00059] train_loss: 0.022109 kl_loss: 0.187238 normal_loss: 0.016492\n",
      "[436/00015] train_loss: 0.022145 kl_loss: 0.187229 normal_loss: 0.016528\n",
      "[436/00115] train_loss: 0.022181 kl_loss: 0.187450 normal_loss: 0.016557\n",
      "[437/00071] train_loss: 0.022083 kl_loss: 0.186664 normal_loss: 0.016483\n",
      "[438/00027] train_loss: 0.022055 kl_loss: 0.186443 normal_loss: 0.016462\n",
      "[438/00127] train_loss: 0.022130 kl_loss: 0.187047 normal_loss: 0.016518\n",
      "[439/00083] train_loss: 0.022026 kl_loss: 0.186225 normal_loss: 0.016439\n",
      "[440/00039] train_loss: 0.022049 kl_loss: 0.186846 normal_loss: 0.016444\n",
      "[440/00139] train_loss: 0.022130 kl_loss: 0.186890 normal_loss: 0.016524\n",
      "[441/00095] train_loss: 0.022077 kl_loss: 0.186826 normal_loss: 0.016472\n",
      "[442/00051] train_loss: 0.021973 kl_loss: 0.186448 normal_loss: 0.016379\n",
      "[443/00007] train_loss: 0.022140 kl_loss: 0.186537 normal_loss: 0.016544\n",
      "[443/00107] train_loss: 0.022051 kl_loss: 0.186653 normal_loss: 0.016451\n",
      "[444/00063] train_loss: 0.021973 kl_loss: 0.185874 normal_loss: 0.016397\n",
      "[445/00019] train_loss: 0.022032 kl_loss: 0.186739 normal_loss: 0.016430\n",
      "[445/00119] train_loss: 0.022079 kl_loss: 0.186063 normal_loss: 0.016497\n",
      "[446/00075] train_loss: 0.022013 kl_loss: 0.185932 normal_loss: 0.016435\n",
      "[447/00031] train_loss: 0.022108 kl_loss: 0.186881 normal_loss: 0.016502\n",
      "[447/00131] train_loss: 0.021976 kl_loss: 0.185799 normal_loss: 0.016402\n",
      "[448/00087] train_loss: 0.022026 kl_loss: 0.186048 normal_loss: 0.016445\n",
      "[449/00043] train_loss: 0.021982 kl_loss: 0.185845 normal_loss: 0.016407\n",
      "[449/00143] train_loss: 0.021996 kl_loss: 0.185980 normal_loss: 0.016416\n",
      "[450/00099] train_loss: 0.022074 kl_loss: 0.186424 normal_loss: 0.016481\n",
      "[451/00055] train_loss: 0.021908 kl_loss: 0.184933 normal_loss: 0.016360\n",
      "[452/00011] train_loss: 0.021993 kl_loss: 0.186364 normal_loss: 0.016403\n",
      "[452/00111] train_loss: 0.021934 kl_loss: 0.185584 normal_loss: 0.016366\n",
      "[453/00067] train_loss: 0.021984 kl_loss: 0.185617 normal_loss: 0.016415\n",
      "[454/00023] train_loss: 0.022026 kl_loss: 0.186226 normal_loss: 0.016439\n",
      "[454/00123] train_loss: 0.021856 kl_loss: 0.184810 normal_loss: 0.016311\n",
      "[455/00079] train_loss: 0.021894 kl_loss: 0.185068 normal_loss: 0.016342\n",
      "[456/00035] train_loss: 0.022045 kl_loss: 0.186409 normal_loss: 0.016453\n",
      "[456/00135] train_loss: 0.021964 kl_loss: 0.185454 normal_loss: 0.016400\n",
      "[457/00091] train_loss: 0.021972 kl_loss: 0.185193 normal_loss: 0.016416\n",
      "[458/00047] train_loss: 0.021982 kl_loss: 0.185788 normal_loss: 0.016408\n",
      "[459/00003] train_loss: 0.021959 kl_loss: 0.185342 normal_loss: 0.016399\n",
      "[459/00103] train_loss: 0.021894 kl_loss: 0.184836 normal_loss: 0.016349\n",
      "[460/00059] train_loss: 0.021993 kl_loss: 0.185171 normal_loss: 0.016438\n",
      "[461/00015] train_loss: 0.021861 kl_loss: 0.185393 normal_loss: 0.016299\n",
      "[461/00115] train_loss: 0.021954 kl_loss: 0.185715 normal_loss: 0.016383\n",
      "[462/00071] train_loss: 0.021833 kl_loss: 0.184116 normal_loss: 0.016310\n",
      "[463/00027] train_loss: 0.021944 kl_loss: 0.185551 normal_loss: 0.016377\n",
      "[463/00127] train_loss: 0.021849 kl_loss: 0.184731 normal_loss: 0.016307\n",
      "[464/00083] train_loss: 0.021891 kl_loss: 0.184687 normal_loss: 0.016351\n",
      "[465/00039] train_loss: 0.021941 kl_loss: 0.185631 normal_loss: 0.016372\n",
      "[465/00139] train_loss: 0.021906 kl_loss: 0.184625 normal_loss: 0.016368\n",
      "[466/00095] train_loss: 0.021953 kl_loss: 0.185550 normal_loss: 0.016387\n",
      "[467/00051] train_loss: 0.021810 kl_loss: 0.184397 normal_loss: 0.016278\n",
      "[468/00007] train_loss: 0.021814 kl_loss: 0.184562 normal_loss: 0.016277\n",
      "[468/00107] train_loss: 0.021920 kl_loss: 0.185123 normal_loss: 0.016366\n",
      "[469/00063] train_loss: 0.021827 kl_loss: 0.184790 normal_loss: 0.016283\n",
      "[470/00019] train_loss: 0.021789 kl_loss: 0.184026 normal_loss: 0.016268\n",
      "[470/00119] train_loss: 0.021824 kl_loss: 0.184361 normal_loss: 0.016293\n",
      "[471/00075] train_loss: 0.021855 kl_loss: 0.184993 normal_loss: 0.016305\n",
      "[472/00031] train_loss: 0.021765 kl_loss: 0.184106 normal_loss: 0.016242\n",
      "[472/00131] train_loss: 0.021845 kl_loss: 0.184786 normal_loss: 0.016301\n",
      "[473/00087] train_loss: 0.021777 kl_loss: 0.184066 normal_loss: 0.016255\n",
      "[474/00043] train_loss: 0.021815 kl_loss: 0.184141 normal_loss: 0.016291\n",
      "[474/00143] train_loss: 0.021831 kl_loss: 0.184477 normal_loss: 0.016297\n",
      "[475/00099] train_loss: 0.021783 kl_loss: 0.184439 normal_loss: 0.016250\n",
      "[476/00055] train_loss: 0.021674 kl_loss: 0.183516 normal_loss: 0.016168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msofa_chair_vad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOFA & CHAIR VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_chair_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa_chair',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3173\n",
      "Training params: 3\n",
      "[001/00049] train_loss: 0.158797 kl_loss: 0.485190 normal_loss: 0.144241\n",
      "[003/00049] train_loss: 0.103193 kl_loss: 0.413880 normal_loss: 0.090777\n",
      "[005/00049] train_loss: 0.092658 kl_loss: 0.348836 normal_loss: 0.082193\n",
      "[007/00049] train_loss: 0.089486 kl_loss: 0.300002 normal_loss: 0.080486\n",
      "[009/00049] train_loss: 0.084510 kl_loss: 0.266648 normal_loss: 0.076511\n",
      "[011/00049] train_loss: 0.080834 kl_loss: 0.244796 normal_loss: 0.073490\n",
      "[013/00049] train_loss: 0.075477 kl_loss: 0.232309 normal_loss: 0.068508\n",
      "[015/00049] train_loss: 0.072623 kl_loss: 0.224983 normal_loss: 0.065873\n",
      "[017/00049] train_loss: 0.070084 kl_loss: 0.216381 normal_loss: 0.063592\n",
      "[019/00049] train_loss: 0.068342 kl_loss: 0.208961 normal_loss: 0.062073\n",
      "[021/00049] train_loss: 0.065641 kl_loss: 0.202974 normal_loss: 0.059552\n",
      "[023/00049] train_loss: 0.062942 kl_loss: 0.197744 normal_loss: 0.057010\n",
      "[025/00049] train_loss: 0.061899 kl_loss: 0.192079 normal_loss: 0.056137\n",
      "[027/00049] train_loss: 0.059484 kl_loss: 0.187513 normal_loss: 0.053859\n",
      "[029/00049] train_loss: 0.058626 kl_loss: 0.183452 normal_loss: 0.053122\n",
      "[031/00049] train_loss: 0.058840 kl_loss: 0.180400 normal_loss: 0.053428\n",
      "[033/00049] train_loss: 0.057099 kl_loss: 0.178188 normal_loss: 0.051754\n",
      "[035/00049] train_loss: 0.055559 kl_loss: 0.176012 normal_loss: 0.050278\n",
      "[037/00049] train_loss: 0.053583 kl_loss: 0.174599 normal_loss: 0.048345\n",
      "[039/00049] train_loss: 0.054169 kl_loss: 0.173451 normal_loss: 0.048966\n",
      "[041/00049] train_loss: 0.052052 kl_loss: 0.172690 normal_loss: 0.046871\n",
      "[043/00049] train_loss: 0.052532 kl_loss: 0.171810 normal_loss: 0.047378\n",
      "[045/00049] train_loss: 0.050681 kl_loss: 0.170677 normal_loss: 0.045561\n",
      "[047/00049] train_loss: 0.049992 kl_loss: 0.170478 normal_loss: 0.044878\n",
      "[049/00049] train_loss: 0.049948 kl_loss: 0.170287 normal_loss: 0.044839\n",
      "[051/00049] train_loss: 0.048990 kl_loss: 0.170063 normal_loss: 0.043888\n",
      "[053/00049] train_loss: 0.048732 kl_loss: 0.169346 normal_loss: 0.043652\n",
      "[055/00049] train_loss: 0.048494 kl_loss: 0.169516 normal_loss: 0.043409\n",
      "[057/00049] train_loss: 0.047484 kl_loss: 0.169542 normal_loss: 0.042398\n",
      "[059/00049] train_loss: 0.045750 kl_loss: 0.169900 normal_loss: 0.040653\n",
      "[061/00049] train_loss: 0.046257 kl_loss: 0.170086 normal_loss: 0.041154\n",
      "[063/00049] train_loss: 0.045244 kl_loss: 0.170302 normal_loss: 0.040135\n",
      "[065/00049] train_loss: 0.045412 kl_loss: 0.170895 normal_loss: 0.040286\n",
      "[067/00049] train_loss: 0.044639 kl_loss: 0.171221 normal_loss: 0.039503\n",
      "[069/00049] train_loss: 0.044389 kl_loss: 0.171689 normal_loss: 0.039239\n",
      "[071/00049] train_loss: 0.044083 kl_loss: 0.171934 normal_loss: 0.038925\n",
      "[073/00049] train_loss: 0.043876 kl_loss: 0.173409 normal_loss: 0.038674\n",
      "[075/00049] train_loss: 0.042309 kl_loss: 0.174524 normal_loss: 0.037073\n",
      "[077/00049] train_loss: 0.042777 kl_loss: 0.175583 normal_loss: 0.037509\n",
      "[079/00049] train_loss: 0.042217 kl_loss: 0.176266 normal_loss: 0.036929\n",
      "[081/00049] train_loss: 0.041699 kl_loss: 0.177738 normal_loss: 0.036367\n",
      "[083/00049] train_loss: 0.041630 kl_loss: 0.179094 normal_loss: 0.036257\n",
      "[085/00049] train_loss: 0.040626 kl_loss: 0.180166 normal_loss: 0.035221\n",
      "[087/00049] train_loss: 0.040599 kl_loss: 0.181578 normal_loss: 0.035151\n",
      "[089/00049] train_loss: 0.040178 kl_loss: 0.182782 normal_loss: 0.034694\n",
      "[091/00049] train_loss: 0.039949 kl_loss: 0.184032 normal_loss: 0.034428\n",
      "[093/00049] train_loss: 0.039943 kl_loss: 0.184855 normal_loss: 0.034397\n",
      "[095/00049] train_loss: 0.039653 kl_loss: 0.185723 normal_loss: 0.034081\n",
      "[097/00049] train_loss: 0.039297 kl_loss: 0.186599 normal_loss: 0.033699\n",
      "[099/00049] train_loss: 0.039506 kl_loss: 0.188096 normal_loss: 0.033864\n",
      "[101/00049] train_loss: 0.035651 kl_loss: 0.188075 normal_loss: 0.030009\n",
      "[103/00049] train_loss: 0.035288 kl_loss: 0.186099 normal_loss: 0.029705\n",
      "[105/00049] train_loss: 0.034492 kl_loss: 0.184361 normal_loss: 0.028961\n",
      "[107/00049] train_loss: 0.034181 kl_loss: 0.182726 normal_loss: 0.028699\n",
      "[109/00049] train_loss: 0.034218 kl_loss: 0.181609 normal_loss: 0.028770\n",
      "[111/00049] train_loss: 0.033851 kl_loss: 0.180504 normal_loss: 0.028436\n",
      "[113/00049] train_loss: 0.033729 kl_loss: 0.179692 normal_loss: 0.028339\n",
      "[115/00049] train_loss: 0.033467 kl_loss: 0.178709 normal_loss: 0.028106\n",
      "[117/00049] train_loss: 0.033524 kl_loss: 0.177987 normal_loss: 0.028185\n",
      "[119/00049] train_loss: 0.033528 kl_loss: 0.177737 normal_loss: 0.028196\n",
      "[121/00049] train_loss: 0.033043 kl_loss: 0.177363 normal_loss: 0.027722\n",
      "[123/00049] train_loss: 0.032972 kl_loss: 0.177073 normal_loss: 0.027660\n",
      "[125/00049] train_loss: 0.032669 kl_loss: 0.176627 normal_loss: 0.027370\n",
      "[127/00049] train_loss: 0.032384 kl_loss: 0.176546 normal_loss: 0.027088\n",
      "[129/00049] train_loss: 0.032324 kl_loss: 0.176525 normal_loss: 0.027028\n",
      "[131/00049] train_loss: 0.032250 kl_loss: 0.176592 normal_loss: 0.026952\n",
      "[133/00049] train_loss: 0.032227 kl_loss: 0.176547 normal_loss: 0.026931\n",
      "[135/00049] train_loss: 0.031908 kl_loss: 0.176931 normal_loss: 0.026600\n",
      "[137/00049] train_loss: 0.032233 kl_loss: 0.177011 normal_loss: 0.026923\n",
      "[139/00049] train_loss: 0.032038 kl_loss: 0.177213 normal_loss: 0.026722\n",
      "[141/00049] train_loss: 0.031605 kl_loss: 0.177699 normal_loss: 0.026274\n",
      "[143/00049] train_loss: 0.031743 kl_loss: 0.177965 normal_loss: 0.026404\n",
      "[145/00049] train_loss: 0.031229 kl_loss: 0.178326 normal_loss: 0.025879\n",
      "[147/00049] train_loss: 0.031140 kl_loss: 0.178554 normal_loss: 0.025784\n",
      "[149/00049] train_loss: 0.030914 kl_loss: 0.179021 normal_loss: 0.025544\n",
      "[151/00049] train_loss: 0.030670 kl_loss: 0.179293 normal_loss: 0.025291\n",
      "[153/00049] train_loss: 0.030855 kl_loss: 0.179572 normal_loss: 0.025468\n",
      "[155/00049] train_loss: 0.030587 kl_loss: 0.179908 normal_loss: 0.025189\n",
      "[157/00049] train_loss: 0.029987 kl_loss: 0.180173 normal_loss: 0.024582\n",
      "[159/00049] train_loss: 0.030099 kl_loss: 0.180335 normal_loss: 0.024689\n",
      "[161/00049] train_loss: 0.030461 kl_loss: 0.180657 normal_loss: 0.025041\n",
      "[163/00049] train_loss: 0.030057 kl_loss: 0.181056 normal_loss: 0.024626\n",
      "[165/00049] train_loss: 0.029992 kl_loss: 0.181596 normal_loss: 0.024544\n",
      "[167/00049] train_loss: 0.029740 kl_loss: 0.181741 normal_loss: 0.024287\n",
      "[169/00049] train_loss: 0.029392 kl_loss: 0.181966 normal_loss: 0.023933\n",
      "[171/00049] train_loss: 0.029154 kl_loss: 0.182248 normal_loss: 0.023687\n",
      "[173/00049] train_loss: 0.029491 kl_loss: 0.182706 normal_loss: 0.024010\n",
      "[175/00049] train_loss: 0.029005 kl_loss: 0.182898 normal_loss: 0.023518\n",
      "[177/00049] train_loss: 0.028615 kl_loss: 0.183165 normal_loss: 0.023120\n",
      "[179/00049] train_loss: 0.028678 kl_loss: 0.183436 normal_loss: 0.023175\n",
      "[181/00049] train_loss: 0.028636 kl_loss: 0.183565 normal_loss: 0.023129\n",
      "[183/00049] train_loss: 0.028922 kl_loss: 0.183896 normal_loss: 0.023405\n",
      "[185/00049] train_loss: 0.028800 kl_loss: 0.184217 normal_loss: 0.023274\n",
      "[187/00049] train_loss: 0.028287 kl_loss: 0.184347 normal_loss: 0.022757\n",
      "[189/00049] train_loss: 0.028455 kl_loss: 0.184610 normal_loss: 0.022917\n",
      "[191/00049] train_loss: 0.028019 kl_loss: 0.184813 normal_loss: 0.022474\n",
      "[193/00049] train_loss: 0.027900 kl_loss: 0.184912 normal_loss: 0.022352\n",
      "[195/00049] train_loss: 0.027723 kl_loss: 0.185343 normal_loss: 0.022162\n",
      "[197/00049] train_loss: 0.027717 kl_loss: 0.185384 normal_loss: 0.022156\n",
      "[199/00049] train_loss: 0.027726 kl_loss: 0.185875 normal_loss: 0.022149\n",
      "[201/00049] train_loss: 0.026158 kl_loss: 0.185587 normal_loss: 0.020591\n",
      "[203/00049] train_loss: 0.025869 kl_loss: 0.185479 normal_loss: 0.020305\n",
      "[205/00049] train_loss: 0.025548 kl_loss: 0.184956 normal_loss: 0.019999\n",
      "[207/00049] train_loss: 0.025465 kl_loss: 0.184499 normal_loss: 0.019930\n",
      "[209/00049] train_loss: 0.025254 kl_loss: 0.184125 normal_loss: 0.019730\n",
      "[211/00049] train_loss: 0.025229 kl_loss: 0.183703 normal_loss: 0.019718\n",
      "[213/00049] train_loss: 0.025097 kl_loss: 0.183236 normal_loss: 0.019600\n",
      "[215/00049] train_loss: 0.024943 kl_loss: 0.182732 normal_loss: 0.019461\n",
      "[217/00049] train_loss: 0.025139 kl_loss: 0.182408 normal_loss: 0.019667\n",
      "[219/00049] train_loss: 0.025077 kl_loss: 0.182216 normal_loss: 0.019610\n",
      "[221/00049] train_loss: 0.025002 kl_loss: 0.181818 normal_loss: 0.019547\n",
      "[223/00049] train_loss: 0.024814 kl_loss: 0.181485 normal_loss: 0.019370\n",
      "[225/00049] train_loss: 0.024834 kl_loss: 0.181221 normal_loss: 0.019397\n",
      "[227/00049] train_loss: 0.024689 kl_loss: 0.180899 normal_loss: 0.019262\n",
      "[229/00049] train_loss: 0.024377 kl_loss: 0.180745 normal_loss: 0.018954\n",
      "[231/00049] train_loss: 0.024501 kl_loss: 0.180540 normal_loss: 0.019085\n",
      "[233/00049] train_loss: 0.024324 kl_loss: 0.180141 normal_loss: 0.018920\n",
      "[235/00049] train_loss: 0.024363 kl_loss: 0.179955 normal_loss: 0.018964\n",
      "[237/00049] train_loss: 0.024252 kl_loss: 0.179787 normal_loss: 0.018858\n",
      "[239/00049] train_loss: 0.024158 kl_loss: 0.179605 normal_loss: 0.018770\n",
      "[241/00049] train_loss: 0.024240 kl_loss: 0.179561 normal_loss: 0.018853\n",
      "[243/00049] train_loss: 0.024350 kl_loss: 0.179312 normal_loss: 0.018971\n",
      "[245/00049] train_loss: 0.024075 kl_loss: 0.179325 normal_loss: 0.018695\n",
      "[247/00049] train_loss: 0.024145 kl_loss: 0.179106 normal_loss: 0.018772\n",
      "[249/00049] train_loss: 0.023969 kl_loss: 0.178880 normal_loss: 0.018602\n",
      "[251/00049] train_loss: 0.024001 kl_loss: 0.178768 normal_loss: 0.018638\n",
      "[253/00049] train_loss: 0.023967 kl_loss: 0.178838 normal_loss: 0.018602\n",
      "[255/00049] train_loss: 0.023779 kl_loss: 0.178616 normal_loss: 0.018421\n",
      "[257/00049] train_loss: 0.023683 kl_loss: 0.178582 normal_loss: 0.018326\n",
      "[259/00049] train_loss: 0.023678 kl_loss: 0.178373 normal_loss: 0.018327\n",
      "[261/00049] train_loss: 0.023760 kl_loss: 0.178394 normal_loss: 0.018408\n",
      "[263/00049] train_loss: 0.023516 kl_loss: 0.178227 normal_loss: 0.018169\n",
      "[265/00049] train_loss: 0.023459 kl_loss: 0.178157 normal_loss: 0.018114\n",
      "[267/00049] train_loss: 0.023535 kl_loss: 0.178088 normal_loss: 0.018192\n",
      "[269/00049] train_loss: 0.023216 kl_loss: 0.178185 normal_loss: 0.017870\n",
      "[271/00049] train_loss: 0.023478 kl_loss: 0.178087 normal_loss: 0.018135\n",
      "[273/00049] train_loss: 0.023274 kl_loss: 0.178033 normal_loss: 0.017933\n",
      "[275/00049] train_loss: 0.023182 kl_loss: 0.177997 normal_loss: 0.017842\n",
      "[277/00049] train_loss: 0.023134 kl_loss: 0.177804 normal_loss: 0.017800\n",
      "[279/00049] train_loss: 0.023227 kl_loss: 0.177872 normal_loss: 0.017891\n",
      "[281/00049] train_loss: 0.023134 kl_loss: 0.177822 normal_loss: 0.017799\n",
      "[283/00049] train_loss: 0.022995 kl_loss: 0.177784 normal_loss: 0.017662\n",
      "[285/00049] train_loss: 0.022884 kl_loss: 0.177783 normal_loss: 0.017551\n",
      "[287/00049] train_loss: 0.023202 kl_loss: 0.177633 normal_loss: 0.017873\n",
      "[289/00049] train_loss: 0.023050 kl_loss: 0.177741 normal_loss: 0.017718\n",
      "[291/00049] train_loss: 0.022876 kl_loss: 0.177732 normal_loss: 0.017544\n",
      "[293/00049] train_loss: 0.022834 kl_loss: 0.177653 normal_loss: 0.017504\n",
      "[295/00049] train_loss: 0.022793 kl_loss: 0.177580 normal_loss: 0.017466\n",
      "[297/00049] train_loss: 0.022545 kl_loss: 0.177588 normal_loss: 0.017217\n",
      "[299/00049] train_loss: 0.022638 kl_loss: 0.177423 normal_loss: 0.017316\n",
      "[301/00049] train_loss: 0.022014 kl_loss: 0.177131 normal_loss: 0.016700\n",
      "[303/00049] train_loss: 0.021783 kl_loss: 0.177116 normal_loss: 0.016470\n",
      "[305/00049] train_loss: 0.021748 kl_loss: 0.176914 normal_loss: 0.016441\n",
      "[307/00049] train_loss: 0.021618 kl_loss: 0.176799 normal_loss: 0.016314\n",
      "[309/00049] train_loss: 0.021609 kl_loss: 0.176716 normal_loss: 0.016308\n",
      "[311/00049] train_loss: 0.021615 kl_loss: 0.176545 normal_loss: 0.016319\n",
      "[313/00049] train_loss: 0.021493 kl_loss: 0.176303 normal_loss: 0.016204\n",
      "[315/00049] train_loss: 0.021467 kl_loss: 0.176164 normal_loss: 0.016182\n",
      "[317/00049] train_loss: 0.021416 kl_loss: 0.175951 normal_loss: 0.016137\n",
      "[319/00049] train_loss: 0.021430 kl_loss: 0.175806 normal_loss: 0.016156\n",
      "[321/00049] train_loss: 0.021467 kl_loss: 0.175710 normal_loss: 0.016195\n",
      "[323/00049] train_loss: 0.021447 kl_loss: 0.175533 normal_loss: 0.016181\n",
      "[325/00049] train_loss: 0.021262 kl_loss: 0.175355 normal_loss: 0.016001\n",
      "[327/00049] train_loss: 0.021281 kl_loss: 0.175218 normal_loss: 0.016025\n",
      "[329/00049] train_loss: 0.021234 kl_loss: 0.174976 normal_loss: 0.015985\n",
      "[331/00049] train_loss: 0.021301 kl_loss: 0.174921 normal_loss: 0.016053\n",
      "[333/00049] train_loss: 0.021245 kl_loss: 0.174771 normal_loss: 0.016002\n",
      "[335/00049] train_loss: 0.021169 kl_loss: 0.174648 normal_loss: 0.015930\n",
      "[337/00049] train_loss: 0.021251 kl_loss: 0.174502 normal_loss: 0.016016\n",
      "[339/00049] train_loss: 0.021105 kl_loss: 0.174483 normal_loss: 0.015871\n",
      "[341/00049] train_loss: 0.021088 kl_loss: 0.174287 normal_loss: 0.015859\n",
      "[343/00049] train_loss: 0.021080 kl_loss: 0.174137 normal_loss: 0.015856\n",
      "[345/00049] train_loss: 0.021112 kl_loss: 0.174035 normal_loss: 0.015891\n",
      "[347/00049] train_loss: 0.021022 kl_loss: 0.173902 normal_loss: 0.015805\n",
      "[349/00049] train_loss: 0.021061 kl_loss: 0.173786 normal_loss: 0.015847\n",
      "[351/00049] train_loss: 0.020983 kl_loss: 0.173591 normal_loss: 0.015775\n",
      "[353/00049] train_loss: 0.020949 kl_loss: 0.173379 normal_loss: 0.015748\n",
      "[355/00049] train_loss: 0.020869 kl_loss: 0.173346 normal_loss: 0.015668\n",
      "[357/00049] train_loss: 0.020854 kl_loss: 0.173260 normal_loss: 0.015656\n",
      "[359/00049] train_loss: 0.020884 kl_loss: 0.173135 normal_loss: 0.015690\n",
      "[361/00049] train_loss: 0.020913 kl_loss: 0.173051 normal_loss: 0.015722\n",
      "[363/00049] train_loss: 0.020771 kl_loss: 0.172923 normal_loss: 0.015583\n",
      "[365/00049] train_loss: 0.020787 kl_loss: 0.172795 normal_loss: 0.015603\n",
      "[367/00049] train_loss: 0.020746 kl_loss: 0.172787 normal_loss: 0.015562\n",
      "[369/00049] train_loss: 0.020866 kl_loss: 0.172658 normal_loss: 0.015686\n",
      "[371/00049] train_loss: 0.020671 kl_loss: 0.172478 normal_loss: 0.015497\n",
      "[373/00049] train_loss: 0.020683 kl_loss: 0.172489 normal_loss: 0.015508\n",
      "[375/00049] train_loss: 0.020624 kl_loss: 0.172283 normal_loss: 0.015455\n",
      "[377/00049] train_loss: 0.020498 kl_loss: 0.172187 normal_loss: 0.015333\n",
      "[379/00049] train_loss: 0.020644 kl_loss: 0.172089 normal_loss: 0.015481\n",
      "[381/00049] train_loss: 0.020661 kl_loss: 0.171948 normal_loss: 0.015503\n",
      "[383/00049] train_loss: 0.020562 kl_loss: 0.171919 normal_loss: 0.015404\n",
      "[385/00049] train_loss: 0.020541 kl_loss: 0.171867 normal_loss: 0.015385\n",
      "[387/00049] train_loss: 0.020526 kl_loss: 0.171841 normal_loss: 0.015371\n",
      "[389/00049] train_loss: 0.020425 kl_loss: 0.171752 normal_loss: 0.015273\n",
      "[391/00049] train_loss: 0.020395 kl_loss: 0.171685 normal_loss: 0.015244\n",
      "[393/00049] train_loss: 0.020512 kl_loss: 0.171605 normal_loss: 0.015364\n",
      "[395/00049] train_loss: 0.020410 kl_loss: 0.171481 normal_loss: 0.015266\n",
      "[397/00049] train_loss: 0.020266 kl_loss: 0.171439 normal_loss: 0.015123\n",
      "[399/00049] train_loss: 0.020305 kl_loss: 0.171254 normal_loss: 0.015167\n",
      "[401/00049] train_loss: 0.020047 kl_loss: 0.171241 normal_loss: 0.014910\n",
      "[403/00049] train_loss: 0.020031 kl_loss: 0.171195 normal_loss: 0.014896\n",
      "[405/00049] train_loss: 0.019981 kl_loss: 0.171212 normal_loss: 0.014844\n",
      "[407/00049] train_loss: 0.019939 kl_loss: 0.171080 normal_loss: 0.014807\n",
      "[409/00049] train_loss: 0.019964 kl_loss: 0.170919 normal_loss: 0.014837\n",
      "[411/00049] train_loss: 0.019873 kl_loss: 0.170838 normal_loss: 0.014748\n",
      "[413/00049] train_loss: 0.019919 kl_loss: 0.170830 normal_loss: 0.014794\n",
      "[415/00049] train_loss: 0.019831 kl_loss: 0.170729 normal_loss: 0.014709\n",
      "[417/00049] train_loss: 0.019866 kl_loss: 0.170656 normal_loss: 0.014746\n",
      "[419/00049] train_loss: 0.019943 kl_loss: 0.170548 normal_loss: 0.014826\n",
      "[421/00049] train_loss: 0.019831 kl_loss: 0.170525 normal_loss: 0.014715\n",
      "[423/00049] train_loss: 0.019792 kl_loss: 0.170445 normal_loss: 0.014679\n",
      "[425/00049] train_loss: 0.019797 kl_loss: 0.170346 normal_loss: 0.014687\n",
      "[427/00049] train_loss: 0.019769 kl_loss: 0.170243 normal_loss: 0.014662\n",
      "[429/00049] train_loss: 0.019775 kl_loss: 0.170280 normal_loss: 0.014666\n",
      "[431/00049] train_loss: 0.019739 kl_loss: 0.170085 normal_loss: 0.014636\n",
      "[433/00049] train_loss: 0.019728 kl_loss: 0.170046 normal_loss: 0.014627\n",
      "[435/00049] train_loss: 0.019741 kl_loss: 0.170012 normal_loss: 0.014641\n",
      "[437/00049] train_loss: 0.019713 kl_loss: 0.169901 normal_loss: 0.014616\n",
      "[439/00049] train_loss: 0.019623 kl_loss: 0.169828 normal_loss: 0.014528\n",
      "[441/00049] train_loss: 0.019677 kl_loss: 0.169907 normal_loss: 0.014580\n",
      "[443/00049] train_loss: 0.019659 kl_loss: 0.169708 normal_loss: 0.014568\n",
      "[445/00049] train_loss: 0.019646 kl_loss: 0.169616 normal_loss: 0.014558\n",
      "[447/00049] train_loss: 0.019608 kl_loss: 0.169558 normal_loss: 0.014522\n",
      "[449/00049] train_loss: 0.019661 kl_loss: 0.169399 normal_loss: 0.014579\n",
      "[451/00049] train_loss: 0.019577 kl_loss: 0.169463 normal_loss: 0.014493\n",
      "[453/00049] train_loss: 0.019569 kl_loss: 0.169361 normal_loss: 0.014489\n",
      "[455/00049] train_loss: 0.019589 kl_loss: 0.169272 normal_loss: 0.014511\n",
      "[457/00049] train_loss: 0.019573 kl_loss: 0.169153 normal_loss: 0.014498\n",
      "[459/00049] train_loss: 0.019558 kl_loss: 0.169156 normal_loss: 0.014483\n",
      "[461/00049] train_loss: 0.019515 kl_loss: 0.169094 normal_loss: 0.014442\n",
      "[463/00049] train_loss: 0.019538 kl_loss: 0.168979 normal_loss: 0.014469\n",
      "[465/00049] train_loss: 0.019515 kl_loss: 0.168962 normal_loss: 0.014446\n",
      "[467/00049] train_loss: 0.019466 kl_loss: 0.168890 normal_loss: 0.014399\n",
      "[469/00049] train_loss: 0.019453 kl_loss: 0.168773 normal_loss: 0.014390\n",
      "[471/00049] train_loss: 0.019428 kl_loss: 0.168749 normal_loss: 0.014365\n",
      "[473/00049] train_loss: 0.019454 kl_loss: 0.168576 normal_loss: 0.014397\n",
      "[475/00049] train_loss: 0.019458 kl_loss: 0.168565 normal_loss: 0.014401\n",
      "[477/00049] train_loss: 0.019497 kl_loss: 0.168535 normal_loss: 0.014441\n",
      "[479/00049] train_loss: 0.019410 kl_loss: 0.168581 normal_loss: 0.014353\n",
      "[481/00049] train_loss: 0.019414 kl_loss: 0.168393 normal_loss: 0.014362\n",
      "[483/00049] train_loss: 0.019320 kl_loss: 0.168440 normal_loss: 0.014267\n",
      "[485/00049] train_loss: 0.019430 kl_loss: 0.168287 normal_loss: 0.014381\n",
      "[487/00049] train_loss: 0.019338 kl_loss: 0.168266 normal_loss: 0.014290\n",
      "[489/00049] train_loss: 0.019372 kl_loss: 0.168235 normal_loss: 0.014325\n",
      "[491/00049] train_loss: 0.019382 kl_loss: 0.168193 normal_loss: 0.014336\n",
      "[493/00049] train_loss: 0.019283 kl_loss: 0.168007 normal_loss: 0.014243\n",
      "[495/00049] train_loss: 0.019289 kl_loss: 0.167999 normal_loss: 0.014249\n",
      "[497/00049] train_loss: 0.019290 kl_loss: 0.167932 normal_loss: 0.014252\n",
      "[499/00049] train_loss: 0.019259 kl_loss: 0.167835 normal_loss: 0.014224\n"
     ]
    }
   ],
   "source": [
    "# SOFA VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_vad_0_03kl',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3173\n",
      "Training params: 2\n",
      "[001/00049] train_loss: 0.147046 kl_loss: 0.000000 normal_loss: 0.147046\n",
      "[003/00049] train_loss: 0.086997 kl_loss: 0.000000 normal_loss: 0.086997\n",
      "[005/00049] train_loss: 0.078271 kl_loss: 0.000000 normal_loss: 0.078271\n",
      "[007/00049] train_loss: 0.072125 kl_loss: 0.000000 normal_loss: 0.072125\n",
      "[009/00049] train_loss: 0.063552 kl_loss: 0.000000 normal_loss: 0.063552\n",
      "[011/00049] train_loss: 0.060275 kl_loss: 0.000000 normal_loss: 0.060275\n",
      "[013/00049] train_loss: 0.056823 kl_loss: 0.000000 normal_loss: 0.056823\n",
      "[015/00049] train_loss: 0.056097 kl_loss: 0.000000 normal_loss: 0.056097\n",
      "[017/00049] train_loss: 0.051553 kl_loss: 0.000000 normal_loss: 0.051553\n",
      "[019/00049] train_loss: 0.050349 kl_loss: 0.000000 normal_loss: 0.050349\n",
      "[021/00049] train_loss: 0.047069 kl_loss: 0.000000 normal_loss: 0.047069\n",
      "[023/00049] train_loss: 0.044120 kl_loss: 0.000000 normal_loss: 0.044120\n",
      "[025/00049] train_loss: 0.043236 kl_loss: 0.000000 normal_loss: 0.043236\n",
      "[027/00049] train_loss: 0.041510 kl_loss: 0.000000 normal_loss: 0.041510\n",
      "[029/00049] train_loss: 0.040672 kl_loss: 0.000000 normal_loss: 0.040672\n",
      "[031/00049] train_loss: 0.039042 kl_loss: 0.000000 normal_loss: 0.039042\n",
      "[033/00049] train_loss: 0.037533 kl_loss: 0.000000 normal_loss: 0.037533\n",
      "[035/00049] train_loss: 0.035808 kl_loss: 0.000000 normal_loss: 0.035808\n",
      "[037/00049] train_loss: 0.034947 kl_loss: 0.000000 normal_loss: 0.034947\n",
      "[039/00049] train_loss: 0.033185 kl_loss: 0.000000 normal_loss: 0.033185\n",
      "[041/00049] train_loss: 0.032495 kl_loss: 0.000000 normal_loss: 0.032495\n",
      "[043/00049] train_loss: 0.031494 kl_loss: 0.000000 normal_loss: 0.031494\n",
      "[045/00049] train_loss: 0.030708 kl_loss: 0.000000 normal_loss: 0.030708\n",
      "[047/00049] train_loss: 0.030595 kl_loss: 0.000000 normal_loss: 0.030595\n",
      "[049/00049] train_loss: 0.029956 kl_loss: 0.000000 normal_loss: 0.029956\n",
      "[051/00049] train_loss: 0.028813 kl_loss: 0.000000 normal_loss: 0.028813\n",
      "[053/00049] train_loss: 0.027714 kl_loss: 0.000000 normal_loss: 0.027714\n",
      "[055/00049] train_loss: 0.027332 kl_loss: 0.000000 normal_loss: 0.027332\n",
      "[057/00049] train_loss: 0.027308 kl_loss: 0.000000 normal_loss: 0.027308\n",
      "[059/00049] train_loss: 0.026427 kl_loss: 0.000000 normal_loss: 0.026427\n",
      "[061/00049] train_loss: 0.026049 kl_loss: 0.000000 normal_loss: 0.026049\n",
      "[063/00049] train_loss: 0.024755 kl_loss: 0.000000 normal_loss: 0.024755\n",
      "[065/00049] train_loss: 0.024271 kl_loss: 0.000000 normal_loss: 0.024271\n",
      "[067/00049] train_loss: 0.024266 kl_loss: 0.000000 normal_loss: 0.024266\n",
      "[069/00049] train_loss: 0.024478 kl_loss: 0.000000 normal_loss: 0.024478\n",
      "[071/00049] train_loss: 0.024100 kl_loss: 0.000000 normal_loss: 0.024100\n",
      "[073/00049] train_loss: 0.022835 kl_loss: 0.000000 normal_loss: 0.022835\n",
      "[075/00049] train_loss: 0.022686 kl_loss: 0.000000 normal_loss: 0.022686\n",
      "[077/00049] train_loss: 0.023004 kl_loss: 0.000000 normal_loss: 0.023004\n",
      "[079/00049] train_loss: 0.022281 kl_loss: 0.000000 normal_loss: 0.022281\n",
      "[081/00049] train_loss: 0.021605 kl_loss: 0.000000 normal_loss: 0.021605\n",
      "[083/00049] train_loss: 0.020298 kl_loss: 0.000000 normal_loss: 0.020298\n",
      "[085/00049] train_loss: 0.020655 kl_loss: 0.000000 normal_loss: 0.020655\n",
      "[087/00049] train_loss: 0.021509 kl_loss: 0.000000 normal_loss: 0.021509\n",
      "[089/00049] train_loss: 0.022649 kl_loss: 0.000000 normal_loss: 0.022649\n",
      "[091/00049] train_loss: 0.020297 kl_loss: 0.000000 normal_loss: 0.020297\n",
      "[093/00049] train_loss: 0.021210 kl_loss: 0.000000 normal_loss: 0.021210\n",
      "[095/00049] train_loss: 0.019058 kl_loss: 0.000000 normal_loss: 0.019058\n",
      "[097/00049] train_loss: 0.019488 kl_loss: 0.000000 normal_loss: 0.019488\n",
      "[099/00049] train_loss: 0.018620 kl_loss: 0.000000 normal_loss: 0.018620\n",
      "[101/00049] train_loss: 0.016498 kl_loss: 0.000000 normal_loss: 0.016498\n",
      "[103/00049] train_loss: 0.014815 kl_loss: 0.000000 normal_loss: 0.014815\n",
      "[105/00049] train_loss: 0.014853 kl_loss: 0.000000 normal_loss: 0.014853\n",
      "[107/00049] train_loss: 0.015392 kl_loss: 0.000000 normal_loss: 0.015392\n",
      "[109/00049] train_loss: 0.016997 kl_loss: 0.000000 normal_loss: 0.016997\n",
      "[111/00049] train_loss: 0.014858 kl_loss: 0.000000 normal_loss: 0.014858\n",
      "[113/00049] train_loss: 0.014544 kl_loss: 0.000000 normal_loss: 0.014544\n",
      "[115/00049] train_loss: 0.014657 kl_loss: 0.000000 normal_loss: 0.014657\n",
      "[117/00049] train_loss: 0.014345 kl_loss: 0.000000 normal_loss: 0.014345\n",
      "[119/00049] train_loss: 0.013949 kl_loss: 0.000000 normal_loss: 0.013949\n",
      "[121/00049] train_loss: 0.014472 kl_loss: 0.000000 normal_loss: 0.014472\n",
      "[123/00049] train_loss: 0.015266 kl_loss: 0.000000 normal_loss: 0.015266\n",
      "[125/00049] train_loss: 0.014614 kl_loss: 0.000000 normal_loss: 0.014614\n",
      "[127/00049] train_loss: 0.013951 kl_loss: 0.000000 normal_loss: 0.013951\n",
      "[129/00049] train_loss: 0.014564 kl_loss: 0.000000 normal_loss: 0.014564\n",
      "[131/00049] train_loss: 0.013915 kl_loss: 0.000000 normal_loss: 0.013915\n",
      "[133/00049] train_loss: 0.013754 kl_loss: 0.000000 normal_loss: 0.013754\n",
      "[135/00049] train_loss: 0.013780 kl_loss: 0.000000 normal_loss: 0.013780\n",
      "[137/00049] train_loss: 0.013444 kl_loss: 0.000000 normal_loss: 0.013444\n",
      "[139/00049] train_loss: 0.013381 kl_loss: 0.000000 normal_loss: 0.013381\n",
      "[141/00049] train_loss: 0.013236 kl_loss: 0.000000 normal_loss: 0.013236\n",
      "[143/00049] train_loss: 0.013419 kl_loss: 0.000000 normal_loss: 0.013419\n",
      "[145/00049] train_loss: 0.013631 kl_loss: 0.000000 normal_loss: 0.013631\n",
      "[147/00049] train_loss: 0.013256 kl_loss: 0.000000 normal_loss: 0.013256\n",
      "[149/00049] train_loss: 0.013501 kl_loss: 0.000000 normal_loss: 0.013501\n",
      "[151/00049] train_loss: 0.013316 kl_loss: 0.000000 normal_loss: 0.013316\n",
      "[153/00049] train_loss: 0.012754 kl_loss: 0.000000 normal_loss: 0.012754\n",
      "[155/00049] train_loss: 0.012450 kl_loss: 0.000000 normal_loss: 0.012450\n",
      "[157/00049] train_loss: 0.013149 kl_loss: 0.000000 normal_loss: 0.013149\n",
      "[159/00049] train_loss: 0.012789 kl_loss: 0.000000 normal_loss: 0.012789\n",
      "[161/00049] train_loss: 0.013052 kl_loss: 0.000000 normal_loss: 0.013052\n",
      "[163/00049] train_loss: 0.012452 kl_loss: 0.000000 normal_loss: 0.012452\n",
      "[165/00049] train_loss: 0.012844 kl_loss: 0.000000 normal_loss: 0.012844\n",
      "[167/00049] train_loss: 0.012598 kl_loss: 0.000000 normal_loss: 0.012598\n",
      "[169/00049] train_loss: 0.012433 kl_loss: 0.000000 normal_loss: 0.012433\n",
      "[171/00049] train_loss: 0.012264 kl_loss: 0.000000 normal_loss: 0.012264\n",
      "[173/00049] train_loss: 0.012649 kl_loss: 0.000000 normal_loss: 0.012649\n",
      "[175/00049] train_loss: 0.012375 kl_loss: 0.000000 normal_loss: 0.012375\n",
      "[177/00049] train_loss: 0.012224 kl_loss: 0.000000 normal_loss: 0.012224\n",
      "[179/00049] train_loss: 0.012347 kl_loss: 0.000000 normal_loss: 0.012347\n",
      "[181/00049] train_loss: 0.012235 kl_loss: 0.000000 normal_loss: 0.012235\n",
      "[183/00049] train_loss: 0.012057 kl_loss: 0.000000 normal_loss: 0.012057\n",
      "[185/00049] train_loss: 0.012528 kl_loss: 0.000000 normal_loss: 0.012528\n",
      "[187/00049] train_loss: 0.012125 kl_loss: 0.000000 normal_loss: 0.012125\n",
      "[189/00049] train_loss: 0.012086 kl_loss: 0.000000 normal_loss: 0.012086\n",
      "[191/00049] train_loss: 0.012190 kl_loss: 0.000000 normal_loss: 0.012190\n",
      "[193/00049] train_loss: 0.012084 kl_loss: 0.000000 normal_loss: 0.012084\n",
      "[195/00049] train_loss: 0.011601 kl_loss: 0.000000 normal_loss: 0.011601\n",
      "[197/00049] train_loss: 0.011606 kl_loss: 0.000000 normal_loss: 0.011606\n",
      "[199/00049] train_loss: 0.011835 kl_loss: 0.000000 normal_loss: 0.011835\n",
      "[201/00049] train_loss: 0.010245 kl_loss: 0.000000 normal_loss: 0.010245\n",
      "[203/00049] train_loss: 0.009693 kl_loss: 0.000000 normal_loss: 0.009693\n",
      "[205/00049] train_loss: 0.009638 kl_loss: 0.000000 normal_loss: 0.009638\n",
      "[207/00049] train_loss: 0.009711 kl_loss: 0.000000 normal_loss: 0.009711\n",
      "[209/00049] train_loss: 0.009666 kl_loss: 0.000000 normal_loss: 0.009666\n",
      "[211/00049] train_loss: 0.009929 kl_loss: 0.000000 normal_loss: 0.009929\n",
      "[213/00049] train_loss: 0.009723 kl_loss: 0.000000 normal_loss: 0.009723\n",
      "[215/00049] train_loss: 0.009604 kl_loss: 0.000000 normal_loss: 0.009604\n",
      "[217/00049] train_loss: 0.009741 kl_loss: 0.000000 normal_loss: 0.009741\n",
      "[219/00049] train_loss: 0.009573 kl_loss: 0.000000 normal_loss: 0.009573\n",
      "[221/00049] train_loss: 0.009610 kl_loss: 0.000000 normal_loss: 0.009610\n",
      "[223/00049] train_loss: 0.009645 kl_loss: 0.000000 normal_loss: 0.009645\n",
      "[225/00049] train_loss: 0.010178 kl_loss: 0.000000 normal_loss: 0.010178\n",
      "[227/00049] train_loss: 0.009635 kl_loss: 0.000000 normal_loss: 0.009635\n",
      "[229/00049] train_loss: 0.009561 kl_loss: 0.000000 normal_loss: 0.009561\n",
      "[231/00049] train_loss: 0.009556 kl_loss: 0.000000 normal_loss: 0.009556\n",
      "[233/00049] train_loss: 0.009592 kl_loss: 0.000000 normal_loss: 0.009592\n",
      "[235/00049] train_loss: 0.009393 kl_loss: 0.000000 normal_loss: 0.009393\n",
      "[237/00049] train_loss: 0.009423 kl_loss: 0.000000 normal_loss: 0.009423\n",
      "[239/00049] train_loss: 0.009530 kl_loss: 0.000000 normal_loss: 0.009530\n",
      "[241/00049] train_loss: 0.009409 kl_loss: 0.000000 normal_loss: 0.009409\n",
      "[243/00049] train_loss: 0.009378 kl_loss: 0.000000 normal_loss: 0.009378\n",
      "[245/00049] train_loss: 0.009269 kl_loss: 0.000000 normal_loss: 0.009269\n",
      "[247/00049] train_loss: 0.009874 kl_loss: 0.000000 normal_loss: 0.009874\n",
      "[249/00049] train_loss: 0.009428 kl_loss: 0.000000 normal_loss: 0.009428\n",
      "[251/00049] train_loss: 0.009331 kl_loss: 0.000000 normal_loss: 0.009331\n",
      "[253/00049] train_loss: 0.009261 kl_loss: 0.000000 normal_loss: 0.009261\n",
      "[255/00049] train_loss: 0.009186 kl_loss: 0.000000 normal_loss: 0.009186\n",
      "[257/00049] train_loss: 0.009217 kl_loss: 0.000000 normal_loss: 0.009217\n",
      "[259/00049] train_loss: 0.009274 kl_loss: 0.000000 normal_loss: 0.009274\n",
      "[261/00049] train_loss: 0.009253 kl_loss: 0.000000 normal_loss: 0.009253\n",
      "[263/00049] train_loss: 0.009154 kl_loss: 0.000000 normal_loss: 0.009154\n",
      "[265/00049] train_loss: 0.009312 kl_loss: 0.000000 normal_loss: 0.009312\n",
      "[267/00049] train_loss: 0.009182 kl_loss: 0.000000 normal_loss: 0.009182\n",
      "[269/00049] train_loss: 0.009103 kl_loss: 0.000000 normal_loss: 0.009103\n",
      "[271/00049] train_loss: 0.009021 kl_loss: 0.000000 normal_loss: 0.009021\n",
      "[273/00049] train_loss: 0.009119 kl_loss: 0.000000 normal_loss: 0.009119\n",
      "[275/00049] train_loss: 0.009051 kl_loss: 0.000000 normal_loss: 0.009051\n",
      "[277/00049] train_loss: 0.009004 kl_loss: 0.000000 normal_loss: 0.009004\n",
      "[279/00049] train_loss: 0.009004 kl_loss: 0.000000 normal_loss: 0.009004\n",
      "[281/00049] train_loss: 0.009110 kl_loss: 0.000000 normal_loss: 0.009110\n",
      "[283/00049] train_loss: 0.008988 kl_loss: 0.000000 normal_loss: 0.008988\n",
      "[285/00049] train_loss: 0.009043 kl_loss: 0.000000 normal_loss: 0.009043\n",
      "[287/00049] train_loss: 0.009085 kl_loss: 0.000000 normal_loss: 0.009085\n",
      "[289/00049] train_loss: 0.009061 kl_loss: 0.000000 normal_loss: 0.009061\n",
      "[291/00049] train_loss: 0.009048 kl_loss: 0.000000 normal_loss: 0.009048\n",
      "[293/00049] train_loss: 0.009074 kl_loss: 0.000000 normal_loss: 0.009074\n",
      "[295/00049] train_loss: 0.008822 kl_loss: 0.000000 normal_loss: 0.008822\n",
      "[297/00049] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[299/00049] train_loss: 0.009084 kl_loss: 0.000000 normal_loss: 0.009084\n",
      "[301/00049] train_loss: 0.008269 kl_loss: 0.000000 normal_loss: 0.008269\n",
      "[303/00049] train_loss: 0.008235 kl_loss: 0.000000 normal_loss: 0.008235\n",
      "[305/00049] train_loss: 0.007957 kl_loss: 0.000000 normal_loss: 0.007957\n",
      "[307/00049] train_loss: 0.007999 kl_loss: 0.000000 normal_loss: 0.007999\n",
      "[309/00049] train_loss: 0.008008 kl_loss: 0.000000 normal_loss: 0.008008\n",
      "[311/00049] train_loss: 0.007940 kl_loss: 0.000000 normal_loss: 0.007940\n",
      "[313/00049] train_loss: 0.007987 kl_loss: 0.000000 normal_loss: 0.007987\n",
      "[315/00049] train_loss: 0.007912 kl_loss: 0.000000 normal_loss: 0.007912\n",
      "[317/00049] train_loss: 0.007917 kl_loss: 0.000000 normal_loss: 0.007917\n",
      "[319/00049] train_loss: 0.008026 kl_loss: 0.000000 normal_loss: 0.008026\n",
      "[321/00049] train_loss: 0.007986 kl_loss: 0.000000 normal_loss: 0.007986\n",
      "[323/00049] train_loss: 0.008022 kl_loss: 0.000000 normal_loss: 0.008022\n",
      "[325/00049] train_loss: 0.007902 kl_loss: 0.000000 normal_loss: 0.007902\n",
      "[327/00049] train_loss: 0.007901 kl_loss: 0.000000 normal_loss: 0.007901\n",
      "[329/00049] train_loss: 0.007892 kl_loss: 0.000000 normal_loss: 0.007892\n",
      "[331/00049] train_loss: 0.007906 kl_loss: 0.000000 normal_loss: 0.007906\n",
      "[333/00049] train_loss: 0.007951 kl_loss: 0.000000 normal_loss: 0.007951\n",
      "[335/00049] train_loss: 0.007999 kl_loss: 0.000000 normal_loss: 0.007999\n",
      "[337/00049] train_loss: 0.007971 kl_loss: 0.000000 normal_loss: 0.007971\n",
      "[339/00049] train_loss: 0.007929 kl_loss: 0.000000 normal_loss: 0.007929\n",
      "[341/00049] train_loss: 0.007864 kl_loss: 0.000000 normal_loss: 0.007864\n",
      "[343/00049] train_loss: 0.007930 kl_loss: 0.000000 normal_loss: 0.007930\n",
      "[345/00049] train_loss: 0.007838 kl_loss: 0.000000 normal_loss: 0.007838\n",
      "[347/00049] train_loss: 0.007828 kl_loss: 0.000000 normal_loss: 0.007828\n",
      "[349/00049] train_loss: 0.007752 kl_loss: 0.000000 normal_loss: 0.007752\n",
      "[351/00049] train_loss: 0.007833 kl_loss: 0.000000 normal_loss: 0.007833\n",
      "[353/00049] train_loss: 0.007786 kl_loss: 0.000000 normal_loss: 0.007786\n",
      "[355/00049] train_loss: 0.007723 kl_loss: 0.000000 normal_loss: 0.007723\n",
      "[357/00049] train_loss: 0.007965 kl_loss: 0.000000 normal_loss: 0.007965\n",
      "[359/00049] train_loss: 0.007852 kl_loss: 0.000000 normal_loss: 0.007852\n",
      "[361/00049] train_loss: 0.007754 kl_loss: 0.000000 normal_loss: 0.007754\n",
      "[363/00049] train_loss: 0.007941 kl_loss: 0.000000 normal_loss: 0.007941\n",
      "[365/00049] train_loss: 0.007849 kl_loss: 0.000000 normal_loss: 0.007849\n",
      "[367/00049] train_loss: 0.007733 kl_loss: 0.000000 normal_loss: 0.007733\n",
      "[369/00049] train_loss: 0.007711 kl_loss: 0.000000 normal_loss: 0.007711\n",
      "[371/00049] train_loss: 0.007828 kl_loss: 0.000000 normal_loss: 0.007828\n",
      "[373/00049] train_loss: 0.007737 kl_loss: 0.000000 normal_loss: 0.007737\n",
      "[375/00049] train_loss: 0.007653 kl_loss: 0.000000 normal_loss: 0.007653\n",
      "[377/00049] train_loss: 0.007701 kl_loss: 0.000000 normal_loss: 0.007701\n",
      "[379/00049] train_loss: 0.007686 kl_loss: 0.000000 normal_loss: 0.007686\n",
      "[381/00049] train_loss: 0.007610 kl_loss: 0.000000 normal_loss: 0.007610\n",
      "[383/00049] train_loss: 0.007665 kl_loss: 0.000000 normal_loss: 0.007665\n",
      "[385/00049] train_loss: 0.007645 kl_loss: 0.000000 normal_loss: 0.007645\n",
      "[387/00049] train_loss: 0.007624 kl_loss: 0.000000 normal_loss: 0.007624\n",
      "[389/00049] train_loss: 0.007573 kl_loss: 0.000000 normal_loss: 0.007573\n",
      "[391/00049] train_loss: 0.007549 kl_loss: 0.000000 normal_loss: 0.007549\n",
      "[393/00049] train_loss: 0.007596 kl_loss: 0.000000 normal_loss: 0.007596\n",
      "[395/00049] train_loss: 0.007568 kl_loss: 0.000000 normal_loss: 0.007568\n",
      "[397/00049] train_loss: 0.007679 kl_loss: 0.000000 normal_loss: 0.007679\n",
      "[399/00049] train_loss: 0.007571 kl_loss: 0.000000 normal_loss: 0.007571\n",
      "[401/00049] train_loss: 0.007387 kl_loss: 0.000000 normal_loss: 0.007387\n",
      "[403/00049] train_loss: 0.007264 kl_loss: 0.000000 normal_loss: 0.007264\n",
      "[405/00049] train_loss: 0.007265 kl_loss: 0.000000 normal_loss: 0.007265\n",
      "[407/00049] train_loss: 0.007248 kl_loss: 0.000000 normal_loss: 0.007248\n",
      "[409/00049] train_loss: 0.007270 kl_loss: 0.000000 normal_loss: 0.007270\n",
      "[411/00049] train_loss: 0.007240 kl_loss: 0.000000 normal_loss: 0.007240\n",
      "[413/00049] train_loss: 0.007286 kl_loss: 0.000000 normal_loss: 0.007286\n",
      "[415/00049] train_loss: 0.007280 kl_loss: 0.000000 normal_loss: 0.007280\n",
      "[417/00049] train_loss: 0.007265 kl_loss: 0.000000 normal_loss: 0.007265\n",
      "[419/00049] train_loss: 0.007276 kl_loss: 0.000000 normal_loss: 0.007276\n",
      "[421/00049] train_loss: 0.007284 kl_loss: 0.000000 normal_loss: 0.007284\n",
      "[423/00049] train_loss: 0.007252 kl_loss: 0.000000 normal_loss: 0.007252\n",
      "[425/00049] train_loss: 0.007216 kl_loss: 0.000000 normal_loss: 0.007216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msofa_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOFA AD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 5949\n",
      "Training params: 3\n",
      "[001/00006] train_loss: 0.128625 kl_loss: 0.495828 normal_loss: 0.113750\n",
      "[002/00013] train_loss: 0.064590 kl_loss: 0.449093 normal_loss: 0.051117\n",
      "[003/00020] train_loss: 0.054780 kl_loss: 0.394209 normal_loss: 0.042954\n",
      "[004/00027] train_loss: 0.052251 kl_loss: 0.344885 normal_loss: 0.041905\n",
      "[005/00034] train_loss: 0.047728 kl_loss: 0.301620 normal_loss: 0.038679\n",
      "[006/00041] train_loss: 0.046487 kl_loss: 0.263708 normal_loss: 0.038576\n",
      "[007/00048] train_loss: 0.047152 kl_loss: 0.231018 normal_loss: 0.040222\n",
      "[008/00055] train_loss: 0.045063 kl_loss: 0.204679 normal_loss: 0.038922\n",
      "[009/00062] train_loss: 0.043648 kl_loss: 0.182405 normal_loss: 0.038176\n",
      "[010/00069] train_loss: 0.042102 kl_loss: 0.166742 normal_loss: 0.037100\n",
      "[011/00076] train_loss: 0.040511 kl_loss: 0.154277 normal_loss: 0.035882\n",
      "[012/00083] train_loss: 0.039628 kl_loss: 0.144000 normal_loss: 0.035308\n",
      "[013/00090] train_loss: 0.038446 kl_loss: 0.132178 normal_loss: 0.034480\n",
      "[015/00004] train_loss: 0.036895 kl_loss: 0.120673 normal_loss: 0.033274\n",
      "[016/00011] train_loss: 0.036078 kl_loss: 0.110031 normal_loss: 0.032777\n",
      "[017/00018] train_loss: 0.035701 kl_loss: 0.100870 normal_loss: 0.032675\n",
      "[018/00025] train_loss: 0.035575 kl_loss: 0.094324 normal_loss: 0.032745\n",
      "[019/00032] train_loss: 0.034990 kl_loss: 0.088328 normal_loss: 0.032340\n",
      "[020/00039] train_loss: 0.033722 kl_loss: 0.082920 normal_loss: 0.031234\n",
      "[021/00046] train_loss: 0.033521 kl_loss: 0.077984 normal_loss: 0.031182\n",
      "[022/00053] train_loss: 0.033104 kl_loss: 0.075035 normal_loss: 0.030853\n",
      "[023/00060] train_loss: 0.032935 kl_loss: 0.072458 normal_loss: 0.030761\n",
      "[024/00067] train_loss: 0.032163 kl_loss: 0.072182 normal_loss: 0.029998\n",
      "[025/00074] train_loss: 0.032322 kl_loss: 0.071417 normal_loss: 0.030180\n",
      "[026/00081] train_loss: 0.031696 kl_loss: 0.072819 normal_loss: 0.029511\n",
      "[027/00088] train_loss: 0.031074 kl_loss: 0.074549 normal_loss: 0.028837\n",
      "[029/00002] train_loss: 0.030719 kl_loss: 0.075989 normal_loss: 0.028440\n",
      "[030/00009] train_loss: 0.030417 kl_loss: 0.077656 normal_loss: 0.028088\n",
      "[031/00016] train_loss: 0.029266 kl_loss: 0.078521 normal_loss: 0.026910\n",
      "[032/00023] train_loss: 0.028788 kl_loss: 0.080542 normal_loss: 0.026371\n",
      "[033/00030] train_loss: 0.029200 kl_loss: 0.081324 normal_loss: 0.026760\n",
      "[034/00037] train_loss: 0.028644 kl_loss: 0.083184 normal_loss: 0.026149\n",
      "[035/00044] train_loss: 0.028165 kl_loss: 0.084940 normal_loss: 0.025617\n",
      "[036/00051] train_loss: 0.027509 kl_loss: 0.086187 normal_loss: 0.024923\n",
      "[037/00058] train_loss: 0.027920 kl_loss: 0.086951 normal_loss: 0.025311\n",
      "[038/00065] train_loss: 0.027506 kl_loss: 0.088042 normal_loss: 0.024864\n",
      "[039/00072] train_loss: 0.027244 kl_loss: 0.088915 normal_loss: 0.024577\n",
      "[040/00079] train_loss: 0.027026 kl_loss: 0.088916 normal_loss: 0.024359\n",
      "[041/00086] train_loss: 0.026795 kl_loss: 0.089382 normal_loss: 0.024114\n",
      "[043/00000] train_loss: 0.026685 kl_loss: 0.089658 normal_loss: 0.023995\n",
      "[044/00007] train_loss: 0.026357 kl_loss: 0.089757 normal_loss: 0.023665\n",
      "[045/00014] train_loss: 0.025959 kl_loss: 0.089980 normal_loss: 0.023260\n",
      "[046/00021] train_loss: 0.025682 kl_loss: 0.090187 normal_loss: 0.022976\n",
      "[047/00028] train_loss: 0.026143 kl_loss: 0.090059 normal_loss: 0.023441\n",
      "[048/00035] train_loss: 0.026151 kl_loss: 0.090587 normal_loss: 0.023433\n",
      "[049/00042] train_loss: 0.025765 kl_loss: 0.090969 normal_loss: 0.023036\n",
      "[050/00049] train_loss: 0.025819 kl_loss: 0.090858 normal_loss: 0.023093\n",
      "[051/00056] train_loss: 0.025508 kl_loss: 0.091612 normal_loss: 0.022759\n",
      "[052/00063] train_loss: 0.025656 kl_loss: 0.091177 normal_loss: 0.022921\n",
      "[053/00070] train_loss: 0.025308 kl_loss: 0.091721 normal_loss: 0.022556\n",
      "[054/00077] train_loss: 0.024945 kl_loss: 0.091914 normal_loss: 0.022187\n",
      "[055/00084] train_loss: 0.025335 kl_loss: 0.091259 normal_loss: 0.022598\n",
      "[056/00091] train_loss: 0.025017 kl_loss: 0.091614 normal_loss: 0.022268\n",
      "[058/00005] train_loss: 0.024981 kl_loss: 0.092123 normal_loss: 0.022218\n",
      "[059/00012] train_loss: 0.025022 kl_loss: 0.091383 normal_loss: 0.022280\n",
      "[060/00019] train_loss: 0.025013 kl_loss: 0.093131 normal_loss: 0.022219\n",
      "[061/00026] train_loss: 0.024847 kl_loss: 0.093484 normal_loss: 0.022042\n",
      "[062/00033] train_loss: 0.024228 kl_loss: 0.093882 normal_loss: 0.021411\n",
      "[063/00040] train_loss: 0.024153 kl_loss: 0.093920 normal_loss: 0.021336\n",
      "[064/00047] train_loss: 0.024689 kl_loss: 0.094616 normal_loss: 0.021851\n",
      "[065/00054] train_loss: 0.024423 kl_loss: 0.094286 normal_loss: 0.021594\n",
      "[066/00061] train_loss: 0.024136 kl_loss: 0.095359 normal_loss: 0.021275\n",
      "[067/00068] train_loss: 0.024148 kl_loss: 0.095360 normal_loss: 0.021287\n",
      "[068/00075] train_loss: 0.024369 kl_loss: 0.095719 normal_loss: 0.021497\n",
      "[069/00082] train_loss: 0.024199 kl_loss: 0.095035 normal_loss: 0.021348\n",
      "[070/00089] train_loss: 0.023930 kl_loss: 0.096099 normal_loss: 0.021047\n",
      "[072/00003] train_loss: 0.024171 kl_loss: 0.096825 normal_loss: 0.021267\n",
      "[073/00010] train_loss: 0.023992 kl_loss: 0.096863 normal_loss: 0.021086\n",
      "[074/00017] train_loss: 0.023779 kl_loss: 0.097156 normal_loss: 0.020864\n",
      "[075/00024] train_loss: 0.023705 kl_loss: 0.097717 normal_loss: 0.020774\n",
      "[076/00031] train_loss: 0.023777 kl_loss: 0.097246 normal_loss: 0.020859\n",
      "[077/00038] train_loss: 0.023831 kl_loss: 0.099055 normal_loss: 0.020860\n",
      "[078/00045] train_loss: 0.023121 kl_loss: 0.098549 normal_loss: 0.020164\n",
      "[079/00052] train_loss: 0.023595 kl_loss: 0.099629 normal_loss: 0.020606\n",
      "[080/00059] train_loss: 0.023312 kl_loss: 0.098783 normal_loss: 0.020348\n",
      "[081/00066] train_loss: 0.023491 kl_loss: 0.100003 normal_loss: 0.020491\n",
      "[082/00073] train_loss: 0.023496 kl_loss: 0.100462 normal_loss: 0.020482\n",
      "[083/00080] train_loss: 0.022990 kl_loss: 0.099807 normal_loss: 0.019996\n",
      "[084/00087] train_loss: 0.023220 kl_loss: 0.100818 normal_loss: 0.020195\n",
      "[086/00001] train_loss: 0.023300 kl_loss: 0.100756 normal_loss: 0.020278\n",
      "[087/00008] train_loss: 0.023220 kl_loss: 0.101267 normal_loss: 0.020182\n",
      "[088/00015] train_loss: 0.023204 kl_loss: 0.101253 normal_loss: 0.020166\n",
      "[089/00022] train_loss: 0.023198 kl_loss: 0.102525 normal_loss: 0.020123\n",
      "[090/00029] train_loss: 0.022957 kl_loss: 0.102089 normal_loss: 0.019894\n",
      "[091/00036] train_loss: 0.022579 kl_loss: 0.102619 normal_loss: 0.019500\n",
      "[092/00043] train_loss: 0.023054 kl_loss: 0.102650 normal_loss: 0.019975\n",
      "[093/00050] train_loss: 0.022775 kl_loss: 0.101984 normal_loss: 0.019716\n",
      "[094/00057] train_loss: 0.022909 kl_loss: 0.102694 normal_loss: 0.019828\n",
      "[095/00064] train_loss: 0.022556 kl_loss: 0.102349 normal_loss: 0.019485\n",
      "[096/00071] train_loss: 0.022755 kl_loss: 0.102963 normal_loss: 0.019666\n",
      "[097/00078] train_loss: 0.022624 kl_loss: 0.103319 normal_loss: 0.019525\n",
      "[098/00085] train_loss: 0.022221 kl_loss: 0.103441 normal_loss: 0.019118\n",
      "[099/00092] train_loss: 0.022315 kl_loss: 0.102821 normal_loss: 0.019230\n",
      "[101/00006] train_loss: 0.020332 kl_loss: 0.102579 normal_loss: 0.017255\n",
      "[102/00013] train_loss: 0.020097 kl_loss: 0.100405 normal_loss: 0.017085\n",
      "[103/00020] train_loss: 0.019910 kl_loss: 0.098218 normal_loss: 0.016964\n",
      "[104/00027] train_loss: 0.019730 kl_loss: 0.097397 normal_loss: 0.016808\n",
      "[105/00034] train_loss: 0.019792 kl_loss: 0.094457 normal_loss: 0.016959\n",
      "[106/00041] train_loss: 0.019803 kl_loss: 0.094312 normal_loss: 0.016973\n",
      "[107/00048] train_loss: 0.019705 kl_loss: 0.093286 normal_loss: 0.016907\n",
      "[108/00055] train_loss: 0.019878 kl_loss: 0.092030 normal_loss: 0.017118\n",
      "[109/00062] train_loss: 0.019856 kl_loss: 0.091508 normal_loss: 0.017111\n",
      "[110/00069] train_loss: 0.019760 kl_loss: 0.090599 normal_loss: 0.017042\n",
      "[111/00076] train_loss: 0.019661 kl_loss: 0.090141 normal_loss: 0.016957\n",
      "[112/00083] train_loss: 0.019561 kl_loss: 0.090017 normal_loss: 0.016860\n",
      "[113/00090] train_loss: 0.019474 kl_loss: 0.089019 normal_loss: 0.016803\n",
      "[115/00004] train_loss: 0.019524 kl_loss: 0.089011 normal_loss: 0.016854\n",
      "[116/00011] train_loss: 0.019607 kl_loss: 0.088775 normal_loss: 0.016944\n",
      "[117/00018] train_loss: 0.019258 kl_loss: 0.088267 normal_loss: 0.016610\n",
      "[118/00025] train_loss: 0.019506 kl_loss: 0.088587 normal_loss: 0.016848\n",
      "[119/00032] train_loss: 0.019543 kl_loss: 0.088350 normal_loss: 0.016893\n",
      "[120/00039] train_loss: 0.019604 kl_loss: 0.087843 normal_loss: 0.016969\n",
      "[121/00046] train_loss: 0.019552 kl_loss: 0.088541 normal_loss: 0.016896\n",
      "[122/00053] train_loss: 0.019086 kl_loss: 0.087972 normal_loss: 0.016447\n",
      "[123/00060] train_loss: 0.019266 kl_loss: 0.087836 normal_loss: 0.016631\n",
      "[124/00067] train_loss: 0.019389 kl_loss: 0.088358 normal_loss: 0.016738\n",
      "[125/00074] train_loss: 0.019317 kl_loss: 0.087928 normal_loss: 0.016679\n",
      "[126/00081] train_loss: 0.019503 kl_loss: 0.088552 normal_loss: 0.016846\n",
      "[127/00088] train_loss: 0.019438 kl_loss: 0.087987 normal_loss: 0.016798\n",
      "[129/00002] train_loss: 0.018953 kl_loss: 0.088258 normal_loss: 0.016306\n",
      "[130/00009] train_loss: 0.019362 kl_loss: 0.088344 normal_loss: 0.016712\n",
      "[131/00016] train_loss: 0.019235 kl_loss: 0.088569 normal_loss: 0.016578\n",
      "[132/00023] train_loss: 0.019292 kl_loss: 0.087961 normal_loss: 0.016653\n",
      "[133/00030] train_loss: 0.019050 kl_loss: 0.088717 normal_loss: 0.016388\n",
      "[134/00037] train_loss: 0.019228 kl_loss: 0.088400 normal_loss: 0.016576\n",
      "[135/00044] train_loss: 0.019119 kl_loss: 0.089058 normal_loss: 0.016447\n",
      "[136/00051] train_loss: 0.019151 kl_loss: 0.088179 normal_loss: 0.016506\n",
      "[137/00058] train_loss: 0.019228 kl_loss: 0.088524 normal_loss: 0.016572\n",
      "[138/00065] train_loss: 0.018993 kl_loss: 0.088180 normal_loss: 0.016347\n",
      "[139/00072] train_loss: 0.019123 kl_loss: 0.089276 normal_loss: 0.016445\n",
      "[140/00079] train_loss: 0.019256 kl_loss: 0.089082 normal_loss: 0.016583\n",
      "[141/00086] train_loss: 0.019136 kl_loss: 0.088978 normal_loss: 0.016466\n",
      "[143/00000] train_loss: 0.019067 kl_loss: 0.089109 normal_loss: 0.016394\n",
      "[144/00007] train_loss: 0.018849 kl_loss: 0.088965 normal_loss: 0.016180\n",
      "[145/00014] train_loss: 0.018940 kl_loss: 0.088951 normal_loss: 0.016271\n",
      "[146/00021] train_loss: 0.018811 kl_loss: 0.089417 normal_loss: 0.016129\n",
      "[147/00028] train_loss: 0.018813 kl_loss: 0.089152 normal_loss: 0.016138\n",
      "[148/00035] train_loss: 0.019011 kl_loss: 0.089260 normal_loss: 0.016334\n",
      "[149/00042] train_loss: 0.018875 kl_loss: 0.090258 normal_loss: 0.016167\n",
      "[150/00049] train_loss: 0.018762 kl_loss: 0.088819 normal_loss: 0.016098\n",
      "[151/00056] train_loss: 0.018776 kl_loss: 0.089809 normal_loss: 0.016081\n",
      "[152/00063] train_loss: 0.018740 kl_loss: 0.089739 normal_loss: 0.016048\n",
      "[153/00070] train_loss: 0.018971 kl_loss: 0.089680 normal_loss: 0.016280\n",
      "[154/00077] train_loss: 0.018998 kl_loss: 0.090385 normal_loss: 0.016287\n",
      "[155/00084] train_loss: 0.018760 kl_loss: 0.089683 normal_loss: 0.016069\n",
      "[156/00091] train_loss: 0.018892 kl_loss: 0.089989 normal_loss: 0.016192\n",
      "[158/00005] train_loss: 0.018650 kl_loss: 0.090188 normal_loss: 0.015944\n",
      "[159/00012] train_loss: 0.018460 kl_loss: 0.090183 normal_loss: 0.015754\n",
      "[160/00019] train_loss: 0.018687 kl_loss: 0.090378 normal_loss: 0.015975\n",
      "[161/00026] train_loss: 0.018637 kl_loss: 0.090740 normal_loss: 0.015915\n",
      "[162/00033] train_loss: 0.018704 kl_loss: 0.090753 normal_loss: 0.015982\n",
      "[163/00040] train_loss: 0.018636 kl_loss: 0.089929 normal_loss: 0.015938\n",
      "[164/00047] train_loss: 0.018696 kl_loss: 0.090683 normal_loss: 0.015976\n",
      "[165/00054] train_loss: 0.018478 kl_loss: 0.090502 normal_loss: 0.015763\n",
      "[166/00061] train_loss: 0.018632 kl_loss: 0.090658 normal_loss: 0.015912\n",
      "[167/00068] train_loss: 0.018582 kl_loss: 0.090795 normal_loss: 0.015858\n",
      "[168/00075] train_loss: 0.018304 kl_loss: 0.090095 normal_loss: 0.015601\n",
      "[169/00082] train_loss: 0.018715 kl_loss: 0.090882 normal_loss: 0.015988\n",
      "[170/00089] train_loss: 0.018397 kl_loss: 0.090589 normal_loss: 0.015679\n",
      "[172/00003] train_loss: 0.018244 kl_loss: 0.090699 normal_loss: 0.015523\n",
      "[173/00010] train_loss: 0.018544 kl_loss: 0.090518 normal_loss: 0.015828\n",
      "[174/00017] train_loss: 0.018572 kl_loss: 0.091643 normal_loss: 0.015823\n",
      "[175/00024] train_loss: 0.018361 kl_loss: 0.090815 normal_loss: 0.015637\n",
      "[176/00031] train_loss: 0.018341 kl_loss: 0.090622 normal_loss: 0.015623\n",
      "[177/00038] train_loss: 0.018199 kl_loss: 0.091202 normal_loss: 0.015463\n",
      "[178/00045] train_loss: 0.017980 kl_loss: 0.091383 normal_loss: 0.015238\n",
      "[179/00052] train_loss: 0.018520 kl_loss: 0.090978 normal_loss: 0.015791\n",
      "[180/00059] train_loss: 0.018275 kl_loss: 0.091160 normal_loss: 0.015540\n",
      "[181/00066] train_loss: 0.018408 kl_loss: 0.091515 normal_loss: 0.015663\n",
      "[182/00073] train_loss: 0.018436 kl_loss: 0.091435 normal_loss: 0.015693\n",
      "[183/00080] train_loss: 0.018323 kl_loss: 0.091787 normal_loss: 0.015570\n",
      "[184/00087] train_loss: 0.017958 kl_loss: 0.091220 normal_loss: 0.015222\n",
      "[186/00001] train_loss: 0.018188 kl_loss: 0.091339 normal_loss: 0.015448\n",
      "[187/00008] train_loss: 0.018004 kl_loss: 0.091497 normal_loss: 0.015259\n",
      "[188/00015] train_loss: 0.018103 kl_loss: 0.091294 normal_loss: 0.015364\n",
      "[189/00022] train_loss: 0.017942 kl_loss: 0.090940 normal_loss: 0.015214\n",
      "[190/00029] train_loss: 0.018047 kl_loss: 0.091917 normal_loss: 0.015289\n",
      "[191/00036] train_loss: 0.018241 kl_loss: 0.091765 normal_loss: 0.015488\n",
      "[192/00043] train_loss: 0.018079 kl_loss: 0.090532 normal_loss: 0.015363\n",
      "[193/00050] train_loss: 0.018159 kl_loss: 0.092111 normal_loss: 0.015396\n",
      "[194/00057] train_loss: 0.018174 kl_loss: 0.091522 normal_loss: 0.015428\n",
      "[195/00064] train_loss: 0.017974 kl_loss: 0.091319 normal_loss: 0.015235\n",
      "[196/00071] train_loss: 0.018167 kl_loss: 0.092055 normal_loss: 0.015406\n",
      "[197/00078] train_loss: 0.017843 kl_loss: 0.092055 normal_loss: 0.015082\n",
      "[198/00085] train_loss: 0.018042 kl_loss: 0.091824 normal_loss: 0.015288\n",
      "[199/00092] train_loss: 0.017996 kl_loss: 0.091916 normal_loss: 0.015238\n",
      "[201/00006] train_loss: 0.016953 kl_loss: 0.092109 normal_loss: 0.014190\n",
      "[202/00013] train_loss: 0.016910 kl_loss: 0.091548 normal_loss: 0.014163\n",
      "[203/00020] train_loss: 0.016782 kl_loss: 0.090875 normal_loss: 0.014056\n",
      "[204/00027] train_loss: 0.016940 kl_loss: 0.090739 normal_loss: 0.014218\n",
      "[205/00034] train_loss: 0.016616 kl_loss: 0.089434 normal_loss: 0.013933\n",
      "[206/00041] train_loss: 0.016807 kl_loss: 0.089913 normal_loss: 0.014110\n",
      "[207/00048] train_loss: 0.016741 kl_loss: 0.089572 normal_loss: 0.014054\n",
      "[208/00055] train_loss: 0.016735 kl_loss: 0.089111 normal_loss: 0.014062\n",
      "[209/00062] train_loss: 0.016748 kl_loss: 0.088476 normal_loss: 0.014094\n",
      "[210/00069] train_loss: 0.016615 kl_loss: 0.088213 normal_loss: 0.013969\n",
      "[211/00076] train_loss: 0.016621 kl_loss: 0.088246 normal_loss: 0.013974\n",
      "[212/00083] train_loss: 0.016658 kl_loss: 0.087793 normal_loss: 0.014024\n",
      "[213/00090] train_loss: 0.016705 kl_loss: 0.087642 normal_loss: 0.014076\n",
      "[215/00004] train_loss: 0.016566 kl_loss: 0.087196 normal_loss: 0.013950\n",
      "[216/00011] train_loss: 0.016594 kl_loss: 0.087223 normal_loss: 0.013977\n",
      "[217/00018] train_loss: 0.016620 kl_loss: 0.087224 normal_loss: 0.014003\n",
      "[218/00025] train_loss: 0.016463 kl_loss: 0.086608 normal_loss: 0.013865\n",
      "[219/00032] train_loss: 0.016668 kl_loss: 0.086771 normal_loss: 0.014065\n",
      "[220/00039] train_loss: 0.016480 kl_loss: 0.086754 normal_loss: 0.013878\n",
      "[221/00046] train_loss: 0.016529 kl_loss: 0.086735 normal_loss: 0.013927\n",
      "[222/00053] train_loss: 0.016552 kl_loss: 0.086440 normal_loss: 0.013959\n",
      "[223/00060] train_loss: 0.016575 kl_loss: 0.086025 normal_loss: 0.013995\n",
      "[224/00067] train_loss: 0.016507 kl_loss: 0.086655 normal_loss: 0.013907\n",
      "[225/00074] train_loss: 0.016578 kl_loss: 0.086314 normal_loss: 0.013989\n",
      "[226/00081] train_loss: 0.016393 kl_loss: 0.086013 normal_loss: 0.013813\n",
      "[227/00088] train_loss: 0.016406 kl_loss: 0.085813 normal_loss: 0.013832\n",
      "[229/00002] train_loss: 0.016406 kl_loss: 0.086069 normal_loss: 0.013824\n",
      "[230/00009] train_loss: 0.016360 kl_loss: 0.085915 normal_loss: 0.013783\n",
      "[231/00016] train_loss: 0.016335 kl_loss: 0.086049 normal_loss: 0.013753\n",
      "[232/00023] train_loss: 0.016273 kl_loss: 0.085794 normal_loss: 0.013699\n",
      "[233/00030] train_loss: 0.016394 kl_loss: 0.085904 normal_loss: 0.013816\n",
      "[234/00037] train_loss: 0.016349 kl_loss: 0.085497 normal_loss: 0.013784\n",
      "[235/00044] train_loss: 0.016321 kl_loss: 0.085546 normal_loss: 0.013755\n",
      "[236/00051] train_loss: 0.016446 kl_loss: 0.085452 normal_loss: 0.013882\n",
      "[237/00058] train_loss: 0.016421 kl_loss: 0.085918 normal_loss: 0.013843\n",
      "[238/00065] train_loss: 0.016317 kl_loss: 0.085637 normal_loss: 0.013748\n",
      "[239/00072] train_loss: 0.016364 kl_loss: 0.085547 normal_loss: 0.013798\n",
      "[240/00079] train_loss: 0.016343 kl_loss: 0.085254 normal_loss: 0.013785\n",
      "[241/00086] train_loss: 0.016196 kl_loss: 0.085967 normal_loss: 0.013617\n",
      "[243/00000] train_loss: 0.016219 kl_loss: 0.085547 normal_loss: 0.013653\n",
      "[244/00007] train_loss: 0.016239 kl_loss: 0.085527 normal_loss: 0.013673\n",
      "[245/00014] train_loss: 0.016310 kl_loss: 0.085750 normal_loss: 0.013737\n",
      "[246/00021] train_loss: 0.016183 kl_loss: 0.084968 normal_loss: 0.013634\n",
      "[247/00028] train_loss: 0.016291 kl_loss: 0.085907 normal_loss: 0.013714\n",
      "[248/00035] train_loss: 0.016165 kl_loss: 0.085597 normal_loss: 0.013597\n",
      "[249/00042] train_loss: 0.016181 kl_loss: 0.085247 normal_loss: 0.013624\n",
      "[250/00049] train_loss: 0.016184 kl_loss: 0.085388 normal_loss: 0.013622\n",
      "[251/00056] train_loss: 0.016192 kl_loss: 0.086236 normal_loss: 0.013605\n",
      "[252/00063] train_loss: 0.016250 kl_loss: 0.085507 normal_loss: 0.013685\n",
      "[253/00070] train_loss: 0.016186 kl_loss: 0.085508 normal_loss: 0.013621\n",
      "[254/00077] train_loss: 0.016213 kl_loss: 0.085714 normal_loss: 0.013642\n",
      "[255/00084] train_loss: 0.016033 kl_loss: 0.085622 normal_loss: 0.013464\n",
      "[256/00091] train_loss: 0.016104 kl_loss: 0.085809 normal_loss: 0.013529\n",
      "[258/00005] train_loss: 0.016129 kl_loss: 0.085575 normal_loss: 0.013562\n",
      "[259/00012] train_loss: 0.016084 kl_loss: 0.085643 normal_loss: 0.013514\n",
      "[260/00019] train_loss: 0.016196 kl_loss: 0.086012 normal_loss: 0.013616\n",
      "[261/00026] train_loss: 0.016118 kl_loss: 0.085362 normal_loss: 0.013558\n",
      "[262/00033] train_loss: 0.015907 kl_loss: 0.085397 normal_loss: 0.013345\n",
      "[263/00040] train_loss: 0.015959 kl_loss: 0.085606 normal_loss: 0.013391\n",
      "[264/00047] train_loss: 0.016084 kl_loss: 0.085874 normal_loss: 0.013508\n",
      "[265/00054] train_loss: 0.016066 kl_loss: 0.085439 normal_loss: 0.013503\n",
      "[266/00061] train_loss: 0.015914 kl_loss: 0.086153 normal_loss: 0.013329\n",
      "[267/00068] train_loss: 0.016098 kl_loss: 0.085749 normal_loss: 0.013526\n",
      "[268/00075] train_loss: 0.016083 kl_loss: 0.085828 normal_loss: 0.013508\n",
      "[269/00082] train_loss: 0.016023 kl_loss: 0.085357 normal_loss: 0.013462\n",
      "[270/00089] train_loss: 0.016075 kl_loss: 0.085728 normal_loss: 0.013504\n",
      "[272/00003] train_loss: 0.016059 kl_loss: 0.085905 normal_loss: 0.013482\n",
      "[273/00010] train_loss: 0.015843 kl_loss: 0.085823 normal_loss: 0.013268\n",
      "[274/00017] train_loss: 0.015972 kl_loss: 0.085843 normal_loss: 0.013397\n",
      "[275/00024] train_loss: 0.015778 kl_loss: 0.085788 normal_loss: 0.013204\n",
      "[276/00031] train_loss: 0.015994 kl_loss: 0.085318 normal_loss: 0.013434\n",
      "[277/00038] train_loss: 0.016007 kl_loss: 0.085753 normal_loss: 0.013434\n",
      "[278/00045] train_loss: 0.015996 kl_loss: 0.085997 normal_loss: 0.013416\n",
      "[279/00052] train_loss: 0.015951 kl_loss: 0.086016 normal_loss: 0.013371\n",
      "[280/00059] train_loss: 0.015847 kl_loss: 0.086174 normal_loss: 0.013261\n",
      "[281/00066] train_loss: 0.015856 kl_loss: 0.085151 normal_loss: 0.013301\n",
      "[282/00073] train_loss: 0.015890 kl_loss: 0.085984 normal_loss: 0.013311\n",
      "[283/00080] train_loss: 0.015883 kl_loss: 0.086150 normal_loss: 0.013298\n",
      "[284/00087] train_loss: 0.015698 kl_loss: 0.085996 normal_loss: 0.013118\n",
      "[286/00001] train_loss: 0.015819 kl_loss: 0.085958 normal_loss: 0.013240\n",
      "[287/00008] train_loss: 0.015834 kl_loss: 0.085762 normal_loss: 0.013261\n",
      "[288/00015] train_loss: 0.015909 kl_loss: 0.086526 normal_loss: 0.013313\n",
      "[289/00022] train_loss: 0.015813 kl_loss: 0.085660 normal_loss: 0.013243\n",
      "[290/00029] train_loss: 0.015785 kl_loss: 0.086056 normal_loss: 0.013204\n",
      "[291/00036] train_loss: 0.015729 kl_loss: 0.086052 normal_loss: 0.013147\n",
      "[292/00043] train_loss: 0.015777 kl_loss: 0.085801 normal_loss: 0.013203\n",
      "[293/00050] train_loss: 0.015833 kl_loss: 0.086308 normal_loss: 0.013244\n",
      "[294/00057] train_loss: 0.015686 kl_loss: 0.085645 normal_loss: 0.013117\n",
      "[295/00064] train_loss: 0.015691 kl_loss: 0.086434 normal_loss: 0.013098\n",
      "[296/00071] train_loss: 0.015734 kl_loss: 0.086501 normal_loss: 0.013139\n",
      "[297/00078] train_loss: 0.015697 kl_loss: 0.086129 normal_loss: 0.013114\n",
      "[298/00085] train_loss: 0.015681 kl_loss: 0.086046 normal_loss: 0.013099\n",
      "[299/00092] train_loss: 0.015691 kl_loss: 0.086214 normal_loss: 0.013104\n",
      "[301/00006] train_loss: 0.015221 kl_loss: 0.086312 normal_loss: 0.012631\n",
      "[302/00013] train_loss: 0.015244 kl_loss: 0.085920 normal_loss: 0.012666\n",
      "[303/00020] train_loss: 0.015235 kl_loss: 0.086305 normal_loss: 0.012646\n",
      "[304/00027] train_loss: 0.015161 kl_loss: 0.085413 normal_loss: 0.012599\n",
      "[305/00034] train_loss: 0.015133 kl_loss: 0.085762 normal_loss: 0.012561\n",
      "[306/00041] train_loss: 0.015268 kl_loss: 0.085968 normal_loss: 0.012689\n",
      "[307/00048] train_loss: 0.015094 kl_loss: 0.085640 normal_loss: 0.012525\n",
      "[308/00055] train_loss: 0.015145 kl_loss: 0.085274 normal_loss: 0.012587\n",
      "[309/00062] train_loss: 0.015089 kl_loss: 0.085332 normal_loss: 0.012529\n",
      "[310/00069] train_loss: 0.015134 kl_loss: 0.085204 normal_loss: 0.012578\n",
      "[311/00076] train_loss: 0.015023 kl_loss: 0.085144 normal_loss: 0.012468\n",
      "[312/00083] train_loss: 0.015139 kl_loss: 0.085009 normal_loss: 0.012589\n",
      "[313/00090] train_loss: 0.015093 kl_loss: 0.085028 normal_loss: 0.012542\n",
      "[315/00004] train_loss: 0.015099 kl_loss: 0.085021 normal_loss: 0.012549\n",
      "[316/00011] train_loss: 0.015074 kl_loss: 0.084806 normal_loss: 0.012530\n",
      "[317/00018] train_loss: 0.015017 kl_loss: 0.084414 normal_loss: 0.012485\n",
      "[318/00025] train_loss: 0.015044 kl_loss: 0.084882 normal_loss: 0.012498\n",
      "[319/00032] train_loss: 0.015107 kl_loss: 0.084871 normal_loss: 0.012561\n",
      "[320/00039] train_loss: 0.015012 kl_loss: 0.084696 normal_loss: 0.012471\n",
      "[321/00046] train_loss: 0.015086 kl_loss: 0.084238 normal_loss: 0.012559\n",
      "[322/00053] train_loss: 0.015080 kl_loss: 0.084839 normal_loss: 0.012534\n",
      "[323/00060] train_loss: 0.014901 kl_loss: 0.083803 normal_loss: 0.012387\n",
      "[324/00067] train_loss: 0.014929 kl_loss: 0.084580 normal_loss: 0.012392\n",
      "[325/00074] train_loss: 0.014972 kl_loss: 0.084236 normal_loss: 0.012445\n",
      "[326/00081] train_loss: 0.015081 kl_loss: 0.084328 normal_loss: 0.012552\n",
      "[327/00088] train_loss: 0.014988 kl_loss: 0.084343 normal_loss: 0.012458\n",
      "[329/00002] train_loss: 0.014936 kl_loss: 0.084163 normal_loss: 0.012411\n",
      "[330/00009] train_loss: 0.014990 kl_loss: 0.084214 normal_loss: 0.012463\n",
      "[331/00016] train_loss: 0.014971 kl_loss: 0.084084 normal_loss: 0.012449\n",
      "[332/00023] train_loss: 0.014850 kl_loss: 0.083679 normal_loss: 0.012340\n",
      "[333/00030] train_loss: 0.015008 kl_loss: 0.083869 normal_loss: 0.012492\n",
      "[334/00037] train_loss: 0.014976 kl_loss: 0.084326 normal_loss: 0.012446\n",
      "[335/00044] train_loss: 0.014900 kl_loss: 0.083699 normal_loss: 0.012389\n",
      "[336/00051] train_loss: 0.014953 kl_loss: 0.084313 normal_loss: 0.012423\n",
      "[337/00058] train_loss: 0.014789 kl_loss: 0.083266 normal_loss: 0.012291\n",
      "[338/00065] train_loss: 0.014976 kl_loss: 0.084569 normal_loss: 0.012439\n",
      "[339/00072] train_loss: 0.014859 kl_loss: 0.083659 normal_loss: 0.012349\n",
      "[340/00079] train_loss: 0.014775 kl_loss: 0.083608 normal_loss: 0.012266\n",
      "[341/00086] train_loss: 0.014931 kl_loss: 0.083945 normal_loss: 0.012412\n",
      "[343/00000] train_loss: 0.014955 kl_loss: 0.083669 normal_loss: 0.012444\n",
      "[344/00007] train_loss: 0.014895 kl_loss: 0.083731 normal_loss: 0.012383\n",
      "[345/00014] train_loss: 0.014860 kl_loss: 0.083820 normal_loss: 0.012346\n",
      "[346/00021] train_loss: 0.014758 kl_loss: 0.083442 normal_loss: 0.012255\n",
      "[347/00028] train_loss: 0.014905 kl_loss: 0.083831 normal_loss: 0.012390\n",
      "[348/00035] train_loss: 0.014864 kl_loss: 0.083433 normal_loss: 0.012361\n",
      "[349/00042] train_loss: 0.014908 kl_loss: 0.083942 normal_loss: 0.012389\n",
      "[350/00049] train_loss: 0.014715 kl_loss: 0.083612 normal_loss: 0.012206\n",
      "[351/00056] train_loss: 0.014866 kl_loss: 0.083481 normal_loss: 0.012361\n",
      "[352/00063] train_loss: 0.014828 kl_loss: 0.083782 normal_loss: 0.012315\n",
      "[353/00070] train_loss: 0.014922 kl_loss: 0.084056 normal_loss: 0.012400\n",
      "[354/00077] train_loss: 0.014761 kl_loss: 0.083493 normal_loss: 0.012256\n",
      "[355/00084] train_loss: 0.014703 kl_loss: 0.083596 normal_loss: 0.012195\n",
      "[356/00091] train_loss: 0.014827 kl_loss: 0.083714 normal_loss: 0.012315\n",
      "[358/00005] train_loss: 0.014773 kl_loss: 0.083534 normal_loss: 0.012267\n",
      "[359/00012] train_loss: 0.014768 kl_loss: 0.083588 normal_loss: 0.012260\n",
      "[360/00019] train_loss: 0.014818 kl_loss: 0.083279 normal_loss: 0.012319\n",
      "[361/00026] train_loss: 0.014699 kl_loss: 0.083630 normal_loss: 0.012190\n",
      "[362/00033] train_loss: 0.014770 kl_loss: 0.084012 normal_loss: 0.012250\n",
      "[363/00040] train_loss: 0.014785 kl_loss: 0.083471 normal_loss: 0.012280\n",
      "[364/00047] train_loss: 0.014743 kl_loss: 0.084010 normal_loss: 0.012222\n",
      "[365/00054] train_loss: 0.014740 kl_loss: 0.083098 normal_loss: 0.012247\n",
      "[366/00061] train_loss: 0.014757 kl_loss: 0.084131 normal_loss: 0.012233\n",
      "[367/00068] train_loss: 0.014765 kl_loss: 0.083628 normal_loss: 0.012256\n",
      "[368/00075] train_loss: 0.014658 kl_loss: 0.082848 normal_loss: 0.012172\n",
      "[369/00082] train_loss: 0.014794 kl_loss: 0.083818 normal_loss: 0.012279\n",
      "[370/00089] train_loss: 0.014690 kl_loss: 0.083531 normal_loss: 0.012184\n",
      "[372/00003] train_loss: 0.014738 kl_loss: 0.083583 normal_loss: 0.012231\n",
      "[373/00010] train_loss: 0.014678 kl_loss: 0.083508 normal_loss: 0.012173\n",
      "[374/00017] train_loss: 0.014640 kl_loss: 0.083625 normal_loss: 0.012132\n",
      "[375/00024] train_loss: 0.014638 kl_loss: 0.083739 normal_loss: 0.012126\n",
      "[376/00031] train_loss: 0.014691 kl_loss: 0.083381 normal_loss: 0.012190\n",
      "[377/00038] train_loss: 0.014653 kl_loss: 0.083683 normal_loss: 0.012142\n",
      "[378/00045] train_loss: 0.014569 kl_loss: 0.083454 normal_loss: 0.012066\n",
      "[379/00052] train_loss: 0.014711 kl_loss: 0.083522 normal_loss: 0.012205\n",
      "[380/00059] train_loss: 0.014740 kl_loss: 0.083498 normal_loss: 0.012235\n",
      "[381/00066] train_loss: 0.014635 kl_loss: 0.083713 normal_loss: 0.012124\n",
      "[382/00073] train_loss: 0.014585 kl_loss: 0.083603 normal_loss: 0.012077\n",
      "[383/00080] train_loss: 0.014601 kl_loss: 0.083624 normal_loss: 0.012092\n",
      "[384/00087] train_loss: 0.014581 kl_loss: 0.083110 normal_loss: 0.012088\n",
      "[386/00001] train_loss: 0.014658 kl_loss: 0.083818 normal_loss: 0.012143\n",
      "[387/00008] train_loss: 0.014596 kl_loss: 0.083594 normal_loss: 0.012088\n",
      "[388/00015] train_loss: 0.014572 kl_loss: 0.083509 normal_loss: 0.012066\n",
      "[389/00022] train_loss: 0.014685 kl_loss: 0.083748 normal_loss: 0.012172\n",
      "[390/00029] train_loss: 0.014526 kl_loss: 0.083193 normal_loss: 0.012030\n",
      "[391/00036] train_loss: 0.014582 kl_loss: 0.083603 normal_loss: 0.012074\n",
      "[392/00043] train_loss: 0.014645 kl_loss: 0.083915 normal_loss: 0.012127\n",
      "[393/00050] train_loss: 0.014446 kl_loss: 0.083048 normal_loss: 0.011955\n",
      "[394/00057] train_loss: 0.014594 kl_loss: 0.083927 normal_loss: 0.012076\n",
      "[395/00064] train_loss: 0.014655 kl_loss: 0.084067 normal_loss: 0.012133\n",
      "[396/00071] train_loss: 0.014510 kl_loss: 0.083077 normal_loss: 0.012018\n",
      "[397/00078] train_loss: 0.014532 kl_loss: 0.083821 normal_loss: 0.012017\n",
      "[398/00085] train_loss: 0.014547 kl_loss: 0.083368 normal_loss: 0.012046\n",
      "[399/00092] train_loss: 0.014598 kl_loss: 0.083644 normal_loss: 0.012088\n",
      "[401/00006] train_loss: 0.014354 kl_loss: 0.083597 normal_loss: 0.011846\n",
      "[402/00013] train_loss: 0.014379 kl_loss: 0.083625 normal_loss: 0.011870\n",
      "[403/00020] train_loss: 0.014300 kl_loss: 0.083143 normal_loss: 0.011806\n",
      "[404/00027] train_loss: 0.014326 kl_loss: 0.083933 normal_loss: 0.011808\n",
      "[405/00034] train_loss: 0.014338 kl_loss: 0.083399 normal_loss: 0.011836\n",
      "[406/00041] train_loss: 0.014322 kl_loss: 0.083316 normal_loss: 0.011822\n",
      "[407/00048] train_loss: 0.014339 kl_loss: 0.083793 normal_loss: 0.011825\n",
      "[408/00055] train_loss: 0.014283 kl_loss: 0.083049 normal_loss: 0.011792\n",
      "[409/00062] train_loss: 0.014355 kl_loss: 0.083412 normal_loss: 0.011853\n",
      "[410/00069] train_loss: 0.014250 kl_loss: 0.083061 normal_loss: 0.011758\n",
      "[411/00076] train_loss: 0.014342 kl_loss: 0.083309 normal_loss: 0.011842\n",
      "[412/00083] train_loss: 0.014273 kl_loss: 0.083141 normal_loss: 0.011778\n",
      "[413/00090] train_loss: 0.014277 kl_loss: 0.083318 normal_loss: 0.011777\n",
      "[415/00004] train_loss: 0.014269 kl_loss: 0.083342 normal_loss: 0.011769\n",
      "[416/00011] train_loss: 0.014241 kl_loss: 0.083038 normal_loss: 0.011750\n",
      "[417/00018] train_loss: 0.014259 kl_loss: 0.083068 normal_loss: 0.011767\n",
      "[418/00025] train_loss: 0.014230 kl_loss: 0.083191 normal_loss: 0.011734\n",
      "[419/00032] train_loss: 0.014300 kl_loss: 0.083613 normal_loss: 0.011791\n",
      "[420/00039] train_loss: 0.014243 kl_loss: 0.083114 normal_loss: 0.011750\n",
      "[421/00046] train_loss: 0.014212 kl_loss: 0.082864 normal_loss: 0.011726\n",
      "[422/00053] train_loss: 0.014305 kl_loss: 0.083054 normal_loss: 0.011813\n",
      "[423/00060] train_loss: 0.014223 kl_loss: 0.083242 normal_loss: 0.011726\n",
      "[424/00067] train_loss: 0.014272 kl_loss: 0.082977 normal_loss: 0.011783\n",
      "[425/00074] train_loss: 0.014218 kl_loss: 0.082986 normal_loss: 0.011728\n",
      "[426/00081] train_loss: 0.014215 kl_loss: 0.082516 normal_loss: 0.011740\n",
      "[427/00088] train_loss: 0.014233 kl_loss: 0.083357 normal_loss: 0.011732\n",
      "[429/00002] train_loss: 0.014239 kl_loss: 0.083039 normal_loss: 0.011748\n",
      "[430/00009] train_loss: 0.014219 kl_loss: 0.083026 normal_loss: 0.011728\n",
      "[431/00016] train_loss: 0.014206 kl_loss: 0.082756 normal_loss: 0.011723\n",
      "[432/00023] train_loss: 0.014225 kl_loss: 0.082492 normal_loss: 0.011751\n",
      "[433/00030] train_loss: 0.014167 kl_loss: 0.083239 normal_loss: 0.011670\n",
      "[434/00037] train_loss: 0.014226 kl_loss: 0.083290 normal_loss: 0.011727\n",
      "[435/00044] train_loss: 0.014227 kl_loss: 0.083117 normal_loss: 0.011733\n",
      "[436/00051] train_loss: 0.014174 kl_loss: 0.082722 normal_loss: 0.011692\n",
      "[437/00058] train_loss: 0.014101 kl_loss: 0.082192 normal_loss: 0.011636\n",
      "[438/00065] train_loss: 0.014201 kl_loss: 0.082938 normal_loss: 0.011713\n",
      "[439/00072] train_loss: 0.014182 kl_loss: 0.083158 normal_loss: 0.011687\n",
      "[440/00079] train_loss: 0.014161 kl_loss: 0.082284 normal_loss: 0.011692\n",
      "[441/00086] train_loss: 0.014160 kl_loss: 0.083035 normal_loss: 0.011669\n",
      "[443/00000] train_loss: 0.014209 kl_loss: 0.082896 normal_loss: 0.011722\n",
      "[444/00007] train_loss: 0.014131 kl_loss: 0.082663 normal_loss: 0.011651\n",
      "[445/00014] train_loss: 0.014162 kl_loss: 0.082788 normal_loss: 0.011678\n",
      "[446/00021] train_loss: 0.014203 kl_loss: 0.082829 normal_loss: 0.011718\n",
      "[447/00028] train_loss: 0.014115 kl_loss: 0.082997 normal_loss: 0.011625\n",
      "[448/00035] train_loss: 0.014154 kl_loss: 0.082627 normal_loss: 0.011675\n",
      "[449/00042] train_loss: 0.014191 kl_loss: 0.083080 normal_loss: 0.011698\n",
      "[450/00049] train_loss: 0.014077 kl_loss: 0.082323 normal_loss: 0.011607\n",
      "[451/00056] train_loss: 0.014101 kl_loss: 0.082678 normal_loss: 0.011621\n",
      "[452/00063] train_loss: 0.014157 kl_loss: 0.082590 normal_loss: 0.011679\n",
      "[453/00070] train_loss: 0.014087 kl_loss: 0.082467 normal_loss: 0.011613\n",
      "[454/00077] train_loss: 0.014145 kl_loss: 0.082653 normal_loss: 0.011665\n",
      "[455/00084] train_loss: 0.014093 kl_loss: 0.082748 normal_loss: 0.011611\n",
      "[456/00091] train_loss: 0.014154 kl_loss: 0.082725 normal_loss: 0.011672\n",
      "[458/00005] train_loss: 0.014059 kl_loss: 0.082760 normal_loss: 0.011577\n",
      "[459/00012] train_loss: 0.014128 kl_loss: 0.082600 normal_loss: 0.011650\n",
      "[460/00019] train_loss: 0.014043 kl_loss: 0.082577 normal_loss: 0.011566\n",
      "[461/00026] train_loss: 0.014067 kl_loss: 0.082930 normal_loss: 0.011579\n",
      "[462/00033] train_loss: 0.014070 kl_loss: 0.082271 normal_loss: 0.011602\n",
      "[463/00040] train_loss: 0.014169 kl_loss: 0.082765 normal_loss: 0.011686\n",
      "[464/00047] train_loss: 0.014113 kl_loss: 0.082520 normal_loss: 0.011637\n",
      "[465/00054] train_loss: 0.014044 kl_loss: 0.082103 normal_loss: 0.011581\n",
      "[466/00061] train_loss: 0.014122 kl_loss: 0.083174 normal_loss: 0.011626\n",
      "[467/00068] train_loss: 0.014011 kl_loss: 0.082465 normal_loss: 0.011537\n",
      "[468/00075] train_loss: 0.014024 kl_loss: 0.082524 normal_loss: 0.011548\n",
      "[469/00082] train_loss: 0.014045 kl_loss: 0.082714 normal_loss: 0.011564\n",
      "[470/00089] train_loss: 0.014065 kl_loss: 0.082581 normal_loss: 0.011588\n",
      "[472/00003] train_loss: 0.014044 kl_loss: 0.082636 normal_loss: 0.011565\n",
      "[473/00010] train_loss: 0.014041 kl_loss: 0.082520 normal_loss: 0.011565\n",
      "[474/00017] train_loss: 0.014051 kl_loss: 0.082614 normal_loss: 0.011573\n",
      "[475/00024] train_loss: 0.014043 kl_loss: 0.082244 normal_loss: 0.011576\n",
      "[476/00031] train_loss: 0.014055 kl_loss: 0.083117 normal_loss: 0.011562\n",
      "[477/00038] train_loss: 0.014004 kl_loss: 0.082224 normal_loss: 0.011537\n",
      "[478/00045] train_loss: 0.014087 kl_loss: 0.082495 normal_loss: 0.011612\n",
      "[479/00052] train_loss: 0.014021 kl_loss: 0.082735 normal_loss: 0.011539\n",
      "[480/00059] train_loss: 0.014042 kl_loss: 0.082593 normal_loss: 0.011564\n",
      "[481/00066] train_loss: 0.014013 kl_loss: 0.082394 normal_loss: 0.011541\n",
      "[482/00073] train_loss: 0.013965 kl_loss: 0.082350 normal_loss: 0.011495\n",
      "[483/00080] train_loss: 0.014026 kl_loss: 0.082579 normal_loss: 0.011549\n",
      "[484/00087] train_loss: 0.013969 kl_loss: 0.082504 normal_loss: 0.011493\n",
      "[486/00001] train_loss: 0.014084 kl_loss: 0.082426 normal_loss: 0.011611\n",
      "[487/00008] train_loss: 0.013959 kl_loss: 0.082720 normal_loss: 0.011477\n",
      "[488/00015] train_loss: 0.013956 kl_loss: 0.082462 normal_loss: 0.011482\n",
      "[489/00022] train_loss: 0.013965 kl_loss: 0.082353 normal_loss: 0.011494\n",
      "[490/00029] train_loss: 0.014010 kl_loss: 0.082679 normal_loss: 0.011530\n",
      "[491/00036] train_loss: 0.014031 kl_loss: 0.082377 normal_loss: 0.011560\n",
      "[492/00043] train_loss: 0.013933 kl_loss: 0.082426 normal_loss: 0.011461\n",
      "[493/00050] train_loss: 0.014006 kl_loss: 0.082256 normal_loss: 0.011538\n",
      "[494/00057] train_loss: 0.013949 kl_loss: 0.082299 normal_loss: 0.011480\n",
      "[495/00064] train_loss: 0.013982 kl_loss: 0.082835 normal_loss: 0.011496\n",
      "[496/00071] train_loss: 0.013982 kl_loss: 0.082724 normal_loss: 0.011500\n",
      "[497/00078] train_loss: 0.013991 kl_loss: 0.082524 normal_loss: 0.011515\n",
      "[498/00085] train_loss: 0.013955 kl_loss: 0.082374 normal_loss: 0.011484\n",
      "[499/00092] train_loss: 0.013997 kl_loss: 0.082472 normal_loss: 0.011523\n"
     ]
    }
   ],
   "source": [
    "# SOFA VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'car_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 5949\n",
      "Training params: 2\n",
      "[001/00006] train_loss: 0.112116 kl_loss: 0.000000 normal_loss: 0.112116\n",
      "[002/00013] train_loss: 0.049877 kl_loss: 0.000000 normal_loss: 0.049877\n",
      "[003/00020] train_loss: 0.042903 kl_loss: 0.000000 normal_loss: 0.042903\n",
      "[004/00027] train_loss: 0.040008 kl_loss: 0.000000 normal_loss: 0.040008\n",
      "[005/00034] train_loss: 0.040174 kl_loss: 0.000000 normal_loss: 0.040174\n",
      "[006/00041] train_loss: 0.038132 kl_loss: 0.000000 normal_loss: 0.038132\n",
      "[007/00048] train_loss: 0.034730 kl_loss: 0.000000 normal_loss: 0.034730\n",
      "[008/00055] train_loss: 0.034613 kl_loss: 0.000000 normal_loss: 0.034613\n",
      "[009/00062] train_loss: 0.032794 kl_loss: 0.000000 normal_loss: 0.032794\n",
      "[010/00069] train_loss: 0.031808 kl_loss: 0.000000 normal_loss: 0.031808\n",
      "[011/00076] train_loss: 0.030735 kl_loss: 0.000000 normal_loss: 0.030735\n",
      "[012/00083] train_loss: 0.030599 kl_loss: 0.000000 normal_loss: 0.030599\n",
      "[013/00090] train_loss: 0.029965 kl_loss: 0.000000 normal_loss: 0.029965\n",
      "[015/00004] train_loss: 0.028993 kl_loss: 0.000000 normal_loss: 0.028993\n",
      "[016/00011] train_loss: 0.027056 kl_loss: 0.000000 normal_loss: 0.027056\n",
      "[017/00018] train_loss: 0.027284 kl_loss: 0.000000 normal_loss: 0.027284\n",
      "[018/00025] train_loss: 0.026861 kl_loss: 0.000000 normal_loss: 0.026861\n",
      "[019/00032] train_loss: 0.025727 kl_loss: 0.000000 normal_loss: 0.025727\n",
      "[020/00039] train_loss: 0.025234 kl_loss: 0.000000 normal_loss: 0.025234\n",
      "[021/00046] train_loss: 0.024505 kl_loss: 0.000000 normal_loss: 0.024505\n",
      "[022/00053] train_loss: 0.023743 kl_loss: 0.000000 normal_loss: 0.023743\n",
      "[023/00060] train_loss: 0.023106 kl_loss: 0.000000 normal_loss: 0.023106\n",
      "[024/00067] train_loss: 0.021966 kl_loss: 0.000000 normal_loss: 0.021966\n",
      "[025/00074] train_loss: 0.022841 kl_loss: 0.000000 normal_loss: 0.022841\n",
      "[026/00081] train_loss: 0.022479 kl_loss: 0.000000 normal_loss: 0.022479\n",
      "[027/00088] train_loss: 0.021860 kl_loss: 0.000000 normal_loss: 0.021860\n",
      "[029/00002] train_loss: 0.021227 kl_loss: 0.000000 normal_loss: 0.021227\n",
      "[030/00009] train_loss: 0.021554 kl_loss: 0.000000 normal_loss: 0.021554\n",
      "[031/00016] train_loss: 0.020874 kl_loss: 0.000000 normal_loss: 0.020874\n",
      "[032/00023] train_loss: 0.021093 kl_loss: 0.000000 normal_loss: 0.021093\n",
      "[033/00030] train_loss: 0.020685 kl_loss: 0.000000 normal_loss: 0.020685\n",
      "[034/00037] train_loss: 0.020258 kl_loss: 0.000000 normal_loss: 0.020258\n",
      "[035/00044] train_loss: 0.020323 kl_loss: 0.000000 normal_loss: 0.020323\n",
      "[036/00051] train_loss: 0.019859 kl_loss: 0.000000 normal_loss: 0.019859\n",
      "[037/00058] train_loss: 0.019706 kl_loss: 0.000000 normal_loss: 0.019706\n",
      "[038/00065] train_loss: 0.019188 kl_loss: 0.000000 normal_loss: 0.019188\n",
      "[039/00072] train_loss: 0.019370 kl_loss: 0.000000 normal_loss: 0.019370\n",
      "[040/00079] train_loss: 0.019206 kl_loss: 0.000000 normal_loss: 0.019206\n",
      "[041/00086] train_loss: 0.018249 kl_loss: 0.000000 normal_loss: 0.018249\n",
      "[043/00000] train_loss: 0.018469 kl_loss: 0.000000 normal_loss: 0.018469\n",
      "[044/00007] train_loss: 0.017837 kl_loss: 0.000000 normal_loss: 0.017837\n",
      "[045/00014] train_loss: 0.017862 kl_loss: 0.000000 normal_loss: 0.017862\n",
      "[046/00021] train_loss: 0.017897 kl_loss: 0.000000 normal_loss: 0.017897\n",
      "[047/00028] train_loss: 0.017754 kl_loss: 0.000000 normal_loss: 0.017754\n",
      "[048/00035] train_loss: 0.017640 kl_loss: 0.000000 normal_loss: 0.017640\n",
      "[049/00042] train_loss: 0.017573 kl_loss: 0.000000 normal_loss: 0.017573\n",
      "[050/00049] train_loss: 0.016900 kl_loss: 0.000000 normal_loss: 0.016900\n",
      "[051/00056] train_loss: 0.016703 kl_loss: 0.000000 normal_loss: 0.016703\n",
      "[052/00063] train_loss: 0.016674 kl_loss: 0.000000 normal_loss: 0.016674\n",
      "[053/00070] train_loss: 0.015997 kl_loss: 0.000000 normal_loss: 0.015997\n",
      "[054/00077] train_loss: 0.016484 kl_loss: 0.000000 normal_loss: 0.016484\n",
      "[055/00084] train_loss: 0.016483 kl_loss: 0.000000 normal_loss: 0.016483\n",
      "[056/00091] train_loss: 0.016207 kl_loss: 0.000000 normal_loss: 0.016207\n",
      "[058/00005] train_loss: 0.015754 kl_loss: 0.000000 normal_loss: 0.015754\n",
      "[059/00012] train_loss: 0.016044 kl_loss: 0.000000 normal_loss: 0.016044\n",
      "[060/00019] train_loss: 0.015375 kl_loss: 0.000000 normal_loss: 0.015375\n",
      "[061/00026] train_loss: 0.015182 kl_loss: 0.000000 normal_loss: 0.015182\n",
      "[062/00033] train_loss: 0.015228 kl_loss: 0.000000 normal_loss: 0.015228\n",
      "[063/00040] train_loss: 0.015282 kl_loss: 0.000000 normal_loss: 0.015282\n",
      "[064/00047] train_loss: 0.015289 kl_loss: 0.000000 normal_loss: 0.015289\n",
      "[065/00054] train_loss: 0.014399 kl_loss: 0.000000 normal_loss: 0.014399\n",
      "[066/00061] train_loss: 0.014779 kl_loss: 0.000000 normal_loss: 0.014779\n",
      "[067/00068] train_loss: 0.014853 kl_loss: 0.000000 normal_loss: 0.014853\n",
      "[068/00075] train_loss: 0.014757 kl_loss: 0.000000 normal_loss: 0.014757\n",
      "[069/00082] train_loss: 0.014013 kl_loss: 0.000000 normal_loss: 0.014013\n",
      "[070/00089] train_loss: 0.014159 kl_loss: 0.000000 normal_loss: 0.014159\n",
      "[072/00003] train_loss: 0.014243 kl_loss: 0.000000 normal_loss: 0.014243\n",
      "[073/00010] train_loss: 0.014053 kl_loss: 0.000000 normal_loss: 0.014053\n",
      "[074/00017] train_loss: 0.014116 kl_loss: 0.000000 normal_loss: 0.014116\n",
      "[075/00024] train_loss: 0.014004 kl_loss: 0.000000 normal_loss: 0.014004\n",
      "[076/00031] train_loss: 0.013833 kl_loss: 0.000000 normal_loss: 0.013833\n",
      "[077/00038] train_loss: 0.013716 kl_loss: 0.000000 normal_loss: 0.013716\n",
      "[078/00045] train_loss: 0.013776 kl_loss: 0.000000 normal_loss: 0.013776\n",
      "[079/00052] train_loss: 0.013572 kl_loss: 0.000000 normal_loss: 0.013572\n",
      "[080/00059] train_loss: 0.013308 kl_loss: 0.000000 normal_loss: 0.013308\n",
      "[081/00066] train_loss: 0.012803 kl_loss: 0.000000 normal_loss: 0.012803\n",
      "[082/00073] train_loss: 0.013334 kl_loss: 0.000000 normal_loss: 0.013334\n",
      "[083/00080] train_loss: 0.012926 kl_loss: 0.000000 normal_loss: 0.012926\n",
      "[084/00087] train_loss: 0.013273 kl_loss: 0.000000 normal_loss: 0.013273\n",
      "[086/00001] train_loss: 0.013204 kl_loss: 0.000000 normal_loss: 0.013204\n",
      "[087/00008] train_loss: 0.013055 kl_loss: 0.000000 normal_loss: 0.013055\n",
      "[088/00015] train_loss: 0.012993 kl_loss: 0.000000 normal_loss: 0.012993\n",
      "[089/00022] train_loss: 0.012580 kl_loss: 0.000000 normal_loss: 0.012580\n",
      "[090/00029] train_loss: 0.012469 kl_loss: 0.000000 normal_loss: 0.012469\n",
      "[091/00036] train_loss: 0.012742 kl_loss: 0.000000 normal_loss: 0.012742\n",
      "[092/00043] train_loss: 0.012453 kl_loss: 0.000000 normal_loss: 0.012453\n",
      "[093/00050] train_loss: 0.012344 kl_loss: 0.000000 normal_loss: 0.012344\n",
      "[094/00057] train_loss: 0.012419 kl_loss: 0.000000 normal_loss: 0.012419\n",
      "[095/00064] train_loss: 0.012429 kl_loss: 0.000000 normal_loss: 0.012429\n",
      "[096/00071] train_loss: 0.012061 kl_loss: 0.000000 normal_loss: 0.012061\n",
      "[097/00078] train_loss: 0.011729 kl_loss: 0.000000 normal_loss: 0.011729\n",
      "[098/00085] train_loss: 0.012116 kl_loss: 0.000000 normal_loss: 0.012116\n",
      "[099/00092] train_loss: 0.011742 kl_loss: 0.000000 normal_loss: 0.011742\n",
      "[101/00006] train_loss: 0.010504 kl_loss: 0.000000 normal_loss: 0.010504\n",
      "[102/00013] train_loss: 0.009791 kl_loss: 0.000000 normal_loss: 0.009791\n",
      "[103/00020] train_loss: 0.009532 kl_loss: 0.000000 normal_loss: 0.009532\n",
      "[104/00027] train_loss: 0.009250 kl_loss: 0.000000 normal_loss: 0.009250\n",
      "[105/00034] train_loss: 0.009523 kl_loss: 0.000000 normal_loss: 0.009523\n",
      "[106/00041] train_loss: 0.009282 kl_loss: 0.000000 normal_loss: 0.009282\n",
      "[107/00048] train_loss: 0.009293 kl_loss: 0.000000 normal_loss: 0.009293\n",
      "[108/00055] train_loss: 0.009307 kl_loss: 0.000000 normal_loss: 0.009307\n",
      "[109/00062] train_loss: 0.009653 kl_loss: 0.000000 normal_loss: 0.009653\n",
      "[110/00069] train_loss: 0.009411 kl_loss: 0.000000 normal_loss: 0.009411\n",
      "[111/00076] train_loss: 0.009356 kl_loss: 0.000000 normal_loss: 0.009356\n",
      "[112/00083] train_loss: 0.009440 kl_loss: 0.000000 normal_loss: 0.009440\n",
      "[113/00090] train_loss: 0.009426 kl_loss: 0.000000 normal_loss: 0.009426\n",
      "[115/00004] train_loss: 0.009598 kl_loss: 0.000000 normal_loss: 0.009598\n",
      "[116/00011] train_loss: 0.009318 kl_loss: 0.000000 normal_loss: 0.009318\n",
      "[117/00018] train_loss: 0.009342 kl_loss: 0.000000 normal_loss: 0.009342\n",
      "[118/00025] train_loss: 0.009421 kl_loss: 0.000000 normal_loss: 0.009421\n",
      "[119/00032] train_loss: 0.009300 kl_loss: 0.000000 normal_loss: 0.009300\n",
      "[120/00039] train_loss: 0.009138 kl_loss: 0.000000 normal_loss: 0.009138\n",
      "[121/00046] train_loss: 0.009324 kl_loss: 0.000000 normal_loss: 0.009324\n",
      "[122/00053] train_loss: 0.009080 kl_loss: 0.000000 normal_loss: 0.009080\n",
      "[123/00060] train_loss: 0.009236 kl_loss: 0.000000 normal_loss: 0.009236\n",
      "[124/00067] train_loss: 0.009198 kl_loss: 0.000000 normal_loss: 0.009198\n",
      "[125/00074] train_loss: 0.009038 kl_loss: 0.000000 normal_loss: 0.009038\n",
      "[126/00081] train_loss: 0.008739 kl_loss: 0.000000 normal_loss: 0.008739\n",
      "[127/00088] train_loss: 0.009042 kl_loss: 0.000000 normal_loss: 0.009042\n",
      "[129/00002] train_loss: 0.009161 kl_loss: 0.000000 normal_loss: 0.009161\n",
      "[130/00009] train_loss: 0.008981 kl_loss: 0.000000 normal_loss: 0.008981\n",
      "[131/00016] train_loss: 0.008748 kl_loss: 0.000000 normal_loss: 0.008748\n",
      "[132/00023] train_loss: 0.008946 kl_loss: 0.000000 normal_loss: 0.008946\n",
      "[133/00030] train_loss: 0.008925 kl_loss: 0.000000 normal_loss: 0.008925\n",
      "[134/00037] train_loss: 0.008762 kl_loss: 0.000000 normal_loss: 0.008762\n",
      "[135/00044] train_loss: 0.008903 kl_loss: 0.000000 normal_loss: 0.008903\n",
      "[136/00051] train_loss: 0.008882 kl_loss: 0.000000 normal_loss: 0.008882\n",
      "[137/00058] train_loss: 0.008752 kl_loss: 0.000000 normal_loss: 0.008752\n",
      "[138/00065] train_loss: 0.008948 kl_loss: 0.000000 normal_loss: 0.008948\n",
      "[139/00072] train_loss: 0.008654 kl_loss: 0.000000 normal_loss: 0.008654\n",
      "[140/00079] train_loss: 0.008535 kl_loss: 0.000000 normal_loss: 0.008535\n",
      "[141/00086] train_loss: 0.008779 kl_loss: 0.000000 normal_loss: 0.008779\n",
      "[143/00000] train_loss: 0.008781 kl_loss: 0.000000 normal_loss: 0.008781\n",
      "[144/00007] train_loss: 0.008744 kl_loss: 0.000000 normal_loss: 0.008744\n",
      "[145/00014] train_loss: 0.008726 kl_loss: 0.000000 normal_loss: 0.008726\n",
      "[146/00021] train_loss: 0.008805 kl_loss: 0.000000 normal_loss: 0.008805\n",
      "[147/00028] train_loss: 0.008594 kl_loss: 0.000000 normal_loss: 0.008594\n",
      "[148/00035] train_loss: 0.008427 kl_loss: 0.000000 normal_loss: 0.008427\n",
      "[149/00042] train_loss: 0.008740 kl_loss: 0.000000 normal_loss: 0.008740\n",
      "[150/00049] train_loss: 0.008683 kl_loss: 0.000000 normal_loss: 0.008683\n",
      "[151/00056] train_loss: 0.008670 kl_loss: 0.000000 normal_loss: 0.008670\n",
      "[152/00063] train_loss: 0.008303 kl_loss: 0.000000 normal_loss: 0.008303\n",
      "[153/00070] train_loss: 0.008368 kl_loss: 0.000000 normal_loss: 0.008368\n",
      "[154/00077] train_loss: 0.008294 kl_loss: 0.000000 normal_loss: 0.008294\n",
      "[155/00084] train_loss: 0.008554 kl_loss: 0.000000 normal_loss: 0.008554\n",
      "[156/00091] train_loss: 0.008593 kl_loss: 0.000000 normal_loss: 0.008593\n",
      "[158/00005] train_loss: 0.008386 kl_loss: 0.000000 normal_loss: 0.008386\n",
      "[159/00012] train_loss: 0.008519 kl_loss: 0.000000 normal_loss: 0.008519\n",
      "[160/00019] train_loss: 0.008363 kl_loss: 0.000000 normal_loss: 0.008363\n",
      "[161/00026] train_loss: 0.008165 kl_loss: 0.000000 normal_loss: 0.008165\n",
      "[162/00033] train_loss: 0.008462 kl_loss: 0.000000 normal_loss: 0.008462\n",
      "[163/00040] train_loss: 0.008381 kl_loss: 0.000000 normal_loss: 0.008381\n",
      "[164/00047] train_loss: 0.008295 kl_loss: 0.000000 normal_loss: 0.008295\n",
      "[165/00054] train_loss: 0.008313 kl_loss: 0.000000 normal_loss: 0.008313\n",
      "[166/00061] train_loss: 0.008193 kl_loss: 0.000000 normal_loss: 0.008193\n",
      "[167/00068] train_loss: 0.008107 kl_loss: 0.000000 normal_loss: 0.008107\n",
      "[168/00075] train_loss: 0.008087 kl_loss: 0.000000 normal_loss: 0.008087\n",
      "[169/00082] train_loss: 0.008367 kl_loss: 0.000000 normal_loss: 0.008367\n",
      "[170/00089] train_loss: 0.008176 kl_loss: 0.000000 normal_loss: 0.008176\n",
      "[172/00003] train_loss: 0.008119 kl_loss: 0.000000 normal_loss: 0.008119\n",
      "[173/00010] train_loss: 0.008232 kl_loss: 0.000000 normal_loss: 0.008232\n",
      "[174/00017] train_loss: 0.008225 kl_loss: 0.000000 normal_loss: 0.008225\n",
      "[175/00024] train_loss: 0.007934 kl_loss: 0.000000 normal_loss: 0.007934\n",
      "[176/00031] train_loss: 0.008113 kl_loss: 0.000000 normal_loss: 0.008113\n",
      "[177/00038] train_loss: 0.008065 kl_loss: 0.000000 normal_loss: 0.008065\n",
      "[178/00045] train_loss: 0.007935 kl_loss: 0.000000 normal_loss: 0.007935\n",
      "[179/00052] train_loss: 0.008087 kl_loss: 0.000000 normal_loss: 0.008087\n",
      "[180/00059] train_loss: 0.007948 kl_loss: 0.000000 normal_loss: 0.007948\n",
      "[181/00066] train_loss: 0.008048 kl_loss: 0.000000 normal_loss: 0.008048\n",
      "[182/00073] train_loss: 0.008045 kl_loss: 0.000000 normal_loss: 0.008045\n",
      "[183/00080] train_loss: 0.008173 kl_loss: 0.000000 normal_loss: 0.008173\n",
      "[184/00087] train_loss: 0.008056 kl_loss: 0.000000 normal_loss: 0.008056\n",
      "[186/00001] train_loss: 0.007941 kl_loss: 0.000000 normal_loss: 0.007941\n",
      "[187/00008] train_loss: 0.007975 kl_loss: 0.000000 normal_loss: 0.007975\n",
      "[188/00015] train_loss: 0.007994 kl_loss: 0.000000 normal_loss: 0.007994\n",
      "[189/00022] train_loss: 0.007952 kl_loss: 0.000000 normal_loss: 0.007952\n",
      "[190/00029] train_loss: 0.007735 kl_loss: 0.000000 normal_loss: 0.007735\n",
      "[191/00036] train_loss: 0.008014 kl_loss: 0.000000 normal_loss: 0.008014\n",
      "[192/00043] train_loss: 0.007849 kl_loss: 0.000000 normal_loss: 0.007849\n",
      "[193/00050] train_loss: 0.008024 kl_loss: 0.000000 normal_loss: 0.008024\n",
      "[194/00057] train_loss: 0.007574 kl_loss: 0.000000 normal_loss: 0.007574\n",
      "[195/00064] train_loss: 0.008034 kl_loss: 0.000000 normal_loss: 0.008034\n",
      "[196/00071] train_loss: 0.007878 kl_loss: 0.000000 normal_loss: 0.007878\n",
      "[197/00078] train_loss: 0.007838 kl_loss: 0.000000 normal_loss: 0.007838\n",
      "[198/00085] train_loss: 0.007838 kl_loss: 0.000000 normal_loss: 0.007838\n",
      "[199/00092] train_loss: 0.007732 kl_loss: 0.000000 normal_loss: 0.007732\n",
      "[201/00006] train_loss: 0.007062 kl_loss: 0.000000 normal_loss: 0.007062\n",
      "[202/00013] train_loss: 0.006761 kl_loss: 0.000000 normal_loss: 0.006761\n",
      "[203/00020] train_loss: 0.006657 kl_loss: 0.000000 normal_loss: 0.006657\n",
      "[204/00027] train_loss: 0.006602 kl_loss: 0.000000 normal_loss: 0.006602\n",
      "[205/00034] train_loss: 0.006614 kl_loss: 0.000000 normal_loss: 0.006614\n",
      "[206/00041] train_loss: 0.006652 kl_loss: 0.000000 normal_loss: 0.006652\n",
      "[207/00048] train_loss: 0.006613 kl_loss: 0.000000 normal_loss: 0.006613\n",
      "[208/00055] train_loss: 0.006767 kl_loss: 0.000000 normal_loss: 0.006767\n",
      "[209/00062] train_loss: 0.006667 kl_loss: 0.000000 normal_loss: 0.006667\n",
      "[210/00069] train_loss: 0.006593 kl_loss: 0.000000 normal_loss: 0.006593\n",
      "[211/00076] train_loss: 0.006601 kl_loss: 0.000000 normal_loss: 0.006601\n",
      "[212/00083] train_loss: 0.006572 kl_loss: 0.000000 normal_loss: 0.006572\n",
      "[213/00090] train_loss: 0.006625 kl_loss: 0.000000 normal_loss: 0.006625\n",
      "[215/00004] train_loss: 0.006625 kl_loss: 0.000000 normal_loss: 0.006625\n",
      "[216/00011] train_loss: 0.006733 kl_loss: 0.000000 normal_loss: 0.006733\n",
      "[217/00018] train_loss: 0.006660 kl_loss: 0.000000 normal_loss: 0.006660\n",
      "[218/00025] train_loss: 0.006669 kl_loss: 0.000000 normal_loss: 0.006669\n",
      "[219/00032] train_loss: 0.006648 kl_loss: 0.000000 normal_loss: 0.006648\n",
      "[220/00039] train_loss: 0.006590 kl_loss: 0.000000 normal_loss: 0.006590\n",
      "[221/00046] train_loss: 0.006542 kl_loss: 0.000000 normal_loss: 0.006542\n",
      "[222/00053] train_loss: 0.006616 kl_loss: 0.000000 normal_loss: 0.006616\n",
      "[223/00060] train_loss: 0.006559 kl_loss: 0.000000 normal_loss: 0.006559\n",
      "[224/00067] train_loss: 0.006716 kl_loss: 0.000000 normal_loss: 0.006716\n",
      "[225/00074] train_loss: 0.006624 kl_loss: 0.000000 normal_loss: 0.006624\n",
      "[226/00081] train_loss: 0.006664 kl_loss: 0.000000 normal_loss: 0.006664\n",
      "[227/00088] train_loss: 0.006592 kl_loss: 0.000000 normal_loss: 0.006592\n",
      "[229/00002] train_loss: 0.006610 kl_loss: 0.000000 normal_loss: 0.006610\n",
      "[230/00009] train_loss: 0.006563 kl_loss: 0.000000 normal_loss: 0.006563\n",
      "[231/00016] train_loss: 0.006589 kl_loss: 0.000000 normal_loss: 0.006589\n",
      "[232/00023] train_loss: 0.006499 kl_loss: 0.000000 normal_loss: 0.006499\n",
      "[233/00030] train_loss: 0.006574 kl_loss: 0.000000 normal_loss: 0.006574\n",
      "[234/00037] train_loss: 0.006553 kl_loss: 0.000000 normal_loss: 0.006553\n",
      "[235/00044] train_loss: 0.006384 kl_loss: 0.000000 normal_loss: 0.006384\n",
      "[236/00051] train_loss: 0.006455 kl_loss: 0.000000 normal_loss: 0.006455\n",
      "[237/00058] train_loss: 0.006547 kl_loss: 0.000000 normal_loss: 0.006547\n",
      "[238/00065] train_loss: 0.006411 kl_loss: 0.000000 normal_loss: 0.006411\n",
      "[239/00072] train_loss: 0.006371 kl_loss: 0.000000 normal_loss: 0.006371\n",
      "[240/00079] train_loss: 0.006458 kl_loss: 0.000000 normal_loss: 0.006458\n",
      "[241/00086] train_loss: 0.006536 kl_loss: 0.000000 normal_loss: 0.006536\n",
      "[243/00000] train_loss: 0.006433 kl_loss: 0.000000 normal_loss: 0.006433\n",
      "[244/00007] train_loss: 0.006520 kl_loss: 0.000000 normal_loss: 0.006520\n",
      "[245/00014] train_loss: 0.006600 kl_loss: 0.000000 normal_loss: 0.006600\n",
      "[246/00021] train_loss: 0.006587 kl_loss: 0.000000 normal_loss: 0.006587\n",
      "[247/00028] train_loss: 0.006483 kl_loss: 0.000000 normal_loss: 0.006483\n",
      "[248/00035] train_loss: 0.006359 kl_loss: 0.000000 normal_loss: 0.006359\n",
      "[249/00042] train_loss: 0.006533 kl_loss: 0.000000 normal_loss: 0.006533\n",
      "[250/00049] train_loss: 0.006430 kl_loss: 0.000000 normal_loss: 0.006430\n",
      "[251/00056] train_loss: 0.006483 kl_loss: 0.000000 normal_loss: 0.006483\n",
      "[252/00063] train_loss: 0.006278 kl_loss: 0.000000 normal_loss: 0.006278\n",
      "[253/00070] train_loss: 0.006541 kl_loss: 0.000000 normal_loss: 0.006541\n",
      "[254/00077] train_loss: 0.006545 kl_loss: 0.000000 normal_loss: 0.006545\n",
      "[255/00084] train_loss: 0.006397 kl_loss: 0.000000 normal_loss: 0.006397\n",
      "[256/00091] train_loss: 0.006463 kl_loss: 0.000000 normal_loss: 0.006463\n",
      "[258/00005] train_loss: 0.006310 kl_loss: 0.000000 normal_loss: 0.006310\n",
      "[259/00012] train_loss: 0.006399 kl_loss: 0.000000 normal_loss: 0.006399\n",
      "[260/00019] train_loss: 0.006416 kl_loss: 0.000000 normal_loss: 0.006416\n",
      "[261/00026] train_loss: 0.006401 kl_loss: 0.000000 normal_loss: 0.006401\n",
      "[262/00033] train_loss: 0.006385 kl_loss: 0.000000 normal_loss: 0.006385\n",
      "[263/00040] train_loss: 0.006398 kl_loss: 0.000000 normal_loss: 0.006398\n",
      "[264/00047] train_loss: 0.006367 kl_loss: 0.000000 normal_loss: 0.006367\n",
      "[265/00054] train_loss: 0.006350 kl_loss: 0.000000 normal_loss: 0.006350\n",
      "[266/00061] train_loss: 0.006349 kl_loss: 0.000000 normal_loss: 0.006349\n",
      "[267/00068] train_loss: 0.006388 kl_loss: 0.000000 normal_loss: 0.006388\n",
      "[268/00075] train_loss: 0.006382 kl_loss: 0.000000 normal_loss: 0.006382\n",
      "[269/00082] train_loss: 0.006312 kl_loss: 0.000000 normal_loss: 0.006312\n",
      "[270/00089] train_loss: 0.006328 kl_loss: 0.000000 normal_loss: 0.006328\n",
      "[272/00003] train_loss: 0.006384 kl_loss: 0.000000 normal_loss: 0.006384\n",
      "[273/00010] train_loss: 0.006332 kl_loss: 0.000000 normal_loss: 0.006332\n",
      "[274/00017] train_loss: 0.006247 kl_loss: 0.000000 normal_loss: 0.006247\n",
      "[275/00024] train_loss: 0.006403 kl_loss: 0.000000 normal_loss: 0.006403\n",
      "[276/00031] train_loss: 0.006313 kl_loss: 0.000000 normal_loss: 0.006313\n",
      "[277/00038] train_loss: 0.006178 kl_loss: 0.000000 normal_loss: 0.006178\n",
      "[278/00045] train_loss: 0.006207 kl_loss: 0.000000 normal_loss: 0.006207\n",
      "[279/00052] train_loss: 0.006243 kl_loss: 0.000000 normal_loss: 0.006243\n",
      "[280/00059] train_loss: 0.006362 kl_loss: 0.000000 normal_loss: 0.006362\n",
      "[281/00066] train_loss: 0.006196 kl_loss: 0.000000 normal_loss: 0.006196\n",
      "[282/00073] train_loss: 0.006208 kl_loss: 0.000000 normal_loss: 0.006208\n",
      "[283/00080] train_loss: 0.006217 kl_loss: 0.000000 normal_loss: 0.006217\n",
      "[284/00087] train_loss: 0.006213 kl_loss: 0.000000 normal_loss: 0.006213\n",
      "[286/00001] train_loss: 0.006247 kl_loss: 0.000000 normal_loss: 0.006247\n",
      "[287/00008] train_loss: 0.006247 kl_loss: 0.000000 normal_loss: 0.006247\n",
      "[288/00015] train_loss: 0.006156 kl_loss: 0.000000 normal_loss: 0.006156\n",
      "[289/00022] train_loss: 0.006181 kl_loss: 0.000000 normal_loss: 0.006181\n",
      "[290/00029] train_loss: 0.006360 kl_loss: 0.000000 normal_loss: 0.006360\n",
      "[291/00036] train_loss: 0.006329 kl_loss: 0.000000 normal_loss: 0.006329\n",
      "[292/00043] train_loss: 0.006202 kl_loss: 0.000000 normal_loss: 0.006202\n",
      "[293/00050] train_loss: 0.006345 kl_loss: 0.000000 normal_loss: 0.006345\n",
      "[294/00057] train_loss: 0.006338 kl_loss: 0.000000 normal_loss: 0.006338\n",
      "[295/00064] train_loss: 0.006254 kl_loss: 0.000000 normal_loss: 0.006254\n",
      "[296/00071] train_loss: 0.006223 kl_loss: 0.000000 normal_loss: 0.006223\n",
      "[297/00078] train_loss: 0.006159 kl_loss: 0.000000 normal_loss: 0.006159\n",
      "[298/00085] train_loss: 0.006263 kl_loss: 0.000000 normal_loss: 0.006263\n",
      "[299/00092] train_loss: 0.006246 kl_loss: 0.000000 normal_loss: 0.006246\n",
      "[301/00006] train_loss: 0.005748 kl_loss: 0.000000 normal_loss: 0.005748\n",
      "[302/00013] train_loss: 0.005722 kl_loss: 0.000000 normal_loss: 0.005722\n",
      "[303/00020] train_loss: 0.005738 kl_loss: 0.000000 normal_loss: 0.005738\n",
      "[304/00027] train_loss: 0.005753 kl_loss: 0.000000 normal_loss: 0.005753\n",
      "[305/00034] train_loss: 0.005656 kl_loss: 0.000000 normal_loss: 0.005656\n",
      "[306/00041] train_loss: 0.005699 kl_loss: 0.000000 normal_loss: 0.005699\n",
      "[307/00048] train_loss: 0.005761 kl_loss: 0.000000 normal_loss: 0.005761\n",
      "[308/00055] train_loss: 0.005691 kl_loss: 0.000000 normal_loss: 0.005691\n",
      "[309/00062] train_loss: 0.005707 kl_loss: 0.000000 normal_loss: 0.005707\n",
      "[310/00069] train_loss: 0.005741 kl_loss: 0.000000 normal_loss: 0.005741\n",
      "[311/00076] train_loss: 0.005714 kl_loss: 0.000000 normal_loss: 0.005714\n",
      "[312/00083] train_loss: 0.005691 kl_loss: 0.000000 normal_loss: 0.005691\n",
      "[313/00090] train_loss: 0.005702 kl_loss: 0.000000 normal_loss: 0.005702\n",
      "[315/00004] train_loss: 0.005680 kl_loss: 0.000000 normal_loss: 0.005680\n",
      "[316/00011] train_loss: 0.005658 kl_loss: 0.000000 normal_loss: 0.005658\n",
      "[317/00018] train_loss: 0.005695 kl_loss: 0.000000 normal_loss: 0.005695\n",
      "[318/00025] train_loss: 0.005665 kl_loss: 0.000000 normal_loss: 0.005665\n",
      "[319/00032] train_loss: 0.005728 kl_loss: 0.000000 normal_loss: 0.005728\n",
      "[320/00039] train_loss: 0.005701 kl_loss: 0.000000 normal_loss: 0.005701\n",
      "[321/00046] train_loss: 0.005656 kl_loss: 0.000000 normal_loss: 0.005656\n",
      "[322/00053] train_loss: 0.005675 kl_loss: 0.000000 normal_loss: 0.005675\n",
      "[323/00060] train_loss: 0.005661 kl_loss: 0.000000 normal_loss: 0.005661\n",
      "[324/00067] train_loss: 0.005610 kl_loss: 0.000000 normal_loss: 0.005610\n",
      "[325/00074] train_loss: 0.005674 kl_loss: 0.000000 normal_loss: 0.005674\n",
      "[326/00081] train_loss: 0.005735 kl_loss: 0.000000 normal_loss: 0.005735\n",
      "[327/00088] train_loss: 0.005643 kl_loss: 0.000000 normal_loss: 0.005643\n",
      "[329/00002] train_loss: 0.005655 kl_loss: 0.000000 normal_loss: 0.005655\n",
      "[330/00009] train_loss: 0.005694 kl_loss: 0.000000 normal_loss: 0.005694\n",
      "[331/00016] train_loss: 0.005661 kl_loss: 0.000000 normal_loss: 0.005661\n",
      "[332/00023] train_loss: 0.005634 kl_loss: 0.000000 normal_loss: 0.005634\n",
      "[333/00030] train_loss: 0.005643 kl_loss: 0.000000 normal_loss: 0.005643\n",
      "[334/00037] train_loss: 0.005652 kl_loss: 0.000000 normal_loss: 0.005652\n",
      "[335/00044] train_loss: 0.005621 kl_loss: 0.000000 normal_loss: 0.005621\n",
      "[336/00051] train_loss: 0.005585 kl_loss: 0.000000 normal_loss: 0.005585\n",
      "[337/00058] train_loss: 0.005679 kl_loss: 0.000000 normal_loss: 0.005679\n",
      "[338/00065] train_loss: 0.005635 kl_loss: 0.000000 normal_loss: 0.005635\n",
      "[339/00072] train_loss: 0.005664 kl_loss: 0.000000 normal_loss: 0.005664\n",
      "[340/00079] train_loss: 0.005577 kl_loss: 0.000000 normal_loss: 0.005577\n",
      "[341/00086] train_loss: 0.005585 kl_loss: 0.000000 normal_loss: 0.005585\n",
      "[343/00000] train_loss: 0.005666 kl_loss: 0.000000 normal_loss: 0.005666\n",
      "[344/00007] train_loss: 0.005586 kl_loss: 0.000000 normal_loss: 0.005586\n",
      "[345/00014] train_loss: 0.005567 kl_loss: 0.000000 normal_loss: 0.005567\n",
      "[346/00021] train_loss: 0.005574 kl_loss: 0.000000 normal_loss: 0.005574\n",
      "[347/00028] train_loss: 0.005663 kl_loss: 0.000000 normal_loss: 0.005663\n",
      "[348/00035] train_loss: 0.005653 kl_loss: 0.000000 normal_loss: 0.005653\n",
      "[349/00042] train_loss: 0.005566 kl_loss: 0.000000 normal_loss: 0.005566\n",
      "[350/00049] train_loss: 0.005627 kl_loss: 0.000000 normal_loss: 0.005627\n",
      "[351/00056] train_loss: 0.005627 kl_loss: 0.000000 normal_loss: 0.005627\n",
      "[352/00063] train_loss: 0.005643 kl_loss: 0.000000 normal_loss: 0.005643\n",
      "[353/00070] train_loss: 0.005633 kl_loss: 0.000000 normal_loss: 0.005633\n",
      "[354/00077] train_loss: 0.005601 kl_loss: 0.000000 normal_loss: 0.005601\n",
      "[355/00084] train_loss: 0.005590 kl_loss: 0.000000 normal_loss: 0.005590\n",
      "[356/00091] train_loss: 0.005578 kl_loss: 0.000000 normal_loss: 0.005578\n",
      "[358/00005] train_loss: 0.005608 kl_loss: 0.000000 normal_loss: 0.005608\n",
      "[359/00012] train_loss: 0.005530 kl_loss: 0.000000 normal_loss: 0.005530\n",
      "[360/00019] train_loss: 0.005592 kl_loss: 0.000000 normal_loss: 0.005592\n",
      "[361/00026] train_loss: 0.005563 kl_loss: 0.000000 normal_loss: 0.005563\n",
      "[362/00033] train_loss: 0.005571 kl_loss: 0.000000 normal_loss: 0.005571\n",
      "[363/00040] train_loss: 0.005625 kl_loss: 0.000000 normal_loss: 0.005625\n",
      "[364/00047] train_loss: 0.005564 kl_loss: 0.000000 normal_loss: 0.005564\n",
      "[365/00054] train_loss: 0.005550 kl_loss: 0.000000 normal_loss: 0.005550\n",
      "[366/00061] train_loss: 0.005531 kl_loss: 0.000000 normal_loss: 0.005531\n",
      "[367/00068] train_loss: 0.005547 kl_loss: 0.000000 normal_loss: 0.005547\n",
      "[368/00075] train_loss: 0.005604 kl_loss: 0.000000 normal_loss: 0.005604\n",
      "[369/00082] train_loss: 0.005608 kl_loss: 0.000000 normal_loss: 0.005608\n",
      "[370/00089] train_loss: 0.005583 kl_loss: 0.000000 normal_loss: 0.005583\n",
      "[372/00003] train_loss: 0.005548 kl_loss: 0.000000 normal_loss: 0.005548\n",
      "[373/00010] train_loss: 0.005510 kl_loss: 0.000000 normal_loss: 0.005510\n",
      "[374/00017] train_loss: 0.005541 kl_loss: 0.000000 normal_loss: 0.005541\n",
      "[375/00024] train_loss: 0.005535 kl_loss: 0.000000 normal_loss: 0.005535\n",
      "[376/00031] train_loss: 0.005505 kl_loss: 0.000000 normal_loss: 0.005505\n",
      "[377/00038] train_loss: 0.005603 kl_loss: 0.000000 normal_loss: 0.005603\n",
      "[378/00045] train_loss: 0.005564 kl_loss: 0.000000 normal_loss: 0.005564\n",
      "[379/00052] train_loss: 0.005480 kl_loss: 0.000000 normal_loss: 0.005480\n",
      "[380/00059] train_loss: 0.005563 kl_loss: 0.000000 normal_loss: 0.005563\n",
      "[381/00066] train_loss: 0.005581 kl_loss: 0.000000 normal_loss: 0.005581\n",
      "[382/00073] train_loss: 0.005507 kl_loss: 0.000000 normal_loss: 0.005507\n",
      "[383/00080] train_loss: 0.005463 kl_loss: 0.000000 normal_loss: 0.005463\n",
      "[384/00087] train_loss: 0.005567 kl_loss: 0.000000 normal_loss: 0.005567\n",
      "[386/00001] train_loss: 0.005544 kl_loss: 0.000000 normal_loss: 0.005544\n",
      "[387/00008] train_loss: 0.005510 kl_loss: 0.000000 normal_loss: 0.005510\n",
      "[388/00015] train_loss: 0.005532 kl_loss: 0.000000 normal_loss: 0.005532\n",
      "[389/00022] train_loss: 0.005548 kl_loss: 0.000000 normal_loss: 0.005548\n",
      "[390/00029] train_loss: 0.005460 kl_loss: 0.000000 normal_loss: 0.005460\n",
      "[391/00036] train_loss: 0.005545 kl_loss: 0.000000 normal_loss: 0.005545\n",
      "[392/00043] train_loss: 0.005506 kl_loss: 0.000000 normal_loss: 0.005506\n",
      "[393/00050] train_loss: 0.005545 kl_loss: 0.000000 normal_loss: 0.005545\n",
      "[394/00057] train_loss: 0.005481 kl_loss: 0.000000 normal_loss: 0.005481\n",
      "[395/00064] train_loss: 0.005515 kl_loss: 0.000000 normal_loss: 0.005515\n",
      "[396/00071] train_loss: 0.005489 kl_loss: 0.000000 normal_loss: 0.005489\n",
      "[397/00078] train_loss: 0.005536 kl_loss: 0.000000 normal_loss: 0.005536\n",
      "[398/00085] train_loss: 0.005468 kl_loss: 0.000000 normal_loss: 0.005468\n",
      "[399/00092] train_loss: 0.005521 kl_loss: 0.000000 normal_loss: 0.005521\n",
      "[401/00006] train_loss: 0.005306 kl_loss: 0.000000 normal_loss: 0.005306\n",
      "[402/00013] train_loss: 0.005310 kl_loss: 0.000000 normal_loss: 0.005310\n",
      "[403/00020] train_loss: 0.005305 kl_loss: 0.000000 normal_loss: 0.005305\n",
      "[404/00027] train_loss: 0.005288 kl_loss: 0.000000 normal_loss: 0.005288\n",
      "[405/00034] train_loss: 0.005276 kl_loss: 0.000000 normal_loss: 0.005276\n",
      "[406/00041] train_loss: 0.005302 kl_loss: 0.000000 normal_loss: 0.005302\n",
      "[407/00048] train_loss: 0.005329 kl_loss: 0.000000 normal_loss: 0.005329\n",
      "[408/00055] train_loss: 0.005335 kl_loss: 0.000000 normal_loss: 0.005335\n",
      "[409/00062] train_loss: 0.005295 kl_loss: 0.000000 normal_loss: 0.005295\n",
      "[410/00069] train_loss: 0.005290 kl_loss: 0.000000 normal_loss: 0.005290\n",
      "[411/00076] train_loss: 0.005326 kl_loss: 0.000000 normal_loss: 0.005326\n",
      "[412/00083] train_loss: 0.005298 kl_loss: 0.000000 normal_loss: 0.005298\n",
      "[413/00090] train_loss: 0.005338 kl_loss: 0.000000 normal_loss: 0.005338\n",
      "[415/00004] train_loss: 0.005325 kl_loss: 0.000000 normal_loss: 0.005325\n",
      "[416/00011] train_loss: 0.005293 kl_loss: 0.000000 normal_loss: 0.005293\n",
      "[417/00018] train_loss: 0.005268 kl_loss: 0.000000 normal_loss: 0.005268\n",
      "[418/00025] train_loss: 0.005275 kl_loss: 0.000000 normal_loss: 0.005275\n",
      "[419/00032] train_loss: 0.005354 kl_loss: 0.000000 normal_loss: 0.005354\n",
      "[420/00039] train_loss: 0.005268 kl_loss: 0.000000 normal_loss: 0.005268\n",
      "[421/00046] train_loss: 0.005255 kl_loss: 0.000000 normal_loss: 0.005255\n",
      "[422/00053] train_loss: 0.005281 kl_loss: 0.000000 normal_loss: 0.005281\n",
      "[423/00060] train_loss: 0.005281 kl_loss: 0.000000 normal_loss: 0.005281\n",
      "[424/00067] train_loss: 0.005284 kl_loss: 0.000000 normal_loss: 0.005284\n",
      "[425/00074] train_loss: 0.005267 kl_loss: 0.000000 normal_loss: 0.005267\n",
      "[426/00081] train_loss: 0.005280 kl_loss: 0.000000 normal_loss: 0.005280\n",
      "[427/00088] train_loss: 0.005263 kl_loss: 0.000000 normal_loss: 0.005263\n",
      "[429/00002] train_loss: 0.005257 kl_loss: 0.000000 normal_loss: 0.005257\n",
      "[430/00009] train_loss: 0.005281 kl_loss: 0.000000 normal_loss: 0.005281\n",
      "[431/00016] train_loss: 0.005246 kl_loss: 0.000000 normal_loss: 0.005246\n",
      "[432/00023] train_loss: 0.005241 kl_loss: 0.000000 normal_loss: 0.005241\n",
      "[433/00030] train_loss: 0.005313 kl_loss: 0.000000 normal_loss: 0.005313\n",
      "[434/00037] train_loss: 0.005276 kl_loss: 0.000000 normal_loss: 0.005276\n",
      "[435/00044] train_loss: 0.005285 kl_loss: 0.000000 normal_loss: 0.005285\n",
      "[436/00051] train_loss: 0.005251 kl_loss: 0.000000 normal_loss: 0.005251\n",
      "[437/00058] train_loss: 0.005309 kl_loss: 0.000000 normal_loss: 0.005309\n",
      "[438/00065] train_loss: 0.005320 kl_loss: 0.000000 normal_loss: 0.005320\n",
      "[439/00072] train_loss: 0.005234 kl_loss: 0.000000 normal_loss: 0.005234\n",
      "[440/00079] train_loss: 0.005269 kl_loss: 0.000000 normal_loss: 0.005269\n",
      "[441/00086] train_loss: 0.005253 kl_loss: 0.000000 normal_loss: 0.005253\n",
      "[443/00000] train_loss: 0.005260 kl_loss: 0.000000 normal_loss: 0.005260\n",
      "[444/00007] train_loss: 0.005247 kl_loss: 0.000000 normal_loss: 0.005247\n",
      "[445/00014] train_loss: 0.005250 kl_loss: 0.000000 normal_loss: 0.005250\n",
      "[446/00021] train_loss: 0.005251 kl_loss: 0.000000 normal_loss: 0.005251\n",
      "[447/00028] train_loss: 0.005241 kl_loss: 0.000000 normal_loss: 0.005241\n",
      "[448/00035] train_loss: 0.005237 kl_loss: 0.000000 normal_loss: 0.005237\n",
      "[449/00042] train_loss: 0.005256 kl_loss: 0.000000 normal_loss: 0.005256\n",
      "[450/00049] train_loss: 0.005241 kl_loss: 0.000000 normal_loss: 0.005241\n",
      "[451/00056] train_loss: 0.005296 kl_loss: 0.000000 normal_loss: 0.005296\n",
      "[452/00063] train_loss: 0.005243 kl_loss: 0.000000 normal_loss: 0.005243\n",
      "[453/00070] train_loss: 0.005263 kl_loss: 0.000000 normal_loss: 0.005263\n",
      "[454/00077] train_loss: 0.005246 kl_loss: 0.000000 normal_loss: 0.005246\n",
      "[455/00084] train_loss: 0.005211 kl_loss: 0.000000 normal_loss: 0.005211\n",
      "[456/00091] train_loss: 0.005235 kl_loss: 0.000000 normal_loss: 0.005235\n",
      "[458/00005] train_loss: 0.005239 kl_loss: 0.000000 normal_loss: 0.005239\n",
      "[459/00012] train_loss: 0.005248 kl_loss: 0.000000 normal_loss: 0.005248\n",
      "[460/00019] train_loss: 0.005206 kl_loss: 0.000000 normal_loss: 0.005206\n",
      "[461/00026] train_loss: 0.005223 kl_loss: 0.000000 normal_loss: 0.005223\n",
      "[462/00033] train_loss: 0.005211 kl_loss: 0.000000 normal_loss: 0.005211\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcar_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000019?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:69\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=66'>67</a>\u001b[0m n \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=67'>68</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mmax_epochs\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=68'>69</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(train_dataloader):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=69'>70</a>\u001b[0m         \u001b[39m# Move batch to device, set optimizer gradients to zero, perform forward pass\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=70'>71</a>\u001b[0m         ShapeNet\u001b[39m.\u001b[39mmove_batch_to_device(batch, device)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=71'>72</a>\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=365'>366</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=366'>367</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=367'>368</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:314\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=311'>312</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=312'>313</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=313'>314</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:927\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=919'>920</a>\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=920'>921</a>\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=921'>922</a>\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=922'>923</a>\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=923'>924</a>\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=924'>925</a>\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=925'>926</a>\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=926'>927</a>\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=927'>928</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/utils/data/dataloader.py?line=928'>929</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=117'>118</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=118'>119</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=119'>120</a>\u001b[0m _cleanup()\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=120'>121</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=121'>122</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=122'>123</a>\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/process.py?line=123'>124</a>\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=221'>222</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=222'>223</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=223'>224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=323'>324</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=324'>325</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=325'>326</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/context.py?line=326'>327</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=91'>92</a>\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=92'>93</a>\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=93'>94</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/popen_spawn_win32.py?line=94'>95</a>\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=57'>58</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=58'>59</a>\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/multiprocessing/reduction.py?line=59'>60</a>\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CAR AD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'car_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.0,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 9994\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 2\n",
      "[000/00099] train_loss: 0.012841 kl_loss: 0.000000 normal_loss: 0.012841\n",
      "[001/00042] train_loss: 0.009136 kl_loss: 0.000000 normal_loss: 0.009136\n",
      "[001/00142] train_loss: 0.009702 kl_loss: 0.000000 normal_loss: 0.009702\n",
      "[002/00085] train_loss: 0.009820 kl_loss: 0.000000 normal_loss: 0.009820\n",
      "[003/00028] train_loss: 0.010000 kl_loss: 0.000000 normal_loss: 0.010000\n",
      "[003/00128] train_loss: 0.009638 kl_loss: 0.000000 normal_loss: 0.009638\n",
      "[004/00071] train_loss: 0.009730 kl_loss: 0.000000 normal_loss: 0.009730\n",
      "[005/00014] train_loss: 0.009661 kl_loss: 0.000000 normal_loss: 0.009661\n",
      "[005/00114] train_loss: 0.009315 kl_loss: 0.000000 normal_loss: 0.009315\n",
      "[006/00057] train_loss: 0.009637 kl_loss: 0.000000 normal_loss: 0.009637\n",
      "[007/00000] train_loss: 0.009447 kl_loss: 0.000000 normal_loss: 0.009447\n",
      "[007/00100] train_loss: 0.009116 kl_loss: 0.000000 normal_loss: 0.009116\n",
      "[008/00043] train_loss: 0.008983 kl_loss: 0.000000 normal_loss: 0.008983\n",
      "[008/00143] train_loss: 0.009491 kl_loss: 0.000000 normal_loss: 0.009491\n",
      "[009/00086] train_loss: 0.009559 kl_loss: 0.000000 normal_loss: 0.009559\n",
      "[010/00029] train_loss: 0.009401 kl_loss: 0.000000 normal_loss: 0.009401\n",
      "[010/00129] train_loss: 0.009312 kl_loss: 0.000000 normal_loss: 0.009312\n",
      "[011/00072] train_loss: 0.009221 kl_loss: 0.000000 normal_loss: 0.009221\n",
      "[012/00015] train_loss: 0.008730 kl_loss: 0.000000 normal_loss: 0.008730\n",
      "[012/00115] train_loss: 0.009322 kl_loss: 0.000000 normal_loss: 0.009322\n",
      "[013/00058] train_loss: 0.009264 kl_loss: 0.000000 normal_loss: 0.009264\n",
      "[014/00001] train_loss: 0.008898 kl_loss: 0.000000 normal_loss: 0.008898\n",
      "[014/00101] train_loss: 0.009165 kl_loss: 0.000000 normal_loss: 0.009165\n",
      "[015/00044] train_loss: 0.009166 kl_loss: 0.000000 normal_loss: 0.009166\n",
      "[015/00144] train_loss: 0.009054 kl_loss: 0.000000 normal_loss: 0.009054\n",
      "[016/00087] train_loss: 0.009160 kl_loss: 0.000000 normal_loss: 0.009160\n",
      "[017/00030] train_loss: 0.009117 kl_loss: 0.000000 normal_loss: 0.009117\n",
      "[017/00130] train_loss: 0.008945 kl_loss: 0.000000 normal_loss: 0.008945\n",
      "[018/00073] train_loss: 0.008784 kl_loss: 0.000000 normal_loss: 0.008784\n",
      "[019/00016] train_loss: 0.009053 kl_loss: 0.000000 normal_loss: 0.009053\n",
      "[019/00116] train_loss: 0.008852 kl_loss: 0.000000 normal_loss: 0.008852\n",
      "[020/00059] train_loss: 0.008999 kl_loss: 0.000000 normal_loss: 0.008999\n",
      "[021/00002] train_loss: 0.008837 kl_loss: 0.000000 normal_loss: 0.008837\n",
      "[021/00102] train_loss: 0.009063 kl_loss: 0.000000 normal_loss: 0.009063\n",
      "[022/00045] train_loss: 0.008688 kl_loss: 0.000000 normal_loss: 0.008688\n",
      "[022/00145] train_loss: 0.008926 kl_loss: 0.000000 normal_loss: 0.008926\n",
      "[023/00088] train_loss: 0.008787 kl_loss: 0.000000 normal_loss: 0.008787\n",
      "[024/00031] train_loss: 0.008585 kl_loss: 0.000000 normal_loss: 0.008585\n",
      "[024/00131] train_loss: 0.008889 kl_loss: 0.000000 normal_loss: 0.008889\n",
      "[025/00074] train_loss: 0.008487 kl_loss: 0.000000 normal_loss: 0.008487\n",
      "[026/00017] train_loss: 0.008674 kl_loss: 0.000000 normal_loss: 0.008674\n",
      "[026/00117] train_loss: 0.008555 kl_loss: 0.000000 normal_loss: 0.008555\n",
      "[027/00060] train_loss: 0.008676 kl_loss: 0.000000 normal_loss: 0.008676\n",
      "[028/00003] train_loss: 0.008895 kl_loss: 0.000000 normal_loss: 0.008895\n",
      "[028/00103] train_loss: 0.008696 kl_loss: 0.000000 normal_loss: 0.008696\n",
      "[029/00046] train_loss: 0.008727 kl_loss: 0.000000 normal_loss: 0.008727\n",
      "[029/00146] train_loss: 0.008677 kl_loss: 0.000000 normal_loss: 0.008677\n",
      "[030/00089] train_loss: 0.008579 kl_loss: 0.000000 normal_loss: 0.008579\n",
      "[031/00032] train_loss: 0.008555 kl_loss: 0.000000 normal_loss: 0.008555\n",
      "[031/00132] train_loss: 0.008569 kl_loss: 0.000000 normal_loss: 0.008569\n",
      "[032/00075] train_loss: 0.008544 kl_loss: 0.000000 normal_loss: 0.008544\n",
      "[033/00018] train_loss: 0.008249 kl_loss: 0.000000 normal_loss: 0.008249\n",
      "[033/00118] train_loss: 0.008058 kl_loss: 0.000000 normal_loss: 0.008058\n",
      "[034/00061] train_loss: 0.008148 kl_loss: 0.000000 normal_loss: 0.008148\n",
      "[035/00004] train_loss: 0.008735 kl_loss: 0.000000 normal_loss: 0.008735\n",
      "[035/00104] train_loss: 0.008700 kl_loss: 0.000000 normal_loss: 0.008700\n",
      "[036/00047] train_loss: 0.008477 kl_loss: 0.000000 normal_loss: 0.008477\n",
      "[036/00147] train_loss: 0.008539 kl_loss: 0.000000 normal_loss: 0.008539\n",
      "[037/00090] train_loss: 0.008530 kl_loss: 0.000000 normal_loss: 0.008530\n",
      "[038/00033] train_loss: 0.008526 kl_loss: 0.000000 normal_loss: 0.008526\n",
      "[038/00133] train_loss: 0.008662 kl_loss: 0.000000 normal_loss: 0.008662\n",
      "[039/00076] train_loss: 0.008210 kl_loss: 0.000000 normal_loss: 0.008210\n",
      "[040/00019] train_loss: 0.008562 kl_loss: 0.000000 normal_loss: 0.008562\n",
      "[040/00119] train_loss: 0.008379 kl_loss: 0.000000 normal_loss: 0.008379\n",
      "[041/00062] train_loss: 0.008061 kl_loss: 0.000000 normal_loss: 0.008061\n",
      "[042/00005] train_loss: 0.008466 kl_loss: 0.000000 normal_loss: 0.008466\n",
      "[042/00105] train_loss: 0.008464 kl_loss: 0.000000 normal_loss: 0.008464\n",
      "[043/00048] train_loss: 0.008400 kl_loss: 0.000000 normal_loss: 0.008400\n",
      "[043/00148] train_loss: 0.008381 kl_loss: 0.000000 normal_loss: 0.008381\n",
      "[044/00091] train_loss: 0.008068 kl_loss: 0.000000 normal_loss: 0.008068\n",
      "[045/00034] train_loss: 0.008273 kl_loss: 0.000000 normal_loss: 0.008273\n",
      "[045/00134] train_loss: 0.008360 kl_loss: 0.000000 normal_loss: 0.008360\n",
      "[046/00077] train_loss: 0.008281 kl_loss: 0.000000 normal_loss: 0.008281\n",
      "[047/00020] train_loss: 0.008030 kl_loss: 0.000000 normal_loss: 0.008030\n",
      "[047/00120] train_loss: 0.008056 kl_loss: 0.000000 normal_loss: 0.008056\n",
      "[048/00063] train_loss: 0.007948 kl_loss: 0.000000 normal_loss: 0.007948\n",
      "[049/00006] train_loss: 0.008273 kl_loss: 0.000000 normal_loss: 0.008273\n",
      "[049/00106] train_loss: 0.008204 kl_loss: 0.000000 normal_loss: 0.008204\n",
      "[050/00049] train_loss: 0.008137 kl_loss: 0.000000 normal_loss: 0.008137\n",
      "[050/00149] train_loss: 0.008109 kl_loss: 0.000000 normal_loss: 0.008109\n",
      "[051/00092] train_loss: 0.008246 kl_loss: 0.000000 normal_loss: 0.008246\n",
      "[052/00035] train_loss: 0.007944 kl_loss: 0.000000 normal_loss: 0.007944\n",
      "[052/00135] train_loss: 0.007801 kl_loss: 0.000000 normal_loss: 0.007801\n",
      "[053/00078] train_loss: 0.008261 kl_loss: 0.000000 normal_loss: 0.008261\n",
      "[054/00021] train_loss: 0.008163 kl_loss: 0.000000 normal_loss: 0.008163\n",
      "[054/00121] train_loss: 0.008145 kl_loss: 0.000000 normal_loss: 0.008145\n",
      "[055/00064] train_loss: 0.008056 kl_loss: 0.000000 normal_loss: 0.008056\n",
      "[056/00007] train_loss: 0.007822 kl_loss: 0.000000 normal_loss: 0.007822\n",
      "[056/00107] train_loss: 0.007879 kl_loss: 0.000000 normal_loss: 0.007879\n",
      "[057/00050] train_loss: 0.007680 kl_loss: 0.000000 normal_loss: 0.007680\n",
      "[057/00150] train_loss: 0.007895 kl_loss: 0.000000 normal_loss: 0.007895\n",
      "[058/00093] train_loss: 0.007995 kl_loss: 0.000000 normal_loss: 0.007995\n",
      "[059/00036] train_loss: 0.008038 kl_loss: 0.000000 normal_loss: 0.008038\n",
      "[059/00136] train_loss: 0.008004 kl_loss: 0.000000 normal_loss: 0.008004\n",
      "[060/00079] train_loss: 0.008100 kl_loss: 0.000000 normal_loss: 0.008100\n",
      "[061/00022] train_loss: 0.007916 kl_loss: 0.000000 normal_loss: 0.007916\n",
      "[061/00122] train_loss: 0.007865 kl_loss: 0.000000 normal_loss: 0.007865\n",
      "[062/00065] train_loss: 0.007961 kl_loss: 0.000000 normal_loss: 0.007961\n",
      "[063/00008] train_loss: 0.007891 kl_loss: 0.000000 normal_loss: 0.007891\n",
      "[063/00108] train_loss: 0.007570 kl_loss: 0.000000 normal_loss: 0.007570\n",
      "[064/00051] train_loss: 0.007582 kl_loss: 0.000000 normal_loss: 0.007582\n",
      "[064/00151] train_loss: 0.007596 kl_loss: 0.000000 normal_loss: 0.007596\n",
      "[065/00094] train_loss: 0.007939 kl_loss: 0.000000 normal_loss: 0.007939\n",
      "[066/00037] train_loss: 0.007901 kl_loss: 0.000000 normal_loss: 0.007901\n",
      "[066/00137] train_loss: 0.007740 kl_loss: 0.000000 normal_loss: 0.007740\n",
      "[067/00080] train_loss: 0.007760 kl_loss: 0.000000 normal_loss: 0.007760\n",
      "[068/00023] train_loss: 0.007935 kl_loss: 0.000000 normal_loss: 0.007935\n",
      "[068/00123] train_loss: 0.007745 kl_loss: 0.000000 normal_loss: 0.007745\n",
      "[069/00066] train_loss: 0.007743 kl_loss: 0.000000 normal_loss: 0.007743\n",
      "[070/00009] train_loss: 0.007463 kl_loss: 0.000000 normal_loss: 0.007463\n",
      "[070/00109] train_loss: 0.007455 kl_loss: 0.000000 normal_loss: 0.007455\n",
      "[071/00052] train_loss: 0.007281 kl_loss: 0.000000 normal_loss: 0.007281\n",
      "[071/00152] train_loss: 0.007579 kl_loss: 0.000000 normal_loss: 0.007579\n",
      "[072/00095] train_loss: 0.007840 kl_loss: 0.000000 normal_loss: 0.007840\n",
      "[073/00038] train_loss: 0.007948 kl_loss: 0.000000 normal_loss: 0.007948\n",
      "[073/00138] train_loss: 0.007725 kl_loss: 0.000000 normal_loss: 0.007725\n",
      "[074/00081] train_loss: 0.007367 kl_loss: 0.000000 normal_loss: 0.007367\n",
      "[075/00024] train_loss: 0.007596 kl_loss: 0.000000 normal_loss: 0.007596\n",
      "[075/00124] train_loss: 0.007277 kl_loss: 0.000000 normal_loss: 0.007277\n",
      "[076/00067] train_loss: 0.007395 kl_loss: 0.000000 normal_loss: 0.007395\n",
      "[077/00010] train_loss: 0.007750 kl_loss: 0.000000 normal_loss: 0.007750\n",
      "[077/00110] train_loss: 0.007646 kl_loss: 0.000000 normal_loss: 0.007646\n",
      "[078/00053] train_loss: 0.007326 kl_loss: 0.000000 normal_loss: 0.007326\n",
      "[078/00153] train_loss: 0.007623 kl_loss: 0.000000 normal_loss: 0.007623\n",
      "[079/00096] train_loss: 0.007582 kl_loss: 0.000000 normal_loss: 0.007582\n",
      "[080/00039] train_loss: 0.007535 kl_loss: 0.000000 normal_loss: 0.007535\n",
      "[080/00139] train_loss: 0.007732 kl_loss: 0.000000 normal_loss: 0.007732\n",
      "[081/00082] train_loss: 0.007193 kl_loss: 0.000000 normal_loss: 0.007193\n",
      "[082/00025] train_loss: 0.007349 kl_loss: 0.000000 normal_loss: 0.007349\n",
      "[082/00125] train_loss: 0.007401 kl_loss: 0.000000 normal_loss: 0.007401\n",
      "[083/00068] train_loss: 0.007704 kl_loss: 0.000000 normal_loss: 0.007704\n",
      "[084/00011] train_loss: 0.007356 kl_loss: 0.000000 normal_loss: 0.007356\n",
      "[084/00111] train_loss: 0.007533 kl_loss: 0.000000 normal_loss: 0.007533\n",
      "[085/00054] train_loss: 0.007521 kl_loss: 0.000000 normal_loss: 0.007521\n",
      "[085/00154] train_loss: 0.007374 kl_loss: 0.000000 normal_loss: 0.007374\n",
      "[086/00097] train_loss: 0.007499 kl_loss: 0.000000 normal_loss: 0.007499\n",
      "[087/00040] train_loss: 0.007664 kl_loss: 0.000000 normal_loss: 0.007664\n",
      "[087/00140] train_loss: 0.007713 kl_loss: 0.000000 normal_loss: 0.007713\n",
      "[088/00083] train_loss: 0.007569 kl_loss: 0.000000 normal_loss: 0.007569\n",
      "[089/00026] train_loss: 0.007512 kl_loss: 0.000000 normal_loss: 0.007512\n",
      "[089/00126] train_loss: 0.007559 kl_loss: 0.000000 normal_loss: 0.007559\n",
      "[090/00069] train_loss: 0.007192 kl_loss: 0.000000 normal_loss: 0.007192\n",
      "[091/00012] train_loss: 0.007431 kl_loss: 0.000000 normal_loss: 0.007431\n",
      "[091/00112] train_loss: 0.007308 kl_loss: 0.000000 normal_loss: 0.007308\n",
      "[092/00055] train_loss: 0.007362 kl_loss: 0.000000 normal_loss: 0.007362\n",
      "[092/00155] train_loss: 0.007621 kl_loss: 0.000000 normal_loss: 0.007621\n",
      "[093/00098] train_loss: 0.007546 kl_loss: 0.000000 normal_loss: 0.007546\n",
      "[094/00041] train_loss: 0.007538 kl_loss: 0.000000 normal_loss: 0.007538\n",
      "[094/00141] train_loss: 0.007415 kl_loss: 0.000000 normal_loss: 0.007415\n",
      "[095/00084] train_loss: 0.007334 kl_loss: 0.000000 normal_loss: 0.007334\n",
      "[096/00027] train_loss: 0.007364 kl_loss: 0.000000 normal_loss: 0.007364\n",
      "[096/00127] train_loss: 0.007320 kl_loss: 0.000000 normal_loss: 0.007320\n",
      "[097/00070] train_loss: 0.007326 kl_loss: 0.000000 normal_loss: 0.007326\n",
      "[098/00013] train_loss: 0.007280 kl_loss: 0.000000 normal_loss: 0.007280\n",
      "[098/00113] train_loss: 0.007194 kl_loss: 0.000000 normal_loss: 0.007194\n",
      "[099/00056] train_loss: 0.007384 kl_loss: 0.000000 normal_loss: 0.007384\n",
      "[099/00156] train_loss: 0.007328 kl_loss: 0.000000 normal_loss: 0.007328\n",
      "[100/00099] train_loss: 0.006028 kl_loss: 0.000000 normal_loss: 0.006028\n",
      "[101/00042] train_loss: 0.005783 kl_loss: 0.000000 normal_loss: 0.005783\n",
      "[101/00142] train_loss: 0.005590 kl_loss: 0.000000 normal_loss: 0.005590\n",
      "[102/00085] train_loss: 0.005685 kl_loss: 0.000000 normal_loss: 0.005685\n",
      "[103/00028] train_loss: 0.005534 kl_loss: 0.000000 normal_loss: 0.005534\n",
      "[103/00128] train_loss: 0.005658 kl_loss: 0.000000 normal_loss: 0.005658\n",
      "[104/00071] train_loss: 0.005610 kl_loss: 0.000000 normal_loss: 0.005610\n",
      "[105/00014] train_loss: 0.005576 kl_loss: 0.000000 normal_loss: 0.005576\n",
      "[105/00114] train_loss: 0.005721 kl_loss: 0.000000 normal_loss: 0.005721\n",
      "[106/00057] train_loss: 0.005520 kl_loss: 0.000000 normal_loss: 0.005520\n",
      "[107/00000] train_loss: 0.005603 kl_loss: 0.000000 normal_loss: 0.005603\n",
      "[107/00100] train_loss: 0.005623 kl_loss: 0.000000 normal_loss: 0.005623\n",
      "[108/00043] train_loss: 0.005761 kl_loss: 0.000000 normal_loss: 0.005761\n",
      "[108/00143] train_loss: 0.005689 kl_loss: 0.000000 normal_loss: 0.005689\n",
      "[109/00086] train_loss: 0.005737 kl_loss: 0.000000 normal_loss: 0.005737\n",
      "[110/00029] train_loss: 0.005731 kl_loss: 0.000000 normal_loss: 0.005731\n",
      "[110/00129] train_loss: 0.005767 kl_loss: 0.000000 normal_loss: 0.005767\n",
      "[111/00072] train_loss: 0.005792 kl_loss: 0.000000 normal_loss: 0.005792\n",
      "[112/00015] train_loss: 0.005683 kl_loss: 0.000000 normal_loss: 0.005683\n",
      "[112/00115] train_loss: 0.005505 kl_loss: 0.000000 normal_loss: 0.005505\n",
      "[113/00058] train_loss: 0.005549 kl_loss: 0.000000 normal_loss: 0.005549\n",
      "[114/00001] train_loss: 0.005500 kl_loss: 0.000000 normal_loss: 0.005500\n",
      "[114/00101] train_loss: 0.005755 kl_loss: 0.000000 normal_loss: 0.005755\n",
      "[115/00044] train_loss: 0.005773 kl_loss: 0.000000 normal_loss: 0.005773\n",
      "[115/00144] train_loss: 0.005815 kl_loss: 0.000000 normal_loss: 0.005815\n",
      "[116/00087] train_loss: 0.005634 kl_loss: 0.000000 normal_loss: 0.005634\n",
      "[117/00030] train_loss: 0.005635 kl_loss: 0.000000 normal_loss: 0.005635\n",
      "[117/00130] train_loss: 0.005523 kl_loss: 0.000000 normal_loss: 0.005523\n",
      "[118/00073] train_loss: 0.005641 kl_loss: 0.000000 normal_loss: 0.005641\n",
      "[119/00016] train_loss: 0.005594 kl_loss: 0.000000 normal_loss: 0.005594\n",
      "[119/00116] train_loss: 0.005733 kl_loss: 0.000000 normal_loss: 0.005733\n",
      "[120/00059] train_loss: 0.005708 kl_loss: 0.000000 normal_loss: 0.005708\n",
      "[121/00002] train_loss: 0.005503 kl_loss: 0.000000 normal_loss: 0.005503\n",
      "[121/00102] train_loss: 0.005639 kl_loss: 0.000000 normal_loss: 0.005639\n",
      "[122/00045] train_loss: 0.005630 kl_loss: 0.000000 normal_loss: 0.005630\n",
      "[122/00145] train_loss: 0.005653 kl_loss: 0.000000 normal_loss: 0.005653\n",
      "[123/00088] train_loss: 0.005789 kl_loss: 0.000000 normal_loss: 0.005789\n",
      "[124/00031] train_loss: 0.005617 kl_loss: 0.000000 normal_loss: 0.005617\n",
      "[124/00131] train_loss: 0.005652 kl_loss: 0.000000 normal_loss: 0.005652\n",
      "[125/00074] train_loss: 0.005618 kl_loss: 0.000000 normal_loss: 0.005618\n",
      "[126/00017] train_loss: 0.005656 kl_loss: 0.000000 normal_loss: 0.005656\n",
      "[126/00117] train_loss: 0.005554 kl_loss: 0.000000 normal_loss: 0.005554\n",
      "[127/00060] train_loss: 0.005579 kl_loss: 0.000000 normal_loss: 0.005579\n",
      "[128/00003] train_loss: 0.005638 kl_loss: 0.000000 normal_loss: 0.005638\n",
      "[128/00103] train_loss: 0.005519 kl_loss: 0.000000 normal_loss: 0.005519\n",
      "[129/00046] train_loss: 0.005546 kl_loss: 0.000000 normal_loss: 0.005546\n",
      "[129/00146] train_loss: 0.005419 kl_loss: 0.000000 normal_loss: 0.005419\n",
      "[130/00089] train_loss: 0.005513 kl_loss: 0.000000 normal_loss: 0.005513\n",
      "[131/00032] train_loss: 0.005636 kl_loss: 0.000000 normal_loss: 0.005636\n",
      "[131/00132] train_loss: 0.005708 kl_loss: 0.000000 normal_loss: 0.005708\n",
      "[132/00075] train_loss: 0.005607 kl_loss: 0.000000 normal_loss: 0.005607\n",
      "[133/00018] train_loss: 0.005697 kl_loss: 0.000000 normal_loss: 0.005697\n",
      "[133/00118] train_loss: 0.005561 kl_loss: 0.000000 normal_loss: 0.005561\n",
      "[134/00061] train_loss: 0.005559 kl_loss: 0.000000 normal_loss: 0.005559\n",
      "[135/00004] train_loss: 0.005497 kl_loss: 0.000000 normal_loss: 0.005497\n",
      "[135/00104] train_loss: 0.005496 kl_loss: 0.000000 normal_loss: 0.005496\n",
      "[136/00047] train_loss: 0.005471 kl_loss: 0.000000 normal_loss: 0.005471\n",
      "[136/00147] train_loss: 0.005570 kl_loss: 0.000000 normal_loss: 0.005570\n",
      "[137/00090] train_loss: 0.005525 kl_loss: 0.000000 normal_loss: 0.005525\n",
      "[138/00033] train_loss: 0.005596 kl_loss: 0.000000 normal_loss: 0.005596\n",
      "[138/00133] train_loss: 0.005489 kl_loss: 0.000000 normal_loss: 0.005489\n",
      "[139/00076] train_loss: 0.005613 kl_loss: 0.000000 normal_loss: 0.005613\n",
      "[140/00019] train_loss: 0.005526 kl_loss: 0.000000 normal_loss: 0.005526\n",
      "[140/00119] train_loss: 0.005388 kl_loss: 0.000000 normal_loss: 0.005388\n",
      "[141/00062] train_loss: 0.005530 kl_loss: 0.000000 normal_loss: 0.005530\n",
      "[142/00005] train_loss: 0.005553 kl_loss: 0.000000 normal_loss: 0.005553\n",
      "[142/00105] train_loss: 0.005492 kl_loss: 0.000000 normal_loss: 0.005492\n",
      "[143/00048] train_loss: 0.005425 kl_loss: 0.000000 normal_loss: 0.005425\n",
      "[143/00148] train_loss: 0.005542 kl_loss: 0.000000 normal_loss: 0.005542\n",
      "[144/00091] train_loss: 0.005524 kl_loss: 0.000000 normal_loss: 0.005524\n",
      "[145/00034] train_loss: 0.005553 kl_loss: 0.000000 normal_loss: 0.005553\n",
      "[145/00134] train_loss: 0.005516 kl_loss: 0.000000 normal_loss: 0.005516\n",
      "[146/00077] train_loss: 0.005297 kl_loss: 0.000000 normal_loss: 0.005297\n",
      "[147/00020] train_loss: 0.005459 kl_loss: 0.000000 normal_loss: 0.005459\n",
      "[147/00120] train_loss: 0.005444 kl_loss: 0.000000 normal_loss: 0.005444\n",
      "[148/00063] train_loss: 0.005462 kl_loss: 0.000000 normal_loss: 0.005462\n",
      "[149/00006] train_loss: 0.005445 kl_loss: 0.000000 normal_loss: 0.005445\n",
      "[149/00106] train_loss: 0.005383 kl_loss: 0.000000 normal_loss: 0.005383\n",
      "[150/00049] train_loss: 0.005657 kl_loss: 0.000000 normal_loss: 0.005657\n",
      "[150/00149] train_loss: 0.005583 kl_loss: 0.000000 normal_loss: 0.005583\n",
      "[151/00092] train_loss: 0.005540 kl_loss: 0.000000 normal_loss: 0.005540\n",
      "[152/00035] train_loss: 0.005524 kl_loss: 0.000000 normal_loss: 0.005524\n",
      "[152/00135] train_loss: 0.005546 kl_loss: 0.000000 normal_loss: 0.005546\n",
      "[153/00078] train_loss: 0.005523 kl_loss: 0.000000 normal_loss: 0.005523\n",
      "[154/00021] train_loss: 0.005522 kl_loss: 0.000000 normal_loss: 0.005522\n",
      "[154/00121] train_loss: 0.005432 kl_loss: 0.000000 normal_loss: 0.005432\n",
      "[155/00064] train_loss: 0.005532 kl_loss: 0.000000 normal_loss: 0.005532\n",
      "[156/00007] train_loss: 0.005477 kl_loss: 0.000000 normal_loss: 0.005477\n",
      "[156/00107] train_loss: 0.005534 kl_loss: 0.000000 normal_loss: 0.005534\n",
      "[157/00050] train_loss: 0.005511 kl_loss: 0.000000 normal_loss: 0.005511\n",
      "[157/00150] train_loss: 0.005296 kl_loss: 0.000000 normal_loss: 0.005296\n",
      "[158/00093] train_loss: 0.005345 kl_loss: 0.000000 normal_loss: 0.005345\n",
      "[159/00036] train_loss: 0.005453 kl_loss: 0.000000 normal_loss: 0.005453\n",
      "[159/00136] train_loss: 0.005390 kl_loss: 0.000000 normal_loss: 0.005390\n",
      "[160/00079] train_loss: 0.005469 kl_loss: 0.000000 normal_loss: 0.005469\n",
      "[161/00022] train_loss: 0.005409 kl_loss: 0.000000 normal_loss: 0.005409\n",
      "[161/00122] train_loss: 0.005454 kl_loss: 0.000000 normal_loss: 0.005454\n",
      "[162/00065] train_loss: 0.005500 kl_loss: 0.000000 normal_loss: 0.005500\n",
      "[163/00008] train_loss: 0.005456 kl_loss: 0.000000 normal_loss: 0.005456\n",
      "[163/00108] train_loss: 0.005356 kl_loss: 0.000000 normal_loss: 0.005356\n",
      "[164/00051] train_loss: 0.005320 kl_loss: 0.000000 normal_loss: 0.005320\n",
      "[164/00151] train_loss: 0.005496 kl_loss: 0.000000 normal_loss: 0.005496\n",
      "[165/00094] train_loss: 0.005422 kl_loss: 0.000000 normal_loss: 0.005422\n",
      "[166/00037] train_loss: 0.005311 kl_loss: 0.000000 normal_loss: 0.005311\n",
      "[166/00137] train_loss: 0.005423 kl_loss: 0.000000 normal_loss: 0.005423\n",
      "[167/00080] train_loss: 0.005470 kl_loss: 0.000000 normal_loss: 0.005470\n",
      "[168/00023] train_loss: 0.005452 kl_loss: 0.000000 normal_loss: 0.005452\n",
      "[168/00123] train_loss: 0.005417 kl_loss: 0.000000 normal_loss: 0.005417\n",
      "[169/00066] train_loss: 0.005342 kl_loss: 0.000000 normal_loss: 0.005342\n",
      "[170/00009] train_loss: 0.005386 kl_loss: 0.000000 normal_loss: 0.005386\n",
      "[170/00109] train_loss: 0.005368 kl_loss: 0.000000 normal_loss: 0.005368\n",
      "[171/00052] train_loss: 0.005444 kl_loss: 0.000000 normal_loss: 0.005444\n",
      "[171/00152] train_loss: 0.005413 kl_loss: 0.000000 normal_loss: 0.005413\n",
      "[172/00095] train_loss: 0.005426 kl_loss: 0.000000 normal_loss: 0.005426\n",
      "[173/00038] train_loss: 0.005405 kl_loss: 0.000000 normal_loss: 0.005405\n",
      "[173/00138] train_loss: 0.005400 kl_loss: 0.000000 normal_loss: 0.005400\n",
      "[174/00081] train_loss: 0.005384 kl_loss: 0.000000 normal_loss: 0.005384\n",
      "[175/00024] train_loss: 0.005388 kl_loss: 0.000000 normal_loss: 0.005388\n",
      "[175/00124] train_loss: 0.005323 kl_loss: 0.000000 normal_loss: 0.005323\n",
      "[176/00067] train_loss: 0.005317 kl_loss: 0.000000 normal_loss: 0.005317\n",
      "[177/00010] train_loss: 0.005234 kl_loss: 0.000000 normal_loss: 0.005234\n",
      "[177/00110] train_loss: 0.005418 kl_loss: 0.000000 normal_loss: 0.005418\n",
      "[178/00053] train_loss: 0.005342 kl_loss: 0.000000 normal_loss: 0.005342\n",
      "[178/00153] train_loss: 0.005391 kl_loss: 0.000000 normal_loss: 0.005391\n",
      "[179/00096] train_loss: 0.005398 kl_loss: 0.000000 normal_loss: 0.005398\n",
      "[180/00039] train_loss: 0.005326 kl_loss: 0.000000 normal_loss: 0.005326\n",
      "[180/00139] train_loss: 0.005387 kl_loss: 0.000000 normal_loss: 0.005387\n",
      "[181/00082] train_loss: 0.005300 kl_loss: 0.000000 normal_loss: 0.005300\n",
      "[182/00025] train_loss: 0.005357 kl_loss: 0.000000 normal_loss: 0.005357\n",
      "[182/00125] train_loss: 0.005345 kl_loss: 0.000000 normal_loss: 0.005345\n",
      "[183/00068] train_loss: 0.005247 kl_loss: 0.000000 normal_loss: 0.005247\n",
      "[184/00011] train_loss: 0.005246 kl_loss: 0.000000 normal_loss: 0.005246\n",
      "[184/00111] train_loss: 0.005353 kl_loss: 0.000000 normal_loss: 0.005353\n",
      "[185/00054] train_loss: 0.005486 kl_loss: 0.000000 normal_loss: 0.005486\n",
      "[185/00154] train_loss: 0.005316 kl_loss: 0.000000 normal_loss: 0.005316\n",
      "[186/00097] train_loss: 0.005399 kl_loss: 0.000000 normal_loss: 0.005399\n",
      "[187/00040] train_loss: 0.005421 kl_loss: 0.000000 normal_loss: 0.005421\n",
      "[187/00140] train_loss: 0.005371 kl_loss: 0.000000 normal_loss: 0.005371\n",
      "[188/00083] train_loss: 0.005306 kl_loss: 0.000000 normal_loss: 0.005306\n",
      "[189/00026] train_loss: 0.005160 kl_loss: 0.000000 normal_loss: 0.005160\n",
      "[189/00126] train_loss: 0.005381 kl_loss: 0.000000 normal_loss: 0.005381\n",
      "[190/00069] train_loss: 0.005372 kl_loss: 0.000000 normal_loss: 0.005372\n",
      "[191/00012] train_loss: 0.005401 kl_loss: 0.000000 normal_loss: 0.005401\n",
      "[191/00112] train_loss: 0.005331 kl_loss: 0.000000 normal_loss: 0.005331\n",
      "[192/00055] train_loss: 0.005298 kl_loss: 0.000000 normal_loss: 0.005298\n",
      "[192/00155] train_loss: 0.005331 kl_loss: 0.000000 normal_loss: 0.005331\n",
      "[193/00098] train_loss: 0.005310 kl_loss: 0.000000 normal_loss: 0.005310\n",
      "[194/00041] train_loss: 0.005193 kl_loss: 0.000000 normal_loss: 0.005193\n",
      "[194/00141] train_loss: 0.005403 kl_loss: 0.000000 normal_loss: 0.005403\n",
      "[195/00084] train_loss: 0.005336 kl_loss: 0.000000 normal_loss: 0.005336\n",
      "[196/00027] train_loss: 0.005365 kl_loss: 0.000000 normal_loss: 0.005365\n",
      "[196/00127] train_loss: 0.005255 kl_loss: 0.000000 normal_loss: 0.005255\n",
      "[197/00070] train_loss: 0.005266 kl_loss: 0.000000 normal_loss: 0.005266\n",
      "[198/00013] train_loss: 0.005390 kl_loss: 0.000000 normal_loss: 0.005390\n",
      "[198/00113] train_loss: 0.005199 kl_loss: 0.000000 normal_loss: 0.005199\n",
      "[199/00056] train_loss: 0.005252 kl_loss: 0.000000 normal_loss: 0.005252\n",
      "[199/00156] train_loss: 0.005336 kl_loss: 0.000000 normal_loss: 0.005336\n",
      "[200/00099] train_loss: 0.004685 kl_loss: 0.000000 normal_loss: 0.004685\n",
      "[201/00042] train_loss: 0.004634 kl_loss: 0.000000 normal_loss: 0.004634\n",
      "[201/00142] train_loss: 0.004499 kl_loss: 0.000000 normal_loss: 0.004499\n",
      "[202/00085] train_loss: 0.004606 kl_loss: 0.000000 normal_loss: 0.004606\n",
      "[203/00028] train_loss: 0.004485 kl_loss: 0.000000 normal_loss: 0.004485\n",
      "[203/00128] train_loss: 0.004533 kl_loss: 0.000000 normal_loss: 0.004533\n",
      "[204/00071] train_loss: 0.004623 kl_loss: 0.000000 normal_loss: 0.004623\n",
      "[205/00014] train_loss: 0.004706 kl_loss: 0.000000 normal_loss: 0.004706\n",
      "[205/00114] train_loss: 0.004601 kl_loss: 0.000000 normal_loss: 0.004601\n",
      "[206/00057] train_loss: 0.004535 kl_loss: 0.000000 normal_loss: 0.004535\n",
      "[207/00000] train_loss: 0.004561 kl_loss: 0.000000 normal_loss: 0.004561\n",
      "[207/00100] train_loss: 0.004449 kl_loss: 0.000000 normal_loss: 0.004449\n",
      "[208/00043] train_loss: 0.004570 kl_loss: 0.000000 normal_loss: 0.004570\n",
      "[208/00143] train_loss: 0.004542 kl_loss: 0.000000 normal_loss: 0.004542\n",
      "[209/00086] train_loss: 0.004541 kl_loss: 0.000000 normal_loss: 0.004541\n",
      "[210/00029] train_loss: 0.004577 kl_loss: 0.000000 normal_loss: 0.004577\n",
      "[210/00129] train_loss: 0.004627 kl_loss: 0.000000 normal_loss: 0.004627\n",
      "[211/00072] train_loss: 0.004626 kl_loss: 0.000000 normal_loss: 0.004626\n",
      "[212/00015] train_loss: 0.004528 kl_loss: 0.000000 normal_loss: 0.004528\n",
      "[212/00115] train_loss: 0.004656 kl_loss: 0.000000 normal_loss: 0.004656\n",
      "[213/00058] train_loss: 0.004522 kl_loss: 0.000000 normal_loss: 0.004522\n",
      "[214/00001] train_loss: 0.004586 kl_loss: 0.000000 normal_loss: 0.004586\n",
      "[214/00101] train_loss: 0.004591 kl_loss: 0.000000 normal_loss: 0.004591\n",
      "[215/00044] train_loss: 0.004573 kl_loss: 0.000000 normal_loss: 0.004573\n",
      "[215/00144] train_loss: 0.004574 kl_loss: 0.000000 normal_loss: 0.004574\n",
      "[216/00087] train_loss: 0.004590 kl_loss: 0.000000 normal_loss: 0.004590\n",
      "[217/00030] train_loss: 0.004493 kl_loss: 0.000000 normal_loss: 0.004493\n",
      "[217/00130] train_loss: 0.004524 kl_loss: 0.000000 normal_loss: 0.004524\n",
      "[218/00073] train_loss: 0.004514 kl_loss: 0.000000 normal_loss: 0.004514\n",
      "[219/00016] train_loss: 0.004645 kl_loss: 0.000000 normal_loss: 0.004645\n",
      "[219/00116] train_loss: 0.004577 kl_loss: 0.000000 normal_loss: 0.004577\n",
      "[220/00059] train_loss: 0.004568 kl_loss: 0.000000 normal_loss: 0.004568\n",
      "[221/00002] train_loss: 0.004598 kl_loss: 0.000000 normal_loss: 0.004598\n",
      "[221/00102] train_loss: 0.004583 kl_loss: 0.000000 normal_loss: 0.004583\n",
      "[222/00045] train_loss: 0.004555 kl_loss: 0.000000 normal_loss: 0.004555\n",
      "[222/00145] train_loss: 0.004488 kl_loss: 0.000000 normal_loss: 0.004488\n",
      "[223/00088] train_loss: 0.004526 kl_loss: 0.000000 normal_loss: 0.004526\n",
      "[224/00031] train_loss: 0.004593 kl_loss: 0.000000 normal_loss: 0.004593\n",
      "[224/00131] train_loss: 0.004566 kl_loss: 0.000000 normal_loss: 0.004566\n",
      "[225/00074] train_loss: 0.004572 kl_loss: 0.000000 normal_loss: 0.004572\n",
      "[226/00017] train_loss: 0.004545 kl_loss: 0.000000 normal_loss: 0.004545\n",
      "[226/00117] train_loss: 0.004587 kl_loss: 0.000000 normal_loss: 0.004587\n",
      "[227/00060] train_loss: 0.004621 kl_loss: 0.000000 normal_loss: 0.004621\n",
      "[228/00003] train_loss: 0.004536 kl_loss: 0.000000 normal_loss: 0.004536\n",
      "[228/00103] train_loss: 0.004576 kl_loss: 0.000000 normal_loss: 0.004576\n",
      "[229/00046] train_loss: 0.004481 kl_loss: 0.000000 normal_loss: 0.004481\n",
      "[229/00146] train_loss: 0.004568 kl_loss: 0.000000 normal_loss: 0.004568\n",
      "[230/00089] train_loss: 0.004528 kl_loss: 0.000000 normal_loss: 0.004528\n",
      "[231/00032] train_loss: 0.004552 kl_loss: 0.000000 normal_loss: 0.004552\n",
      "[231/00132] train_loss: 0.004427 kl_loss: 0.000000 normal_loss: 0.004427\n",
      "[232/00075] train_loss: 0.004520 kl_loss: 0.000000 normal_loss: 0.004520\n",
      "[233/00018] train_loss: 0.004489 kl_loss: 0.000000 normal_loss: 0.004489\n",
      "[233/00118] train_loss: 0.004492 kl_loss: 0.000000 normal_loss: 0.004492\n",
      "[234/00061] train_loss: 0.004507 kl_loss: 0.000000 normal_loss: 0.004507\n",
      "[235/00004] train_loss: 0.004557 kl_loss: 0.000000 normal_loss: 0.004557\n",
      "[235/00104] train_loss: 0.004539 kl_loss: 0.000000 normal_loss: 0.004539\n",
      "[236/00047] train_loss: 0.004556 kl_loss: 0.000000 normal_loss: 0.004556\n",
      "[236/00147] train_loss: 0.004499 kl_loss: 0.000000 normal_loss: 0.004499\n",
      "[237/00090] train_loss: 0.004555 kl_loss: 0.000000 normal_loss: 0.004555\n",
      "[238/00033] train_loss: 0.004562 kl_loss: 0.000000 normal_loss: 0.004562\n",
      "[238/00133] train_loss: 0.004476 kl_loss: 0.000000 normal_loss: 0.004476\n",
      "[239/00076] train_loss: 0.004579 kl_loss: 0.000000 normal_loss: 0.004579\n",
      "[240/00019] train_loss: 0.004525 kl_loss: 0.000000 normal_loss: 0.004525\n",
      "[240/00119] train_loss: 0.004490 kl_loss: 0.000000 normal_loss: 0.004490\n",
      "[241/00062] train_loss: 0.004485 kl_loss: 0.000000 normal_loss: 0.004485\n",
      "[242/00005] train_loss: 0.004538 kl_loss: 0.000000 normal_loss: 0.004538\n",
      "[242/00105] train_loss: 0.004473 kl_loss: 0.000000 normal_loss: 0.004473\n",
      "[243/00048] train_loss: 0.004553 kl_loss: 0.000000 normal_loss: 0.004553\n",
      "[243/00148] train_loss: 0.004552 kl_loss: 0.000000 normal_loss: 0.004552\n",
      "[244/00091] train_loss: 0.004554 kl_loss: 0.000000 normal_loss: 0.004554\n",
      "[245/00034] train_loss: 0.004494 kl_loss: 0.000000 normal_loss: 0.004494\n",
      "[245/00134] train_loss: 0.004508 kl_loss: 0.000000 normal_loss: 0.004508\n",
      "[246/00077] train_loss: 0.004478 kl_loss: 0.000000 normal_loss: 0.004478\n",
      "[247/00020] train_loss: 0.004564 kl_loss: 0.000000 normal_loss: 0.004564\n",
      "[247/00120] train_loss: 0.004459 kl_loss: 0.000000 normal_loss: 0.004459\n",
      "[248/00063] train_loss: 0.004541 kl_loss: 0.000000 normal_loss: 0.004541\n",
      "[249/00006] train_loss: 0.004476 kl_loss: 0.000000 normal_loss: 0.004476\n",
      "[249/00106] train_loss: 0.004439 kl_loss: 0.000000 normal_loss: 0.004439\n",
      "[250/00049] train_loss: 0.004465 kl_loss: 0.000000 normal_loss: 0.004465\n",
      "[250/00149] train_loss: 0.004487 kl_loss: 0.000000 normal_loss: 0.004487\n",
      "[251/00092] train_loss: 0.004548 kl_loss: 0.000000 normal_loss: 0.004548\n",
      "[252/00035] train_loss: 0.004469 kl_loss: 0.000000 normal_loss: 0.004469\n",
      "[252/00135] train_loss: 0.004488 kl_loss: 0.000000 normal_loss: 0.004488\n",
      "[253/00078] train_loss: 0.004393 kl_loss: 0.000000 normal_loss: 0.004393\n",
      "[254/00021] train_loss: 0.004532 kl_loss: 0.000000 normal_loss: 0.004532\n",
      "[254/00121] train_loss: 0.004541 kl_loss: 0.000000 normal_loss: 0.004541\n",
      "[255/00064] train_loss: 0.004491 kl_loss: 0.000000 normal_loss: 0.004491\n",
      "[256/00007] train_loss: 0.004507 kl_loss: 0.000000 normal_loss: 0.004507\n",
      "[256/00107] train_loss: 0.004407 kl_loss: 0.000000 normal_loss: 0.004407\n",
      "[257/00050] train_loss: 0.004455 kl_loss: 0.000000 normal_loss: 0.004455\n",
      "[257/00150] train_loss: 0.004493 kl_loss: 0.000000 normal_loss: 0.004493\n",
      "[258/00093] train_loss: 0.004392 kl_loss: 0.000000 normal_loss: 0.004392\n",
      "[259/00036] train_loss: 0.004509 kl_loss: 0.000000 normal_loss: 0.004509\n",
      "[259/00136] train_loss: 0.004480 kl_loss: 0.000000 normal_loss: 0.004480\n",
      "[260/00079] train_loss: 0.004484 kl_loss: 0.000000 normal_loss: 0.004484\n",
      "[261/00022] train_loss: 0.004515 kl_loss: 0.000000 normal_loss: 0.004515\n",
      "[261/00122] train_loss: 0.004461 kl_loss: 0.000000 normal_loss: 0.004461\n",
      "[262/00065] train_loss: 0.004533 kl_loss: 0.000000 normal_loss: 0.004533\n",
      "[263/00008] train_loss: 0.004414 kl_loss: 0.000000 normal_loss: 0.004414\n",
      "[263/00108] train_loss: 0.004434 kl_loss: 0.000000 normal_loss: 0.004434\n",
      "[264/00051] train_loss: 0.004490 kl_loss: 0.000000 normal_loss: 0.004490\n",
      "[264/00151] train_loss: 0.004462 kl_loss: 0.000000 normal_loss: 0.004462\n",
      "[265/00094] train_loss: 0.004501 kl_loss: 0.000000 normal_loss: 0.004501\n",
      "[266/00037] train_loss: 0.004405 kl_loss: 0.000000 normal_loss: 0.004405\n",
      "[266/00137] train_loss: 0.004532 kl_loss: 0.000000 normal_loss: 0.004532\n",
      "[267/00080] train_loss: 0.004498 kl_loss: 0.000000 normal_loss: 0.004498\n",
      "[268/00023] train_loss: 0.004446 kl_loss: 0.000000 normal_loss: 0.004446\n",
      "[268/00123] train_loss: 0.004383 kl_loss: 0.000000 normal_loss: 0.004383\n",
      "[269/00066] train_loss: 0.004510 kl_loss: 0.000000 normal_loss: 0.004510\n",
      "[270/00009] train_loss: 0.004516 kl_loss: 0.000000 normal_loss: 0.004516\n",
      "[270/00109] train_loss: 0.004481 kl_loss: 0.000000 normal_loss: 0.004481\n",
      "[271/00052] train_loss: 0.004356 kl_loss: 0.000000 normal_loss: 0.004356\n",
      "[271/00152] train_loss: 0.004462 kl_loss: 0.000000 normal_loss: 0.004462\n",
      "[272/00095] train_loss: 0.004419 kl_loss: 0.000000 normal_loss: 0.004419\n",
      "[273/00038] train_loss: 0.004484 kl_loss: 0.000000 normal_loss: 0.004484\n",
      "[273/00138] train_loss: 0.004456 kl_loss: 0.000000 normal_loss: 0.004456\n",
      "[274/00081] train_loss: 0.004452 kl_loss: 0.000000 normal_loss: 0.004452\n",
      "[275/00024] train_loss: 0.004467 kl_loss: 0.000000 normal_loss: 0.004467\n",
      "[275/00124] train_loss: 0.004500 kl_loss: 0.000000 normal_loss: 0.004500\n",
      "[276/00067] train_loss: 0.004490 kl_loss: 0.000000 normal_loss: 0.004490\n",
      "[277/00010] train_loss: 0.004462 kl_loss: 0.000000 normal_loss: 0.004462\n",
      "[277/00110] train_loss: 0.004412 kl_loss: 0.000000 normal_loss: 0.004412\n",
      "[278/00053] train_loss: 0.004465 kl_loss: 0.000000 normal_loss: 0.004465\n",
      "[278/00153] train_loss: 0.004464 kl_loss: 0.000000 normal_loss: 0.004464\n",
      "[279/00096] train_loss: 0.004345 kl_loss: 0.000000 normal_loss: 0.004345\n",
      "[280/00039] train_loss: 0.004494 kl_loss: 0.000000 normal_loss: 0.004494\n",
      "[280/00139] train_loss: 0.004489 kl_loss: 0.000000 normal_loss: 0.004489\n",
      "[281/00082] train_loss: 0.004411 kl_loss: 0.000000 normal_loss: 0.004411\n",
      "[282/00025] train_loss: 0.004385 kl_loss: 0.000000 normal_loss: 0.004385\n",
      "[282/00125] train_loss: 0.004501 kl_loss: 0.000000 normal_loss: 0.004501\n",
      "[283/00068] train_loss: 0.004409 kl_loss: 0.000000 normal_loss: 0.004409\n",
      "[284/00011] train_loss: 0.004477 kl_loss: 0.000000 normal_loss: 0.004477\n",
      "[284/00111] train_loss: 0.004399 kl_loss: 0.000000 normal_loss: 0.004399\n",
      "[285/00054] train_loss: 0.004449 kl_loss: 0.000000 normal_loss: 0.004449\n",
      "[285/00154] train_loss: 0.004472 kl_loss: 0.000000 normal_loss: 0.004472\n",
      "[286/00097] train_loss: 0.004476 kl_loss: 0.000000 normal_loss: 0.004476\n",
      "[287/00040] train_loss: 0.004381 kl_loss: 0.000000 normal_loss: 0.004381\n",
      "[287/00140] train_loss: 0.004442 kl_loss: 0.000000 normal_loss: 0.004442\n",
      "[288/00083] train_loss: 0.004461 kl_loss: 0.000000 normal_loss: 0.004461\n",
      "[289/00026] train_loss: 0.004404 kl_loss: 0.000000 normal_loss: 0.004404\n",
      "[289/00126] train_loss: 0.004469 kl_loss: 0.000000 normal_loss: 0.004469\n",
      "[290/00069] train_loss: 0.004473 kl_loss: 0.000000 normal_loss: 0.004473\n",
      "[291/00012] train_loss: 0.004380 kl_loss: 0.000000 normal_loss: 0.004380\n",
      "[291/00112] train_loss: 0.004287 kl_loss: 0.000000 normal_loss: 0.004287\n",
      "[292/00055] train_loss: 0.004470 kl_loss: 0.000000 normal_loss: 0.004470\n",
      "[292/00155] train_loss: 0.004472 kl_loss: 0.000000 normal_loss: 0.004472\n",
      "[293/00098] train_loss: 0.004415 kl_loss: 0.000000 normal_loss: 0.004415\n",
      "[294/00041] train_loss: 0.004454 kl_loss: 0.000000 normal_loss: 0.004454\n",
      "[294/00141] train_loss: 0.004272 kl_loss: 0.000000 normal_loss: 0.004272\n",
      "[295/00084] train_loss: 0.004386 kl_loss: 0.000000 normal_loss: 0.004386\n",
      "[296/00027] train_loss: 0.004480 kl_loss: 0.000000 normal_loss: 0.004480\n",
      "[296/00127] train_loss: 0.004419 kl_loss: 0.000000 normal_loss: 0.004419\n",
      "[297/00070] train_loss: 0.004339 kl_loss: 0.000000 normal_loss: 0.004339\n",
      "[298/00013] train_loss: 0.004410 kl_loss: 0.000000 normal_loss: 0.004410\n",
      "[298/00113] train_loss: 0.004435 kl_loss: 0.000000 normal_loss: 0.004435\n",
      "[299/00056] train_loss: 0.004405 kl_loss: 0.000000 normal_loss: 0.004405\n",
      "[299/00156] train_loss: 0.004427 kl_loss: 0.000000 normal_loss: 0.004427\n",
      "[300/00099] train_loss: 0.004151 kl_loss: 0.000000 normal_loss: 0.004151\n",
      "[301/00042] train_loss: 0.004141 kl_loss: 0.000000 normal_loss: 0.004141\n",
      "[301/00142] train_loss: 0.004093 kl_loss: 0.000000 normal_loss: 0.004093\n",
      "[302/00085] train_loss: 0.004187 kl_loss: 0.000000 normal_loss: 0.004187\n",
      "[303/00028] train_loss: 0.004095 kl_loss: 0.000000 normal_loss: 0.004095\n",
      "[303/00128] train_loss: 0.004081 kl_loss: 0.000000 normal_loss: 0.004081\n",
      "[304/00071] train_loss: 0.004093 kl_loss: 0.000000 normal_loss: 0.004093\n",
      "[305/00014] train_loss: 0.004102 kl_loss: 0.000000 normal_loss: 0.004102\n",
      "[305/00114] train_loss: 0.004118 kl_loss: 0.000000 normal_loss: 0.004118\n",
      "[306/00057] train_loss: 0.004151 kl_loss: 0.000000 normal_loss: 0.004151\n",
      "[307/00000] train_loss: 0.004082 kl_loss: 0.000000 normal_loss: 0.004082\n",
      "[307/00100] train_loss: 0.004150 kl_loss: 0.000000 normal_loss: 0.004150\n",
      "[308/00043] train_loss: 0.004089 kl_loss: 0.000000 normal_loss: 0.004089\n",
      "[308/00143] train_loss: 0.004126 kl_loss: 0.000000 normal_loss: 0.004126\n",
      "[309/00086] train_loss: 0.004148 kl_loss: 0.000000 normal_loss: 0.004148\n",
      "[310/00029] train_loss: 0.004098 kl_loss: 0.000000 normal_loss: 0.004098\n",
      "[310/00129] train_loss: 0.004103 kl_loss: 0.000000 normal_loss: 0.004103\n",
      "[311/00072] train_loss: 0.004133 kl_loss: 0.000000 normal_loss: 0.004133\n",
      "[312/00015] train_loss: 0.004094 kl_loss: 0.000000 normal_loss: 0.004094\n",
      "[312/00115] train_loss: 0.004098 kl_loss: 0.000000 normal_loss: 0.004098\n",
      "[313/00058] train_loss: 0.004125 kl_loss: 0.000000 normal_loss: 0.004125\n",
      "[314/00001] train_loss: 0.004118 kl_loss: 0.000000 normal_loss: 0.004118\n",
      "[314/00101] train_loss: 0.004101 kl_loss: 0.000000 normal_loss: 0.004101\n",
      "[315/00044] train_loss: 0.004105 kl_loss: 0.000000 normal_loss: 0.004105\n",
      "[315/00144] train_loss: 0.004113 kl_loss: 0.000000 normal_loss: 0.004113\n",
      "[316/00087] train_loss: 0.004146 kl_loss: 0.000000 normal_loss: 0.004146\n",
      "[317/00030] train_loss: 0.004055 kl_loss: 0.000000 normal_loss: 0.004055\n",
      "[317/00130] train_loss: 0.004104 kl_loss: 0.000000 normal_loss: 0.004104\n",
      "[318/00073] train_loss: 0.004123 kl_loss: 0.000000 normal_loss: 0.004123\n",
      "[319/00016] train_loss: 0.004119 kl_loss: 0.000000 normal_loss: 0.004119\n",
      "[319/00116] train_loss: 0.004118 kl_loss: 0.000000 normal_loss: 0.004118\n",
      "[320/00059] train_loss: 0.004137 kl_loss: 0.000000 normal_loss: 0.004137\n",
      "[321/00002] train_loss: 0.004111 kl_loss: 0.000000 normal_loss: 0.004111\n",
      "[321/00102] train_loss: 0.004067 kl_loss: 0.000000 normal_loss: 0.004067\n",
      "[322/00045] train_loss: 0.004138 kl_loss: 0.000000 normal_loss: 0.004138\n",
      "[322/00145] train_loss: 0.004123 kl_loss: 0.000000 normal_loss: 0.004123\n",
      "[323/00088] train_loss: 0.004134 kl_loss: 0.000000 normal_loss: 0.004134\n",
      "[324/00031] train_loss: 0.004067 kl_loss: 0.000000 normal_loss: 0.004067\n",
      "[324/00131] train_loss: 0.004072 kl_loss: 0.000000 normal_loss: 0.004072\n",
      "[325/00074] train_loss: 0.004105 kl_loss: 0.000000 normal_loss: 0.004105\n",
      "[326/00017] train_loss: 0.004115 kl_loss: 0.000000 normal_loss: 0.004115\n",
      "[326/00117] train_loss: 0.004108 kl_loss: 0.000000 normal_loss: 0.004108\n",
      "[327/00060] train_loss: 0.004056 kl_loss: 0.000000 normal_loss: 0.004056\n",
      "[328/00003] train_loss: 0.004088 kl_loss: 0.000000 normal_loss: 0.004088\n",
      "[328/00103] train_loss: 0.004072 kl_loss: 0.000000 normal_loss: 0.004072\n",
      "[329/00046] train_loss: 0.004083 kl_loss: 0.000000 normal_loss: 0.004083\n",
      "[329/00146] train_loss: 0.004066 kl_loss: 0.000000 normal_loss: 0.004066\n",
      "[330/00089] train_loss: 0.004114 kl_loss: 0.000000 normal_loss: 0.004114\n",
      "[331/00032] train_loss: 0.004021 kl_loss: 0.000000 normal_loss: 0.004021\n",
      "[331/00132] train_loss: 0.004082 kl_loss: 0.000000 normal_loss: 0.004082\n",
      "[332/00075] train_loss: 0.004129 kl_loss: 0.000000 normal_loss: 0.004129\n",
      "[333/00018] train_loss: 0.004061 kl_loss: 0.000000 normal_loss: 0.004061\n",
      "[333/00118] train_loss: 0.004110 kl_loss: 0.000000 normal_loss: 0.004110\n",
      "[334/00061] train_loss: 0.004106 kl_loss: 0.000000 normal_loss: 0.004106\n",
      "[335/00004] train_loss: 0.004050 kl_loss: 0.000000 normal_loss: 0.004050\n",
      "[335/00104] train_loss: 0.004130 kl_loss: 0.000000 normal_loss: 0.004130\n",
      "[336/00047] train_loss: 0.004077 kl_loss: 0.000000 normal_loss: 0.004077\n",
      "[336/00147] train_loss: 0.004090 kl_loss: 0.000000 normal_loss: 0.004090\n",
      "[337/00090] train_loss: 0.004089 kl_loss: 0.000000 normal_loss: 0.004089\n",
      "[338/00033] train_loss: 0.004066 kl_loss: 0.000000 normal_loss: 0.004066\n",
      "[338/00133] train_loss: 0.004071 kl_loss: 0.000000 normal_loss: 0.004071\n",
      "[339/00076] train_loss: 0.004090 kl_loss: 0.000000 normal_loss: 0.004090\n",
      "[340/00019] train_loss: 0.004067 kl_loss: 0.000000 normal_loss: 0.004067\n",
      "[340/00119] train_loss: 0.004039 kl_loss: 0.000000 normal_loss: 0.004039\n",
      "[341/00062] train_loss: 0.004066 kl_loss: 0.000000 normal_loss: 0.004066\n",
      "[342/00005] train_loss: 0.004056 kl_loss: 0.000000 normal_loss: 0.004056\n",
      "[342/00105] train_loss: 0.004069 kl_loss: 0.000000 normal_loss: 0.004069\n",
      "[343/00048] train_loss: 0.004109 kl_loss: 0.000000 normal_loss: 0.004109\n",
      "[343/00148] train_loss: 0.004086 kl_loss: 0.000000 normal_loss: 0.004086\n",
      "[344/00091] train_loss: 0.004079 kl_loss: 0.000000 normal_loss: 0.004079\n",
      "[345/00034] train_loss: 0.004029 kl_loss: 0.000000 normal_loss: 0.004029\n",
      "[345/00134] train_loss: 0.004026 kl_loss: 0.000000 normal_loss: 0.004026\n",
      "[346/00077] train_loss: 0.004117 kl_loss: 0.000000 normal_loss: 0.004117\n",
      "[347/00020] train_loss: 0.004032 kl_loss: 0.000000 normal_loss: 0.004032\n",
      "[347/00120] train_loss: 0.004104 kl_loss: 0.000000 normal_loss: 0.004104\n",
      "[348/00063] train_loss: 0.004042 kl_loss: 0.000000 normal_loss: 0.004042\n",
      "[349/00006] train_loss: 0.004104 kl_loss: 0.000000 normal_loss: 0.004104\n",
      "[349/00106] train_loss: 0.004078 kl_loss: 0.000000 normal_loss: 0.004078\n",
      "[350/00049] train_loss: 0.004052 kl_loss: 0.000000 normal_loss: 0.004052\n",
      "[350/00149] train_loss: 0.004070 kl_loss: 0.000000 normal_loss: 0.004070\n",
      "[351/00092] train_loss: 0.004032 kl_loss: 0.000000 normal_loss: 0.004032\n",
      "[352/00035] train_loss: 0.004054 kl_loss: 0.000000 normal_loss: 0.004054\n",
      "[352/00135] train_loss: 0.004045 kl_loss: 0.000000 normal_loss: 0.004045\n",
      "[353/00078] train_loss: 0.004053 kl_loss: 0.000000 normal_loss: 0.004053\n",
      "[354/00021] train_loss: 0.004067 kl_loss: 0.000000 normal_loss: 0.004067\n",
      "[354/00121] train_loss: 0.004075 kl_loss: 0.000000 normal_loss: 0.004075\n",
      "[355/00064] train_loss: 0.004138 kl_loss: 0.000000 normal_loss: 0.004138\n",
      "[356/00007] train_loss: 0.004027 kl_loss: 0.000000 normal_loss: 0.004027\n",
      "[356/00107] train_loss: 0.004066 kl_loss: 0.000000 normal_loss: 0.004066\n",
      "[357/00050] train_loss: 0.004071 kl_loss: 0.000000 normal_loss: 0.004071\n",
      "[357/00150] train_loss: 0.004021 kl_loss: 0.000000 normal_loss: 0.004021\n",
      "[358/00093] train_loss: 0.004123 kl_loss: 0.000000 normal_loss: 0.004123\n",
      "[359/00036] train_loss: 0.004046 kl_loss: 0.000000 normal_loss: 0.004046\n",
      "[359/00136] train_loss: 0.004073 kl_loss: 0.000000 normal_loss: 0.004073\n",
      "[360/00079] train_loss: 0.004035 kl_loss: 0.000000 normal_loss: 0.004035\n",
      "[361/00022] train_loss: 0.004037 kl_loss: 0.000000 normal_loss: 0.004037\n",
      "[361/00122] train_loss: 0.004069 kl_loss: 0.000000 normal_loss: 0.004069\n",
      "[362/00065] train_loss: 0.004022 kl_loss: 0.000000 normal_loss: 0.004022\n",
      "[363/00008] train_loss: 0.004051 kl_loss: 0.000000 normal_loss: 0.004051\n",
      "[363/00108] train_loss: 0.004047 kl_loss: 0.000000 normal_loss: 0.004047\n",
      "[364/00051] train_loss: 0.004123 kl_loss: 0.000000 normal_loss: 0.004123\n",
      "[364/00151] train_loss: 0.004042 kl_loss: 0.000000 normal_loss: 0.004042\n",
      "[365/00094] train_loss: 0.004065 kl_loss: 0.000000 normal_loss: 0.004065\n",
      "[366/00037] train_loss: 0.004043 kl_loss: 0.000000 normal_loss: 0.004043\n",
      "[366/00137] train_loss: 0.004041 kl_loss: 0.000000 normal_loss: 0.004041\n",
      "[367/00080] train_loss: 0.004012 kl_loss: 0.000000 normal_loss: 0.004012\n",
      "[368/00023] train_loss: 0.004087 kl_loss: 0.000000 normal_loss: 0.004087\n",
      "[368/00123] train_loss: 0.003990 kl_loss: 0.000000 normal_loss: 0.003990\n",
      "[369/00066] train_loss: 0.004042 kl_loss: 0.000000 normal_loss: 0.004042\n",
      "[370/00009] train_loss: 0.004006 kl_loss: 0.000000 normal_loss: 0.004006\n",
      "[370/00109] train_loss: 0.004078 kl_loss: 0.000000 normal_loss: 0.004078\n",
      "[371/00052] train_loss: 0.004029 kl_loss: 0.000000 normal_loss: 0.004029\n",
      "[371/00152] train_loss: 0.004065 kl_loss: 0.000000 normal_loss: 0.004065\n",
      "[372/00095] train_loss: 0.004004 kl_loss: 0.000000 normal_loss: 0.004004\n",
      "[373/00038] train_loss: 0.004088 kl_loss: 0.000000 normal_loss: 0.004088\n",
      "[373/00138] train_loss: 0.004001 kl_loss: 0.000000 normal_loss: 0.004001\n",
      "[374/00081] train_loss: 0.004105 kl_loss: 0.000000 normal_loss: 0.004105\n",
      "[375/00024] train_loss: 0.004001 kl_loss: 0.000000 normal_loss: 0.004001\n",
      "[375/00124] train_loss: 0.004048 kl_loss: 0.000000 normal_loss: 0.004048\n",
      "[376/00067] train_loss: 0.004045 kl_loss: 0.000000 normal_loss: 0.004045\n",
      "[377/00010] train_loss: 0.004051 kl_loss: 0.000000 normal_loss: 0.004051\n",
      "[377/00110] train_loss: 0.004003 kl_loss: 0.000000 normal_loss: 0.004003\n",
      "[378/00053] train_loss: 0.004067 kl_loss: 0.000000 normal_loss: 0.004067\n",
      "[378/00153] train_loss: 0.004023 kl_loss: 0.000000 normal_loss: 0.004023\n",
      "[379/00096] train_loss: 0.004035 kl_loss: 0.000000 normal_loss: 0.004035\n",
      "[380/00039] train_loss: 0.004081 kl_loss: 0.000000 normal_loss: 0.004081\n",
      "[380/00139] train_loss: 0.004025 kl_loss: 0.000000 normal_loss: 0.004025\n",
      "[381/00082] train_loss: 0.004016 kl_loss: 0.000000 normal_loss: 0.004016\n",
      "[382/00025] train_loss: 0.004044 kl_loss: 0.000000 normal_loss: 0.004044\n",
      "[382/00125] train_loss: 0.004037 kl_loss: 0.000000 normal_loss: 0.004037\n",
      "[383/00068] train_loss: 0.004074 kl_loss: 0.000000 normal_loss: 0.004074\n",
      "[384/00011] train_loss: 0.003988 kl_loss: 0.000000 normal_loss: 0.003988\n",
      "[384/00111] train_loss: 0.004056 kl_loss: 0.000000 normal_loss: 0.004056\n",
      "[385/00054] train_loss: 0.004053 kl_loss: 0.000000 normal_loss: 0.004053\n",
      "[385/00154] train_loss: 0.003989 kl_loss: 0.000000 normal_loss: 0.003989\n",
      "[386/00097] train_loss: 0.004037 kl_loss: 0.000000 normal_loss: 0.004037\n",
      "[387/00040] train_loss: 0.003999 kl_loss: 0.000000 normal_loss: 0.003999\n",
      "[387/00140] train_loss: 0.004025 kl_loss: 0.000000 normal_loss: 0.004025\n",
      "[388/00083] train_loss: 0.003987 kl_loss: 0.000000 normal_loss: 0.003987\n",
      "[389/00026] train_loss: 0.004091 kl_loss: 0.000000 normal_loss: 0.004091\n",
      "[389/00126] train_loss: 0.004093 kl_loss: 0.000000 normal_loss: 0.004093\n",
      "[390/00069] train_loss: 0.004039 kl_loss: 0.000000 normal_loss: 0.004039\n",
      "[391/00012] train_loss: 0.003998 kl_loss: 0.000000 normal_loss: 0.003998\n",
      "[391/00112] train_loss: 0.004036 kl_loss: 0.000000 normal_loss: 0.004036\n",
      "[392/00055] train_loss: 0.004032 kl_loss: 0.000000 normal_loss: 0.004032\n",
      "[392/00155] train_loss: 0.004002 kl_loss: 0.000000 normal_loss: 0.004002\n",
      "[393/00098] train_loss: 0.004052 kl_loss: 0.000000 normal_loss: 0.004052\n",
      "[394/00041] train_loss: 0.003988 kl_loss: 0.000000 normal_loss: 0.003988\n",
      "[394/00141] train_loss: 0.004035 kl_loss: 0.000000 normal_loss: 0.004035\n",
      "[395/00084] train_loss: 0.004038 kl_loss: 0.000000 normal_loss: 0.004038\n",
      "[396/00027] train_loss: 0.004024 kl_loss: 0.000000 normal_loss: 0.004024\n",
      "[396/00127] train_loss: 0.003980 kl_loss: 0.000000 normal_loss: 0.003980\n",
      "[397/00070] train_loss: 0.004043 kl_loss: 0.000000 normal_loss: 0.004043\n",
      "[398/00013] train_loss: 0.004015 kl_loss: 0.000000 normal_loss: 0.004015\n",
      "[398/00113] train_loss: 0.004021 kl_loss: 0.000000 normal_loss: 0.004021\n",
      "[399/00056] train_loss: 0.004021 kl_loss: 0.000000 normal_loss: 0.004021\n",
      "[399/00156] train_loss: 0.004023 kl_loss: 0.000000 normal_loss: 0.004023\n",
      "[400/00099] train_loss: 0.003891 kl_loss: 0.000000 normal_loss: 0.003891\n",
      "[401/00042] train_loss: 0.003914 kl_loss: 0.000000 normal_loss: 0.003914\n",
      "[401/00142] train_loss: 0.003907 kl_loss: 0.000000 normal_loss: 0.003907\n",
      "[402/00085] train_loss: 0.003908 kl_loss: 0.000000 normal_loss: 0.003908\n",
      "[403/00028] train_loss: 0.003908 kl_loss: 0.000000 normal_loss: 0.003908\n",
      "[403/00128] train_loss: 0.003892 kl_loss: 0.000000 normal_loss: 0.003892\n",
      "[404/00071] train_loss: 0.003896 kl_loss: 0.000000 normal_loss: 0.003896\n",
      "[405/00014] train_loss: 0.003937 kl_loss: 0.000000 normal_loss: 0.003937\n",
      "[405/00114] train_loss: 0.003930 kl_loss: 0.000000 normal_loss: 0.003930\n",
      "[406/00057] train_loss: 0.003904 kl_loss: 0.000000 normal_loss: 0.003904\n",
      "[407/00000] train_loss: 0.003902 kl_loss: 0.000000 normal_loss: 0.003902\n",
      "[407/00100] train_loss: 0.003918 kl_loss: 0.000000 normal_loss: 0.003918\n",
      "[408/00043] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[408/00143] train_loss: 0.003901 kl_loss: 0.000000 normal_loss: 0.003901\n",
      "[409/00086] train_loss: 0.003898 kl_loss: 0.000000 normal_loss: 0.003898\n",
      "[410/00029] train_loss: 0.003927 kl_loss: 0.000000 normal_loss: 0.003927\n",
      "[410/00129] train_loss: 0.003878 kl_loss: 0.000000 normal_loss: 0.003878\n",
      "[411/00072] train_loss: 0.003903 kl_loss: 0.000000 normal_loss: 0.003903\n",
      "[412/00015] train_loss: 0.003883 kl_loss: 0.000000 normal_loss: 0.003883\n",
      "[412/00115] train_loss: 0.003896 kl_loss: 0.000000 normal_loss: 0.003896\n",
      "[413/00058] train_loss: 0.003917 kl_loss: 0.000000 normal_loss: 0.003917\n",
      "[414/00001] train_loss: 0.003882 kl_loss: 0.000000 normal_loss: 0.003882\n",
      "[414/00101] train_loss: 0.003911 kl_loss: 0.000000 normal_loss: 0.003911\n",
      "[415/00044] train_loss: 0.003884 kl_loss: 0.000000 normal_loss: 0.003884\n",
      "[415/00144] train_loss: 0.003910 kl_loss: 0.000000 normal_loss: 0.003910\n",
      "[416/00087] train_loss: 0.003892 kl_loss: 0.000000 normal_loss: 0.003892\n",
      "[417/00030] train_loss: 0.003868 kl_loss: 0.000000 normal_loss: 0.003868\n",
      "[417/00130] train_loss: 0.003912 kl_loss: 0.000000 normal_loss: 0.003912\n",
      "[418/00073] train_loss: 0.003888 kl_loss: 0.000000 normal_loss: 0.003888\n",
      "[419/00016] train_loss: 0.003897 kl_loss: 0.000000 normal_loss: 0.003897\n",
      "[419/00116] train_loss: 0.003917 kl_loss: 0.000000 normal_loss: 0.003917\n",
      "[420/00059] train_loss: 0.003875 kl_loss: 0.000000 normal_loss: 0.003875\n",
      "[421/00002] train_loss: 0.003914 kl_loss: 0.000000 normal_loss: 0.003914\n",
      "[421/00102] train_loss: 0.003897 kl_loss: 0.000000 normal_loss: 0.003897\n",
      "[422/00045] train_loss: 0.003913 kl_loss: 0.000000 normal_loss: 0.003913\n",
      "[422/00145] train_loss: 0.003884 kl_loss: 0.000000 normal_loss: 0.003884\n",
      "[423/00088] train_loss: 0.003908 kl_loss: 0.000000 normal_loss: 0.003908\n",
      "[424/00031] train_loss: 0.003867 kl_loss: 0.000000 normal_loss: 0.003867\n",
      "[424/00131] train_loss: 0.003883 kl_loss: 0.000000 normal_loss: 0.003883\n",
      "[425/00074] train_loss: 0.003894 kl_loss: 0.000000 normal_loss: 0.003894\n",
      "[426/00017] train_loss: 0.003908 kl_loss: 0.000000 normal_loss: 0.003908\n",
      "[426/00117] train_loss: 0.003900 kl_loss: 0.000000 normal_loss: 0.003900\n",
      "[427/00060] train_loss: 0.003945 kl_loss: 0.000000 normal_loss: 0.003945\n",
      "[428/00003] train_loss: 0.003868 kl_loss: 0.000000 normal_loss: 0.003868\n",
      "[428/00103] train_loss: 0.003858 kl_loss: 0.000000 normal_loss: 0.003858\n",
      "[429/00046] train_loss: 0.003917 kl_loss: 0.000000 normal_loss: 0.003917\n",
      "[429/00146] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[430/00089] train_loss: 0.003869 kl_loss: 0.000000 normal_loss: 0.003869\n",
      "[431/00032] train_loss: 0.003882 kl_loss: 0.000000 normal_loss: 0.003882\n",
      "[431/00132] train_loss: 0.003884 kl_loss: 0.000000 normal_loss: 0.003884\n",
      "[432/00075] train_loss: 0.003885 kl_loss: 0.000000 normal_loss: 0.003885\n",
      "[433/00018] train_loss: 0.003874 kl_loss: 0.000000 normal_loss: 0.003874\n",
      "[433/00118] train_loss: 0.003879 kl_loss: 0.000000 normal_loss: 0.003879\n",
      "[434/00061] train_loss: 0.003921 kl_loss: 0.000000 normal_loss: 0.003921\n",
      "[435/00004] train_loss: 0.003905 kl_loss: 0.000000 normal_loss: 0.003905\n",
      "[435/00104] train_loss: 0.003900 kl_loss: 0.000000 normal_loss: 0.003900\n",
      "[436/00047] train_loss: 0.003898 kl_loss: 0.000000 normal_loss: 0.003898\n",
      "[436/00147] train_loss: 0.003877 kl_loss: 0.000000 normal_loss: 0.003877\n",
      "[437/00090] train_loss: 0.003905 kl_loss: 0.000000 normal_loss: 0.003905\n",
      "[438/00033] train_loss: 0.003845 kl_loss: 0.000000 normal_loss: 0.003845\n",
      "[438/00133] train_loss: 0.003881 kl_loss: 0.000000 normal_loss: 0.003881\n",
      "[439/00076] train_loss: 0.003937 kl_loss: 0.000000 normal_loss: 0.003937\n",
      "[440/00019] train_loss: 0.003893 kl_loss: 0.000000 normal_loss: 0.003893\n",
      "[440/00119] train_loss: 0.003893 kl_loss: 0.000000 normal_loss: 0.003893\n",
      "[441/00062] train_loss: 0.003952 kl_loss: 0.000000 normal_loss: 0.003952\n",
      "[442/00005] train_loss: 0.003893 kl_loss: 0.000000 normal_loss: 0.003893\n",
      "[442/00105] train_loss: 0.003895 kl_loss: 0.000000 normal_loss: 0.003895\n",
      "[443/00048] train_loss: 0.003886 kl_loss: 0.000000 normal_loss: 0.003886\n",
      "[443/00148] train_loss: 0.003863 kl_loss: 0.000000 normal_loss: 0.003863\n",
      "[444/00091] train_loss: 0.003895 kl_loss: 0.000000 normal_loss: 0.003895\n",
      "[445/00034] train_loss: 0.003890 kl_loss: 0.000000 normal_loss: 0.003890\n",
      "[445/00134] train_loss: 0.003853 kl_loss: 0.000000 normal_loss: 0.003853\n",
      "[446/00077] train_loss: 0.003862 kl_loss: 0.000000 normal_loss: 0.003862\n",
      "[447/00020] train_loss: 0.003922 kl_loss: 0.000000 normal_loss: 0.003922\n",
      "[447/00120] train_loss: 0.003909 kl_loss: 0.000000 normal_loss: 0.003909\n",
      "[448/00063] train_loss: 0.003852 kl_loss: 0.000000 normal_loss: 0.003852\n",
      "[449/00006] train_loss: 0.003860 kl_loss: 0.000000 normal_loss: 0.003860\n",
      "[449/00106] train_loss: 0.003874 kl_loss: 0.000000 normal_loss: 0.003874\n",
      "[450/00049] train_loss: 0.003881 kl_loss: 0.000000 normal_loss: 0.003881\n",
      "[450/00149] train_loss: 0.003866 kl_loss: 0.000000 normal_loss: 0.003866\n",
      "[451/00092] train_loss: 0.003913 kl_loss: 0.000000 normal_loss: 0.003913\n",
      "[452/00035] train_loss: 0.003931 kl_loss: 0.000000 normal_loss: 0.003931\n",
      "[452/00135] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[453/00078] train_loss: 0.003867 kl_loss: 0.000000 normal_loss: 0.003867\n",
      "[454/00021] train_loss: 0.003914 kl_loss: 0.000000 normal_loss: 0.003914\n",
      "[454/00121] train_loss: 0.003858 kl_loss: 0.000000 normal_loss: 0.003858\n",
      "[455/00064] train_loss: 0.003892 kl_loss: 0.000000 normal_loss: 0.003892\n",
      "[456/00007] train_loss: 0.003863 kl_loss: 0.000000 normal_loss: 0.003863\n",
      "[456/00107] train_loss: 0.003842 kl_loss: 0.000000 normal_loss: 0.003842\n",
      "[457/00050] train_loss: 0.003953 kl_loss: 0.000000 normal_loss: 0.003953\n",
      "[457/00150] train_loss: 0.003879 kl_loss: 0.000000 normal_loss: 0.003879\n",
      "[458/00093] train_loss: 0.003875 kl_loss: 0.000000 normal_loss: 0.003875\n",
      "[459/00036] train_loss: 0.003902 kl_loss: 0.000000 normal_loss: 0.003902\n",
      "[459/00136] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[460/00079] train_loss: 0.003883 kl_loss: 0.000000 normal_loss: 0.003883\n",
      "[461/00022] train_loss: 0.003874 kl_loss: 0.000000 normal_loss: 0.003874\n",
      "[461/00122] train_loss: 0.003911 kl_loss: 0.000000 normal_loss: 0.003911\n",
      "[462/00065] train_loss: 0.003853 kl_loss: 0.000000 normal_loss: 0.003853\n",
      "[463/00008] train_loss: 0.003880 kl_loss: 0.000000 normal_loss: 0.003880\n",
      "[463/00108] train_loss: 0.003906 kl_loss: 0.000000 normal_loss: 0.003906\n",
      "[464/00051] train_loss: 0.003858 kl_loss: 0.000000 normal_loss: 0.003858\n",
      "[464/00151] train_loss: 0.003850 kl_loss: 0.000000 normal_loss: 0.003850\n",
      "[465/00094] train_loss: 0.003901 kl_loss: 0.000000 normal_loss: 0.003901\n",
      "[466/00037] train_loss: 0.003843 kl_loss: 0.000000 normal_loss: 0.003843\n",
      "[466/00137] train_loss: 0.003881 kl_loss: 0.000000 normal_loss: 0.003881\n",
      "[467/00080] train_loss: 0.003864 kl_loss: 0.000000 normal_loss: 0.003864\n",
      "[468/00023] train_loss: 0.003864 kl_loss: 0.000000 normal_loss: 0.003864\n",
      "[468/00123] train_loss: 0.003869 kl_loss: 0.000000 normal_loss: 0.003869\n",
      "[469/00066] train_loss: 0.003890 kl_loss: 0.000000 normal_loss: 0.003890\n",
      "[470/00009] train_loss: 0.003841 kl_loss: 0.000000 normal_loss: 0.003841\n",
      "[470/00109] train_loss: 0.003883 kl_loss: 0.000000 normal_loss: 0.003883\n",
      "[471/00052] train_loss: 0.003865 kl_loss: 0.000000 normal_loss: 0.003865\n",
      "[471/00152] train_loss: 0.003890 kl_loss: 0.000000 normal_loss: 0.003890\n",
      "[472/00095] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[473/00038] train_loss: 0.003839 kl_loss: 0.000000 normal_loss: 0.003839\n",
      "[473/00138] train_loss: 0.003868 kl_loss: 0.000000 normal_loss: 0.003868\n",
      "[474/00081] train_loss: 0.003889 kl_loss: 0.000000 normal_loss: 0.003889\n",
      "[475/00024] train_loss: 0.003849 kl_loss: 0.000000 normal_loss: 0.003849\n",
      "[475/00124] train_loss: 0.003866 kl_loss: 0.000000 normal_loss: 0.003866\n",
      "[476/00067] train_loss: 0.003842 kl_loss: 0.000000 normal_loss: 0.003842\n",
      "[477/00010] train_loss: 0.003858 kl_loss: 0.000000 normal_loss: 0.003858\n",
      "[477/00110] train_loss: 0.003863 kl_loss: 0.000000 normal_loss: 0.003863\n",
      "[478/00053] train_loss: 0.003860 kl_loss: 0.000000 normal_loss: 0.003860\n",
      "[478/00153] train_loss: 0.003884 kl_loss: 0.000000 normal_loss: 0.003884\n",
      "[479/00096] train_loss: 0.003829 kl_loss: 0.000000 normal_loss: 0.003829\n",
      "[480/00039] train_loss: 0.003852 kl_loss: 0.000000 normal_loss: 0.003852\n",
      "[480/00139] train_loss: 0.003886 kl_loss: 0.000000 normal_loss: 0.003886\n",
      "[481/00082] train_loss: 0.003894 kl_loss: 0.000000 normal_loss: 0.003894\n",
      "[482/00025] train_loss: 0.003856 kl_loss: 0.000000 normal_loss: 0.003856\n",
      "[482/00125] train_loss: 0.003904 kl_loss: 0.000000 normal_loss: 0.003904\n",
      "[483/00068] train_loss: 0.003833 kl_loss: 0.000000 normal_loss: 0.003833\n",
      "[484/00011] train_loss: 0.003867 kl_loss: 0.000000 normal_loss: 0.003867\n",
      "[484/00111] train_loss: 0.003857 kl_loss: 0.000000 normal_loss: 0.003857\n",
      "[485/00054] train_loss: 0.003884 kl_loss: 0.000000 normal_loss: 0.003884\n",
      "[485/00154] train_loss: 0.003868 kl_loss: 0.000000 normal_loss: 0.003868\n",
      "[486/00097] train_loss: 0.003858 kl_loss: 0.000000 normal_loss: 0.003858\n",
      "[487/00040] train_loss: 0.003876 kl_loss: 0.000000 normal_loss: 0.003876\n",
      "[487/00140] train_loss: 0.003866 kl_loss: 0.000000 normal_loss: 0.003866\n",
      "[488/00083] train_loss: 0.003894 kl_loss: 0.000000 normal_loss: 0.003894\n",
      "[489/00026] train_loss: 0.003819 kl_loss: 0.000000 normal_loss: 0.003819\n",
      "[489/00126] train_loss: 0.003859 kl_loss: 0.000000 normal_loss: 0.003859\n",
      "[490/00069] train_loss: 0.003851 kl_loss: 0.000000 normal_loss: 0.003851\n",
      "[491/00012] train_loss: 0.003859 kl_loss: 0.000000 normal_loss: 0.003859\n",
      "[491/00112] train_loss: 0.003844 kl_loss: 0.000000 normal_loss: 0.003844\n",
      "[492/00055] train_loss: 0.003856 kl_loss: 0.000000 normal_loss: 0.003856\n",
      "[492/00155] train_loss: 0.003853 kl_loss: 0.000000 normal_loss: 0.003853\n",
      "[493/00098] train_loss: 0.003847 kl_loss: 0.000000 normal_loss: 0.003847\n",
      "[494/00041] train_loss: 0.003867 kl_loss: 0.000000 normal_loss: 0.003867\n",
      "[494/00141] train_loss: 0.003840 kl_loss: 0.000000 normal_loss: 0.003840\n",
      "[495/00084] train_loss: 0.003868 kl_loss: 0.000000 normal_loss: 0.003868\n",
      "[496/00027] train_loss: 0.003857 kl_loss: 0.000000 normal_loss: 0.003857\n",
      "[496/00127] train_loss: 0.003839 kl_loss: 0.000000 normal_loss: 0.003839\n",
      "[497/00070] train_loss: 0.003836 kl_loss: 0.000000 normal_loss: 0.003836\n",
      "[498/00013] train_loss: 0.003831 kl_loss: 0.000000 normal_loss: 0.003831\n",
      "[498/00113] train_loss: 0.003869 kl_loss: 0.000000 normal_loss: 0.003869\n",
      "[499/00056] train_loss: 0.003831 kl_loss: 0.000000 normal_loss: 0.003831\n",
      "[499/00156] train_loss: 0.003857 kl_loss: 0.000000 normal_loss: 0.003857\n"
     ]
    }
   ],
   "source": [
    "# CAR AIRPLANE AD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'car_airplane_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.0,\n",
    "    'resume_ckpt': 'car_airplane_ad',\n",
    "    'filter_class': 'car_airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#                   #\n",
    "#    VISUALIZING    #\n",
    "#                   #\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "\n",
    "def visualize_dataset_sample(filter_class, index):\n",
    "    dataset = ShapeNet('train', filter_class = filter_class)\n",
    "    sample = dataset[index]\n",
    "    input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "    visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "\n",
    "def visualize_ad(experiment, index):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x = latent_vectors[index].unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_vad(experiment, index):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    log_vars = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x = latent_vectors[index]\n",
    "    Dist = dist.Normal(x, torch.exp(log_vars[index]))\n",
    "    x_vad = Dist.rsample().unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x_vad)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_vad_norm(experiment):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Sample\n",
    "    # Dist = dist.Normal(torch.zeros(256), torch.ones(256))\n",
    "    x_vad = torch.randn(256).unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x_vad)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_interpolation_ad(experiment, index1, index2, a1=0.5, a2=0.5):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x1 = latent_vectors[index1]\n",
    "    x2 = latent_vectors[index2]\n",
    "    x = (a1*x1 + a2*x2).unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339335a07f1b43489864992cd2a44003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 342\n",
    "index1 = 312\n",
    "index2 = 33\n",
    "a1 = 0.5\n",
    "a2 = 1 - a1\n",
    "# experiment = \"sofa_ad\"\n",
    "# filter_class = \"sofa\"\n",
    "experiment = \"airplane_vad\"\n",
    "# experiment2 = \"sofa_vad_0_03kl\"\n",
    "filter_class = \"car_airplane\"\n",
    "#-------\n",
    "# visualize_vad_norm(experiment2)\n",
    "visualize_vad_norm(experiment)\n",
    "# visualize_ad(experiment, index)\n",
    "#-------\n",
    "# visualize_vad_norm(experiment)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_ad(experiment2, index)\n",
    "# visualize_dataset_sample(filter_class, index)\n",
    "#-------\n",
    "# visualize_interpolation_ad(experiment, index1, index2, a1, a2)\n",
    "# visualize_ad(experiment, index1)\n",
    "# visualize_ad(experiment, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00862b34921449e983f816a3f5dffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc318834a3ea4d7e8763838ee854af57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6e37737e142a18ba21a339a0c4f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "name_temp = 'sofa_ad'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "Dist = dist.Normal(latent_vectors[1002], torch.exp(log_vars[1002]))\n",
    "Dist2 = dist.Normal(latent_vectors[1000], torch.exp(log_vars[1000]))\n",
    "#x_vad_n = torch.randn(256, device='cpu')\n",
    "x_vad = Dist.rsample()#.unsqueeze(0)\n",
    "x_vad2 = Dist2.rsample()#.unsqueeze(0)\n",
    "\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((x_vad*a1 + x_vad2*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad2.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca351708eaa4addbfc5b3ae6957994c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d6af7f7ae4b758d8db8fa80246362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c37264fb2946c5989b9589d5329c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c0b1f2ca8f437286882ddd1b59d2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[5]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "# load model\n",
    "name_temp = 'generalization_airplane_no_vad'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "\n",
    "# load latent codes and latent variances\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "\n",
    "x_vad_n = x_vad_n.unsqueeze(0)\n",
    "output_meshes = model(latent_vectors[5].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(latent_vectors[105].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((latent_vectors[f1]*a1 + latent_vectors[f2]*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a9db08bae3858a47e0749c92dd3faa3fcc5b478149a89cc50b2ccab095deb2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
