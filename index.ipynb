{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 197376  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 131584  \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | decoder1     | Sequential       | 8389376 \n",
      "6  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "7  | decoder1.1   | BatchNorm3d      | 512     \n",
      "8  | decoder1.2   | ReLU             | 0       \n",
      "9  | decoder2     | Sequential       | 2097536 \n",
      "10 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "11 | decoder2.1   | BatchNorm3d      | 256     \n",
      "12 | decoder2.2   | ReLU             | 0       \n",
      "13 | decoder3     | Sequential       | 524480  \n",
      "14 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "15 | decoder3.1   | BatchNorm3d      | 128     \n",
      "16 | decoder3.2   | ReLU             | 0       \n",
      "17 | decoder4     | Sequential       | 4097    \n",
      "18 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "19 | TOTAL        | ThreeDEPNDecoder | 11212865\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1854\n",
      "Length of val set: 232\n",
      "Length of test set: 232\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class='lamp')\n",
    "val_dataset = ShapeNet('val', filter_class='lamp')\n",
    "test_dataset = ShapeNet('test', filter_class='lamp')\n",
    "\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 153540\n",
    "print(f'Length of test set: {len(test_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n",
      "Target DF: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7237be25c6543d9a88f4058de3bf490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = test_dataset[231]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "print(f'Target DF: {type(sample[\"target_df\"])}')  # expected output: <class 'numpy.ndarray'>\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#                #\n",
    "#    TRAINING    #\n",
    "#                #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 9994\n",
      "Training params: 3\n",
      "[000/00099] train_loss: 0.288746 kl_loss: 0.166894 normal_loss: 0.121852\n",
      "[000/00199] train_loss: 0.225641 kl_loss: 0.166353 normal_loss: 0.059288\n",
      "[000/00299] train_loss: 0.219455 kl_loss: 0.166727 normal_loss: 0.052728\n",
      "[001/00086] train_loss: 0.176847 kl_loss: 0.128196 normal_loss: 0.048651\n",
      "[001/00186] train_loss: 0.170828 kl_loss: 0.122256 normal_loss: 0.048573\n",
      "[001/00286] train_loss: 0.169678 kl_loss: 0.122033 normal_loss: 0.047645\n",
      "[002/00073] train_loss: 0.142393 kl_loss: 0.094651 normal_loss: 0.047742\n",
      "[002/00173] train_loss: 0.130873 kl_loss: 0.084667 normal_loss: 0.046206\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39moverfit\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=21'>22</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=22'>23</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000005?line=23'>24</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# OVERFIT\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 4,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 1,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car_airplane',\n",
    "    'decoder_var' : True\n",
    "\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4045\n",
      "Training params: 3\n",
      "[001/00035] train_loss: 0.103237 kl_loss: 0.163130 normal_loss: 0.098343\n",
      "[003/00007] train_loss: 0.048654 kl_loss: 0.136225 normal_loss: 0.044567\n",
      "[004/00043] train_loss: 0.040166 kl_loss: 0.099543 normal_loss: 0.037180\n",
      "[006/00015] train_loss: 0.036385 kl_loss: 0.077942 normal_loss: 0.034047\n",
      "[007/00051] train_loss: 0.034134 kl_loss: 0.067799 normal_loss: 0.032100\n",
      "[009/00023] train_loss: 0.033282 kl_loss: 0.061263 normal_loss: 0.031444\n",
      "[010/00059] train_loss: 0.032224 kl_loss: 0.054992 normal_loss: 0.030574\n",
      "[012/00031] train_loss: 0.031978 kl_loss: 0.048546 normal_loss: 0.030522\n",
      "[014/00003] train_loss: 0.030175 kl_loss: 0.044901 normal_loss: 0.028828\n",
      "[015/00039] train_loss: 0.029017 kl_loss: 0.042187 normal_loss: 0.027751\n",
      "[017/00011] train_loss: 0.028610 kl_loss: 0.041694 normal_loss: 0.027360\n",
      "[018/00047] train_loss: 0.027759 kl_loss: 0.043558 normal_loss: 0.026452\n",
      "[020/00019] train_loss: 0.025344 kl_loss: 0.044607 normal_loss: 0.024005\n",
      "[021/00055] train_loss: 0.022335 kl_loss: 0.043441 normal_loss: 0.021032\n",
      "[023/00027] train_loss: 0.022099 kl_loss: 0.041347 normal_loss: 0.020859\n",
      "[024/00063] train_loss: 0.021862 kl_loss: 0.039888 normal_loss: 0.020666\n",
      "[026/00035] train_loss: 0.021241 kl_loss: 0.039273 normal_loss: 0.020063\n",
      "[028/00007] train_loss: 0.021642 kl_loss: 0.039366 normal_loss: 0.020461\n",
      "[029/00043] train_loss: 0.021308 kl_loss: 0.039634 normal_loss: 0.020119\n",
      "[031/00015] train_loss: 0.021290 kl_loss: 0.040220 normal_loss: 0.020084\n",
      "[032/00051] train_loss: 0.020979 kl_loss: 0.041349 normal_loss: 0.019739\n",
      "[034/00023] train_loss: 0.020941 kl_loss: 0.041863 normal_loss: 0.019685\n",
      "[035/00059] train_loss: 0.020609 kl_loss: 0.042589 normal_loss: 0.019331\n",
      "[037/00031] train_loss: 0.020612 kl_loss: 0.043085 normal_loss: 0.019320\n",
      "[039/00003] train_loss: 0.020472 kl_loss: 0.043539 normal_loss: 0.019166\n",
      "[040/00039] train_loss: 0.019304 kl_loss: 0.044376 normal_loss: 0.017973\n",
      "[042/00011] train_loss: 0.018119 kl_loss: 0.044019 normal_loss: 0.016798\n",
      "[043/00047] train_loss: 0.018201 kl_loss: 0.043702 normal_loss: 0.016890\n",
      "[045/00019] train_loss: 0.018347 kl_loss: 0.043284 normal_loss: 0.017048\n",
      "[046/00055] train_loss: 0.018024 kl_loss: 0.042676 normal_loss: 0.016743\n",
      "[048/00027] train_loss: 0.017840 kl_loss: 0.042858 normal_loss: 0.016554\n",
      "[049/00063] train_loss: 0.018130 kl_loss: 0.042549 normal_loss: 0.016853\n",
      "[051/00035] train_loss: 0.018096 kl_loss: 0.042407 normal_loss: 0.016824\n",
      "[053/00007] train_loss: 0.017944 kl_loss: 0.042347 normal_loss: 0.016674\n",
      "[054/00043] train_loss: 0.017718 kl_loss: 0.042230 normal_loss: 0.016451\n",
      "[056/00015] train_loss: 0.017897 kl_loss: 0.042456 normal_loss: 0.016624\n",
      "[057/00051] train_loss: 0.017836 kl_loss: 0.042320 normal_loss: 0.016566\n",
      "[059/00023] train_loss: 0.017770 kl_loss: 0.042581 normal_loss: 0.016493\n",
      "[060/00059] train_loss: 0.017254 kl_loss: 0.042510 normal_loss: 0.015979\n",
      "[062/00031] train_loss: 0.016665 kl_loss: 0.042568 normal_loss: 0.015388\n",
      "[064/00003] train_loss: 0.016629 kl_loss: 0.042468 normal_loss: 0.015355\n",
      "[065/00039] train_loss: 0.016572 kl_loss: 0.042286 normal_loss: 0.015303\n",
      "[067/00011] train_loss: 0.016652 kl_loss: 0.042392 normal_loss: 0.015381\n",
      "[068/00047] train_loss: 0.016650 kl_loss: 0.042151 normal_loss: 0.015386\n",
      "[070/00019] train_loss: 0.016510 kl_loss: 0.042225 normal_loss: 0.015243\n",
      "[071/00055] train_loss: 0.016530 kl_loss: 0.042032 normal_loss: 0.015270\n",
      "[073/00027] train_loss: 0.016541 kl_loss: 0.042254 normal_loss: 0.015273\n",
      "[074/00063] train_loss: 0.016474 kl_loss: 0.041939 normal_loss: 0.015216\n",
      "[076/00035] train_loss: 0.016429 kl_loss: 0.042005 normal_loss: 0.015168\n",
      "[078/00007] train_loss: 0.016395 kl_loss: 0.041905 normal_loss: 0.015138\n",
      "[079/00043] train_loss: 0.016495 kl_loss: 0.042114 normal_loss: 0.015232\n",
      "[081/00015] train_loss: 0.016153 kl_loss: 0.042122 normal_loss: 0.014889\n",
      "[082/00051] train_loss: 0.015943 kl_loss: 0.041982 normal_loss: 0.014684\n",
      "[084/00023] train_loss: 0.015985 kl_loss: 0.041922 normal_loss: 0.014727\n",
      "[085/00059] train_loss: 0.015966 kl_loss: 0.042194 normal_loss: 0.014700\n",
      "[087/00031] train_loss: 0.015800 kl_loss: 0.041820 normal_loss: 0.014546\n",
      "[089/00003] train_loss: 0.016019 kl_loss: 0.042170 normal_loss: 0.014754\n",
      "[090/00039] train_loss: 0.015857 kl_loss: 0.041838 normal_loss: 0.014602\n",
      "[092/00011] train_loss: 0.016077 kl_loss: 0.042330 normal_loss: 0.014807\n",
      "[093/00047] train_loss: 0.015842 kl_loss: 0.041717 normal_loss: 0.014590\n",
      "[095/00019] train_loss: 0.015895 kl_loss: 0.041949 normal_loss: 0.014637\n",
      "[096/00055] train_loss: 0.015789 kl_loss: 0.041939 normal_loss: 0.014531\n",
      "[098/00027] train_loss: 0.015964 kl_loss: 0.042005 normal_loss: 0.014704\n",
      "[099/00063] train_loss: 0.015880 kl_loss: 0.041877 normal_loss: 0.014624\n",
      "[101/00035] train_loss: 0.015669 kl_loss: 0.041745 normal_loss: 0.014417\n",
      "[103/00007] train_loss: 0.015672 kl_loss: 0.041961 normal_loss: 0.014413\n",
      "[104/00043] train_loss: 0.015643 kl_loss: 0.041923 normal_loss: 0.014385\n",
      "[106/00015] train_loss: 0.015614 kl_loss: 0.041684 normal_loss: 0.014364\n",
      "[107/00051] train_loss: 0.015721 kl_loss: 0.041975 normal_loss: 0.014462\n",
      "[109/00023] train_loss: 0.015646 kl_loss: 0.041880 normal_loss: 0.014390\n",
      "[110/00059] train_loss: 0.015551 kl_loss: 0.041767 normal_loss: 0.014298\n",
      "[112/00031] train_loss: 0.015698 kl_loss: 0.041931 normal_loss: 0.014440\n",
      "[114/00003] train_loss: 0.015596 kl_loss: 0.041891 normal_loss: 0.014339\n",
      "[115/00039] train_loss: 0.015657 kl_loss: 0.041972 normal_loss: 0.014398\n",
      "[117/00011] train_loss: 0.015521 kl_loss: 0.041761 normal_loss: 0.014268\n",
      "[118/00047] train_loss: 0.015651 kl_loss: 0.041868 normal_loss: 0.014395\n",
      "[120/00019] train_loss: 0.015453 kl_loss: 0.041584 normal_loss: 0.014206\n",
      "[121/00055] train_loss: 0.015512 kl_loss: 0.041898 normal_loss: 0.014255\n",
      "[123/00027] train_loss: 0.015449 kl_loss: 0.041912 normal_loss: 0.014191\n",
      "[124/00063] train_loss: 0.015566 kl_loss: 0.041634 normal_loss: 0.014317\n",
      "[126/00035] train_loss: 0.015495 kl_loss: 0.041723 normal_loss: 0.014243\n",
      "[128/00007] train_loss: 0.015553 kl_loss: 0.041921 normal_loss: 0.014295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscripts\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mairplane_vad_final\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# AIRPLANE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_vad_final',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRPLANE AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAIR\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFA & CHAIR VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_chair_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa_chair',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFA VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_vad_0_03kl',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SOFA AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFA VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'car_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'car_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.0,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'car',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR AIRPLANE AD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'car_airplane_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.0,\n",
    "    'resume_ckpt': 'car_airplane_ad',\n",
    "    'filter_class': 'car_airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 3\n",
      "[001/00024] train_loss: 0.169897 kl_loss: 0.163592 normal_loss: 0.164990\n",
      "[002/00049] train_loss: 0.122329 kl_loss: 0.142284 normal_loss: 0.118060\n",
      "[003/00074] train_loss: 0.115421 kl_loss: 0.120233 normal_loss: 0.111814\n",
      "[005/00024] train_loss: 0.109729 kl_loss: 0.118276 normal_loss: 0.106181\n",
      "[006/00049] train_loss: 0.106850 kl_loss: 0.121355 normal_loss: 0.103209\n",
      "[007/00074] train_loss: 0.102989 kl_loss: 0.127613 normal_loss: 0.099161\n",
      "[009/00024] train_loss: 0.102100 kl_loss: 0.138114 normal_loss: 0.097956\n",
      "[010/00049] train_loss: 0.100053 kl_loss: 0.145995 normal_loss: 0.095673\n",
      "[011/00074] train_loss: 0.097527 kl_loss: 0.151523 normal_loss: 0.092981\n",
      "[013/00024] train_loss: 0.093813 kl_loss: 0.154968 normal_loss: 0.089164\n",
      "[014/00049] train_loss: 0.092380 kl_loss: 0.159106 normal_loss: 0.087606\n",
      "[015/00074] train_loss: 0.092439 kl_loss: 0.161559 normal_loss: 0.087593\n",
      "[017/00024] train_loss: 0.090308 kl_loss: 0.163313 normal_loss: 0.085409\n",
      "[018/00049] train_loss: 0.090362 kl_loss: 0.162851 normal_loss: 0.085476\n",
      "[019/00074] train_loss: 0.089260 kl_loss: 0.163612 normal_loss: 0.084352\n",
      "[021/00024] train_loss: 0.088311 kl_loss: 0.163389 normal_loss: 0.083410\n",
      "[022/00049] train_loss: 0.088382 kl_loss: 0.162608 normal_loss: 0.083504\n",
      "[023/00074] train_loss: 0.086948 kl_loss: 0.163260 normal_loss: 0.082050\n",
      "[025/00024] train_loss: 0.085538 kl_loss: 0.165480 normal_loss: 0.080573\n",
      "[026/00049] train_loss: 0.083728 kl_loss: 0.166664 normal_loss: 0.078729\n",
      "[027/00074] train_loss: 0.080997 kl_loss: 0.171634 normal_loss: 0.075848\n",
      "[029/00024] train_loss: 0.078677 kl_loss: 0.175932 normal_loss: 0.073399\n",
      "[030/00049] train_loss: 0.077176 kl_loss: 0.180938 normal_loss: 0.071747\n",
      "[031/00074] train_loss: 0.076603 kl_loss: 0.185691 normal_loss: 0.071032\n",
      "[033/00024] train_loss: 0.074314 kl_loss: 0.189890 normal_loss: 0.068618\n",
      "[034/00049] train_loss: 0.073765 kl_loss: 0.194659 normal_loss: 0.067925\n",
      "[035/00074] train_loss: 0.072475 kl_loss: 0.197490 normal_loss: 0.066550\n",
      "[037/00024] train_loss: 0.070638 kl_loss: 0.201203 normal_loss: 0.064602\n",
      "[038/00049] train_loss: 0.069992 kl_loss: 0.204718 normal_loss: 0.063851\n",
      "[039/00074] train_loss: 0.069053 kl_loss: 0.206156 normal_loss: 0.062868\n",
      "[041/00024] train_loss: 0.068480 kl_loss: 0.209203 normal_loss: 0.062204\n",
      "[042/00049] train_loss: 0.066599 kl_loss: 0.208932 normal_loss: 0.060331\n",
      "[043/00074] train_loss: 0.066786 kl_loss: 0.212078 normal_loss: 0.060424\n",
      "[045/00024] train_loss: 0.065672 kl_loss: 0.213452 normal_loss: 0.059269\n",
      "[046/00049] train_loss: 0.064527 kl_loss: 0.214835 normal_loss: 0.058082\n",
      "[047/00074] train_loss: 0.064520 kl_loss: 0.216063 normal_loss: 0.058039\n",
      "[049/00024] train_loss: 0.064130 kl_loss: 0.218276 normal_loss: 0.057582\n",
      "[050/00049] train_loss: 0.062356 kl_loss: 0.220028 normal_loss: 0.055755\n",
      "[051/00074] train_loss: 0.062656 kl_loss: 0.221254 normal_loss: 0.056019\n",
      "[053/00024] train_loss: 0.061003 kl_loss: 0.223472 normal_loss: 0.054299\n",
      "[054/00049] train_loss: 0.060717 kl_loss: 0.225406 normal_loss: 0.053955\n",
      "[055/00074] train_loss: 0.061282 kl_loss: 0.226481 normal_loss: 0.054487\n",
      "[057/00024] train_loss: 0.060621 kl_loss: 0.228882 normal_loss: 0.053754\n",
      "[058/00049] train_loss: 0.059152 kl_loss: 0.229164 normal_loss: 0.052277\n",
      "[059/00074] train_loss: 0.060152 kl_loss: 0.232026 normal_loss: 0.053191\n",
      "[061/00024] train_loss: 0.058450 kl_loss: 0.232710 normal_loss: 0.051469\n",
      "[062/00049] train_loss: 0.058449 kl_loss: 0.234915 normal_loss: 0.051401\n",
      "[063/00074] train_loss: 0.057587 kl_loss: 0.234126 normal_loss: 0.050563\n",
      "[065/00024] train_loss: 0.056652 kl_loss: 0.237016 normal_loss: 0.049542\n",
      "[066/00049] train_loss: 0.056865 kl_loss: 0.237430 normal_loss: 0.049742\n",
      "[067/00074] train_loss: 0.055740 kl_loss: 0.238296 normal_loss: 0.048591\n",
      "[069/00024] train_loss: 0.055799 kl_loss: 0.240037 normal_loss: 0.048598\n",
      "[070/00049] train_loss: 0.054817 kl_loss: 0.240732 normal_loss: 0.047595\n",
      "[071/00074] train_loss: 0.055988 kl_loss: 0.242310 normal_loss: 0.048719\n",
      "[073/00024] train_loss: 0.055703 kl_loss: 0.242931 normal_loss: 0.048415\n",
      "[074/00049] train_loss: 0.054422 kl_loss: 0.244224 normal_loss: 0.047095\n",
      "[075/00074] train_loss: 0.053943 kl_loss: 0.245074 normal_loss: 0.046591\n",
      "[077/00024] train_loss: 0.053153 kl_loss: 0.245657 normal_loss: 0.045783\n",
      "[078/00049] train_loss: 0.053176 kl_loss: 0.247023 normal_loss: 0.045766\n",
      "[079/00074] train_loss: 0.053004 kl_loss: 0.247196 normal_loss: 0.045588\n",
      "[081/00024] train_loss: 0.052043 kl_loss: 0.248368 normal_loss: 0.044592\n",
      "[082/00049] train_loss: 0.051699 kl_loss: 0.249504 normal_loss: 0.044214\n",
      "[083/00074] train_loss: 0.051955 kl_loss: 0.249471 normal_loss: 0.044471\n",
      "[085/00024] train_loss: 0.051990 kl_loss: 0.250860 normal_loss: 0.044464\n",
      "[086/00049] train_loss: 0.050427 kl_loss: 0.251019 normal_loss: 0.042897\n",
      "[087/00074] train_loss: 0.050324 kl_loss: 0.252375 normal_loss: 0.042752\n",
      "[089/00024] train_loss: 0.050370 kl_loss: 0.252581 normal_loss: 0.042792\n",
      "[090/00049] train_loss: 0.049991 kl_loss: 0.253231 normal_loss: 0.042394\n",
      "[091/00074] train_loss: 0.049893 kl_loss: 0.254904 normal_loss: 0.042246\n",
      "[093/00024] train_loss: 0.049019 kl_loss: 0.255129 normal_loss: 0.041365\n",
      "[094/00049] train_loss: 0.049359 kl_loss: 0.255213 normal_loss: 0.041703\n",
      "[095/00074] train_loss: 0.049446 kl_loss: 0.257685 normal_loss: 0.041715\n",
      "[097/00024] train_loss: 0.047484 kl_loss: 0.256873 normal_loss: 0.039778\n",
      "[098/00049] train_loss: 0.047958 kl_loss: 0.258735 normal_loss: 0.040196\n",
      "[099/00074] train_loss: 0.047696 kl_loss: 0.257871 normal_loss: 0.039960\n",
      "[101/00024] train_loss: 0.045426 kl_loss: 0.258870 normal_loss: 0.037660\n",
      "[102/00049] train_loss: 0.044125 kl_loss: 0.255892 normal_loss: 0.036448\n",
      "[103/00074] train_loss: 0.044230 kl_loss: 0.255383 normal_loss: 0.036568\n",
      "[105/00024] train_loss: 0.044030 kl_loss: 0.252339 normal_loss: 0.036460\n",
      "[106/00049] train_loss: 0.043511 kl_loss: 0.252446 normal_loss: 0.035937\n",
      "[107/00074] train_loss: 0.043041 kl_loss: 0.249571 normal_loss: 0.035554\n",
      "[109/00024] train_loss: 0.042493 kl_loss: 0.248705 normal_loss: 0.035032\n",
      "[110/00049] train_loss: 0.042309 kl_loss: 0.246951 normal_loss: 0.034901\n",
      "[111/00074] train_loss: 0.042160 kl_loss: 0.246264 normal_loss: 0.034772\n",
      "[113/00024] train_loss: 0.041811 kl_loss: 0.244136 normal_loss: 0.034487\n",
      "[114/00049] train_loss: 0.041553 kl_loss: 0.244529 normal_loss: 0.034217\n",
      "[115/00074] train_loss: 0.041827 kl_loss: 0.243473 normal_loss: 0.034523\n",
      "[117/00024] train_loss: 0.041140 kl_loss: 0.241922 normal_loss: 0.033882\n",
      "[118/00049] train_loss: 0.041670 kl_loss: 0.241009 normal_loss: 0.034439\n",
      "[119/00074] train_loss: 0.040949 kl_loss: 0.241398 normal_loss: 0.033707\n",
      "[121/00024] train_loss: 0.041652 kl_loss: 0.240041 normal_loss: 0.034451\n",
      "[122/00049] train_loss: 0.040852 kl_loss: 0.239528 normal_loss: 0.033666\n",
      "[123/00074] train_loss: 0.041268 kl_loss: 0.239724 normal_loss: 0.034076\n",
      "[125/00024] train_loss: 0.040251 kl_loss: 0.238177 normal_loss: 0.033105\n",
      "[126/00049] train_loss: 0.039985 kl_loss: 0.239015 normal_loss: 0.032814\n",
      "[127/00074] train_loss: 0.041332 kl_loss: 0.238738 normal_loss: 0.034170\n",
      "[129/00024] train_loss: 0.039628 kl_loss: 0.239222 normal_loss: 0.032452\n",
      "[130/00049] train_loss: 0.039579 kl_loss: 0.237090 normal_loss: 0.032466\n",
      "[131/00074] train_loss: 0.039738 kl_loss: 0.237636 normal_loss: 0.032609\n",
      "[133/00024] train_loss: 0.039460 kl_loss: 0.237479 normal_loss: 0.032335\n",
      "[134/00049] train_loss: 0.038906 kl_loss: 0.237311 normal_loss: 0.031787\n",
      "[135/00074] train_loss: 0.039968 kl_loss: 0.237965 normal_loss: 0.032829\n",
      "[137/00024] train_loss: 0.039355 kl_loss: 0.238113 normal_loss: 0.032212\n",
      "[138/00049] train_loss: 0.040330 kl_loss: 0.237060 normal_loss: 0.033219\n",
      "[139/00074] train_loss: 0.039840 kl_loss: 0.236983 normal_loss: 0.032731\n",
      "[141/00024] train_loss: 0.038423 kl_loss: 0.236904 normal_loss: 0.031316\n",
      "[142/00049] train_loss: 0.038412 kl_loss: 0.238042 normal_loss: 0.031271\n",
      "[143/00074] train_loss: 0.037705 kl_loss: 0.236326 normal_loss: 0.030615\n",
      "[145/00024] train_loss: 0.037984 kl_loss: 0.236683 normal_loss: 0.030883\n",
      "[146/00049] train_loss: 0.038176 kl_loss: 0.237531 normal_loss: 0.031050\n",
      "[147/00074] train_loss: 0.038347 kl_loss: 0.236867 normal_loss: 0.031241\n",
      "[149/00024] train_loss: 0.037608 kl_loss: 0.237407 normal_loss: 0.030485\n",
      "[150/00049] train_loss: 0.037708 kl_loss: 0.237418 normal_loss: 0.030585\n",
      "[151/00074] train_loss: 0.036954 kl_loss: 0.237051 normal_loss: 0.029843\n",
      "[153/00024] train_loss: 0.036898 kl_loss: 0.236923 normal_loss: 0.029790\n",
      "[154/00049] train_loss: 0.038110 kl_loss: 0.237243 normal_loss: 0.030992\n",
      "[155/00074] train_loss: 0.037524 kl_loss: 0.237901 normal_loss: 0.030387\n",
      "[157/00024] train_loss: 0.036467 kl_loss: 0.237276 normal_loss: 0.029349\n",
      "[158/00049] train_loss: 0.036825 kl_loss: 0.236991 normal_loss: 0.029715\n",
      "[159/00074] train_loss: 0.037139 kl_loss: 0.238904 normal_loss: 0.029972\n",
      "[161/00024] train_loss: 0.037576 kl_loss: 0.237842 normal_loss: 0.030441\n",
      "[162/00049] train_loss: 0.036577 kl_loss: 0.237091 normal_loss: 0.029464\n",
      "[163/00074] train_loss: 0.036218 kl_loss: 0.238226 normal_loss: 0.029071\n",
      "[165/00024] train_loss: 0.036073 kl_loss: 0.238135 normal_loss: 0.028929\n",
      "[166/00049] train_loss: 0.036019 kl_loss: 0.237267 normal_loss: 0.028901\n",
      "[167/00074] train_loss: 0.035509 kl_loss: 0.238284 normal_loss: 0.028360\n",
      "[169/00024] train_loss: 0.035495 kl_loss: 0.237819 normal_loss: 0.028361\n",
      "[170/00049] train_loss: 0.035707 kl_loss: 0.238246 normal_loss: 0.028559\n",
      "[171/00074] train_loss: 0.035466 kl_loss: 0.238262 normal_loss: 0.028318\n",
      "[173/00024] train_loss: 0.035621 kl_loss: 0.238315 normal_loss: 0.028471\n",
      "[174/00049] train_loss: 0.035504 kl_loss: 0.239090 normal_loss: 0.028331\n",
      "[175/00074] train_loss: 0.035636 kl_loss: 0.238124 normal_loss: 0.028492\n",
      "[177/00024] train_loss: 0.035190 kl_loss: 0.239253 normal_loss: 0.028012\n",
      "[178/00049] train_loss: 0.034971 kl_loss: 0.237754 normal_loss: 0.027838\n",
      "[179/00074] train_loss: 0.034864 kl_loss: 0.239023 normal_loss: 0.027694\n",
      "[181/00024] train_loss: 0.034635 kl_loss: 0.239600 normal_loss: 0.027447\n",
      "[182/00049] train_loss: 0.034473 kl_loss: 0.237289 normal_loss: 0.027354\n",
      "[183/00074] train_loss: 0.034589 kl_loss: 0.238493 normal_loss: 0.027434\n",
      "[185/00024] train_loss: 0.034772 kl_loss: 0.238117 normal_loss: 0.027629\n",
      "[186/00049] train_loss: 0.034155 kl_loss: 0.238950 normal_loss: 0.026987\n",
      "[187/00074] train_loss: 0.033999 kl_loss: 0.238269 normal_loss: 0.026851\n",
      "[189/00024] train_loss: 0.034079 kl_loss: 0.238820 normal_loss: 0.026914\n",
      "[190/00049] train_loss: 0.034030 kl_loss: 0.237828 normal_loss: 0.026895\n",
      "[191/00074] train_loss: 0.034196 kl_loss: 0.238900 normal_loss: 0.027029\n",
      "[193/00024] train_loss: 0.034244 kl_loss: 0.238050 normal_loss: 0.027102\n",
      "[194/00049] train_loss: 0.033858 kl_loss: 0.238803 normal_loss: 0.026694\n",
      "[195/00074] train_loss: 0.033455 kl_loss: 0.238543 normal_loss: 0.026299\n",
      "[197/00024] train_loss: 0.033105 kl_loss: 0.238596 normal_loss: 0.025947\n",
      "[198/00049] train_loss: 0.033395 kl_loss: 0.237977 normal_loss: 0.026255\n",
      "[199/00074] train_loss: 0.033300 kl_loss: 0.239183 normal_loss: 0.026125\n",
      "[201/00024] train_loss: 0.032227 kl_loss: 0.238095 normal_loss: 0.025084\n",
      "[202/00049] train_loss: 0.031572 kl_loss: 0.238771 normal_loss: 0.024409\n",
      "[203/00074] train_loss: 0.031488 kl_loss: 0.236962 normal_loss: 0.024379\n",
      "[205/00024] train_loss: 0.031215 kl_loss: 0.236549 normal_loss: 0.024118\n",
      "[206/00049] train_loss: 0.031062 kl_loss: 0.236373 normal_loss: 0.023971\n",
      "[207/00074] train_loss: 0.031030 kl_loss: 0.235870 normal_loss: 0.023954\n",
      "[209/00024] train_loss: 0.030745 kl_loss: 0.234772 normal_loss: 0.023702\n",
      "[210/00049] train_loss: 0.030712 kl_loss: 0.234671 normal_loss: 0.023672\n",
      "[211/00074] train_loss: 0.030806 kl_loss: 0.234511 normal_loss: 0.023770\n",
      "[213/00024] train_loss: 0.030890 kl_loss: 0.233834 normal_loss: 0.023875\n",
      "[214/00049] train_loss: 0.030984 kl_loss: 0.233091 normal_loss: 0.023992\n",
      "[215/00074] train_loss: 0.030487 kl_loss: 0.232366 normal_loss: 0.023516\n",
      "[217/00024] train_loss: 0.030436 kl_loss: 0.232424 normal_loss: 0.023464\n",
      "[218/00049] train_loss: 0.030155 kl_loss: 0.230982 normal_loss: 0.023226\n",
      "[219/00074] train_loss: 0.030604 kl_loss: 0.231766 normal_loss: 0.023651\n",
      "[221/00024] train_loss: 0.030195 kl_loss: 0.230817 normal_loss: 0.023271\n",
      "[222/00049] train_loss: 0.030180 kl_loss: 0.230115 normal_loss: 0.023277\n",
      "[223/00074] train_loss: 0.030423 kl_loss: 0.230662 normal_loss: 0.023503\n",
      "[225/00024] train_loss: 0.029968 kl_loss: 0.230238 normal_loss: 0.023061\n",
      "[226/00049] train_loss: 0.030188 kl_loss: 0.228699 normal_loss: 0.023327\n",
      "[227/00074] train_loss: 0.030009 kl_loss: 0.229046 normal_loss: 0.023138\n",
      "[229/00024] train_loss: 0.029846 kl_loss: 0.228379 normal_loss: 0.022995\n",
      "[230/00049] train_loss: 0.030173 kl_loss: 0.228969 normal_loss: 0.023304\n",
      "[231/00074] train_loss: 0.029746 kl_loss: 0.227250 normal_loss: 0.022928\n",
      "[233/00024] train_loss: 0.029674 kl_loss: 0.227558 normal_loss: 0.022847\n",
      "[234/00049] train_loss: 0.029646 kl_loss: 0.227133 normal_loss: 0.022832\n",
      "[235/00074] train_loss: 0.029483 kl_loss: 0.227078 normal_loss: 0.022670\n",
      "[237/00024] train_loss: 0.029300 kl_loss: 0.226507 normal_loss: 0.022505\n",
      "[238/00049] train_loss: 0.029949 kl_loss: 0.226424 normal_loss: 0.023156\n",
      "[239/00074] train_loss: 0.029587 kl_loss: 0.225834 normal_loss: 0.022812\n",
      "[241/00024] train_loss: 0.029249 kl_loss: 0.225247 normal_loss: 0.022491\n",
      "[242/00049] train_loss: 0.029829 kl_loss: 0.226584 normal_loss: 0.023031\n",
      "[243/00074] train_loss: 0.029156 kl_loss: 0.224751 normal_loss: 0.022413\n",
      "[245/00024] train_loss: 0.029127 kl_loss: 0.224965 normal_loss: 0.022378\n",
      "[246/00049] train_loss: 0.029001 kl_loss: 0.224345 normal_loss: 0.022270\n",
      "[247/00074] train_loss: 0.029135 kl_loss: 0.224970 normal_loss: 0.022386\n",
      "[249/00024] train_loss: 0.028881 kl_loss: 0.223888 normal_loss: 0.022164\n",
      "[250/00049] train_loss: 0.029021 kl_loss: 0.223569 normal_loss: 0.022314\n",
      "[251/00074] train_loss: 0.028955 kl_loss: 0.224402 normal_loss: 0.022223\n",
      "[253/00024] train_loss: 0.028896 kl_loss: 0.222867 normal_loss: 0.022210\n",
      "[254/00049] train_loss: 0.028583 kl_loss: 0.223151 normal_loss: 0.021889\n",
      "[255/00074] train_loss: 0.028973 kl_loss: 0.223723 normal_loss: 0.022261\n",
      "[257/00024] train_loss: 0.028535 kl_loss: 0.222578 normal_loss: 0.021857\n",
      "[258/00049] train_loss: 0.028708 kl_loss: 0.223324 normal_loss: 0.022008\n",
      "[259/00074] train_loss: 0.028450 kl_loss: 0.221994 normal_loss: 0.021790\n",
      "[261/00024] train_loss: 0.028645 kl_loss: 0.222131 normal_loss: 0.021981\n",
      "[262/00049] train_loss: 0.028669 kl_loss: 0.221903 normal_loss: 0.022012\n",
      "[263/00074] train_loss: 0.028428 kl_loss: 0.221987 normal_loss: 0.021768\n",
      "[265/00024] train_loss: 0.028487 kl_loss: 0.222129 normal_loss: 0.021823\n",
      "[266/00049] train_loss: 0.028174 kl_loss: 0.220669 normal_loss: 0.021554\n",
      "[267/00074] train_loss: 0.028482 kl_loss: 0.221266 normal_loss: 0.021844\n",
      "[269/00024] train_loss: 0.028232 kl_loss: 0.220463 normal_loss: 0.021619\n",
      "[270/00049] train_loss: 0.028341 kl_loss: 0.221781 normal_loss: 0.021688\n",
      "[271/00074] train_loss: 0.028126 kl_loss: 0.219858 normal_loss: 0.021531\n",
      "[273/00024] train_loss: 0.028001 kl_loss: 0.219894 normal_loss: 0.021404\n",
      "[274/00049] train_loss: 0.028210 kl_loss: 0.220543 normal_loss: 0.021594\n",
      "[275/00074] train_loss: 0.027910 kl_loss: 0.220072 normal_loss: 0.021308\n",
      "[277/00024] train_loss: 0.028004 kl_loss: 0.219774 normal_loss: 0.021411\n",
      "[278/00049] train_loss: 0.027936 kl_loss: 0.219818 normal_loss: 0.021342\n",
      "[279/00074] train_loss: 0.027741 kl_loss: 0.219447 normal_loss: 0.021157\n",
      "[281/00024] train_loss: 0.027831 kl_loss: 0.220064 normal_loss: 0.021229\n",
      "[282/00049] train_loss: 0.027899 kl_loss: 0.217694 normal_loss: 0.021368\n",
      "[283/00074] train_loss: 0.028339 kl_loss: 0.219848 normal_loss: 0.021744\n",
      "[285/00024] train_loss: 0.028268 kl_loss: 0.219466 normal_loss: 0.021684\n",
      "[286/00049] train_loss: 0.027416 kl_loss: 0.217988 normal_loss: 0.020877\n",
      "[287/00074] train_loss: 0.027743 kl_loss: 0.219050 normal_loss: 0.021171\n",
      "[289/00024] train_loss: 0.027742 kl_loss: 0.219172 normal_loss: 0.021167\n",
      "[290/00049] train_loss: 0.027606 kl_loss: 0.217403 normal_loss: 0.021084\n",
      "[291/00074] train_loss: 0.027394 kl_loss: 0.218525 normal_loss: 0.020839\n",
      "[293/00024] train_loss: 0.027700 kl_loss: 0.218275 normal_loss: 0.021151\n",
      "[294/00049] train_loss: 0.027233 kl_loss: 0.217312 normal_loss: 0.020714\n",
      "[295/00074] train_loss: 0.027457 kl_loss: 0.218142 normal_loss: 0.020912\n",
      "[297/00024] train_loss: 0.027340 kl_loss: 0.217499 normal_loss: 0.020815\n",
      "[298/00049] train_loss: 0.027480 kl_loss: 0.217086 normal_loss: 0.020967\n",
      "[299/00074] train_loss: 0.027328 kl_loss: 0.217902 normal_loss: 0.020791\n",
      "[301/00024] train_loss: 0.026691 kl_loss: 0.217025 normal_loss: 0.020180\n",
      "[302/00049] train_loss: 0.026642 kl_loss: 0.216778 normal_loss: 0.020138\n",
      "[303/00074] train_loss: 0.026497 kl_loss: 0.217131 normal_loss: 0.019983\n",
      "[305/00024] train_loss: 0.026473 kl_loss: 0.216351 normal_loss: 0.019982\n",
      "[306/00049] train_loss: 0.026395 kl_loss: 0.217179 normal_loss: 0.019880\n",
      "[307/00074] train_loss: 0.026348 kl_loss: 0.215550 normal_loss: 0.019882\n",
      "[309/00024] train_loss: 0.026315 kl_loss: 0.215859 normal_loss: 0.019839\n",
      "[310/00049] train_loss: 0.026200 kl_loss: 0.215592 normal_loss: 0.019732\n",
      "[311/00074] train_loss: 0.026301 kl_loss: 0.215645 normal_loss: 0.019832\n",
      "[313/00024] train_loss: 0.026337 kl_loss: 0.215760 normal_loss: 0.019864\n",
      "[314/00049] train_loss: 0.026161 kl_loss: 0.214466 normal_loss: 0.019727\n",
      "[315/00074] train_loss: 0.026008 kl_loss: 0.215069 normal_loss: 0.019556\n",
      "[317/00024] train_loss: 0.025998 kl_loss: 0.213941 normal_loss: 0.019579\n",
      "[318/00049] train_loss: 0.026070 kl_loss: 0.215144 normal_loss: 0.019616\n",
      "[319/00074] train_loss: 0.025898 kl_loss: 0.214334 normal_loss: 0.019468\n",
      "[321/00024] train_loss: 0.026032 kl_loss: 0.214575 normal_loss: 0.019595\n",
      "[322/00049] train_loss: 0.026079 kl_loss: 0.213563 normal_loss: 0.019672\n",
      "[323/00074] train_loss: 0.025894 kl_loss: 0.213387 normal_loss: 0.019493\n",
      "[325/00024] train_loss: 0.025789 kl_loss: 0.213617 normal_loss: 0.019381\n",
      "[326/00049] train_loss: 0.025873 kl_loss: 0.213686 normal_loss: 0.019462\n",
      "[327/00074] train_loss: 0.025841 kl_loss: 0.212528 normal_loss: 0.019465\n",
      "[329/00024] train_loss: 0.025947 kl_loss: 0.213049 normal_loss: 0.019556\n",
      "[330/00049] train_loss: 0.025830 kl_loss: 0.212644 normal_loss: 0.019450\n",
      "[331/00074] train_loss: 0.025756 kl_loss: 0.212408 normal_loss: 0.019383\n",
      "[333/00024] train_loss: 0.025931 kl_loss: 0.212306 normal_loss: 0.019562\n",
      "[334/00049] train_loss: 0.025743 kl_loss: 0.212516 normal_loss: 0.019368\n",
      "[335/00074] train_loss: 0.025640 kl_loss: 0.211475 normal_loss: 0.019295\n",
      "[337/00024] train_loss: 0.025668 kl_loss: 0.211851 normal_loss: 0.019313\n",
      "[338/00049] train_loss: 0.025545 kl_loss: 0.211684 normal_loss: 0.019195\n",
      "[339/00074] train_loss: 0.025648 kl_loss: 0.211049 normal_loss: 0.019317\n",
      "[341/00024] train_loss: 0.025601 kl_loss: 0.211487 normal_loss: 0.019257\n",
      "[342/00049] train_loss: 0.025424 kl_loss: 0.210690 normal_loss: 0.019103\n",
      "[343/00074] train_loss: 0.025576 kl_loss: 0.210762 normal_loss: 0.019253\n",
      "[345/00024] train_loss: 0.025466 kl_loss: 0.210902 normal_loss: 0.019139\n",
      "[346/00049] train_loss: 0.025561 kl_loss: 0.210982 normal_loss: 0.019232\n",
      "[347/00074] train_loss: 0.025364 kl_loss: 0.209613 normal_loss: 0.019076\n",
      "[349/00024] train_loss: 0.025474 kl_loss: 0.211342 normal_loss: 0.019133\n",
      "[350/00049] train_loss: 0.025302 kl_loss: 0.209535 normal_loss: 0.019016\n",
      "[351/00074] train_loss: 0.025395 kl_loss: 0.209136 normal_loss: 0.019121\n",
      "[353/00024] train_loss: 0.025331 kl_loss: 0.209257 normal_loss: 0.019053\n",
      "[354/00049] train_loss: 0.025673 kl_loss: 0.210008 normal_loss: 0.019373\n",
      "[355/00074] train_loss: 0.025201 kl_loss: 0.209301 normal_loss: 0.018922\n",
      "[357/00024] train_loss: 0.025619 kl_loss: 0.209606 normal_loss: 0.019331\n",
      "[358/00049] train_loss: 0.025380 kl_loss: 0.209012 normal_loss: 0.019110\n",
      "[359/00074] train_loss: 0.025261 kl_loss: 0.208729 normal_loss: 0.018999\n",
      "[361/00024] train_loss: 0.025386 kl_loss: 0.208773 normal_loss: 0.019123\n",
      "[362/00049] train_loss: 0.025028 kl_loss: 0.208669 normal_loss: 0.018768\n",
      "[363/00074] train_loss: 0.025187 kl_loss: 0.208573 normal_loss: 0.018929\n",
      "[365/00024] train_loss: 0.025010 kl_loss: 0.208204 normal_loss: 0.018763\n",
      "[366/00049] train_loss: 0.025142 kl_loss: 0.208606 normal_loss: 0.018884\n",
      "[367/00074] train_loss: 0.024976 kl_loss: 0.207865 normal_loss: 0.018740\n",
      "[369/00024] train_loss: 0.025044 kl_loss: 0.207943 normal_loss: 0.018806\n",
      "[370/00049] train_loss: 0.025063 kl_loss: 0.208100 normal_loss: 0.018820\n",
      "[371/00074] train_loss: 0.024829 kl_loss: 0.207427 normal_loss: 0.018607\n",
      "[373/00024] train_loss: 0.024925 kl_loss: 0.207366 normal_loss: 0.018704\n",
      "[374/00049] train_loss: 0.025076 kl_loss: 0.207004 normal_loss: 0.018866\n",
      "[375/00074] train_loss: 0.025113 kl_loss: 0.207921 normal_loss: 0.018876\n",
      "[377/00024] train_loss: 0.024925 kl_loss: 0.207295 normal_loss: 0.018707\n",
      "[378/00049] train_loss: 0.024912 kl_loss: 0.206835 normal_loss: 0.018706\n",
      "[379/00074] train_loss: 0.024955 kl_loss: 0.207012 normal_loss: 0.018745\n",
      "[381/00024] train_loss: 0.024857 kl_loss: 0.207284 normal_loss: 0.018639\n",
      "[382/00049] train_loss: 0.024809 kl_loss: 0.205572 normal_loss: 0.018642\n",
      "[383/00074] train_loss: 0.024899 kl_loss: 0.207147 normal_loss: 0.018684\n",
      "[385/00024] train_loss: 0.024906 kl_loss: 0.206578 normal_loss: 0.018709\n",
      "[386/00049] train_loss: 0.024788 kl_loss: 0.206032 normal_loss: 0.018607\n",
      "[387/00074] train_loss: 0.024727 kl_loss: 0.206211 normal_loss: 0.018541\n",
      "[389/00024] train_loss: 0.024697 kl_loss: 0.206300 normal_loss: 0.018508\n",
      "[390/00049] train_loss: 0.024555 kl_loss: 0.205734 normal_loss: 0.018383\n",
      "[391/00074] train_loss: 0.024756 kl_loss: 0.205493 normal_loss: 0.018592\n",
      "[393/00024] train_loss: 0.024510 kl_loss: 0.205388 normal_loss: 0.018348\n",
      "[394/00049] train_loss: 0.024563 kl_loss: 0.206338 normal_loss: 0.018373\n",
      "[395/00074] train_loss: 0.024649 kl_loss: 0.204609 normal_loss: 0.018511\n",
      "[397/00024] train_loss: 0.024516 kl_loss: 0.205001 normal_loss: 0.018366\n",
      "[398/00049] train_loss: 0.024575 kl_loss: 0.205472 normal_loss: 0.018411\n",
      "[399/00074] train_loss: 0.024628 kl_loss: 0.204592 normal_loss: 0.018490\n",
      "[401/00024] train_loss: 0.024261 kl_loss: 0.204394 normal_loss: 0.018129\n",
      "[402/00049] train_loss: 0.024293 kl_loss: 0.205453 normal_loss: 0.018129\n",
      "[403/00074] train_loss: 0.024107 kl_loss: 0.204181 normal_loss: 0.017982\n",
      "[405/00024] train_loss: 0.024137 kl_loss: 0.203981 normal_loss: 0.018018\n",
      "[406/00049] train_loss: 0.024260 kl_loss: 0.205246 normal_loss: 0.018103\n",
      "[407/00074] train_loss: 0.024051 kl_loss: 0.203918 normal_loss: 0.017933\n",
      "[409/00024] train_loss: 0.024119 kl_loss: 0.204129 normal_loss: 0.017995\n",
      "[410/00049] train_loss: 0.024148 kl_loss: 0.204508 normal_loss: 0.018012\n",
      "[411/00074] train_loss: 0.024055 kl_loss: 0.203638 normal_loss: 0.017946\n",
      "[413/00024] train_loss: 0.023949 kl_loss: 0.204307 normal_loss: 0.017820\n",
      "[414/00049] train_loss: 0.024047 kl_loss: 0.203791 normal_loss: 0.017933\n",
      "[415/00074] train_loss: 0.024038 kl_loss: 0.203315 normal_loss: 0.017938\n",
      "[417/00024] train_loss: 0.023733 kl_loss: 0.203543 normal_loss: 0.017626\n",
      "[418/00049] train_loss: 0.024027 kl_loss: 0.203154 normal_loss: 0.017932\n",
      "[419/00074] train_loss: 0.023908 kl_loss: 0.203832 normal_loss: 0.017793\n",
      "[421/00024] train_loss: 0.023868 kl_loss: 0.203402 normal_loss: 0.017766\n",
      "[422/00049] train_loss: 0.024046 kl_loss: 0.203245 normal_loss: 0.017949\n",
      "[423/00074] train_loss: 0.024039 kl_loss: 0.202912 normal_loss: 0.017951\n",
      "[425/00024] train_loss: 0.023858 kl_loss: 0.203185 normal_loss: 0.017763\n",
      "[426/00049] train_loss: 0.023946 kl_loss: 0.202640 normal_loss: 0.017867\n",
      "[427/00074] train_loss: 0.023845 kl_loss: 0.202804 normal_loss: 0.017761\n",
      "[429/00024] train_loss: 0.023916 kl_loss: 0.202975 normal_loss: 0.017826\n",
      "[430/00049] train_loss: 0.023811 kl_loss: 0.201704 normal_loss: 0.017759\n",
      "[431/00074] train_loss: 0.023997 kl_loss: 0.203007 normal_loss: 0.017907\n",
      "[433/00024] train_loss: 0.023903 kl_loss: 0.202671 normal_loss: 0.017823\n",
      "[434/00049] train_loss: 0.023716 kl_loss: 0.201634 normal_loss: 0.017667\n",
      "[435/00074] train_loss: 0.023875 kl_loss: 0.202539 normal_loss: 0.017799\n",
      "[437/00024] train_loss: 0.023846 kl_loss: 0.202335 normal_loss: 0.017776\n",
      "[438/00049] train_loss: 0.023711 kl_loss: 0.201552 normal_loss: 0.017665\n",
      "[439/00074] train_loss: 0.023882 kl_loss: 0.202080 normal_loss: 0.017819\n",
      "[441/00024] train_loss: 0.023805 kl_loss: 0.201326 normal_loss: 0.017765\n",
      "[442/00049] train_loss: 0.023845 kl_loss: 0.201948 normal_loss: 0.017787\n",
      "[443/00074] train_loss: 0.023675 kl_loss: 0.201924 normal_loss: 0.017617\n",
      "[445/00024] train_loss: 0.023891 kl_loss: 0.202042 normal_loss: 0.017830\n",
      "[446/00049] train_loss: 0.023629 kl_loss: 0.201383 normal_loss: 0.017588\n",
      "[447/00074] train_loss: 0.023739 kl_loss: 0.200991 normal_loss: 0.017710\n",
      "[449/00024] train_loss: 0.023556 kl_loss: 0.200742 normal_loss: 0.017534\n",
      "[450/00049] train_loss: 0.023710 kl_loss: 0.202084 normal_loss: 0.017647\n",
      "[451/00074] train_loss: 0.023613 kl_loss: 0.200728 normal_loss: 0.017591\n",
      "[453/00024] train_loss: 0.023581 kl_loss: 0.200582 normal_loss: 0.017564\n",
      "[454/00049] train_loss: 0.023738 kl_loss: 0.201007 normal_loss: 0.017708\n",
      "[455/00074] train_loss: 0.023675 kl_loss: 0.201129 normal_loss: 0.017641\n",
      "[457/00024] train_loss: 0.023643 kl_loss: 0.201367 normal_loss: 0.017602\n",
      "[458/00049] train_loss: 0.023596 kl_loss: 0.200532 normal_loss: 0.017580\n",
      "[459/00074] train_loss: 0.023622 kl_loss: 0.200081 normal_loss: 0.017620\n",
      "[461/00024] train_loss: 0.023480 kl_loss: 0.200904 normal_loss: 0.017453\n",
      "[462/00049] train_loss: 0.023602 kl_loss: 0.199772 normal_loss: 0.017609\n",
      "[463/00074] train_loss: 0.023568 kl_loss: 0.200580 normal_loss: 0.017551\n",
      "[465/00024] train_loss: 0.023733 kl_loss: 0.200592 normal_loss: 0.017715\n",
      "[466/00049] train_loss: 0.023441 kl_loss: 0.199649 normal_loss: 0.017452\n",
      "[467/00074] train_loss: 0.023545 kl_loss: 0.200275 normal_loss: 0.017536\n",
      "[469/00024] train_loss: 0.023530 kl_loss: 0.200113 normal_loss: 0.017527\n",
      "[470/00049] train_loss: 0.023564 kl_loss: 0.199844 normal_loss: 0.017569\n",
      "[471/00074] train_loss: 0.023359 kl_loss: 0.199874 normal_loss: 0.017362\n",
      "[473/00024] train_loss: 0.023459 kl_loss: 0.200305 normal_loss: 0.017450\n",
      "[474/00049] train_loss: 0.023468 kl_loss: 0.199342 normal_loss: 0.017488\n",
      "[475/00074] train_loss: 0.023327 kl_loss: 0.199473 normal_loss: 0.017343\n",
      "[477/00024] train_loss: 0.023574 kl_loss: 0.199888 normal_loss: 0.017577\n",
      "[478/00049] train_loss: 0.023371 kl_loss: 0.199530 normal_loss: 0.017385\n",
      "[479/00074] train_loss: 0.023413 kl_loss: 0.199027 normal_loss: 0.017443\n",
      "[481/00024] train_loss: 0.023460 kl_loss: 0.199432 normal_loss: 0.017477\n",
      "[482/00049] train_loss: 0.023352 kl_loss: 0.199148 normal_loss: 0.017377\n",
      "[483/00074] train_loss: 0.023346 kl_loss: 0.199216 normal_loss: 0.017369\n",
      "[485/00024] train_loss: 0.023247 kl_loss: 0.198378 normal_loss: 0.017295\n",
      "[486/00049] train_loss: 0.023514 kl_loss: 0.199844 normal_loss: 0.017518\n",
      "[487/00074] train_loss: 0.023302 kl_loss: 0.198879 normal_loss: 0.017336\n",
      "[489/00024] train_loss: 0.023284 kl_loss: 0.198789 normal_loss: 0.017320\n",
      "[490/00049] train_loss: 0.023299 kl_loss: 0.198932 normal_loss: 0.017331\n",
      "[491/00074] train_loss: 0.023338 kl_loss: 0.198654 normal_loss: 0.017378\n",
      "[493/00024] train_loss: 0.023308 kl_loss: 0.198793 normal_loss: 0.017344\n",
      "[494/00049] train_loss: 0.023118 kl_loss: 0.198620 normal_loss: 0.017160\n",
      "[495/00074] train_loss: 0.023304 kl_loss: 0.198243 normal_loss: 0.017356\n",
      "[497/00024] train_loss: 0.023299 kl_loss: 0.198692 normal_loss: 0.017338\n",
      "[498/00049] train_loss: 0.023153 kl_loss: 0.197623 normal_loss: 0.017224\n",
      "[499/00074] train_loss: 0.023369 kl_loss: 0.198728 normal_loss: 0.017407\n",
      "[501/00024] train_loss: 0.023106 kl_loss: 0.197966 normal_loss: 0.017167\n",
      "[502/00049] train_loss: 0.022955 kl_loss: 0.198106 normal_loss: 0.017012\n",
      "[503/00074] train_loss: 0.023060 kl_loss: 0.198440 normal_loss: 0.017107\n",
      "[505/00024] train_loss: 0.023167 kl_loss: 0.198648 normal_loss: 0.017208\n",
      "[506/00049] train_loss: 0.022950 kl_loss: 0.197519 normal_loss: 0.017024\n",
      "[507/00074] train_loss: 0.022999 kl_loss: 0.197890 normal_loss: 0.017062\n",
      "[509/00024] train_loss: 0.022993 kl_loss: 0.197474 normal_loss: 0.017069\n",
      "[510/00049] train_loss: 0.023078 kl_loss: 0.199048 normal_loss: 0.017106\n",
      "[511/00074] train_loss: 0.022930 kl_loss: 0.197099 normal_loss: 0.017017\n",
      "[513/00024] train_loss: 0.022997 kl_loss: 0.198133 normal_loss: 0.017053\n",
      "[514/00049] train_loss: 0.022911 kl_loss: 0.197811 normal_loss: 0.016976\n",
      "[515/00074] train_loss: 0.022947 kl_loss: 0.197236 normal_loss: 0.017030\n",
      "[517/00024] train_loss: 0.022948 kl_loss: 0.197951 normal_loss: 0.017009\n",
      "[518/00049] train_loss: 0.022955 kl_loss: 0.196678 normal_loss: 0.017054\n",
      "[519/00074] train_loss: 0.022987 kl_loss: 0.198083 normal_loss: 0.017045\n",
      "[521/00024] train_loss: 0.023011 kl_loss: 0.197557 normal_loss: 0.017085\n",
      "[522/00049] train_loss: 0.022998 kl_loss: 0.197541 normal_loss: 0.017071\n",
      "[523/00074] train_loss: 0.022956 kl_loss: 0.197180 normal_loss: 0.017040\n",
      "[525/00024] train_loss: 0.022941 kl_loss: 0.197265 normal_loss: 0.017023\n",
      "[526/00049] train_loss: 0.022885 kl_loss: 0.197493 normal_loss: 0.016960\n",
      "[527/00074] train_loss: 0.022877 kl_loss: 0.197081 normal_loss: 0.016965\n",
      "[529/00024] train_loss: 0.022830 kl_loss: 0.196777 normal_loss: 0.016926\n",
      "[530/00049] train_loss: 0.022930 kl_loss: 0.197163 normal_loss: 0.017015\n",
      "[531/00074] train_loss: 0.022892 kl_loss: 0.197462 normal_loss: 0.016968\n",
      "[533/00024] train_loss: 0.022862 kl_loss: 0.196970 normal_loss: 0.016953\n",
      "[534/00049] train_loss: 0.022881 kl_loss: 0.197080 normal_loss: 0.016968\n",
      "[535/00074] train_loss: 0.022853 kl_loss: 0.196937 normal_loss: 0.016945\n",
      "[537/00024] train_loss: 0.022850 kl_loss: 0.196819 normal_loss: 0.016945\n",
      "[538/00049] train_loss: 0.022889 kl_loss: 0.196960 normal_loss: 0.016980\n",
      "[539/00074] train_loss: 0.022804 kl_loss: 0.196810 normal_loss: 0.016899\n",
      "[541/00024] train_loss: 0.022879 kl_loss: 0.196833 normal_loss: 0.016974\n",
      "[542/00049] train_loss: 0.022773 kl_loss: 0.196465 normal_loss: 0.016879\n",
      "[543/00074] train_loss: 0.022842 kl_loss: 0.196875 normal_loss: 0.016936\n",
      "[545/00024] train_loss: 0.022945 kl_loss: 0.196837 normal_loss: 0.017039\n",
      "[546/00049] train_loss: 0.022750 kl_loss: 0.196414 normal_loss: 0.016857\n",
      "[547/00074] train_loss: 0.022840 kl_loss: 0.196507 normal_loss: 0.016945\n",
      "[549/00024] train_loss: 0.022854 kl_loss: 0.196178 normal_loss: 0.016969\n",
      "[550/00049] train_loss: 0.022766 kl_loss: 0.197140 normal_loss: 0.016852\n",
      "[551/00074] train_loss: 0.022784 kl_loss: 0.195979 normal_loss: 0.016904\n",
      "[553/00024] train_loss: 0.022743 kl_loss: 0.196131 normal_loss: 0.016859\n",
      "[554/00049] train_loss: 0.022894 kl_loss: 0.196700 normal_loss: 0.016993\n",
      "[555/00074] train_loss: 0.022718 kl_loss: 0.196031 normal_loss: 0.016837\n",
      "[557/00024] train_loss: 0.022902 kl_loss: 0.196491 normal_loss: 0.017008\n",
      "[558/00049] train_loss: 0.022806 kl_loss: 0.195997 normal_loss: 0.016926\n",
      "[559/00074] train_loss: 0.022723 kl_loss: 0.195976 normal_loss: 0.016844\n",
      "[561/00024] train_loss: 0.022784 kl_loss: 0.196243 normal_loss: 0.016896\n",
      "[562/00049] train_loss: 0.022783 kl_loss: 0.196048 normal_loss: 0.016901\n",
      "[563/00074] train_loss: 0.022778 kl_loss: 0.195762 normal_loss: 0.016905\n",
      "[565/00024] train_loss: 0.022684 kl_loss: 0.196027 normal_loss: 0.016803\n",
      "[566/00049] train_loss: 0.022845 kl_loss: 0.196389 normal_loss: 0.016953\n",
      "[567/00074] train_loss: 0.022590 kl_loss: 0.195216 normal_loss: 0.016733\n",
      "[569/00024] train_loss: 0.022702 kl_loss: 0.195532 normal_loss: 0.016836\n",
      "[570/00049] train_loss: 0.022687 kl_loss: 0.195258 normal_loss: 0.016829\n",
      "[571/00074] train_loss: 0.022797 kl_loss: 0.196445 normal_loss: 0.016903\n",
      "[573/00024] train_loss: 0.022766 kl_loss: 0.195295 normal_loss: 0.016907\n",
      "[574/00049] train_loss: 0.022715 kl_loss: 0.196248 normal_loss: 0.016828\n",
      "[575/00074] train_loss: 0.022683 kl_loss: 0.195311 normal_loss: 0.016824\n",
      "[577/00024] train_loss: 0.022717 kl_loss: 0.195619 normal_loss: 0.016848\n",
      "[578/00049] train_loss: 0.022713 kl_loss: 0.195005 normal_loss: 0.016863\n",
      "[579/00074] train_loss: 0.022638 kl_loss: 0.195849 normal_loss: 0.016763\n",
      "[581/00024] train_loss: 0.022768 kl_loss: 0.194953 normal_loss: 0.016920\n",
      "[582/00049] train_loss: 0.022814 kl_loss: 0.196310 normal_loss: 0.016925\n",
      "[583/00074] train_loss: 0.022527 kl_loss: 0.194800 normal_loss: 0.016683\n",
      "[585/00024] train_loss: 0.022618 kl_loss: 0.195232 normal_loss: 0.016761\n",
      "[586/00049] train_loss: 0.022517 kl_loss: 0.194744 normal_loss: 0.016675\n",
      "[587/00074] train_loss: 0.022662 kl_loss: 0.195710 normal_loss: 0.016791\n",
      "[589/00024] train_loss: 0.022635 kl_loss: 0.195086 normal_loss: 0.016782\n",
      "[590/00049] train_loss: 0.022662 kl_loss: 0.194894 normal_loss: 0.016815\n",
      "[591/00074] train_loss: 0.022588 kl_loss: 0.195325 normal_loss: 0.016729\n",
      "[593/00024] train_loss: 0.022617 kl_loss: 0.195369 normal_loss: 0.016756\n",
      "[594/00049] train_loss: 0.022604 kl_loss: 0.194577 normal_loss: 0.016767\n",
      "[595/00074] train_loss: 0.022642 kl_loss: 0.194990 normal_loss: 0.016792\n",
      "[597/00024] train_loss: 0.022535 kl_loss: 0.194291 normal_loss: 0.016707\n",
      "[598/00049] train_loss: 0.022531 kl_loss: 0.194521 normal_loss: 0.016696\n",
      "[599/00074] train_loss: 0.022648 kl_loss: 0.195744 normal_loss: 0.016776\n",
      "[601/00024] train_loss: 0.022540 kl_loss: 0.194603 normal_loss: 0.016702\n",
      "[602/00049] train_loss: 0.022538 kl_loss: 0.194914 normal_loss: 0.016691\n",
      "[603/00074] train_loss: 0.022448 kl_loss: 0.194743 normal_loss: 0.016606\n",
      "[605/00024] train_loss: 0.022443 kl_loss: 0.194742 normal_loss: 0.016600\n",
      "[606/00049] train_loss: 0.022604 kl_loss: 0.194936 normal_loss: 0.016756\n",
      "[607/00074] train_loss: 0.022467 kl_loss: 0.194379 normal_loss: 0.016636\n",
      "[609/00024] train_loss: 0.022451 kl_loss: 0.195142 normal_loss: 0.016597\n",
      "[610/00049] train_loss: 0.022481 kl_loss: 0.193720 normal_loss: 0.016670\n",
      "[611/00074] train_loss: 0.022418 kl_loss: 0.195005 normal_loss: 0.016568\n",
      "[613/00024] train_loss: 0.022478 kl_loss: 0.194267 normal_loss: 0.016650\n",
      "[614/00049] train_loss: 0.022489 kl_loss: 0.195468 normal_loss: 0.016625\n",
      "[615/00074] train_loss: 0.022404 kl_loss: 0.193920 normal_loss: 0.016586\n",
      "[617/00024] train_loss: 0.022501 kl_loss: 0.195125 normal_loss: 0.016647\n",
      "[618/00049] train_loss: 0.022333 kl_loss: 0.193653 normal_loss: 0.016523\n",
      "[619/00074] train_loss: 0.022482 kl_loss: 0.194652 normal_loss: 0.016643\n",
      "[621/00024] train_loss: 0.022334 kl_loss: 0.194413 normal_loss: 0.016502\n",
      "[622/00049] train_loss: 0.022541 kl_loss: 0.194870 normal_loss: 0.016695\n",
      "[623/00074] train_loss: 0.022392 kl_loss: 0.193940 normal_loss: 0.016574\n",
      "[625/00024] train_loss: 0.022486 kl_loss: 0.194707 normal_loss: 0.016645\n",
      "[626/00049] train_loss: 0.022352 kl_loss: 0.194351 normal_loss: 0.016521\n",
      "[627/00074] train_loss: 0.022437 kl_loss: 0.193964 normal_loss: 0.016619\n",
      "[629/00024] train_loss: 0.022350 kl_loss: 0.194076 normal_loss: 0.016528\n",
      "[630/00049] train_loss: 0.022389 kl_loss: 0.194404 normal_loss: 0.016557\n",
      "[631/00074] train_loss: 0.022447 kl_loss: 0.194327 normal_loss: 0.016617\n",
      "[633/00024] train_loss: 0.022468 kl_loss: 0.194642 normal_loss: 0.016629\n",
      "[634/00049] train_loss: 0.022251 kl_loss: 0.193168 normal_loss: 0.016456\n",
      "[635/00074] train_loss: 0.022484 kl_loss: 0.194771 normal_loss: 0.016641\n",
      "[637/00024] train_loss: 0.022461 kl_loss: 0.194235 normal_loss: 0.016634\n",
      "[638/00049] train_loss: 0.022332 kl_loss: 0.193997 normal_loss: 0.016512\n",
      "[639/00074] train_loss: 0.022395 kl_loss: 0.194137 normal_loss: 0.016570\n",
      "[641/00024] train_loss: 0.022370 kl_loss: 0.194140 normal_loss: 0.016545\n",
      "[642/00049] train_loss: 0.022547 kl_loss: 0.194272 normal_loss: 0.016719\n",
      "[643/00074] train_loss: 0.022350 kl_loss: 0.193735 normal_loss: 0.016538\n",
      "[645/00024] train_loss: 0.022304 kl_loss: 0.193696 normal_loss: 0.016494\n",
      "[646/00049] train_loss: 0.022510 kl_loss: 0.194709 normal_loss: 0.016669\n",
      "[647/00074] train_loss: 0.022290 kl_loss: 0.193542 normal_loss: 0.016484\n",
      "[649/00024] train_loss: 0.022424 kl_loss: 0.193723 normal_loss: 0.016612\n",
      "[650/00049] train_loss: 0.022371 kl_loss: 0.194138 normal_loss: 0.016547\n",
      "[651/00074] train_loss: 0.022334 kl_loss: 0.193896 normal_loss: 0.016517\n",
      "[653/00024] train_loss: 0.022434 kl_loss: 0.193913 normal_loss: 0.016616\n",
      "[654/00049] train_loss: 0.022344 kl_loss: 0.193748 normal_loss: 0.016532\n",
      "[655/00074] train_loss: 0.022380 kl_loss: 0.193877 normal_loss: 0.016563\n",
      "[657/00024] train_loss: 0.022407 kl_loss: 0.193783 normal_loss: 0.016594\n",
      "[658/00049] train_loss: 0.022284 kl_loss: 0.193791 normal_loss: 0.016470\n",
      "[659/00074] train_loss: 0.022423 kl_loss: 0.193757 normal_loss: 0.016610\n",
      "[661/00024] train_loss: 0.022447 kl_loss: 0.193709 normal_loss: 0.016635\n",
      "[662/00049] train_loss: 0.022297 kl_loss: 0.193800 normal_loss: 0.016483\n",
      "[663/00074] train_loss: 0.022439 kl_loss: 0.193629 normal_loss: 0.016630\n",
      "[665/00024] train_loss: 0.022363 kl_loss: 0.193762 normal_loss: 0.016550\n",
      "[666/00049] train_loss: 0.022261 kl_loss: 0.193816 normal_loss: 0.016446\n",
      "[667/00074] train_loss: 0.022351 kl_loss: 0.193362 normal_loss: 0.016550\n",
      "[669/00024] train_loss: 0.022301 kl_loss: 0.193768 normal_loss: 0.016488\n",
      "[670/00049] train_loss: 0.022274 kl_loss: 0.193079 normal_loss: 0.016481\n",
      "[671/00074] train_loss: 0.022369 kl_loss: 0.193889 normal_loss: 0.016552\n",
      "[673/00024] train_loss: 0.022321 kl_loss: 0.193540 normal_loss: 0.016515\n",
      "[674/00049] train_loss: 0.022297 kl_loss: 0.193456 normal_loss: 0.016494\n",
      "[675/00074] train_loss: 0.022324 kl_loss: 0.193528 normal_loss: 0.016519\n",
      "[677/00024] train_loss: 0.022339 kl_loss: 0.193735 normal_loss: 0.016527\n",
      "[678/00049] train_loss: 0.022167 kl_loss: 0.192590 normal_loss: 0.016389\n",
      "[679/00074] train_loss: 0.022351 kl_loss: 0.193990 normal_loss: 0.016532\n",
      "[681/00024] train_loss: 0.022300 kl_loss: 0.193287 normal_loss: 0.016501\n",
      "[682/00049] train_loss: 0.022351 kl_loss: 0.193810 normal_loss: 0.016536\n",
      "[683/00074] train_loss: 0.022246 kl_loss: 0.192996 normal_loss: 0.016456\n",
      "[685/00024] train_loss: 0.022313 kl_loss: 0.193733 normal_loss: 0.016501\n",
      "[686/00049] train_loss: 0.022275 kl_loss: 0.193190 normal_loss: 0.016480\n",
      "[687/00074] train_loss: 0.022204 kl_loss: 0.192963 normal_loss: 0.016416\n",
      "[689/00024] train_loss: 0.022283 kl_loss: 0.192974 normal_loss: 0.016494\n",
      "[690/00049] train_loss: 0.022310 kl_loss: 0.193555 normal_loss: 0.016503\n",
      "[691/00074] train_loss: 0.022298 kl_loss: 0.193151 normal_loss: 0.016504\n",
      "[693/00024] train_loss: 0.022267 kl_loss: 0.193124 normal_loss: 0.016473\n",
      "[694/00049] train_loss: 0.022387 kl_loss: 0.194021 normal_loss: 0.016566\n",
      "[695/00074] train_loss: 0.022197 kl_loss: 0.192334 normal_loss: 0.016426\n",
      "[697/00024] train_loss: 0.022329 kl_loss: 0.192974 normal_loss: 0.016540\n",
      "[698/00049] train_loss: 0.022235 kl_loss: 0.193584 normal_loss: 0.016428\n",
      "[699/00074] train_loss: 0.022273 kl_loss: 0.192727 normal_loss: 0.016491\n",
      "[701/00024] train_loss: 0.022323 kl_loss: 0.192857 normal_loss: 0.016537\n",
      "[702/00049] train_loss: 0.022254 kl_loss: 0.193648 normal_loss: 0.016445\n",
      "[703/00074] train_loss: 0.022123 kl_loss: 0.192630 normal_loss: 0.016344\n",
      "[705/00024] train_loss: 0.022246 kl_loss: 0.193032 normal_loss: 0.016455\n",
      "[706/00049] train_loss: 0.022317 kl_loss: 0.192950 normal_loss: 0.016528\n",
      "[707/00074] train_loss: 0.022183 kl_loss: 0.193055 normal_loss: 0.016392\n",
      "[709/00024] train_loss: 0.022170 kl_loss: 0.192933 normal_loss: 0.016382\n",
      "[710/00049] train_loss: 0.022197 kl_loss: 0.192696 normal_loss: 0.016416\n",
      "[711/00074] train_loss: 0.022234 kl_loss: 0.193308 normal_loss: 0.016435\n",
      "[713/00024] train_loss: 0.022144 kl_loss: 0.193145 normal_loss: 0.016350\n",
      "[714/00049] train_loss: 0.022339 kl_loss: 0.193115 normal_loss: 0.016545\n",
      "[715/00074] train_loss: 0.022145 kl_loss: 0.192574 normal_loss: 0.016368\n",
      "[717/00024] train_loss: 0.022220 kl_loss: 0.193085 normal_loss: 0.016428\n",
      "[718/00049] train_loss: 0.022263 kl_loss: 0.193139 normal_loss: 0.016469\n",
      "[719/00074] train_loss: 0.022102 kl_loss: 0.192513 normal_loss: 0.016326\n",
      "[721/00024] train_loss: 0.022180 kl_loss: 0.192606 normal_loss: 0.016402\n",
      "[722/00049] train_loss: 0.022199 kl_loss: 0.192709 normal_loss: 0.016418\n",
      "[723/00074] train_loss: 0.022217 kl_loss: 0.193315 normal_loss: 0.016418\n",
      "[725/00024] train_loss: 0.022259 kl_loss: 0.193149 normal_loss: 0.016465\n",
      "[726/00049] train_loss: 0.022131 kl_loss: 0.192303 normal_loss: 0.016362\n",
      "[727/00074] train_loss: 0.022175 kl_loss: 0.193073 normal_loss: 0.016383\n",
      "[729/00024] train_loss: 0.022163 kl_loss: 0.193065 normal_loss: 0.016371\n",
      "[730/00049] train_loss: 0.022236 kl_loss: 0.192464 normal_loss: 0.016462\n",
      "[731/00074] train_loss: 0.022144 kl_loss: 0.192894 normal_loss: 0.016357\n",
      "[733/00024] train_loss: 0.022302 kl_loss: 0.193359 normal_loss: 0.016501\n",
      "[734/00049] train_loss: 0.022022 kl_loss: 0.191808 normal_loss: 0.016267\n",
      "[735/00074] train_loss: 0.022240 kl_loss: 0.193152 normal_loss: 0.016446\n",
      "[737/00024] train_loss: 0.022133 kl_loss: 0.192395 normal_loss: 0.016361\n",
      "[738/00049] train_loss: 0.022173 kl_loss: 0.193190 normal_loss: 0.016377\n",
      "[739/00074] train_loss: 0.022207 kl_loss: 0.192627 normal_loss: 0.016429\n",
      "[741/00024] train_loss: 0.022108 kl_loss: 0.192077 normal_loss: 0.016346\n",
      "[742/00049] train_loss: 0.022293 kl_loss: 0.192749 normal_loss: 0.016510\n",
      "[743/00074] train_loss: 0.022134 kl_loss: 0.193287 normal_loss: 0.016335\n",
      "[745/00024] train_loss: 0.022115 kl_loss: 0.192696 normal_loss: 0.016334\n",
      "[746/00049] train_loss: 0.022200 kl_loss: 0.192661 normal_loss: 0.016420\n",
      "[747/00074] train_loss: 0.022151 kl_loss: 0.192652 normal_loss: 0.016372\n",
      "[749/00024] train_loss: 0.022086 kl_loss: 0.192793 normal_loss: 0.016303\n",
      "[750/00049] train_loss: 0.022191 kl_loss: 0.192273 normal_loss: 0.016423\n",
      "[751/00074] train_loss: 0.022151 kl_loss: 0.192836 normal_loss: 0.016366\n",
      "[753/00024] train_loss: 0.022148 kl_loss: 0.192241 normal_loss: 0.016381\n",
      "[754/00049] train_loss: 0.022221 kl_loss: 0.192793 normal_loss: 0.016437\n",
      "[755/00074] train_loss: 0.022127 kl_loss: 0.192770 normal_loss: 0.016344\n",
      "[757/00024] train_loss: 0.022157 kl_loss: 0.192779 normal_loss: 0.016374\n",
      "[758/00049] train_loss: 0.022163 kl_loss: 0.192175 normal_loss: 0.016398\n",
      "[759/00074] train_loss: 0.022111 kl_loss: 0.192749 normal_loss: 0.016329\n",
      "[761/00024] train_loss: 0.022174 kl_loss: 0.192576 normal_loss: 0.016397\n",
      "[762/00049] train_loss: 0.022103 kl_loss: 0.192109 normal_loss: 0.016340\n",
      "[763/00074] train_loss: 0.022131 kl_loss: 0.192919 normal_loss: 0.016344\n",
      "[765/00024] train_loss: 0.022118 kl_loss: 0.192848 normal_loss: 0.016333\n",
      "[766/00049] train_loss: 0.022077 kl_loss: 0.191684 normal_loss: 0.016326\n",
      "[767/00074] train_loss: 0.022233 kl_loss: 0.192960 normal_loss: 0.016445\n",
      "[769/00024] train_loss: 0.022214 kl_loss: 0.192496 normal_loss: 0.016439\n",
      "[770/00049] train_loss: 0.022049 kl_loss: 0.192242 normal_loss: 0.016282\n",
      "[771/00074] train_loss: 0.022085 kl_loss: 0.192649 normal_loss: 0.016305\n",
      "[773/00024] train_loss: 0.022132 kl_loss: 0.192773 normal_loss: 0.016349\n",
      "[774/00049] train_loss: 0.022043 kl_loss: 0.192398 normal_loss: 0.016271\n",
      "[775/00074] train_loss: 0.022141 kl_loss: 0.192103 normal_loss: 0.016378\n",
      "[777/00024] train_loss: 0.022094 kl_loss: 0.192221 normal_loss: 0.016327\n",
      "[778/00049] train_loss: 0.022137 kl_loss: 0.192626 normal_loss: 0.016358\n",
      "[779/00074] train_loss: 0.022127 kl_loss: 0.192317 normal_loss: 0.016358\n",
      "[781/00024] train_loss: 0.022166 kl_loss: 0.192739 normal_loss: 0.016384\n",
      "[782/00049] train_loss: 0.021996 kl_loss: 0.191584 normal_loss: 0.016248\n",
      "[783/00074] train_loss: 0.022131 kl_loss: 0.192738 normal_loss: 0.016349\n",
      "[785/00024] train_loss: 0.022054 kl_loss: 0.192208 normal_loss: 0.016288\n",
      "[786/00049] train_loss: 0.022148 kl_loss: 0.192231 normal_loss: 0.016381\n",
      "[787/00074] train_loss: 0.022115 kl_loss: 0.192520 normal_loss: 0.016339\n",
      "[789/00024] train_loss: 0.022065 kl_loss: 0.191957 normal_loss: 0.016306\n",
      "[790/00049] train_loss: 0.021946 kl_loss: 0.192245 normal_loss: 0.016179\n",
      "[791/00074] train_loss: 0.022178 kl_loss: 0.192648 normal_loss: 0.016399\n",
      "[793/00024] train_loss: 0.022065 kl_loss: 0.192056 normal_loss: 0.016303\n",
      "[794/00049] train_loss: 0.022135 kl_loss: 0.192530 normal_loss: 0.016360\n",
      "[795/00074] train_loss: 0.022075 kl_loss: 0.192156 normal_loss: 0.016311\n",
      "[797/00024] train_loss: 0.022094 kl_loss: 0.191999 normal_loss: 0.016334\n",
      "[798/00049] train_loss: 0.022044 kl_loss: 0.192104 normal_loss: 0.016281\n",
      "[799/00074] train_loss: 0.022097 kl_loss: 0.192534 normal_loss: 0.016321\n",
      "[801/00024] train_loss: 0.022018 kl_loss: 0.192081 normal_loss: 0.016256\n",
      "[802/00049] train_loss: 0.022155 kl_loss: 0.192694 normal_loss: 0.016375\n",
      "[803/00074] train_loss: 0.022019 kl_loss: 0.191781 normal_loss: 0.016265\n",
      "[805/00024] train_loss: 0.022037 kl_loss: 0.191890 normal_loss: 0.016281\n",
      "[806/00049] train_loss: 0.022027 kl_loss: 0.192040 normal_loss: 0.016266\n",
      "[807/00074] train_loss: 0.022061 kl_loss: 0.192573 normal_loss: 0.016284\n",
      "[809/00024] train_loss: 0.022050 kl_loss: 0.191962 normal_loss: 0.016292\n",
      "[810/00049] train_loss: 0.022106 kl_loss: 0.192234 normal_loss: 0.016339\n",
      "[811/00074] train_loss: 0.022069 kl_loss: 0.192251 normal_loss: 0.016301\n",
      "[813/00024] train_loss: 0.022148 kl_loss: 0.192622 normal_loss: 0.016369\n",
      "[814/00049] train_loss: 0.021942 kl_loss: 0.191348 normal_loss: 0.016202\n",
      "[815/00074] train_loss: 0.022161 kl_loss: 0.192426 normal_loss: 0.016388\n",
      "[817/00024] train_loss: 0.022036 kl_loss: 0.191572 normal_loss: 0.016289\n",
      "[818/00049] train_loss: 0.022118 kl_loss: 0.192914 normal_loss: 0.016331\n",
      "[819/00074] train_loss: 0.021932 kl_loss: 0.191857 normal_loss: 0.016176\n",
      "[821/00024] train_loss: 0.022099 kl_loss: 0.192221 normal_loss: 0.016332\n",
      "[822/00049] train_loss: 0.022064 kl_loss: 0.191957 normal_loss: 0.016306\n",
      "[823/00074] train_loss: 0.022024 kl_loss: 0.192109 normal_loss: 0.016261\n",
      "[825/00024] train_loss: 0.022080 kl_loss: 0.192380 normal_loss: 0.016309\n",
      "[826/00049] train_loss: 0.021896 kl_loss: 0.191702 normal_loss: 0.016145\n",
      "[827/00074] train_loss: 0.022146 kl_loss: 0.192153 normal_loss: 0.016381\n",
      "[829/00024] train_loss: 0.022011 kl_loss: 0.192340 normal_loss: 0.016241\n",
      "[830/00049] train_loss: 0.021962 kl_loss: 0.191691 normal_loss: 0.016211\n",
      "[831/00074] train_loss: 0.022067 kl_loss: 0.192152 normal_loss: 0.016302\n",
      "[833/00024] train_loss: 0.021942 kl_loss: 0.191521 normal_loss: 0.016197\n",
      "[834/00049] train_loss: 0.022020 kl_loss: 0.192416 normal_loss: 0.016247\n",
      "[835/00074] train_loss: 0.022012 kl_loss: 0.192189 normal_loss: 0.016247\n",
      "[837/00024] train_loss: 0.022025 kl_loss: 0.192145 normal_loss: 0.016261\n",
      "[838/00049] train_loss: 0.021975 kl_loss: 0.191600 normal_loss: 0.016227\n",
      "[839/00074] train_loss: 0.022036 kl_loss: 0.192324 normal_loss: 0.016266\n",
      "[841/00024] train_loss: 0.021955 kl_loss: 0.192290 normal_loss: 0.016187\n",
      "[842/00049] train_loss: 0.022038 kl_loss: 0.191131 normal_loss: 0.016305\n",
      "[843/00074] train_loss: 0.022104 kl_loss: 0.192595 normal_loss: 0.016327\n",
      "[845/00024] train_loss: 0.022048 kl_loss: 0.192295 normal_loss: 0.016279\n",
      "[846/00049] train_loss: 0.022033 kl_loss: 0.191820 normal_loss: 0.016279\n",
      "[847/00074] train_loss: 0.022006 kl_loss: 0.191848 normal_loss: 0.016250\n",
      "[849/00024] train_loss: 0.022024 kl_loss: 0.192401 normal_loss: 0.016252\n",
      "[850/00049] train_loss: 0.022044 kl_loss: 0.191629 normal_loss: 0.016296\n",
      "[851/00074] train_loss: 0.021987 kl_loss: 0.191881 normal_loss: 0.016231\n",
      "[853/00024] train_loss: 0.022177 kl_loss: 0.192663 normal_loss: 0.016397\n",
      "[854/00049] train_loss: 0.021870 kl_loss: 0.191210 normal_loss: 0.016133\n",
      "[855/00074] train_loss: 0.022091 kl_loss: 0.191986 normal_loss: 0.016332\n",
      "[857/00024] train_loss: 0.021955 kl_loss: 0.191472 normal_loss: 0.016210\n",
      "[858/00049] train_loss: 0.022131 kl_loss: 0.192447 normal_loss: 0.016358\n",
      "[859/00074] train_loss: 0.022023 kl_loss: 0.191888 normal_loss: 0.016266\n",
      "[861/00024] train_loss: 0.021976 kl_loss: 0.191936 normal_loss: 0.016218\n",
      "[862/00049] train_loss: 0.022100 kl_loss: 0.192332 normal_loss: 0.016330\n",
      "[863/00074] train_loss: 0.021975 kl_loss: 0.191487 normal_loss: 0.016230\n",
      "[865/00024] train_loss: 0.021997 kl_loss: 0.191722 normal_loss: 0.016246\n",
      "[866/00049] train_loss: 0.022069 kl_loss: 0.192063 normal_loss: 0.016308\n",
      "[867/00074] train_loss: 0.022007 kl_loss: 0.191920 normal_loss: 0.016250\n",
      "[869/00024] train_loss: 0.022043 kl_loss: 0.191694 normal_loss: 0.016292\n",
      "[870/00049] train_loss: 0.021981 kl_loss: 0.191911 normal_loss: 0.016224\n",
      "[871/00074] train_loss: 0.022041 kl_loss: 0.192047 normal_loss: 0.016279\n",
      "[873/00024] train_loss: 0.022010 kl_loss: 0.191440 normal_loss: 0.016267\n",
      "[874/00049] train_loss: 0.022003 kl_loss: 0.192141 normal_loss: 0.016239\n",
      "[875/00074] train_loss: 0.022037 kl_loss: 0.192021 normal_loss: 0.016276\n",
      "[877/00024] train_loss: 0.022098 kl_loss: 0.192039 normal_loss: 0.016337\n",
      "[878/00049] train_loss: 0.021967 kl_loss: 0.191321 normal_loss: 0.016227\n",
      "[879/00074] train_loss: 0.022031 kl_loss: 0.192188 normal_loss: 0.016266\n",
      "[881/00024] train_loss: 0.022055 kl_loss: 0.192140 normal_loss: 0.016291\n",
      "[882/00049] train_loss: 0.021950 kl_loss: 0.191257 normal_loss: 0.016212\n",
      "[883/00074] train_loss: 0.022064 kl_loss: 0.192096 normal_loss: 0.016301\n",
      "[885/00024] train_loss: 0.021981 kl_loss: 0.191613 normal_loss: 0.016233\n",
      "[886/00049] train_loss: 0.021918 kl_loss: 0.191777 normal_loss: 0.016164\n",
      "[887/00074] train_loss: 0.022057 kl_loss: 0.192051 normal_loss: 0.016296\n",
      "[889/00024] train_loss: 0.022019 kl_loss: 0.191865 normal_loss: 0.016263\n",
      "[890/00049] train_loss: 0.021989 kl_loss: 0.191718 normal_loss: 0.016238\n",
      "[891/00074] train_loss: 0.022006 kl_loss: 0.191805 normal_loss: 0.016252\n",
      "[893/00024] train_loss: 0.021917 kl_loss: 0.191675 normal_loss: 0.016167\n",
      "[894/00049] train_loss: 0.022016 kl_loss: 0.192150 normal_loss: 0.016252\n",
      "[895/00074] train_loss: 0.021974 kl_loss: 0.191511 normal_loss: 0.016228\n",
      "[897/00024] train_loss: 0.021910 kl_loss: 0.191655 normal_loss: 0.016160\n",
      "[898/00049] train_loss: 0.022060 kl_loss: 0.192241 normal_loss: 0.016293\n",
      "[899/00074] train_loss: 0.021925 kl_loss: 0.191383 normal_loss: 0.016183\n",
      "[901/00024] train_loss: 0.022032 kl_loss: 0.191639 normal_loss: 0.016283\n",
      "[902/00049] train_loss: 0.022071 kl_loss: 0.192033 normal_loss: 0.016310\n",
      "[903/00074] train_loss: 0.021889 kl_loss: 0.191566 normal_loss: 0.016142\n",
      "[905/00024] train_loss: 0.022022 kl_loss: 0.191974 normal_loss: 0.016263\n",
      "[906/00049] train_loss: 0.021998 kl_loss: 0.191948 normal_loss: 0.016240\n",
      "[907/00074] train_loss: 0.021940 kl_loss: 0.191290 normal_loss: 0.016201\n",
      "[909/00024] train_loss: 0.022004 kl_loss: 0.191715 normal_loss: 0.016252\n",
      "[910/00049] train_loss: 0.021996 kl_loss: 0.191528 normal_loss: 0.016250\n",
      "[911/00074] train_loss: 0.022047 kl_loss: 0.191942 normal_loss: 0.016289\n",
      "[913/00024] train_loss: 0.022035 kl_loss: 0.191634 normal_loss: 0.016286\n",
      "[914/00049] train_loss: 0.021936 kl_loss: 0.191883 normal_loss: 0.016179\n",
      "[915/00074] train_loss: 0.021969 kl_loss: 0.191643 normal_loss: 0.016220\n",
      "[917/00024] train_loss: 0.022019 kl_loss: 0.191728 normal_loss: 0.016267\n",
      "[918/00049] train_loss: 0.021863 kl_loss: 0.191249 normal_loss: 0.016126\n",
      "[919/00074] train_loss: 0.022016 kl_loss: 0.192155 normal_loss: 0.016252\n",
      "[921/00024] train_loss: 0.022011 kl_loss: 0.191738 normal_loss: 0.016258\n",
      "[922/00049] train_loss: 0.021949 kl_loss: 0.191750 normal_loss: 0.016196\n",
      "[923/00074] train_loss: 0.021926 kl_loss: 0.191617 normal_loss: 0.016177\n",
      "[925/00024] train_loss: 0.022068 kl_loss: 0.191742 normal_loss: 0.016316\n",
      "[926/00049] train_loss: 0.021864 kl_loss: 0.191137 normal_loss: 0.016130\n",
      "[927/00074] train_loss: 0.022128 kl_loss: 0.192200 normal_loss: 0.016362\n",
      "[929/00024] train_loss: 0.021959 kl_loss: 0.191639 normal_loss: 0.016210\n",
      "[930/00049] train_loss: 0.022004 kl_loss: 0.191624 normal_loss: 0.016256\n",
      "[931/00074] train_loss: 0.021999 kl_loss: 0.191791 normal_loss: 0.016246\n",
      "[933/00024] train_loss: 0.021939 kl_loss: 0.191657 normal_loss: 0.016190\n",
      "[934/00049] train_loss: 0.022023 kl_loss: 0.191517 normal_loss: 0.016277\n",
      "[935/00074] train_loss: 0.021997 kl_loss: 0.191854 normal_loss: 0.016241\n",
      "[937/00024] train_loss: 0.022022 kl_loss: 0.191942 normal_loss: 0.016263\n",
      "[938/00049] train_loss: 0.021886 kl_loss: 0.191019 normal_loss: 0.016155\n",
      "[939/00074] train_loss: 0.022011 kl_loss: 0.192040 normal_loss: 0.016250\n",
      "[941/00024] train_loss: 0.021921 kl_loss: 0.191682 normal_loss: 0.016171\n",
      "[942/00049] train_loss: 0.021994 kl_loss: 0.191574 normal_loss: 0.016247\n",
      "[943/00074] train_loss: 0.022012 kl_loss: 0.191719 normal_loss: 0.016261\n",
      "[945/00024] train_loss: 0.021885 kl_loss: 0.191296 normal_loss: 0.016146\n",
      "[946/00049] train_loss: 0.022062 kl_loss: 0.192043 normal_loss: 0.016301\n",
      "[947/00074] train_loss: 0.022026 kl_loss: 0.191610 normal_loss: 0.016278\n",
      "[949/00024] train_loss: 0.022009 kl_loss: 0.191298 normal_loss: 0.016270\n",
      "[950/00049] train_loss: 0.021986 kl_loss: 0.191882 normal_loss: 0.016230\n",
      "[951/00074] train_loss: 0.021951 kl_loss: 0.191745 normal_loss: 0.016199\n",
      "[953/00024] train_loss: 0.021904 kl_loss: 0.191753 normal_loss: 0.016151\n",
      "[954/00049] train_loss: 0.022012 kl_loss: 0.191724 normal_loss: 0.016260\n",
      "[955/00074] train_loss: 0.021964 kl_loss: 0.191421 normal_loss: 0.016221\n",
      "[957/00024] train_loss: 0.021980 kl_loss: 0.191054 normal_loss: 0.016249\n",
      "[958/00049] train_loss: 0.021939 kl_loss: 0.192471 normal_loss: 0.016165\n",
      "[959/00074] train_loss: 0.022035 kl_loss: 0.191347 normal_loss: 0.016295\n",
      "[961/00024] train_loss: 0.022064 kl_loss: 0.192045 normal_loss: 0.016303\n",
      "[962/00049] train_loss: 0.021808 kl_loss: 0.190673 normal_loss: 0.016087\n",
      "[963/00074] train_loss: 0.021969 kl_loss: 0.192129 normal_loss: 0.016205\n",
      "[965/00024] train_loss: 0.021854 kl_loss: 0.191217 normal_loss: 0.016118\n",
      "[966/00049] train_loss: 0.021976 kl_loss: 0.191693 normal_loss: 0.016225\n",
      "[967/00074] train_loss: 0.022077 kl_loss: 0.191909 normal_loss: 0.016319\n",
      "[969/00024] train_loss: 0.022009 kl_loss: 0.191571 normal_loss: 0.016261\n",
      "[970/00049] train_loss: 0.021872 kl_loss: 0.191725 normal_loss: 0.016120\n",
      "[971/00074] train_loss: 0.021964 kl_loss: 0.191497 normal_loss: 0.016219\n",
      "[973/00024] train_loss: 0.021982 kl_loss: 0.191323 normal_loss: 0.016243\n",
      "[974/00049] train_loss: 0.021961 kl_loss: 0.191890 normal_loss: 0.016204\n",
      "[975/00074] train_loss: 0.021908 kl_loss: 0.191551 normal_loss: 0.016162\n",
      "[977/00024] train_loss: 0.021958 kl_loss: 0.191645 normal_loss: 0.016209\n",
      "[978/00049] train_loss: 0.021956 kl_loss: 0.190991 normal_loss: 0.016226\n",
      "[979/00074] train_loss: 0.021998 kl_loss: 0.192100 normal_loss: 0.016235\n",
      "[981/00024] train_loss: 0.021987 kl_loss: 0.191321 normal_loss: 0.016248\n",
      "[982/00049] train_loss: 0.022037 kl_loss: 0.191593 normal_loss: 0.016289\n",
      "[983/00074] train_loss: 0.021924 kl_loss: 0.191798 normal_loss: 0.016170\n",
      "[985/00024] train_loss: 0.021876 kl_loss: 0.191630 normal_loss: 0.016127\n",
      "[986/00049] train_loss: 0.021965 kl_loss: 0.191649 normal_loss: 0.016216\n",
      "[987/00074] train_loss: 0.021941 kl_loss: 0.191403 normal_loss: 0.016198\n",
      "[989/00024] train_loss: 0.021929 kl_loss: 0.191290 normal_loss: 0.016190\n",
      "[990/00049] train_loss: 0.022095 kl_loss: 0.192813 normal_loss: 0.016311\n",
      "[991/00074] train_loss: 0.021918 kl_loss: 0.190552 normal_loss: 0.016202\n",
      "[993/00024] train_loss: 0.021930 kl_loss: 0.191840 normal_loss: 0.016175\n",
      "[994/00049] train_loss: 0.022033 kl_loss: 0.191467 normal_loss: 0.016289\n",
      "[995/00074] train_loss: 0.021902 kl_loss: 0.191323 normal_loss: 0.016163\n",
      "[997/00024] train_loss: 0.021971 kl_loss: 0.191548 normal_loss: 0.016225\n",
      "[998/00049] train_loss: 0.021952 kl_loss: 0.191428 normal_loss: 0.016209\n",
      "[999/00074] train_loss: 0.021858 kl_loss: 0.191627 normal_loss: 0.016109\n"
     ]
    }
   ],
   "source": [
    "# TABLE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'table_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'table',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#                   #\n",
    "#    VISUALIZING    #\n",
    "#                   #\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.visualize import visualize_dataset_sample, visualize_ad, visualize_vad, visualize_vad_norm, visualize_vad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48447773792a41349a5832684ac3814c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd3470b17584f87a083b6b557df136f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "experiment = \"table_vad\"\n",
    "experiment2 = \"sofa_ad\"\n",
    "# experiment2 = \"sofa_ad\"\n",
    "filter_class = \"table\"\n",
    "index = 4123\n",
    "index1 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "index2 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "a1 = 0.5\n",
    "a2 = 1 - a1\n",
    "#-------\n",
    "# visualize_vad_norm(experiment2)\n",
    "visualize_vad_norm(experiment)\n",
    "visualize_ad(experiment, index1)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_ad(experiment, index)\n",
    "#-------\n",
    "# visualize_vad_norm(experiment)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_dataset_sample(filter_class, index)\n",
    "#-------\n",
    "# visualize_interpolation_ad(experiment, index1, index2, a1, a2)\n",
    "# visualize_ad(experiment, index1)\n",
    "# visualize_ad(experiment, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#                  #\n",
    "#    EVALUATION    #\n",
    "#                  #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate import mmd, generate_samples, convert_df_to_point_cloud, visualize_point_cloud, chamfer_distance_numpy, mmd\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1634788faa34f80bf0372959a34e9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc441affc4f4854b37b33d7d5f29e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17727ea305d84fcfbce4ece62823771f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=768, layout=Layout(height='auto', width='100%'), width=1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1531fb5d3b24181955cd4adab3e1345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ViewInteractiveWidget(height=768, layout=Layout(height='auto', width='100%'), width=1024)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MMD\n",
    "\n",
    "# generate n new samples\n",
    "n = 2\n",
    "samples = generate_samples('sofa_vad', n)\n",
    "\n",
    "sample1 = samples[0]\n",
    "sample2 = samples[1]\n",
    "\n",
    "# visualize\n",
    "input_mesh = marching_cubes(sample1.squeeze(0).detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "\n",
    "input_mesh = marching_cubes(sample2.squeeze(0).detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "\n",
    "point_cloud = convert_df_to_point_cloud(sample1.squeeze(0))\n",
    "visualize_point_cloud(point_cloud)\n",
    "\n",
    "point_cloud2 = convert_df_to_point_cloud(sample2.squeeze(0))\n",
    "visualize_point_cloud(point_cloud2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9491640607287115"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate chamfer distance between the 2 samples\n",
    "chamfer_distance_numpy(point_cloud, point_cloud2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.21344659, 1.21994092])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mmd between the 2 samples\n",
    "# Note: the final value changes a bit every time because every time a new point cloud is generated\n",
    "# Note: mmd function takes a lot of time, needs optimization\n",
    "mmd(samples, samples)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8edcb304c74d7de69396267fdd221ef1d3cdc7db9124f1020d58ca6af5038c14"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
