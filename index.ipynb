{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 197376  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 131584  \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | decoder1     | Sequential       | 8389376 \n",
      "6  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "7  | decoder1.1   | BatchNorm3d      | 512     \n",
      "8  | decoder1.2   | ReLU             | 0       \n",
      "9  | decoder2     | Sequential       | 2097536 \n",
      "10 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "11 | decoder2.1   | BatchNorm3d      | 256     \n",
      "12 | decoder2.2   | ReLU             | 0       \n",
      "13 | decoder3     | Sequential       | 524480  \n",
      "14 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "15 | decoder3.1   | BatchNorm3d      | 128     \n",
      "16 | decoder3.2   | ReLU             | 0       \n",
      "17 | decoder4     | Sequential       | 4097    \n",
      "18 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "19 | TOTAL        | ThreeDEPNDecoder | 11212865\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 3173\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class = 'sofa')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a4c43c491f48d09b0558b091bc1748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[0]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#                #\n",
    "#    TRAINING    #\n",
    "#                #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 3\n",
      "[001/00004] train_loss: 0.010115 kl_loss: 0.007572 normal_loss: 0.002543\n",
      "[001/00004] train_loss: 0.010374 kl_loss: 0.007749 normal_loss: 0.002626\n",
      "[002/00004] train_loss: 0.008716 kl_loss: 0.007542 normal_loss: 0.001174\n",
      "[002/00004] train_loss: 0.008393 kl_loss: 0.007422 normal_loss: 0.000971\n",
      "[003/00004] train_loss: 0.008175 kl_loss: 0.007302 normal_loss: 0.000872\n",
      "[003/00004] train_loss: 0.008076 kl_loss: 0.007325 normal_loss: 0.000750\n",
      "[004/00004] train_loss: 0.007750 kl_loss: 0.007125 normal_loss: 0.000625\n",
      "[004/00004] train_loss: 0.007689 kl_loss: 0.007174 normal_loss: 0.000515\n"
     ]
    }
   ],
   "source": [
    "# OVERFIT\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 4,\n",
    "    'print_every_n': 1,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 1,\n",
    "    'resume_ckpt': None\n",
    "\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Training params: 2\n",
      "[001/00035] train_loss: 0.107308 kl_loss: 0.000000 normal_loss: 0.107308\n",
      "[003/00007] train_loss: 0.045517 kl_loss: 0.000000 normal_loss: 0.045517\n",
      "[004/00043] train_loss: 0.036284 kl_loss: 0.000000 normal_loss: 0.036284\n",
      "[006/00015] train_loss: 0.034843 kl_loss: 0.000000 normal_loss: 0.034843\n",
      "[007/00051] train_loss: 0.030958 kl_loss: 0.000000 normal_loss: 0.030958\n",
      "[009/00023] train_loss: 0.029536 kl_loss: 0.000000 normal_loss: 0.029536\n",
      "[010/00059] train_loss: 0.031433 kl_loss: 0.000000 normal_loss: 0.031433\n",
      "[012/00031] train_loss: 0.030174 kl_loss: 0.000000 normal_loss: 0.030174\n",
      "[014/00003] train_loss: 0.029374 kl_loss: 0.000000 normal_loss: 0.029374\n",
      "[015/00039] train_loss: 0.028661 kl_loss: 0.000000 normal_loss: 0.028661\n",
      "[017/00011] train_loss: 0.027764 kl_loss: 0.000000 normal_loss: 0.027764\n",
      "[018/00047] train_loss: 0.026550 kl_loss: 0.000000 normal_loss: 0.026550\n",
      "[020/00019] train_loss: 0.025417 kl_loss: 0.000000 normal_loss: 0.025417\n",
      "[021/00055] train_loss: 0.020581 kl_loss: 0.000000 normal_loss: 0.020581\n",
      "[023/00027] train_loss: 0.020864 kl_loss: 0.000000 normal_loss: 0.020864\n",
      "[024/00063] train_loss: 0.020507 kl_loss: 0.000000 normal_loss: 0.020507\n",
      "[026/00035] train_loss: 0.019881 kl_loss: 0.000000 normal_loss: 0.019881\n",
      "[028/00007] train_loss: 0.020202 kl_loss: 0.000000 normal_loss: 0.020202\n",
      "[029/00043] train_loss: 0.020114 kl_loss: 0.000000 normal_loss: 0.020114\n",
      "[031/00015] train_loss: 0.019694 kl_loss: 0.000000 normal_loss: 0.019694\n",
      "[032/00051] train_loss: 0.019919 kl_loss: 0.000000 normal_loss: 0.019919\n",
      "[034/00023] train_loss: 0.019752 kl_loss: 0.000000 normal_loss: 0.019752\n",
      "[035/00059] train_loss: 0.019399 kl_loss: 0.000000 normal_loss: 0.019399\n",
      "[037/00031] train_loss: 0.019251 kl_loss: 0.000000 normal_loss: 0.019251\n",
      "[039/00003] train_loss: 0.018806 kl_loss: 0.000000 normal_loss: 0.018806\n",
      "[040/00039] train_loss: 0.018089 kl_loss: 0.000000 normal_loss: 0.018089\n",
      "[042/00011] train_loss: 0.016048 kl_loss: 0.000000 normal_loss: 0.016048\n",
      "[043/00047] train_loss: 0.015897 kl_loss: 0.000000 normal_loss: 0.015897\n",
      "[045/00019] train_loss: 0.016128 kl_loss: 0.000000 normal_loss: 0.016128\n",
      "[046/00055] train_loss: 0.015799 kl_loss: 0.000000 normal_loss: 0.015799\n",
      "[048/00027] train_loss: 0.015944 kl_loss: 0.000000 normal_loss: 0.015944\n",
      "[049/00063] train_loss: 0.016014 kl_loss: 0.000000 normal_loss: 0.016014\n",
      "[051/00035] train_loss: 0.015566 kl_loss: 0.000000 normal_loss: 0.015566\n",
      "[053/00007] train_loss: 0.015497 kl_loss: 0.000000 normal_loss: 0.015497\n",
      "[054/00043] train_loss: 0.015785 kl_loss: 0.000000 normal_loss: 0.015785\n",
      "[056/00015] train_loss: 0.015714 kl_loss: 0.000000 normal_loss: 0.015714\n",
      "[057/00051] train_loss: 0.015630 kl_loss: 0.000000 normal_loss: 0.015630\n",
      "[059/00023] train_loss: 0.015047 kl_loss: 0.000000 normal_loss: 0.015047\n",
      "[060/00059] train_loss: 0.014603 kl_loss: 0.000000 normal_loss: 0.014603\n",
      "[062/00031] train_loss: 0.014013 kl_loss: 0.000000 normal_loss: 0.014013\n",
      "[064/00003] train_loss: 0.013866 kl_loss: 0.000000 normal_loss: 0.013866\n",
      "[065/00039] train_loss: 0.013783 kl_loss: 0.000000 normal_loss: 0.013783\n",
      "[067/00011] train_loss: 0.013585 kl_loss: 0.000000 normal_loss: 0.013585\n",
      "[068/00047] train_loss: 0.013819 kl_loss: 0.000000 normal_loss: 0.013819\n",
      "[070/00019] train_loss: 0.013629 kl_loss: 0.000000 normal_loss: 0.013629\n",
      "[071/00055] train_loss: 0.013676 kl_loss: 0.000000 normal_loss: 0.013676\n",
      "[073/00027] train_loss: 0.013468 kl_loss: 0.000000 normal_loss: 0.013468\n",
      "[074/00063] train_loss: 0.013653 kl_loss: 0.000000 normal_loss: 0.013653\n",
      "[076/00035] train_loss: 0.013496 kl_loss: 0.000000 normal_loss: 0.013496\n",
      "[078/00007] train_loss: 0.013367 kl_loss: 0.000000 normal_loss: 0.013367\n",
      "[079/00043] train_loss: 0.013232 kl_loss: 0.000000 normal_loss: 0.013232\n",
      "[081/00015] train_loss: 0.013007 kl_loss: 0.000000 normal_loss: 0.013007\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3397, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Munzer Dwedari\\AppData\\Local\\Temp\\ipykernel_16500\\673902499.py\", line 20, in <cell line: 20>\n",
      "    train.main(config)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py\", line 167, in main\n",
      "  File \"c:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py\", line 55, in train\n",
      "    for epoch in range(config['max_epochs']):\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 368, in __iter__\n",
      "    return self._get_iterator()\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 314, in _get_iterator\n",
      "    return _MultiProcessingDataLoaderIter(self)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 927, in __init__\n",
      "    w.start()\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py\", line 224, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py\", line 327, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\popen_spawn_win32.py\", line 93, in __init__\n",
      "    reduction.dump(process_obj, to_child)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n",
      "    ForkingPickler(file, protocol).dump(obj)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1992, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "# AIRPLANE\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 20,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 6000\n",
      "Training params: 2\n",
      "[001/00005] train_loss: 0.170789 kl_loss: 0.000000 normal_loss: 0.170789\n",
      "[002/00011] train_loss: 0.119310 kl_loss: 0.000000 normal_loss: 0.119310\n",
      "[003/00017] train_loss: 0.112543 kl_loss: 0.000000 normal_loss: 0.112543\n",
      "[004/00023] train_loss: 0.106655 kl_loss: 0.000000 normal_loss: 0.106655\n",
      "[005/00029] train_loss: 0.104214 kl_loss: 0.000000 normal_loss: 0.104214\n",
      "[006/00035] train_loss: 0.098748 kl_loss: 0.000000 normal_loss: 0.098748\n",
      "[007/00041] train_loss: 0.095269 kl_loss: 0.000000 normal_loss: 0.095269\n",
      "[008/00047] train_loss: 0.090359 kl_loss: 0.000000 normal_loss: 0.090359\n",
      "[009/00053] train_loss: 0.089151 kl_loss: 0.000000 normal_loss: 0.089151\n",
      "[010/00059] train_loss: 0.084609 kl_loss: 0.000000 normal_loss: 0.084609\n",
      "[011/00065] train_loss: 0.083053 kl_loss: 0.000000 normal_loss: 0.083053\n",
      "[012/00071] train_loss: 0.081139 kl_loss: 0.000000 normal_loss: 0.081139\n",
      "[013/00077] train_loss: 0.077102 kl_loss: 0.000000 normal_loss: 0.077102\n",
      "[014/00083] train_loss: 0.074768 kl_loss: 0.000000 normal_loss: 0.074768\n",
      "[015/00089] train_loss: 0.073191 kl_loss: 0.000000 normal_loss: 0.073191\n",
      "[017/00001] train_loss: 0.069703 kl_loss: 0.000000 normal_loss: 0.069703\n",
      "[018/00007] train_loss: 0.067230 kl_loss: 0.000000 normal_loss: 0.067230\n",
      "[019/00013] train_loss: 0.064515 kl_loss: 0.000000 normal_loss: 0.064515\n",
      "[020/00019] train_loss: 0.061605 kl_loss: 0.000000 normal_loss: 0.061605\n",
      "[021/00025] train_loss: 0.059996 kl_loss: 0.000000 normal_loss: 0.059996\n",
      "[022/00031] train_loss: 0.058021 kl_loss: 0.000000 normal_loss: 0.058021\n",
      "[023/00037] train_loss: 0.056354 kl_loss: 0.000000 normal_loss: 0.056354\n",
      "[024/00043] train_loss: 0.054989 kl_loss: 0.000000 normal_loss: 0.054989\n",
      "[025/00049] train_loss: 0.054142 kl_loss: 0.000000 normal_loss: 0.054142\n",
      "[026/00055] train_loss: 0.051831 kl_loss: 0.000000 normal_loss: 0.051831\n",
      "[027/00061] train_loss: 0.049995 kl_loss: 0.000000 normal_loss: 0.049995\n",
      "[028/00067] train_loss: 0.049668 kl_loss: 0.000000 normal_loss: 0.049668\n",
      "[029/00073] train_loss: 0.047884 kl_loss: 0.000000 normal_loss: 0.047884\n",
      "[030/00079] train_loss: 0.047094 kl_loss: 0.000000 normal_loss: 0.047094\n",
      "[031/00085] train_loss: 0.044595 kl_loss: 0.000000 normal_loss: 0.044595\n",
      "[032/00091] train_loss: 0.045461 kl_loss: 0.000000 normal_loss: 0.045461\n",
      "[034/00003] train_loss: 0.043912 kl_loss: 0.000000 normal_loss: 0.043912\n",
      "[035/00009] train_loss: 0.042033 kl_loss: 0.000000 normal_loss: 0.042033\n",
      "[036/00015] train_loss: 0.040447 kl_loss: 0.000000 normal_loss: 0.040447\n",
      "[037/00021] train_loss: 0.040736 kl_loss: 0.000000 normal_loss: 0.040736\n",
      "[038/00027] train_loss: 0.038222 kl_loss: 0.000000 normal_loss: 0.038222\n",
      "[039/00033] train_loss: 0.038747 kl_loss: 0.000000 normal_loss: 0.038747\n",
      "[040/00039] train_loss: 0.037341 kl_loss: 0.000000 normal_loss: 0.037341\n",
      "[041/00045] train_loss: 0.036072 kl_loss: 0.000000 normal_loss: 0.036072\n",
      "[042/00051] train_loss: 0.035633 kl_loss: 0.000000 normal_loss: 0.035633\n",
      "[043/00057] train_loss: 0.036622 kl_loss: 0.000000 normal_loss: 0.036622\n",
      "[044/00063] train_loss: 0.035593 kl_loss: 0.000000 normal_loss: 0.035593\n",
      "[045/00069] train_loss: 0.034966 kl_loss: 0.000000 normal_loss: 0.034966\n",
      "[046/00075] train_loss: 0.033288 kl_loss: 0.000000 normal_loss: 0.033288\n",
      "[047/00081] train_loss: 0.033041 kl_loss: 0.000000 normal_loss: 0.033041\n",
      "[048/00087] train_loss: 0.032261 kl_loss: 0.000000 normal_loss: 0.032261\n",
      "[049/00093] train_loss: 0.032458 kl_loss: 0.000000 normal_loss: 0.032458\n",
      "[051/00005] train_loss: 0.031604 kl_loss: 0.000000 normal_loss: 0.031604\n",
      "[052/00011] train_loss: 0.030685 kl_loss: 0.000000 normal_loss: 0.030685\n",
      "[053/00017] train_loss: 0.029773 kl_loss: 0.000000 normal_loss: 0.029773\n",
      "[054/00023] train_loss: 0.029984 kl_loss: 0.000000 normal_loss: 0.029984\n",
      "[055/00029] train_loss: 0.029556 kl_loss: 0.000000 normal_loss: 0.029556\n",
      "[056/00035] train_loss: 0.029127 kl_loss: 0.000000 normal_loss: 0.029127\n",
      "[057/00041] train_loss: 0.028202 kl_loss: 0.000000 normal_loss: 0.028202\n",
      "[058/00047] train_loss: 0.028775 kl_loss: 0.000000 normal_loss: 0.028775\n",
      "[059/00053] train_loss: 0.027824 kl_loss: 0.000000 normal_loss: 0.027824\n",
      "[060/00059] train_loss: 0.026386 kl_loss: 0.000000 normal_loss: 0.026386\n",
      "[061/00065] train_loss: 0.027572 kl_loss: 0.000000 normal_loss: 0.027572\n",
      "[062/00071] train_loss: 0.027168 kl_loss: 0.000000 normal_loss: 0.027168\n",
      "[063/00077] train_loss: 0.026140 kl_loss: 0.000000 normal_loss: 0.026140\n",
      "[064/00083] train_loss: 0.026373 kl_loss: 0.000000 normal_loss: 0.026373\n",
      "[065/00089] train_loss: 0.025501 kl_loss: 0.000000 normal_loss: 0.025501\n",
      "[067/00001] train_loss: 0.026152 kl_loss: 0.000000 normal_loss: 0.026152\n",
      "[068/00007] train_loss: 0.025439 kl_loss: 0.000000 normal_loss: 0.025439\n",
      "[069/00013] train_loss: 0.025331 kl_loss: 0.000000 normal_loss: 0.025331\n",
      "[070/00019] train_loss: 0.024891 kl_loss: 0.000000 normal_loss: 0.024891\n",
      "[071/00025] train_loss: 0.023769 kl_loss: 0.000000 normal_loss: 0.023769\n",
      "[072/00031] train_loss: 0.023991 kl_loss: 0.000000 normal_loss: 0.023991\n",
      "[073/00037] train_loss: 0.023924 kl_loss: 0.000000 normal_loss: 0.023924\n",
      "[074/00043] train_loss: 0.024434 kl_loss: 0.000000 normal_loss: 0.024434\n",
      "[075/00049] train_loss: 0.023806 kl_loss: 0.000000 normal_loss: 0.023806\n",
      "[076/00055] train_loss: 0.022933 kl_loss: 0.000000 normal_loss: 0.022933\n",
      "[077/00061] train_loss: 0.023120 kl_loss: 0.000000 normal_loss: 0.023120\n",
      "[078/00067] train_loss: 0.023447 kl_loss: 0.000000 normal_loss: 0.023447\n",
      "[079/00073] train_loss: 0.023398 kl_loss: 0.000000 normal_loss: 0.023398\n",
      "[080/00079] train_loss: 0.023096 kl_loss: 0.000000 normal_loss: 0.023096\n",
      "[081/00085] train_loss: 0.022661 kl_loss: 0.000000 normal_loss: 0.022661\n",
      "[082/00091] train_loss: 0.022616 kl_loss: 0.000000 normal_loss: 0.022616\n",
      "[084/00003] train_loss: 0.022916 kl_loss: 0.000000 normal_loss: 0.022916\n",
      "[085/00009] train_loss: 0.022678 kl_loss: 0.000000 normal_loss: 0.022678\n",
      "[086/00015] train_loss: 0.022548 kl_loss: 0.000000 normal_loss: 0.022548\n",
      "[087/00021] train_loss: 0.022197 kl_loss: 0.000000 normal_loss: 0.022197\n",
      "[088/00027] train_loss: 0.022057 kl_loss: 0.000000 normal_loss: 0.022057\n",
      "[089/00033] train_loss: 0.021671 kl_loss: 0.000000 normal_loss: 0.021671\n",
      "[090/00039] train_loss: 0.021144 kl_loss: 0.000000 normal_loss: 0.021144\n",
      "[091/00045] train_loss: 0.021607 kl_loss: 0.000000 normal_loss: 0.021607\n",
      "[092/00051] train_loss: 0.021018 kl_loss: 0.000000 normal_loss: 0.021018\n",
      "[093/00057] train_loss: 0.021011 kl_loss: 0.000000 normal_loss: 0.021011\n",
      "[094/00063] train_loss: 0.020048 kl_loss: 0.000000 normal_loss: 0.020048\n",
      "[095/00069] train_loss: 0.020947 kl_loss: 0.000000 normal_loss: 0.020947\n",
      "[096/00075] train_loss: 0.021325 kl_loss: 0.000000 normal_loss: 0.021325\n",
      "[097/00081] train_loss: 0.020538 kl_loss: 0.000000 normal_loss: 0.020538\n",
      "[098/00087] train_loss: 0.021223 kl_loss: 0.000000 normal_loss: 0.021223\n",
      "[099/00093] train_loss: 0.020470 kl_loss: 0.000000 normal_loss: 0.020470\n",
      "[101/00005] train_loss: 0.017555 kl_loss: 0.000000 normal_loss: 0.017555\n",
      "[102/00011] train_loss: 0.015651 kl_loss: 0.000000 normal_loss: 0.015651\n",
      "[103/00017] train_loss: 0.015574 kl_loss: 0.000000 normal_loss: 0.015574\n",
      "[104/00023] train_loss: 0.015257 kl_loss: 0.000000 normal_loss: 0.015257\n",
      "[105/00029] train_loss: 0.015437 kl_loss: 0.000000 normal_loss: 0.015437\n",
      "[106/00035] train_loss: 0.015329 kl_loss: 0.000000 normal_loss: 0.015329\n",
      "[107/00041] train_loss: 0.015530 kl_loss: 0.000000 normal_loss: 0.015530\n",
      "[108/00047] train_loss: 0.015547 kl_loss: 0.000000 normal_loss: 0.015547\n",
      "[109/00053] train_loss: 0.015737 kl_loss: 0.000000 normal_loss: 0.015737\n",
      "[110/00059] train_loss: 0.015648 kl_loss: 0.000000 normal_loss: 0.015648\n",
      "[111/00065] train_loss: 0.015295 kl_loss: 0.000000 normal_loss: 0.015295\n",
      "[112/00071] train_loss: 0.015117 kl_loss: 0.000000 normal_loss: 0.015117\n",
      "[113/00077] train_loss: 0.015775 kl_loss: 0.000000 normal_loss: 0.015775\n",
      "[114/00083] train_loss: 0.016034 kl_loss: 0.000000 normal_loss: 0.016034\n",
      "[115/00089] train_loss: 0.015433 kl_loss: 0.000000 normal_loss: 0.015433\n",
      "[117/00001] train_loss: 0.015185 kl_loss: 0.000000 normal_loss: 0.015185\n",
      "[118/00007] train_loss: 0.015829 kl_loss: 0.000000 normal_loss: 0.015829\n",
      "[119/00013] train_loss: 0.015666 kl_loss: 0.000000 normal_loss: 0.015666\n",
      "[120/00019] train_loss: 0.015286 kl_loss: 0.000000 normal_loss: 0.015286\n",
      "[121/00025] train_loss: 0.015482 kl_loss: 0.000000 normal_loss: 0.015482\n",
      "[122/00031] train_loss: 0.015904 kl_loss: 0.000000 normal_loss: 0.015904\n",
      "[123/00037] train_loss: 0.015639 kl_loss: 0.000000 normal_loss: 0.015639\n",
      "[124/00043] train_loss: 0.015177 kl_loss: 0.000000 normal_loss: 0.015177\n",
      "[125/00049] train_loss: 0.014996 kl_loss: 0.000000 normal_loss: 0.014996\n",
      "[126/00055] train_loss: 0.014901 kl_loss: 0.000000 normal_loss: 0.014901\n",
      "[127/00061] train_loss: 0.015679 kl_loss: 0.000000 normal_loss: 0.015679\n",
      "[128/00067] train_loss: 0.015258 kl_loss: 0.000000 normal_loss: 0.015258\n",
      "[129/00073] train_loss: 0.015340 kl_loss: 0.000000 normal_loss: 0.015340\n",
      "[130/00079] train_loss: 0.015420 kl_loss: 0.000000 normal_loss: 0.015420\n",
      "[131/00085] train_loss: 0.015211 kl_loss: 0.000000 normal_loss: 0.015211\n",
      "[132/00091] train_loss: 0.014825 kl_loss: 0.000000 normal_loss: 0.014825\n",
      "[134/00003] train_loss: 0.014890 kl_loss: 0.000000 normal_loss: 0.014890\n",
      "[135/00009] train_loss: 0.015089 kl_loss: 0.000000 normal_loss: 0.015089\n",
      "[136/00015] train_loss: 0.015091 kl_loss: 0.000000 normal_loss: 0.015091\n",
      "[137/00021] train_loss: 0.015065 kl_loss: 0.000000 normal_loss: 0.015065\n",
      "[138/00027] train_loss: 0.014805 kl_loss: 0.000000 normal_loss: 0.014805\n",
      "[139/00033] train_loss: 0.014967 kl_loss: 0.000000 normal_loss: 0.014967\n",
      "[140/00039] train_loss: 0.014646 kl_loss: 0.000000 normal_loss: 0.014646\n",
      "[141/00045] train_loss: 0.014927 kl_loss: 0.000000 normal_loss: 0.014927\n",
      "[142/00051] train_loss: 0.014857 kl_loss: 0.000000 normal_loss: 0.014857\n",
      "[143/00057] train_loss: 0.014817 kl_loss: 0.000000 normal_loss: 0.014817\n",
      "[144/00063] train_loss: 0.014645 kl_loss: 0.000000 normal_loss: 0.014645\n",
      "[145/00069] train_loss: 0.014525 kl_loss: 0.000000 normal_loss: 0.014525\n",
      "[146/00075] train_loss: 0.014808 kl_loss: 0.000000 normal_loss: 0.014808\n",
      "[147/00081] train_loss: 0.014537 kl_loss: 0.000000 normal_loss: 0.014537\n",
      "[148/00087] train_loss: 0.014322 kl_loss: 0.000000 normal_loss: 0.014322\n",
      "[149/00093] train_loss: 0.014082 kl_loss: 0.000000 normal_loss: 0.014082\n",
      "[151/00005] train_loss: 0.014684 kl_loss: 0.000000 normal_loss: 0.014684\n",
      "[152/00011] train_loss: 0.014507 kl_loss: 0.000000 normal_loss: 0.014507\n",
      "[153/00017] train_loss: 0.015022 kl_loss: 0.000000 normal_loss: 0.015022\n",
      "[154/00023] train_loss: 0.014567 kl_loss: 0.000000 normal_loss: 0.014567\n",
      "[155/00029] train_loss: 0.014520 kl_loss: 0.000000 normal_loss: 0.014520\n",
      "[156/00035] train_loss: 0.014692 kl_loss: 0.000000 normal_loss: 0.014692\n",
      "[157/00041] train_loss: 0.014294 kl_loss: 0.000000 normal_loss: 0.014294\n",
      "[158/00047] train_loss: 0.014516 kl_loss: 0.000000 normal_loss: 0.014516\n",
      "[159/00053] train_loss: 0.013960 kl_loss: 0.000000 normal_loss: 0.013960\n",
      "[160/00059] train_loss: 0.013956 kl_loss: 0.000000 normal_loss: 0.013956\n",
      "[161/00065] train_loss: 0.014307 kl_loss: 0.000000 normal_loss: 0.014307\n",
      "[162/00071] train_loss: 0.014611 kl_loss: 0.000000 normal_loss: 0.014611\n",
      "[163/00077] train_loss: 0.014380 kl_loss: 0.000000 normal_loss: 0.014380\n",
      "[164/00083] train_loss: 0.014190 kl_loss: 0.000000 normal_loss: 0.014190\n",
      "[165/00089] train_loss: 0.014364 kl_loss: 0.000000 normal_loss: 0.014364\n",
      "[167/00001] train_loss: 0.014217 kl_loss: 0.000000 normal_loss: 0.014217\n",
      "[168/00007] train_loss: 0.013905 kl_loss: 0.000000 normal_loss: 0.013905\n",
      "[169/00013] train_loss: 0.013691 kl_loss: 0.000000 normal_loss: 0.013691\n",
      "[170/00019] train_loss: 0.013563 kl_loss: 0.000000 normal_loss: 0.013563\n",
      "[171/00025] train_loss: 0.014289 kl_loss: 0.000000 normal_loss: 0.014289\n",
      "[172/00031] train_loss: 0.014284 kl_loss: 0.000000 normal_loss: 0.014284\n",
      "[173/00037] train_loss: 0.014072 kl_loss: 0.000000 normal_loss: 0.014072\n",
      "[174/00043] train_loss: 0.014386 kl_loss: 0.000000 normal_loss: 0.014386\n",
      "[175/00049] train_loss: 0.014073 kl_loss: 0.000000 normal_loss: 0.014073\n",
      "[176/00055] train_loss: 0.013806 kl_loss: 0.000000 normal_loss: 0.013806\n",
      "[177/00061] train_loss: 0.013736 kl_loss: 0.000000 normal_loss: 0.013736\n",
      "[178/00067] train_loss: 0.014115 kl_loss: 0.000000 normal_loss: 0.014115\n",
      "[179/00073] train_loss: 0.013644 kl_loss: 0.000000 normal_loss: 0.013644\n",
      "[180/00079] train_loss: 0.013642 kl_loss: 0.000000 normal_loss: 0.013642\n",
      "[181/00085] train_loss: 0.013909 kl_loss: 0.000000 normal_loss: 0.013909\n",
      "[182/00091] train_loss: 0.013913 kl_loss: 0.000000 normal_loss: 0.013913\n",
      "[184/00003] train_loss: 0.014096 kl_loss: 0.000000 normal_loss: 0.014096\n",
      "[185/00009] train_loss: 0.013900 kl_loss: 0.000000 normal_loss: 0.013900\n",
      "[186/00015] train_loss: 0.013933 kl_loss: 0.000000 normal_loss: 0.013933\n",
      "[187/00021] train_loss: 0.013852 kl_loss: 0.000000 normal_loss: 0.013852\n",
      "[188/00027] train_loss: 0.013193 kl_loss: 0.000000 normal_loss: 0.013193\n",
      "[189/00033] train_loss: 0.013133 kl_loss: 0.000000 normal_loss: 0.013133\n",
      "[190/00039] train_loss: 0.013644 kl_loss: 0.000000 normal_loss: 0.013644\n",
      "[191/00045] train_loss: 0.013735 kl_loss: 0.000000 normal_loss: 0.013735\n",
      "[192/00051] train_loss: 0.014015 kl_loss: 0.000000 normal_loss: 0.014015\n",
      "[193/00057] train_loss: 0.013924 kl_loss: 0.000000 normal_loss: 0.013924\n",
      "[194/00063] train_loss: 0.013562 kl_loss: 0.000000 normal_loss: 0.013562\n",
      "[195/00069] train_loss: 0.013730 kl_loss: 0.000000 normal_loss: 0.013730\n",
      "[196/00075] train_loss: 0.013645 kl_loss: 0.000000 normal_loss: 0.013645\n",
      "[197/00081] train_loss: 0.013245 kl_loss: 0.000000 normal_loss: 0.013245\n",
      "[198/00087] train_loss: 0.013792 kl_loss: 0.000000 normal_loss: 0.013792\n",
      "[199/00093] train_loss: 0.013519 kl_loss: 0.000000 normal_loss: 0.013519\n",
      "[201/00005] train_loss: 0.012326 kl_loss: 0.000000 normal_loss: 0.012326\n",
      "[202/00011] train_loss: 0.011625 kl_loss: 0.000000 normal_loss: 0.011625\n",
      "[203/00017] train_loss: 0.011559 kl_loss: 0.000000 normal_loss: 0.011559\n",
      "[204/00023] train_loss: 0.011415 kl_loss: 0.000000 normal_loss: 0.011415\n",
      "[205/00029] train_loss: 0.011501 kl_loss: 0.000000 normal_loss: 0.011501\n",
      "[206/00035] train_loss: 0.011453 kl_loss: 0.000000 normal_loss: 0.011453\n",
      "[207/00041] train_loss: 0.011392 kl_loss: 0.000000 normal_loss: 0.011392\n",
      "[208/00047] train_loss: 0.011547 kl_loss: 0.000000 normal_loss: 0.011547\n",
      "[209/00053] train_loss: 0.011568 kl_loss: 0.000000 normal_loss: 0.011568\n",
      "[210/00059] train_loss: 0.011670 kl_loss: 0.000000 normal_loss: 0.011670\n",
      "[211/00065] train_loss: 0.011558 kl_loss: 0.000000 normal_loss: 0.011558\n",
      "[212/00071] train_loss: 0.011680 kl_loss: 0.000000 normal_loss: 0.011680\n",
      "[213/00077] train_loss: 0.011607 kl_loss: 0.000000 normal_loss: 0.011607\n",
      "[214/00083] train_loss: 0.011376 kl_loss: 0.000000 normal_loss: 0.011376\n",
      "[215/00089] train_loss: 0.011502 kl_loss: 0.000000 normal_loss: 0.011502\n",
      "[217/00001] train_loss: 0.011622 kl_loss: 0.000000 normal_loss: 0.011622\n",
      "[218/00007] train_loss: 0.011672 kl_loss: 0.000000 normal_loss: 0.011672\n",
      "[219/00013] train_loss: 0.011692 kl_loss: 0.000000 normal_loss: 0.011692\n",
      "[220/00019] train_loss: 0.011661 kl_loss: 0.000000 normal_loss: 0.011661\n",
      "[221/00025] train_loss: 0.011621 kl_loss: 0.000000 normal_loss: 0.011621\n",
      "[222/00031] train_loss: 0.011576 kl_loss: 0.000000 normal_loss: 0.011576\n",
      "[223/00037] train_loss: 0.011488 kl_loss: 0.000000 normal_loss: 0.011488\n",
      "[224/00043] train_loss: 0.011545 kl_loss: 0.000000 normal_loss: 0.011545\n",
      "[225/00049] train_loss: 0.011521 kl_loss: 0.000000 normal_loss: 0.011521\n",
      "[226/00055] train_loss: 0.011663 kl_loss: 0.000000 normal_loss: 0.011663\n",
      "[227/00061] train_loss: 0.011432 kl_loss: 0.000000 normal_loss: 0.011432\n",
      "[228/00067] train_loss: 0.011706 kl_loss: 0.000000 normal_loss: 0.011706\n",
      "[229/00073] train_loss: 0.011519 kl_loss: 0.000000 normal_loss: 0.011519\n",
      "[230/00079] train_loss: 0.011489 kl_loss: 0.000000 normal_loss: 0.011489\n",
      "[231/00085] train_loss: 0.011500 kl_loss: 0.000000 normal_loss: 0.011500\n",
      "[232/00091] train_loss: 0.011477 kl_loss: 0.000000 normal_loss: 0.011477\n",
      "[234/00003] train_loss: 0.011595 kl_loss: 0.000000 normal_loss: 0.011595\n",
      "[235/00009] train_loss: 0.011436 kl_loss: 0.000000 normal_loss: 0.011436\n",
      "[236/00015] train_loss: 0.011719 kl_loss: 0.000000 normal_loss: 0.011719\n",
      "[237/00021] train_loss: 0.011364 kl_loss: 0.000000 normal_loss: 0.011364\n",
      "[238/00027] train_loss: 0.011530 kl_loss: 0.000000 normal_loss: 0.011530\n",
      "[239/00033] train_loss: 0.011472 kl_loss: 0.000000 normal_loss: 0.011472\n",
      "[240/00039] train_loss: 0.011302 kl_loss: 0.000000 normal_loss: 0.011302\n",
      "[241/00045] train_loss: 0.011359 kl_loss: 0.000000 normal_loss: 0.011359\n",
      "[242/00051] train_loss: 0.011315 kl_loss: 0.000000 normal_loss: 0.011315\n",
      "[243/00057] train_loss: 0.011296 kl_loss: 0.000000 normal_loss: 0.011296\n",
      "[244/00063] train_loss: 0.011389 kl_loss: 0.000000 normal_loss: 0.011389\n",
      "[245/00069] train_loss: 0.011350 kl_loss: 0.000000 normal_loss: 0.011350\n",
      "[246/00075] train_loss: 0.011162 kl_loss: 0.000000 normal_loss: 0.011162\n",
      "[247/00081] train_loss: 0.011681 kl_loss: 0.000000 normal_loss: 0.011681\n",
      "[248/00087] train_loss: 0.011401 kl_loss: 0.000000 normal_loss: 0.011401\n",
      "[249/00093] train_loss: 0.011451 kl_loss: 0.000000 normal_loss: 0.011451\n",
      "[251/00005] train_loss: 0.011298 kl_loss: 0.000000 normal_loss: 0.011298\n",
      "[252/00011] train_loss: 0.011140 kl_loss: 0.000000 normal_loss: 0.011140\n",
      "[253/00017] train_loss: 0.011190 kl_loss: 0.000000 normal_loss: 0.011190\n",
      "[254/00023] train_loss: 0.011457 kl_loss: 0.000000 normal_loss: 0.011457\n",
      "[255/00029] train_loss: 0.011560 kl_loss: 0.000000 normal_loss: 0.011560\n",
      "[256/00035] train_loss: 0.011217 kl_loss: 0.000000 normal_loss: 0.011217\n",
      "[257/00041] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[258/00047] train_loss: 0.011403 kl_loss: 0.000000 normal_loss: 0.011403\n",
      "[259/00053] train_loss: 0.011368 kl_loss: 0.000000 normal_loss: 0.011368\n",
      "[260/00059] train_loss: 0.011352 kl_loss: 0.000000 normal_loss: 0.011352\n",
      "[261/00065] train_loss: 0.011223 kl_loss: 0.000000 normal_loss: 0.011223\n",
      "[262/00071] train_loss: 0.011281 kl_loss: 0.000000 normal_loss: 0.011281\n",
      "[263/00077] train_loss: 0.011121 kl_loss: 0.000000 normal_loss: 0.011121\n",
      "[264/00083] train_loss: 0.011101 kl_loss: 0.000000 normal_loss: 0.011101\n",
      "[265/00089] train_loss: 0.011221 kl_loss: 0.000000 normal_loss: 0.011221\n",
      "[267/00001] train_loss: 0.011442 kl_loss: 0.000000 normal_loss: 0.011442\n",
      "[268/00007] train_loss: 0.011258 kl_loss: 0.000000 normal_loss: 0.011258\n",
      "[269/00013] train_loss: 0.011185 kl_loss: 0.000000 normal_loss: 0.011185\n",
      "[270/00019] train_loss: 0.011169 kl_loss: 0.000000 normal_loss: 0.011169\n",
      "[271/00025] train_loss: 0.011144 kl_loss: 0.000000 normal_loss: 0.011144\n",
      "[272/00031] train_loss: 0.011069 kl_loss: 0.000000 normal_loss: 0.011069\n",
      "[273/00037] train_loss: 0.011306 kl_loss: 0.000000 normal_loss: 0.011306\n",
      "[274/00043] train_loss: 0.011143 kl_loss: 0.000000 normal_loss: 0.011143\n",
      "[275/00049] train_loss: 0.011317 kl_loss: 0.000000 normal_loss: 0.011317\n",
      "[276/00055] train_loss: 0.011050 kl_loss: 0.000000 normal_loss: 0.011050\n",
      "[277/00061] train_loss: 0.011095 kl_loss: 0.000000 normal_loss: 0.011095\n",
      "[278/00067] train_loss: 0.011292 kl_loss: 0.000000 normal_loss: 0.011292\n",
      "[279/00073] train_loss: 0.011090 kl_loss: 0.000000 normal_loss: 0.011090\n",
      "[280/00079] train_loss: 0.011195 kl_loss: 0.000000 normal_loss: 0.011195\n",
      "[281/00085] train_loss: 0.011025 kl_loss: 0.000000 normal_loss: 0.011025\n",
      "[282/00091] train_loss: 0.011012 kl_loss: 0.000000 normal_loss: 0.011012\n",
      "[284/00003] train_loss: 0.010984 kl_loss: 0.000000 normal_loss: 0.010984\n",
      "[285/00009] train_loss: 0.011148 kl_loss: 0.000000 normal_loss: 0.011148\n",
      "[286/00015] train_loss: 0.011265 kl_loss: 0.000000 normal_loss: 0.011265\n",
      "[287/00021] train_loss: 0.011245 kl_loss: 0.000000 normal_loss: 0.011245\n",
      "[288/00027] train_loss: 0.010894 kl_loss: 0.000000 normal_loss: 0.010894\n",
      "[289/00033] train_loss: 0.010918 kl_loss: 0.000000 normal_loss: 0.010918\n",
      "[290/00039] train_loss: 0.011049 kl_loss: 0.000000 normal_loss: 0.011049\n",
      "[291/00045] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[292/00051] train_loss: 0.011161 kl_loss: 0.000000 normal_loss: 0.011161\n",
      "[293/00057] train_loss: 0.011162 kl_loss: 0.000000 normal_loss: 0.011162\n",
      "[294/00063] train_loss: 0.011098 kl_loss: 0.000000 normal_loss: 0.011098\n",
      "[295/00069] train_loss: 0.010842 kl_loss: 0.000000 normal_loss: 0.010842\n",
      "[296/00075] train_loss: 0.011159 kl_loss: 0.000000 normal_loss: 0.011159\n",
      "[297/00081] train_loss: 0.010983 kl_loss: 0.000000 normal_loss: 0.010983\n",
      "[298/00087] train_loss: 0.011065 kl_loss: 0.000000 normal_loss: 0.011065\n",
      "[299/00093] train_loss: 0.011041 kl_loss: 0.000000 normal_loss: 0.011041\n",
      "[301/00005] train_loss: 0.010517 kl_loss: 0.000000 normal_loss: 0.010517\n",
      "[302/00011] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[303/00017] train_loss: 0.010262 kl_loss: 0.000000 normal_loss: 0.010262\n",
      "[304/00023] train_loss: 0.010249 kl_loss: 0.000000 normal_loss: 0.010249\n",
      "[305/00029] train_loss: 0.010271 kl_loss: 0.000000 normal_loss: 0.010271\n",
      "[306/00035] train_loss: 0.010117 kl_loss: 0.000000 normal_loss: 0.010117\n",
      "[307/00041] train_loss: 0.010226 kl_loss: 0.000000 normal_loss: 0.010226\n",
      "[308/00047] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[309/00053] train_loss: 0.010148 kl_loss: 0.000000 normal_loss: 0.010148\n",
      "[310/00059] train_loss: 0.010184 kl_loss: 0.000000 normal_loss: 0.010184\n",
      "[311/00065] train_loss: 0.010260 kl_loss: 0.000000 normal_loss: 0.010260\n",
      "[312/00071] train_loss: 0.010252 kl_loss: 0.000000 normal_loss: 0.010252\n",
      "[313/00077] train_loss: 0.010283 kl_loss: 0.000000 normal_loss: 0.010283\n",
      "[314/00083] train_loss: 0.010220 kl_loss: 0.000000 normal_loss: 0.010220\n",
      "[315/00089] train_loss: 0.010255 kl_loss: 0.000000 normal_loss: 0.010255\n",
      "[317/00001] train_loss: 0.010203 kl_loss: 0.000000 normal_loss: 0.010203\n",
      "[318/00007] train_loss: 0.010226 kl_loss: 0.000000 normal_loss: 0.010226\n",
      "[319/00013] train_loss: 0.010222 kl_loss: 0.000000 normal_loss: 0.010222\n",
      "[320/00019] train_loss: 0.010265 kl_loss: 0.000000 normal_loss: 0.010265\n",
      "[321/00025] train_loss: 0.010266 kl_loss: 0.000000 normal_loss: 0.010266\n",
      "[322/00031] train_loss: 0.010246 kl_loss: 0.000000 normal_loss: 0.010246\n",
      "[323/00037] train_loss: 0.010218 kl_loss: 0.000000 normal_loss: 0.010218\n",
      "[324/00043] train_loss: 0.010209 kl_loss: 0.000000 normal_loss: 0.010209\n",
      "[325/00049] train_loss: 0.010175 kl_loss: 0.000000 normal_loss: 0.010175\n",
      "[326/00055] train_loss: 0.010127 kl_loss: 0.000000 normal_loss: 0.010127\n",
      "[327/00061] train_loss: 0.010312 kl_loss: 0.000000 normal_loss: 0.010312\n",
      "[328/00067] train_loss: 0.010165 kl_loss: 0.000000 normal_loss: 0.010165\n",
      "[329/00073] train_loss: 0.010248 kl_loss: 0.000000 normal_loss: 0.010248\n",
      "[330/00079] train_loss: 0.010207 kl_loss: 0.000000 normal_loss: 0.010207\n",
      "[331/00085] train_loss: 0.010198 kl_loss: 0.000000 normal_loss: 0.010198\n",
      "[332/00091] train_loss: 0.010193 kl_loss: 0.000000 normal_loss: 0.010193\n",
      "[334/00003] train_loss: 0.010178 kl_loss: 0.000000 normal_loss: 0.010178\n",
      "[335/00009] train_loss: 0.010130 kl_loss: 0.000000 normal_loss: 0.010130\n",
      "[336/00015] train_loss: 0.010189 kl_loss: 0.000000 normal_loss: 0.010189\n",
      "[337/00021] train_loss: 0.010203 kl_loss: 0.000000 normal_loss: 0.010203\n",
      "[338/00027] train_loss: 0.010221 kl_loss: 0.000000 normal_loss: 0.010221\n",
      "[339/00033] train_loss: 0.010256 kl_loss: 0.000000 normal_loss: 0.010256\n",
      "[340/00039] train_loss: 0.010155 kl_loss: 0.000000 normal_loss: 0.010155\n",
      "[341/00045] train_loss: 0.010119 kl_loss: 0.000000 normal_loss: 0.010119\n",
      "[342/00051] train_loss: 0.010171 kl_loss: 0.000000 normal_loss: 0.010171\n",
      "[343/00057] train_loss: 0.010198 kl_loss: 0.000000 normal_loss: 0.010198\n",
      "[344/00063] train_loss: 0.010181 kl_loss: 0.000000 normal_loss: 0.010181\n",
      "[345/00069] train_loss: 0.010173 kl_loss: 0.000000 normal_loss: 0.010173\n",
      "[346/00075] train_loss: 0.010180 kl_loss: 0.000000 normal_loss: 0.010180\n",
      "[347/00081] train_loss: 0.010072 kl_loss: 0.000000 normal_loss: 0.010072\n",
      "[348/00087] train_loss: 0.010118 kl_loss: 0.000000 normal_loss: 0.010118\n",
      "[349/00093] train_loss: 0.010134 kl_loss: 0.000000 normal_loss: 0.010134\n",
      "[351/00005] train_loss: 0.010090 kl_loss: 0.000000 normal_loss: 0.010090\n",
      "[352/00011] train_loss: 0.010110 kl_loss: 0.000000 normal_loss: 0.010110\n",
      "[353/00017] train_loss: 0.010092 kl_loss: 0.000000 normal_loss: 0.010092\n",
      "[354/00023] train_loss: 0.010162 kl_loss: 0.000000 normal_loss: 0.010162\n",
      "[355/00029] train_loss: 0.010098 kl_loss: 0.000000 normal_loss: 0.010098\n",
      "[356/00035] train_loss: 0.010046 kl_loss: 0.000000 normal_loss: 0.010046\n",
      "[357/00041] train_loss: 0.010071 kl_loss: 0.000000 normal_loss: 0.010071\n",
      "[358/00047] train_loss: 0.010113 kl_loss: 0.000000 normal_loss: 0.010113\n",
      "[359/00053] train_loss: 0.010060 kl_loss: 0.000000 normal_loss: 0.010060\n",
      "[360/00059] train_loss: 0.010085 kl_loss: 0.000000 normal_loss: 0.010085\n",
      "[361/00065] train_loss: 0.010169 kl_loss: 0.000000 normal_loss: 0.010169\n",
      "[362/00071] train_loss: 0.010050 kl_loss: 0.000000 normal_loss: 0.010050\n",
      "[363/00077] train_loss: 0.010029 kl_loss: 0.000000 normal_loss: 0.010029\n",
      "[364/00083] train_loss: 0.010070 kl_loss: 0.000000 normal_loss: 0.010070\n",
      "[365/00089] train_loss: 0.010169 kl_loss: 0.000000 normal_loss: 0.010169\n",
      "[367/00001] train_loss: 0.010005 kl_loss: 0.000000 normal_loss: 0.010005\n",
      "[368/00007] train_loss: 0.010060 kl_loss: 0.000000 normal_loss: 0.010060\n",
      "[369/00013] train_loss: 0.009988 kl_loss: 0.000000 normal_loss: 0.009988\n",
      "[370/00019] train_loss: 0.010120 kl_loss: 0.000000 normal_loss: 0.010120\n",
      "[371/00025] train_loss: 0.010066 kl_loss: 0.000000 normal_loss: 0.010066\n",
      "[372/00031] train_loss: 0.009933 kl_loss: 0.000000 normal_loss: 0.009933\n",
      "[373/00037] train_loss: 0.010093 kl_loss: 0.000000 normal_loss: 0.010093\n",
      "[374/00043] train_loss: 0.009972 kl_loss: 0.000000 normal_loss: 0.009972\n",
      "[375/00049] train_loss: 0.010037 kl_loss: 0.000000 normal_loss: 0.010037\n",
      "[376/00055] train_loss: 0.009991 kl_loss: 0.000000 normal_loss: 0.009991\n",
      "[377/00061] train_loss: 0.010022 kl_loss: 0.000000 normal_loss: 0.010022\n",
      "[378/00067] train_loss: 0.010008 kl_loss: 0.000000 normal_loss: 0.010008\n",
      "[379/00073] train_loss: 0.009970 kl_loss: 0.000000 normal_loss: 0.009970\n",
      "[380/00079] train_loss: 0.010011 kl_loss: 0.000000 normal_loss: 0.010011\n",
      "[381/00085] train_loss: 0.009967 kl_loss: 0.000000 normal_loss: 0.009967\n",
      "[382/00091] train_loss: 0.009960 kl_loss: 0.000000 normal_loss: 0.009960\n",
      "[384/00003] train_loss: 0.010048 kl_loss: 0.000000 normal_loss: 0.010048\n",
      "[385/00009] train_loss: 0.009959 kl_loss: 0.000000 normal_loss: 0.009959\n",
      "[386/00015] train_loss: 0.010017 kl_loss: 0.000000 normal_loss: 0.010017\n",
      "[387/00021] train_loss: 0.010008 kl_loss: 0.000000 normal_loss: 0.010008\n",
      "[388/00027] train_loss: 0.009994 kl_loss: 0.000000 normal_loss: 0.009994\n",
      "[389/00033] train_loss: 0.010066 kl_loss: 0.000000 normal_loss: 0.010066\n",
      "[390/00039] train_loss: 0.010017 kl_loss: 0.000000 normal_loss: 0.010017\n",
      "[391/00045] train_loss: 0.009861 kl_loss: 0.000000 normal_loss: 0.009861\n",
      "[392/00051] train_loss: 0.010109 kl_loss: 0.000000 normal_loss: 0.010109\n",
      "[393/00057] train_loss: 0.009989 kl_loss: 0.000000 normal_loss: 0.009989\n",
      "[394/00063] train_loss: 0.009996 kl_loss: 0.000000 normal_loss: 0.009996\n",
      "[395/00069] train_loss: 0.009870 kl_loss: 0.000000 normal_loss: 0.009870\n",
      "[396/00075] train_loss: 0.009969 kl_loss: 0.000000 normal_loss: 0.009969\n",
      "[397/00081] train_loss: 0.009880 kl_loss: 0.000000 normal_loss: 0.009880\n",
      "[398/00087] train_loss: 0.009947 kl_loss: 0.000000 normal_loss: 0.009947\n",
      "[399/00093] train_loss: 0.009983 kl_loss: 0.000000 normal_loss: 0.009983\n",
      "[401/00005] train_loss: 0.009788 kl_loss: 0.000000 normal_loss: 0.009788\n",
      "[402/00011] train_loss: 0.009675 kl_loss: 0.000000 normal_loss: 0.009675\n",
      "[403/00017] train_loss: 0.009665 kl_loss: 0.000000 normal_loss: 0.009665\n",
      "[404/00023] train_loss: 0.009673 kl_loss: 0.000000 normal_loss: 0.009673\n",
      "[405/00029] train_loss: 0.009681 kl_loss: 0.000000 normal_loss: 0.009681\n",
      "[406/00035] train_loss: 0.009675 kl_loss: 0.000000 normal_loss: 0.009675\n",
      "[407/00041] train_loss: 0.009686 kl_loss: 0.000000 normal_loss: 0.009686\n",
      "[408/00047] train_loss: 0.009626 kl_loss: 0.000000 normal_loss: 0.009626\n",
      "[409/00053] train_loss: 0.009652 kl_loss: 0.000000 normal_loss: 0.009652\n",
      "[410/00059] train_loss: 0.009662 kl_loss: 0.000000 normal_loss: 0.009662\n",
      "[411/00065] train_loss: 0.009678 kl_loss: 0.000000 normal_loss: 0.009678\n",
      "[412/00071] train_loss: 0.009712 kl_loss: 0.000000 normal_loss: 0.009712\n",
      "[413/00077] train_loss: 0.009683 kl_loss: 0.000000 normal_loss: 0.009683\n",
      "[414/00083] train_loss: 0.009658 kl_loss: 0.000000 normal_loss: 0.009658\n",
      "[415/00089] train_loss: 0.009679 kl_loss: 0.000000 normal_loss: 0.009679\n",
      "[417/00001] train_loss: 0.009674 kl_loss: 0.000000 normal_loss: 0.009674\n",
      "[418/00007] train_loss: 0.009655 kl_loss: 0.000000 normal_loss: 0.009655\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mchair_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000016?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CHAIR\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'chair_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'chair',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 9173\n",
      "Training params: 3\n",
      "[000/00099] train_loss: 0.178056 kl_loss: 0.500034 normal_loss: 0.163055\n",
      "[001/00055] train_loss: 0.132347 kl_loss: 0.470443 normal_loss: 0.118233\n",
      "[002/00011] train_loss: 0.124186 kl_loss: 0.438878 normal_loss: 0.111019\n",
      "[002/00111] train_loss: 0.121336 kl_loss: 0.385560 normal_loss: 0.109769\n",
      "[003/00067] train_loss: 0.119753 kl_loss: 0.350740 normal_loss: 0.109230\n",
      "[004/00023] train_loss: 0.117066 kl_loss: 0.325709 normal_loss: 0.107294\n",
      "[004/00123] train_loss: 0.113976 kl_loss: 0.298345 normal_loss: 0.105026\n",
      "[005/00079] train_loss: 0.110803 kl_loss: 0.291768 normal_loss: 0.102050\n",
      "[006/00035] train_loss: 0.109214 kl_loss: 0.287032 normal_loss: 0.100603\n",
      "[006/00135] train_loss: 0.108182 kl_loss: 0.279403 normal_loss: 0.099800\n",
      "[007/00091] train_loss: 0.104936 kl_loss: 0.269548 normal_loss: 0.096849\n",
      "[008/00047] train_loss: 0.101741 kl_loss: 0.267557 normal_loss: 0.093714\n",
      "[009/00003] train_loss: 0.100989 kl_loss: 0.270261 normal_loss: 0.092881\n",
      "[009/00103] train_loss: 0.096803 kl_loss: 0.271671 normal_loss: 0.088653\n",
      "[010/00059] train_loss: 0.093735 kl_loss: 0.271919 normal_loss: 0.085577\n",
      "[011/00015] train_loss: 0.092613 kl_loss: 0.271636 normal_loss: 0.084464\n",
      "[011/00115] train_loss: 0.091549 kl_loss: 0.270807 normal_loss: 0.083425\n",
      "[012/00071] train_loss: 0.089025 kl_loss: 0.267663 normal_loss: 0.080995\n",
      "[013/00027] train_loss: 0.087582 kl_loss: 0.267876 normal_loss: 0.079546\n",
      "[013/00127] train_loss: 0.086233 kl_loss: 0.267362 normal_loss: 0.078213\n",
      "[014/00083] train_loss: 0.084727 kl_loss: 0.266303 normal_loss: 0.076738\n",
      "[015/00039] train_loss: 0.083200 kl_loss: 0.265377 normal_loss: 0.075239\n",
      "[015/00139] train_loss: 0.082058 kl_loss: 0.264324 normal_loss: 0.074128\n",
      "[016/00095] train_loss: 0.080509 kl_loss: 0.262058 normal_loss: 0.072647\n",
      "[017/00051] train_loss: 0.081169 kl_loss: 0.262728 normal_loss: 0.073287\n",
      "[018/00007] train_loss: 0.078990 kl_loss: 0.260191 normal_loss: 0.071185\n",
      "[018/00107] train_loss: 0.078766 kl_loss: 0.258532 normal_loss: 0.071010\n",
      "[019/00063] train_loss: 0.076132 kl_loss: 0.256643 normal_loss: 0.068433\n",
      "[020/00019] train_loss: 0.075786 kl_loss: 0.255598 normal_loss: 0.068118\n",
      "[020/00119] train_loss: 0.076643 kl_loss: 0.254097 normal_loss: 0.069021\n",
      "[021/00075] train_loss: 0.074903 kl_loss: 0.251185 normal_loss: 0.067367\n",
      "[022/00031] train_loss: 0.073803 kl_loss: 0.251858 normal_loss: 0.066247\n",
      "[022/00131] train_loss: 0.073523 kl_loss: 0.251622 normal_loss: 0.065975\n",
      "[023/00087] train_loss: 0.072428 kl_loss: 0.250056 normal_loss: 0.064926\n",
      "[024/00043] train_loss: 0.070849 kl_loss: 0.247852 normal_loss: 0.063413\n",
      "[024/00143] train_loss: 0.071675 kl_loss: 0.249177 normal_loss: 0.064200\n",
      "[025/00099] train_loss: 0.070496 kl_loss: 0.247242 normal_loss: 0.063079\n",
      "[026/00055] train_loss: 0.070750 kl_loss: 0.246987 normal_loss: 0.063340\n",
      "[027/00011] train_loss: 0.069423 kl_loss: 0.245468 normal_loss: 0.062059\n",
      "[027/00111] train_loss: 0.068992 kl_loss: 0.247674 normal_loss: 0.061561\n",
      "[028/00067] train_loss: 0.067448 kl_loss: 0.247611 normal_loss: 0.060020\n",
      "[029/00023] train_loss: 0.066939 kl_loss: 0.244323 normal_loss: 0.059610\n",
      "[029/00123] train_loss: 0.067726 kl_loss: 0.246355 normal_loss: 0.060335\n",
      "[030/00079] train_loss: 0.065808 kl_loss: 0.245076 normal_loss: 0.058455\n",
      "[031/00035] train_loss: 0.065672 kl_loss: 0.246059 normal_loss: 0.058290\n",
      "[031/00135] train_loss: 0.065236 kl_loss: 0.245531 normal_loss: 0.057870\n",
      "[032/00091] train_loss: 0.064714 kl_loss: 0.246709 normal_loss: 0.057312\n",
      "[033/00047] train_loss: 0.063874 kl_loss: 0.245579 normal_loss: 0.056507\n",
      "[034/00003] train_loss: 0.063412 kl_loss: 0.246808 normal_loss: 0.056008\n",
      "[034/00103] train_loss: 0.062436 kl_loss: 0.246970 normal_loss: 0.055027\n",
      "[035/00059] train_loss: 0.063666 kl_loss: 0.246149 normal_loss: 0.056281\n",
      "[036/00015] train_loss: 0.063026 kl_loss: 0.248889 normal_loss: 0.055559\n",
      "[036/00115] train_loss: 0.062002 kl_loss: 0.247659 normal_loss: 0.054572\n",
      "[037/00071] train_loss: 0.061698 kl_loss: 0.248861 normal_loss: 0.054232\n",
      "[038/00027] train_loss: 0.061683 kl_loss: 0.248673 normal_loss: 0.054223\n",
      "[038/00127] train_loss: 0.060715 kl_loss: 0.247957 normal_loss: 0.053276\n",
      "[039/00083] train_loss: 0.061106 kl_loss: 0.248824 normal_loss: 0.053641\n",
      "[040/00039] train_loss: 0.059661 kl_loss: 0.249238 normal_loss: 0.052184\n",
      "[040/00139] train_loss: 0.059992 kl_loss: 0.249576 normal_loss: 0.052505\n",
      "[041/00095] train_loss: 0.059233 kl_loss: 0.249153 normal_loss: 0.051759\n",
      "[042/00051] train_loss: 0.060006 kl_loss: 0.252168 normal_loss: 0.052441\n",
      "[043/00007] train_loss: 0.059109 kl_loss: 0.249892 normal_loss: 0.051612\n",
      "[043/00107] train_loss: 0.058655 kl_loss: 0.251770 normal_loss: 0.051102\n",
      "[044/00063] train_loss: 0.058663 kl_loss: 0.254003 normal_loss: 0.051043\n",
      "[045/00019] train_loss: 0.057070 kl_loss: 0.249459 normal_loss: 0.049586\n",
      "[045/00119] train_loss: 0.057266 kl_loss: 0.253435 normal_loss: 0.049663\n",
      "[046/00075] train_loss: 0.057590 kl_loss: 0.254977 normal_loss: 0.049940\n",
      "[047/00031] train_loss: 0.057299 kl_loss: 0.253367 normal_loss: 0.049698\n",
      "[047/00131] train_loss: 0.056911 kl_loss: 0.253495 normal_loss: 0.049306\n",
      "[048/00087] train_loss: 0.056909 kl_loss: 0.254600 normal_loss: 0.049271\n",
      "[049/00043] train_loss: 0.056479 kl_loss: 0.256457 normal_loss: 0.048786\n",
      "[049/00143] train_loss: 0.056401 kl_loss: 0.255242 normal_loss: 0.048744\n",
      "[050/00099] train_loss: 0.055483 kl_loss: 0.258583 normal_loss: 0.047726\n",
      "[051/00055] train_loss: 0.054811 kl_loss: 0.255326 normal_loss: 0.047151\n",
      "[052/00011] train_loss: 0.055771 kl_loss: 0.260833 normal_loss: 0.047946\n",
      "[052/00111] train_loss: 0.054686 kl_loss: 0.259473 normal_loss: 0.046902\n",
      "[053/00067] train_loss: 0.055177 kl_loss: 0.259532 normal_loss: 0.047392\n",
      "[054/00023] train_loss: 0.054129 kl_loss: 0.260401 normal_loss: 0.046317\n",
      "[054/00123] train_loss: 0.054392 kl_loss: 0.261862 normal_loss: 0.046536\n",
      "[055/00079] train_loss: 0.053618 kl_loss: 0.260896 normal_loss: 0.045791\n",
      "[056/00035] train_loss: 0.054311 kl_loss: 0.266210 normal_loss: 0.046325\n",
      "[056/00135] train_loss: 0.053341 kl_loss: 0.263181 normal_loss: 0.045445\n",
      "[057/00091] train_loss: 0.053781 kl_loss: 0.264619 normal_loss: 0.045843\n",
      "[058/00047] train_loss: 0.052426 kl_loss: 0.265049 normal_loss: 0.044475\n",
      "[059/00003] train_loss: 0.052965 kl_loss: 0.266419 normal_loss: 0.044972\n",
      "[059/00103] train_loss: 0.052348 kl_loss: 0.267315 normal_loss: 0.044328\n",
      "[060/00059] train_loss: 0.052523 kl_loss: 0.267306 normal_loss: 0.044504\n",
      "[061/00015] train_loss: 0.051938 kl_loss: 0.267502 normal_loss: 0.043913\n",
      "[061/00115] train_loss: 0.052264 kl_loss: 0.268840 normal_loss: 0.044198\n",
      "[062/00071] train_loss: 0.051734 kl_loss: 0.270296 normal_loss: 0.043625\n",
      "[063/00027] train_loss: 0.051298 kl_loss: 0.269993 normal_loss: 0.043198\n",
      "[063/00127] train_loss: 0.051311 kl_loss: 0.270659 normal_loss: 0.043192\n",
      "[064/00083] train_loss: 0.050953 kl_loss: 0.270751 normal_loss: 0.042831\n",
      "[065/00039] train_loss: 0.051084 kl_loss: 0.272073 normal_loss: 0.042922\n",
      "[065/00139] train_loss: 0.050978 kl_loss: 0.272515 normal_loss: 0.042803\n",
      "[066/00095] train_loss: 0.050564 kl_loss: 0.273932 normal_loss: 0.042346\n",
      "[067/00051] train_loss: 0.049713 kl_loss: 0.273193 normal_loss: 0.041517\n",
      "[068/00007] train_loss: 0.050326 kl_loss: 0.272818 normal_loss: 0.042141\n",
      "[068/00107] train_loss: 0.049486 kl_loss: 0.274378 normal_loss: 0.041254\n",
      "[069/00063] train_loss: 0.049896 kl_loss: 0.276724 normal_loss: 0.041594\n",
      "[070/00019] train_loss: 0.049585 kl_loss: 0.275923 normal_loss: 0.041307\n",
      "[070/00119] train_loss: 0.049243 kl_loss: 0.275239 normal_loss: 0.040986\n",
      "[071/00075] train_loss: 0.048726 kl_loss: 0.275780 normal_loss: 0.040452\n",
      "[072/00031] train_loss: 0.048544 kl_loss: 0.278686 normal_loss: 0.040184\n",
      "[072/00131] train_loss: 0.048849 kl_loss: 0.276979 normal_loss: 0.040539\n",
      "[073/00087] train_loss: 0.048719 kl_loss: 0.277878 normal_loss: 0.040383\n",
      "[074/00043] train_loss: 0.048200 kl_loss: 0.278985 normal_loss: 0.039830\n",
      "[074/00143] train_loss: 0.048600 kl_loss: 0.279265 normal_loss: 0.040222\n",
      "[075/00099] train_loss: 0.048193 kl_loss: 0.280765 normal_loss: 0.039770\n",
      "[076/00055] train_loss: 0.047750 kl_loss: 0.279599 normal_loss: 0.039362\n",
      "[077/00011] train_loss: 0.047409 kl_loss: 0.281790 normal_loss: 0.038955\n",
      "[077/00111] train_loss: 0.047186 kl_loss: 0.281502 normal_loss: 0.038741\n",
      "[078/00067] train_loss: 0.047231 kl_loss: 0.281031 normal_loss: 0.038801\n",
      "[079/00023] train_loss: 0.047635 kl_loss: 0.284626 normal_loss: 0.039097\n",
      "[079/00123] train_loss: 0.046417 kl_loss: 0.281763 normal_loss: 0.037964\n",
      "[080/00079] train_loss: 0.046869 kl_loss: 0.284804 normal_loss: 0.038325\n",
      "[081/00035] train_loss: 0.046306 kl_loss: 0.284909 normal_loss: 0.037759\n",
      "[081/00135] train_loss: 0.046672 kl_loss: 0.283368 normal_loss: 0.038171\n",
      "[082/00091] train_loss: 0.046300 kl_loss: 0.285188 normal_loss: 0.037744\n",
      "[083/00047] train_loss: 0.046318 kl_loss: 0.286340 normal_loss: 0.037728\n",
      "[084/00003] train_loss: 0.045584 kl_loss: 0.284953 normal_loss: 0.037036\n",
      "[084/00103] train_loss: 0.045361 kl_loss: 0.287632 normal_loss: 0.036733\n",
      "[085/00059] train_loss: 0.045094 kl_loss: 0.284641 normal_loss: 0.036555\n",
      "[086/00015] train_loss: 0.045904 kl_loss: 0.288443 normal_loss: 0.037251\n",
      "[086/00115] train_loss: 0.045288 kl_loss: 0.287585 normal_loss: 0.036661\n",
      "[087/00071] train_loss: 0.045460 kl_loss: 0.288669 normal_loss: 0.036800\n",
      "[088/00027] train_loss: 0.045100 kl_loss: 0.286324 normal_loss: 0.036511\n",
      "[088/00127] train_loss: 0.044886 kl_loss: 0.290115 normal_loss: 0.036183\n",
      "[089/00083] train_loss: 0.044756 kl_loss: 0.289549 normal_loss: 0.036069\n",
      "[090/00039] train_loss: 0.044304 kl_loss: 0.288877 normal_loss: 0.035638\n",
      "[090/00139] train_loss: 0.044159 kl_loss: 0.288700 normal_loss: 0.035498\n",
      "[091/00095] train_loss: 0.044557 kl_loss: 0.290656 normal_loss: 0.035838\n",
      "[092/00051] train_loss: 0.044263 kl_loss: 0.290524 normal_loss: 0.035548\n",
      "[093/00007] train_loss: 0.044261 kl_loss: 0.289907 normal_loss: 0.035564\n",
      "[093/00107] train_loss: 0.043844 kl_loss: 0.291300 normal_loss: 0.035105\n",
      "[094/00063] train_loss: 0.043386 kl_loss: 0.290729 normal_loss: 0.034664\n",
      "[095/00019] train_loss: 0.043397 kl_loss: 0.291993 normal_loss: 0.034637\n",
      "[095/00119] train_loss: 0.043411 kl_loss: 0.291043 normal_loss: 0.034680\n",
      "[096/00075] train_loss: 0.043530 kl_loss: 0.294686 normal_loss: 0.034689\n",
      "[097/00031] train_loss: 0.043204 kl_loss: 0.292399 normal_loss: 0.034432\n",
      "[097/00131] train_loss: 0.042960 kl_loss: 0.292194 normal_loss: 0.034194\n",
      "[098/00087] train_loss: 0.043005 kl_loss: 0.292429 normal_loss: 0.034232\n",
      "[099/00043] train_loss: 0.042556 kl_loss: 0.293014 normal_loss: 0.033765\n",
      "[099/00143] train_loss: 0.042899 kl_loss: 0.292936 normal_loss: 0.034111\n",
      "[100/00099] train_loss: 0.040152 kl_loss: 0.291792 normal_loss: 0.031398\n",
      "[101/00055] train_loss: 0.039739 kl_loss: 0.291357 normal_loss: 0.030998\n",
      "[102/00011] train_loss: 0.039314 kl_loss: 0.290390 normal_loss: 0.030602\n",
      "[102/00111] train_loss: 0.038563 kl_loss: 0.286350 normal_loss: 0.029973\n",
      "[103/00067] train_loss: 0.038410 kl_loss: 0.285762 normal_loss: 0.029837\n",
      "[104/00023] train_loss: 0.038414 kl_loss: 0.283668 normal_loss: 0.029904\n",
      "[104/00123] train_loss: 0.038022 kl_loss: 0.280938 normal_loss: 0.029594\n",
      "[105/00079] train_loss: 0.038328 kl_loss: 0.281367 normal_loss: 0.029887\n",
      "[106/00035] train_loss: 0.037501 kl_loss: 0.276584 normal_loss: 0.029204\n",
      "[106/00135] train_loss: 0.037915 kl_loss: 0.277469 normal_loss: 0.029591\n",
      "[107/00091] train_loss: 0.037531 kl_loss: 0.273937 normal_loss: 0.029312\n",
      "[108/00047] train_loss: 0.037414 kl_loss: 0.274603 normal_loss: 0.029176\n",
      "[109/00003] train_loss: 0.037234 kl_loss: 0.272516 normal_loss: 0.029058\n",
      "[109/00103] train_loss: 0.037240 kl_loss: 0.271107 normal_loss: 0.029107\n",
      "[110/00059] train_loss: 0.036917 kl_loss: 0.268448 normal_loss: 0.028863\n",
      "[111/00015] train_loss: 0.037223 kl_loss: 0.271296 normal_loss: 0.029084\n",
      "[111/00115] train_loss: 0.036726 kl_loss: 0.266582 normal_loss: 0.028729\n",
      "[112/00071] train_loss: 0.036604 kl_loss: 0.265738 normal_loss: 0.028632\n",
      "[113/00027] train_loss: 0.037043 kl_loss: 0.268455 normal_loss: 0.028989\n",
      "[113/00127] train_loss: 0.036721 kl_loss: 0.264765 normal_loss: 0.028778\n",
      "[114/00083] train_loss: 0.036638 kl_loss: 0.262726 normal_loss: 0.028756\n",
      "[115/00039] train_loss: 0.036525 kl_loss: 0.264790 normal_loss: 0.028582\n",
      "[115/00139] train_loss: 0.036238 kl_loss: 0.261317 normal_loss: 0.028399\n",
      "[116/00095] train_loss: 0.036525 kl_loss: 0.263339 normal_loss: 0.028625\n",
      "[117/00051] train_loss: 0.036052 kl_loss: 0.259873 normal_loss: 0.028255\n",
      "[118/00007] train_loss: 0.036026 kl_loss: 0.260458 normal_loss: 0.028212\n",
      "[118/00107] train_loss: 0.036026 kl_loss: 0.261037 normal_loss: 0.028194\n",
      "[119/00063] train_loss: 0.036064 kl_loss: 0.259948 normal_loss: 0.028266\n",
      "[120/00019] train_loss: 0.035867 kl_loss: 0.260190 normal_loss: 0.028061\n",
      "[120/00119] train_loss: 0.035833 kl_loss: 0.258816 normal_loss: 0.028068\n",
      "[121/00075] train_loss: 0.035450 kl_loss: 0.258133 normal_loss: 0.027706\n",
      "[122/00031] train_loss: 0.035474 kl_loss: 0.257314 normal_loss: 0.027754\n",
      "[122/00131] train_loss: 0.035753 kl_loss: 0.257771 normal_loss: 0.028020\n",
      "[123/00087] train_loss: 0.035855 kl_loss: 0.258949 normal_loss: 0.028087\n",
      "[124/00043] train_loss: 0.035387 kl_loss: 0.255994 normal_loss: 0.027708\n",
      "[124/00143] train_loss: 0.035420 kl_loss: 0.255885 normal_loss: 0.027743\n",
      "[125/00099] train_loss: 0.035161 kl_loss: 0.255795 normal_loss: 0.027487\n",
      "[126/00055] train_loss: 0.034944 kl_loss: 0.255526 normal_loss: 0.027278\n",
      "[127/00011] train_loss: 0.035407 kl_loss: 0.257131 normal_loss: 0.027693\n",
      "[127/00111] train_loss: 0.035076 kl_loss: 0.255190 normal_loss: 0.027420\n",
      "[128/00067] train_loss: 0.034981 kl_loss: 0.255304 normal_loss: 0.027322\n",
      "[129/00023] train_loss: 0.035123 kl_loss: 0.254925 normal_loss: 0.027475\n",
      "[129/00123] train_loss: 0.034909 kl_loss: 0.254930 normal_loss: 0.027261\n",
      "[130/00079] train_loss: 0.035058 kl_loss: 0.254832 normal_loss: 0.027413\n",
      "[131/00035] train_loss: 0.034656 kl_loss: 0.255165 normal_loss: 0.027001\n",
      "[131/00135] train_loss: 0.034711 kl_loss: 0.254287 normal_loss: 0.027083\n",
      "[132/00091] train_loss: 0.034718 kl_loss: 0.254123 normal_loss: 0.027095\n",
      "[133/00047] train_loss: 0.034638 kl_loss: 0.254734 normal_loss: 0.026996\n",
      "[134/00003] train_loss: 0.034474 kl_loss: 0.252737 normal_loss: 0.026892\n",
      "[134/00103] train_loss: 0.034226 kl_loss: 0.254319 normal_loss: 0.026596\n",
      "[135/00059] train_loss: 0.034466 kl_loss: 0.251787 normal_loss: 0.026912\n",
      "[136/00015] train_loss: 0.034527 kl_loss: 0.255032 normal_loss: 0.026876\n",
      "[136/00115] train_loss: 0.034264 kl_loss: 0.253268 normal_loss: 0.026666\n",
      "[137/00071] train_loss: 0.033911 kl_loss: 0.252803 normal_loss: 0.026327\n",
      "[138/00027] train_loss: 0.034124 kl_loss: 0.252411 normal_loss: 0.026552\n",
      "[138/00127] train_loss: 0.034020 kl_loss: 0.252920 normal_loss: 0.026433\n",
      "[139/00083] train_loss: 0.034053 kl_loss: 0.253203 normal_loss: 0.026457\n",
      "[140/00039] train_loss: 0.034056 kl_loss: 0.252114 normal_loss: 0.026493\n",
      "[140/00139] train_loss: 0.033965 kl_loss: 0.252020 normal_loss: 0.026404\n",
      "[141/00095] train_loss: 0.033779 kl_loss: 0.252567 normal_loss: 0.026202\n",
      "[142/00051] train_loss: 0.034255 kl_loss: 0.254233 normal_loss: 0.026628\n",
      "[143/00007] train_loss: 0.033544 kl_loss: 0.250503 normal_loss: 0.026029\n",
      "[143/00107] train_loss: 0.033522 kl_loss: 0.251886 normal_loss: 0.025965\n",
      "[144/00063] train_loss: 0.034000 kl_loss: 0.253323 normal_loss: 0.026400\n",
      "[145/00019] train_loss: 0.033472 kl_loss: 0.251121 normal_loss: 0.025939\n",
      "[145/00119] train_loss: 0.033557 kl_loss: 0.251895 normal_loss: 0.026000\n",
      "[146/00075] train_loss: 0.033173 kl_loss: 0.251709 normal_loss: 0.025622\n",
      "[147/00031] train_loss: 0.033586 kl_loss: 0.252297 normal_loss: 0.026017\n",
      "[147/00131] train_loss: 0.033554 kl_loss: 0.252183 normal_loss: 0.025988\n",
      "[148/00087] train_loss: 0.033209 kl_loss: 0.251545 normal_loss: 0.025663\n",
      "[149/00043] train_loss: 0.033103 kl_loss: 0.251473 normal_loss: 0.025559\n",
      "[149/00143] train_loss: 0.032779 kl_loss: 0.251278 normal_loss: 0.025241\n",
      "[150/00099] train_loss: 0.033400 kl_loss: 0.251151 normal_loss: 0.025866\n",
      "[151/00055] train_loss: 0.032951 kl_loss: 0.250573 normal_loss: 0.025434\n",
      "[152/00011] train_loss: 0.033452 kl_loss: 0.252995 normal_loss: 0.025863\n",
      "[152/00111] train_loss: 0.033063 kl_loss: 0.250283 normal_loss: 0.025555\n",
      "[153/00067] train_loss: 0.033183 kl_loss: 0.252036 normal_loss: 0.025622\n",
      "[154/00023] train_loss: 0.032774 kl_loss: 0.250273 normal_loss: 0.025266\n",
      "[154/00123] train_loss: 0.032730 kl_loss: 0.250082 normal_loss: 0.025227\n",
      "[155/00079] train_loss: 0.032855 kl_loss: 0.249799 normal_loss: 0.025361\n",
      "[156/00035] train_loss: 0.033080 kl_loss: 0.252530 normal_loss: 0.025504\n",
      "[156/00135] train_loss: 0.032622 kl_loss: 0.249968 normal_loss: 0.025123\n",
      "[157/00091] train_loss: 0.032697 kl_loss: 0.249844 normal_loss: 0.025202\n",
      "[158/00047] train_loss: 0.032626 kl_loss: 0.250858 normal_loss: 0.025100\n",
      "[159/00003] train_loss: 0.032543 kl_loss: 0.249370 normal_loss: 0.025062\n",
      "[159/00103] train_loss: 0.032484 kl_loss: 0.250889 normal_loss: 0.024958\n",
      "[160/00059] train_loss: 0.032367 kl_loss: 0.247865 normal_loss: 0.024931\n",
      "[161/00015] train_loss: 0.032243 kl_loss: 0.251793 normal_loss: 0.024689\n",
      "[161/00115] train_loss: 0.032393 kl_loss: 0.249277 normal_loss: 0.024915\n",
      "[162/00071] train_loss: 0.032469 kl_loss: 0.248493 normal_loss: 0.025014\n",
      "[163/00027] train_loss: 0.032412 kl_loss: 0.250827 normal_loss: 0.024887\n",
      "[163/00127] train_loss: 0.032307 kl_loss: 0.248919 normal_loss: 0.024839\n",
      "[164/00083] train_loss: 0.032360 kl_loss: 0.250649 normal_loss: 0.024841\n",
      "[165/00039] train_loss: 0.032119 kl_loss: 0.248557 normal_loss: 0.024662\n",
      "[165/00139] train_loss: 0.032173 kl_loss: 0.249034 normal_loss: 0.024702\n",
      "[166/00095] train_loss: 0.031945 kl_loss: 0.249156 normal_loss: 0.024470\n",
      "[167/00051] train_loss: 0.032121 kl_loss: 0.248106 normal_loss: 0.024678\n",
      "[168/00007] train_loss: 0.032026 kl_loss: 0.249562 normal_loss: 0.024539\n",
      "[168/00107] train_loss: 0.031893 kl_loss: 0.249234 normal_loss: 0.024416\n",
      "[169/00063] train_loss: 0.031878 kl_loss: 0.247390 normal_loss: 0.024456\n",
      "[170/00019] train_loss: 0.031807 kl_loss: 0.248701 normal_loss: 0.024346\n",
      "[170/00119] train_loss: 0.031619 kl_loss: 0.246683 normal_loss: 0.024219\n",
      "[171/00075] train_loss: 0.031845 kl_loss: 0.249977 normal_loss: 0.024346\n",
      "[172/00031] train_loss: 0.031450 kl_loss: 0.247379 normal_loss: 0.024029\n",
      "[172/00131] train_loss: 0.031364 kl_loss: 0.248192 normal_loss: 0.023919\n",
      "[173/00087] train_loss: 0.031657 kl_loss: 0.246939 normal_loss: 0.024248\n",
      "[174/00043] train_loss: 0.031581 kl_loss: 0.247121 normal_loss: 0.024167\n",
      "[174/00143] train_loss: 0.031893 kl_loss: 0.248433 normal_loss: 0.024440\n",
      "[175/00099] train_loss: 0.031280 kl_loss: 0.246741 normal_loss: 0.023877\n",
      "[176/00055] train_loss: 0.031332 kl_loss: 0.246883 normal_loss: 0.023925\n",
      "[177/00011] train_loss: 0.031662 kl_loss: 0.247943 normal_loss: 0.024224\n",
      "[177/00111] train_loss: 0.031403 kl_loss: 0.247128 normal_loss: 0.023989\n",
      "[178/00067] train_loss: 0.030975 kl_loss: 0.245763 normal_loss: 0.023602\n",
      "[179/00023] train_loss: 0.031549 kl_loss: 0.247069 normal_loss: 0.024137\n",
      "[179/00123] train_loss: 0.031120 kl_loss: 0.246204 normal_loss: 0.023734\n",
      "[180/00079] train_loss: 0.031156 kl_loss: 0.246429 normal_loss: 0.023763\n",
      "[181/00035] train_loss: 0.031279 kl_loss: 0.246046 normal_loss: 0.023898\n",
      "[181/00135] train_loss: 0.031043 kl_loss: 0.245660 normal_loss: 0.023673\n",
      "[182/00091] train_loss: 0.031122 kl_loss: 0.246993 normal_loss: 0.023712\n",
      "[183/00047] train_loss: 0.030896 kl_loss: 0.244677 normal_loss: 0.023555\n",
      "[184/00003] train_loss: 0.031237 kl_loss: 0.246721 normal_loss: 0.023835\n",
      "[184/00103] train_loss: 0.031001 kl_loss: 0.244801 normal_loss: 0.023657\n",
      "[185/00059] train_loss: 0.030800 kl_loss: 0.245076 normal_loss: 0.023448\n",
      "[186/00015] train_loss: 0.030624 kl_loss: 0.245247 normal_loss: 0.023266\n",
      "[186/00115] train_loss: 0.030967 kl_loss: 0.245333 normal_loss: 0.023607\n",
      "[187/00071] train_loss: 0.030816 kl_loss: 0.243699 normal_loss: 0.023505\n",
      "[188/00027] train_loss: 0.030659 kl_loss: 0.246207 normal_loss: 0.023273\n",
      "[188/00127] train_loss: 0.030629 kl_loss: 0.244396 normal_loss: 0.023297\n",
      "[189/00083] train_loss: 0.030643 kl_loss: 0.243868 normal_loss: 0.023327\n",
      "[190/00039] train_loss: 0.030732 kl_loss: 0.245113 normal_loss: 0.023379\n",
      "[190/00139] train_loss: 0.030742 kl_loss: 0.243691 normal_loss: 0.023431\n",
      "[191/00095] train_loss: 0.030332 kl_loss: 0.243170 normal_loss: 0.023037\n",
      "[192/00051] train_loss: 0.030616 kl_loss: 0.244095 normal_loss: 0.023293\n",
      "[193/00007] train_loss: 0.030639 kl_loss: 0.244479 normal_loss: 0.023304\n",
      "[193/00107] train_loss: 0.030426 kl_loss: 0.242879 normal_loss: 0.023140\n",
      "[194/00063] train_loss: 0.030319 kl_loss: 0.243214 normal_loss: 0.023022\n",
      "[195/00019] train_loss: 0.030255 kl_loss: 0.243085 normal_loss: 0.022962\n",
      "[195/00119] train_loss: 0.030457 kl_loss: 0.243556 normal_loss: 0.023150\n",
      "[196/00075] train_loss: 0.030312 kl_loss: 0.242279 normal_loss: 0.023043\n",
      "[197/00031] train_loss: 0.030293 kl_loss: 0.241905 normal_loss: 0.023036\n",
      "[197/00131] train_loss: 0.030356 kl_loss: 0.243044 normal_loss: 0.023064\n",
      "[198/00087] train_loss: 0.030215 kl_loss: 0.242264 normal_loss: 0.022947\n",
      "[199/00043] train_loss: 0.030282 kl_loss: 0.242322 normal_loss: 0.023012\n",
      "[199/00143] train_loss: 0.030035 kl_loss: 0.242103 normal_loss: 0.022772\n",
      "[200/00099] train_loss: 0.029288 kl_loss: 0.242344 normal_loss: 0.022018\n",
      "[201/00055] train_loss: 0.028698 kl_loss: 0.240796 normal_loss: 0.021474\n",
      "[202/00011] train_loss: 0.028705 kl_loss: 0.240676 normal_loss: 0.021484\n",
      "[202/00111] train_loss: 0.028508 kl_loss: 0.239821 normal_loss: 0.021313\n",
      "[203/00067] train_loss: 0.028212 kl_loss: 0.238860 normal_loss: 0.021046\n",
      "[204/00023] train_loss: 0.028536 kl_loss: 0.240080 normal_loss: 0.021334\n",
      "[204/00123] train_loss: 0.028309 kl_loss: 0.238343 normal_loss: 0.021158\n",
      "[205/00079] train_loss: 0.028081 kl_loss: 0.236193 normal_loss: 0.020995\n",
      "[206/00035] train_loss: 0.028257 kl_loss: 0.237430 normal_loss: 0.021134\n",
      "[206/00135] train_loss: 0.028246 kl_loss: 0.237272 normal_loss: 0.021128\n",
      "[207/00091] train_loss: 0.028027 kl_loss: 0.235573 normal_loss: 0.020960\n",
      "[208/00047] train_loss: 0.028163 kl_loss: 0.236331 normal_loss: 0.021073\n",
      "[209/00003] train_loss: 0.028048 kl_loss: 0.234455 normal_loss: 0.021015\n",
      "[209/00103] train_loss: 0.027991 kl_loss: 0.234416 normal_loss: 0.020959\n",
      "[210/00059] train_loss: 0.027876 kl_loss: 0.234823 normal_loss: 0.020832\n",
      "[211/00015] train_loss: 0.028004 kl_loss: 0.232953 normal_loss: 0.021016\n",
      "[211/00115] train_loss: 0.027883 kl_loss: 0.232852 normal_loss: 0.020898\n",
      "[212/00071] train_loss: 0.027923 kl_loss: 0.233213 normal_loss: 0.020927\n",
      "[213/00027] train_loss: 0.027739 kl_loss: 0.232021 normal_loss: 0.020779\n",
      "[213/00127] train_loss: 0.027800 kl_loss: 0.231535 normal_loss: 0.020854\n",
      "[214/00083] train_loss: 0.027718 kl_loss: 0.231741 normal_loss: 0.020766\n",
      "[215/00039] train_loss: 0.027568 kl_loss: 0.229301 normal_loss: 0.020689\n",
      "[215/00139] train_loss: 0.027729 kl_loss: 0.231548 normal_loss: 0.020782\n",
      "[216/00095] train_loss: 0.027612 kl_loss: 0.228903 normal_loss: 0.020745\n",
      "[217/00051] train_loss: 0.027538 kl_loss: 0.230413 normal_loss: 0.020626\n",
      "[218/00007] train_loss: 0.027581 kl_loss: 0.228978 normal_loss: 0.020712\n",
      "[218/00107] train_loss: 0.027503 kl_loss: 0.229156 normal_loss: 0.020628\n",
      "[219/00063] train_loss: 0.027708 kl_loss: 0.227942 normal_loss: 0.020869\n",
      "[220/00019] train_loss: 0.027487 kl_loss: 0.228904 normal_loss: 0.020620\n",
      "[220/00119] train_loss: 0.027640 kl_loss: 0.228008 normal_loss: 0.020800\n",
      "[221/00075] train_loss: 0.027558 kl_loss: 0.228408 normal_loss: 0.020706\n",
      "[222/00031] train_loss: 0.027222 kl_loss: 0.224916 normal_loss: 0.020475\n",
      "[222/00131] train_loss: 0.027316 kl_loss: 0.226603 normal_loss: 0.020518\n",
      "[223/00087] train_loss: 0.027285 kl_loss: 0.226650 normal_loss: 0.020485\n",
      "[224/00043] train_loss: 0.027442 kl_loss: 0.226283 normal_loss: 0.020653\n",
      "[224/00143] train_loss: 0.027295 kl_loss: 0.225965 normal_loss: 0.020517\n",
      "[225/00099] train_loss: 0.027124 kl_loss: 0.224906 normal_loss: 0.020377\n",
      "[226/00055] train_loss: 0.027125 kl_loss: 0.224743 normal_loss: 0.020383\n",
      "[227/00011] train_loss: 0.027145 kl_loss: 0.225426 normal_loss: 0.020382\n",
      "[227/00111] train_loss: 0.027252 kl_loss: 0.224806 normal_loss: 0.020508\n",
      "[228/00067] train_loss: 0.027152 kl_loss: 0.222968 normal_loss: 0.020462\n",
      "[229/00023] train_loss: 0.027010 kl_loss: 0.224914 normal_loss: 0.020262\n",
      "[229/00123] train_loss: 0.027239 kl_loss: 0.223178 normal_loss: 0.020544\n",
      "[230/00079] train_loss: 0.027138 kl_loss: 0.223873 normal_loss: 0.020422\n",
      "[231/00035] train_loss: 0.027124 kl_loss: 0.223551 normal_loss: 0.020417\n",
      "[231/00135] train_loss: 0.026916 kl_loss: 0.222339 normal_loss: 0.020246\n",
      "[232/00091] train_loss: 0.027117 kl_loss: 0.223412 normal_loss: 0.020414\n",
      "[233/00047] train_loss: 0.026950 kl_loss: 0.220747 normal_loss: 0.020327\n",
      "[234/00003] train_loss: 0.027094 kl_loss: 0.223118 normal_loss: 0.020400\n",
      "[234/00103] train_loss: 0.026962 kl_loss: 0.222060 normal_loss: 0.020300\n",
      "[235/00059] train_loss: 0.026854 kl_loss: 0.220826 normal_loss: 0.020229\n",
      "[236/00015] train_loss: 0.026877 kl_loss: 0.222270 normal_loss: 0.020209\n",
      "[236/00115] train_loss: 0.026736 kl_loss: 0.220291 normal_loss: 0.020127\n",
      "[237/00071] train_loss: 0.026818 kl_loss: 0.221457 normal_loss: 0.020174\n",
      "[238/00027] train_loss: 0.026837 kl_loss: 0.221054 normal_loss: 0.020205\n",
      "[238/00127] train_loss: 0.026831 kl_loss: 0.220130 normal_loss: 0.020227\n",
      "[239/00083] train_loss: 0.026689 kl_loss: 0.219438 normal_loss: 0.020106\n",
      "[240/00039] train_loss: 0.026696 kl_loss: 0.220478 normal_loss: 0.020082\n",
      "[240/00139] train_loss: 0.026769 kl_loss: 0.220614 normal_loss: 0.020151\n",
      "[241/00095] train_loss: 0.026803 kl_loss: 0.220701 normal_loss: 0.020182\n",
      "[242/00051] train_loss: 0.026677 kl_loss: 0.218704 normal_loss: 0.020116\n",
      "[243/00007] train_loss: 0.026822 kl_loss: 0.219365 normal_loss: 0.020241\n",
      "[243/00107] train_loss: 0.026597 kl_loss: 0.219505 normal_loss: 0.020012\n",
      "[244/00063] train_loss: 0.026312 kl_loss: 0.218016 normal_loss: 0.019772\n",
      "[245/00019] train_loss: 0.026782 kl_loss: 0.219673 normal_loss: 0.020191\n",
      "[245/00119] train_loss: 0.026498 kl_loss: 0.218205 normal_loss: 0.019952\n",
      "[246/00075] train_loss: 0.026459 kl_loss: 0.217374 normal_loss: 0.019937\n",
      "[247/00031] train_loss: 0.026666 kl_loss: 0.219726 normal_loss: 0.020075\n",
      "[247/00131] train_loss: 0.026485 kl_loss: 0.218714 normal_loss: 0.019923\n",
      "[248/00087] train_loss: 0.026288 kl_loss: 0.216772 normal_loss: 0.019785\n",
      "[249/00043] train_loss: 0.026312 kl_loss: 0.216997 normal_loss: 0.019802\n",
      "[249/00143] train_loss: 0.026700 kl_loss: 0.218559 normal_loss: 0.020143\n",
      "[250/00099] train_loss: 0.026352 kl_loss: 0.216679 normal_loss: 0.019851\n",
      "[251/00055] train_loss: 0.026737 kl_loss: 0.218578 normal_loss: 0.020180\n",
      "[252/00011] train_loss: 0.026256 kl_loss: 0.215714 normal_loss: 0.019784\n",
      "[252/00111] train_loss: 0.026396 kl_loss: 0.217727 normal_loss: 0.019864\n",
      "[253/00067] train_loss: 0.026182 kl_loss: 0.216371 normal_loss: 0.019690\n",
      "[254/00023] train_loss: 0.026419 kl_loss: 0.216635 normal_loss: 0.019920\n",
      "[254/00123] train_loss: 0.026417 kl_loss: 0.216018 normal_loss: 0.019936\n",
      "[255/00079] train_loss: 0.026317 kl_loss: 0.217185 normal_loss: 0.019801\n",
      "[256/00035] train_loss: 0.026311 kl_loss: 0.215742 normal_loss: 0.019839\n",
      "[256/00135] train_loss: 0.026125 kl_loss: 0.215462 normal_loss: 0.019661\n",
      "[257/00091] train_loss: 0.026185 kl_loss: 0.216582 normal_loss: 0.019688\n",
      "[258/00047] train_loss: 0.026145 kl_loss: 0.215236 normal_loss: 0.019688\n",
      "[259/00003] train_loss: 0.026283 kl_loss: 0.215270 normal_loss: 0.019825\n",
      "[259/00103] train_loss: 0.026170 kl_loss: 0.214840 normal_loss: 0.019725\n",
      "[260/00059] train_loss: 0.026371 kl_loss: 0.216147 normal_loss: 0.019887\n",
      "[261/00015] train_loss: 0.026070 kl_loss: 0.214582 normal_loss: 0.019632\n",
      "[261/00115] train_loss: 0.026037 kl_loss: 0.214077 normal_loss: 0.019614\n",
      "[262/00071] train_loss: 0.026327 kl_loss: 0.215833 normal_loss: 0.019852\n",
      "[263/00027] train_loss: 0.026043 kl_loss: 0.213437 normal_loss: 0.019640\n",
      "[263/00127] train_loss: 0.026037 kl_loss: 0.214940 normal_loss: 0.019589\n",
      "[264/00083] train_loss: 0.025833 kl_loss: 0.213220 normal_loss: 0.019437\n",
      "[265/00039] train_loss: 0.026056 kl_loss: 0.214317 normal_loss: 0.019627\n",
      "[265/00139] train_loss: 0.026018 kl_loss: 0.214894 normal_loss: 0.019571\n",
      "[266/00095] train_loss: 0.026024 kl_loss: 0.214393 normal_loss: 0.019592\n",
      "[267/00051] train_loss: 0.025813 kl_loss: 0.213123 normal_loss: 0.019420\n",
      "[268/00007] train_loss: 0.025857 kl_loss: 0.213527 normal_loss: 0.019451\n",
      "[268/00107] train_loss: 0.025950 kl_loss: 0.213046 normal_loss: 0.019559\n",
      "[269/00063] train_loss: 0.025989 kl_loss: 0.213294 normal_loss: 0.019590\n",
      "[270/00019] train_loss: 0.026022 kl_loss: 0.213816 normal_loss: 0.019607\n",
      "[270/00119] train_loss: 0.025817 kl_loss: 0.213166 normal_loss: 0.019422\n",
      "[271/00075] train_loss: 0.025793 kl_loss: 0.213255 normal_loss: 0.019395\n",
      "[272/00031] train_loss: 0.025679 kl_loss: 0.212383 normal_loss: 0.019307\n",
      "[272/00131] train_loss: 0.025695 kl_loss: 0.212398 normal_loss: 0.019323\n",
      "[273/00087] train_loss: 0.025682 kl_loss: 0.211609 normal_loss: 0.019334\n",
      "[274/00043] train_loss: 0.025936 kl_loss: 0.213033 normal_loss: 0.019545\n",
      "[274/00143] train_loss: 0.025779 kl_loss: 0.211938 normal_loss: 0.019421\n",
      "[275/00099] train_loss: 0.025921 kl_loss: 0.212455 normal_loss: 0.019547\n",
      "[276/00055] train_loss: 0.025570 kl_loss: 0.211528 normal_loss: 0.019224\n",
      "[277/00011] train_loss: 0.025465 kl_loss: 0.211365 normal_loss: 0.019124\n",
      "[277/00111] train_loss: 0.025719 kl_loss: 0.211575 normal_loss: 0.019372\n",
      "[278/00067] train_loss: 0.025701 kl_loss: 0.211355 normal_loss: 0.019360\n",
      "[279/00023] train_loss: 0.025619 kl_loss: 0.211121 normal_loss: 0.019285\n",
      "[279/00123] train_loss: 0.025725 kl_loss: 0.212143 normal_loss: 0.019360\n",
      "[280/00079] train_loss: 0.025491 kl_loss: 0.210050 normal_loss: 0.019190\n",
      "[281/00035] train_loss: 0.025674 kl_loss: 0.210907 normal_loss: 0.019346\n",
      "[281/00135] train_loss: 0.025693 kl_loss: 0.210748 normal_loss: 0.019371\n",
      "[282/00091] train_loss: 0.025397 kl_loss: 0.210223 normal_loss: 0.019091\n",
      "[283/00047] train_loss: 0.025529 kl_loss: 0.210483 normal_loss: 0.019214\n",
      "[284/00003] train_loss: 0.025500 kl_loss: 0.210785 normal_loss: 0.019176\n",
      "[284/00103] train_loss: 0.025352 kl_loss: 0.210282 normal_loss: 0.019044\n",
      "[285/00059] train_loss: 0.025480 kl_loss: 0.209688 normal_loss: 0.019189\n",
      "[286/00015] train_loss: 0.025345 kl_loss: 0.209926 normal_loss: 0.019047\n",
      "[286/00115] train_loss: 0.025461 kl_loss: 0.209807 normal_loss: 0.019167\n",
      "[287/00071] train_loss: 0.025390 kl_loss: 0.209240 normal_loss: 0.019112\n",
      "[288/00027] train_loss: 0.025395 kl_loss: 0.210024 normal_loss: 0.019094\n",
      "[288/00127] train_loss: 0.025328 kl_loss: 0.209076 normal_loss: 0.019055\n",
      "[289/00083] train_loss: 0.025157 kl_loss: 0.208153 normal_loss: 0.018913\n",
      "[290/00039] train_loss: 0.025452 kl_loss: 0.209910 normal_loss: 0.019154\n",
      "[290/00139] train_loss: 0.025233 kl_loss: 0.209086 normal_loss: 0.018961\n",
      "[291/00095] train_loss: 0.025239 kl_loss: 0.208507 normal_loss: 0.018984\n",
      "[292/00051] train_loss: 0.025429 kl_loss: 0.209521 normal_loss: 0.019143\n",
      "[293/00007] train_loss: 0.025212 kl_loss: 0.208566 normal_loss: 0.018955\n",
      "[293/00107] train_loss: 0.025275 kl_loss: 0.208542 normal_loss: 0.019018\n",
      "[294/00063] train_loss: 0.025044 kl_loss: 0.207210 normal_loss: 0.018828\n",
      "[295/00019] train_loss: 0.025098 kl_loss: 0.208498 normal_loss: 0.018843\n",
      "[295/00119] train_loss: 0.025269 kl_loss: 0.208821 normal_loss: 0.019005\n",
      "[296/00075] train_loss: 0.025042 kl_loss: 0.207735 normal_loss: 0.018810\n",
      "[297/00031] train_loss: 0.025130 kl_loss: 0.206585 normal_loss: 0.018933\n",
      "[297/00131] train_loss: 0.025187 kl_loss: 0.207851 normal_loss: 0.018952\n",
      "[298/00087] train_loss: 0.025074 kl_loss: 0.207049 normal_loss: 0.018863\n",
      "[299/00043] train_loss: 0.025032 kl_loss: 0.207510 normal_loss: 0.018807\n",
      "[299/00143] train_loss: 0.024966 kl_loss: 0.207479 normal_loss: 0.018742\n",
      "[300/00099] train_loss: 0.024610 kl_loss: 0.207288 normal_loss: 0.018391\n",
      "[301/00055] train_loss: 0.024567 kl_loss: 0.206546 normal_loss: 0.018371\n",
      "[302/00011] train_loss: 0.024358 kl_loss: 0.206038 normal_loss: 0.018177\n",
      "[302/00111] train_loss: 0.024447 kl_loss: 0.206630 normal_loss: 0.018248\n",
      "[303/00067] train_loss: 0.024325 kl_loss: 0.205880 normal_loss: 0.018148\n",
      "[304/00023] train_loss: 0.024469 kl_loss: 0.206932 normal_loss: 0.018261\n",
      "[304/00123] train_loss: 0.024344 kl_loss: 0.206088 normal_loss: 0.018162\n",
      "[305/00079] train_loss: 0.024311 kl_loss: 0.205331 normal_loss: 0.018152\n",
      "[306/00035] train_loss: 0.024137 kl_loss: 0.205040 normal_loss: 0.017986\n",
      "[306/00135] train_loss: 0.024351 kl_loss: 0.205517 normal_loss: 0.018186\n",
      "[307/00091] train_loss: 0.024238 kl_loss: 0.205546 normal_loss: 0.018071\n",
      "[308/00047] train_loss: 0.024131 kl_loss: 0.204488 normal_loss: 0.017996\n",
      "[309/00003] train_loss: 0.024260 kl_loss: 0.204729 normal_loss: 0.018118\n",
      "[309/00103] train_loss: 0.024236 kl_loss: 0.204717 normal_loss: 0.018094\n",
      "[310/00059] train_loss: 0.024052 kl_loss: 0.204471 normal_loss: 0.017918\n",
      "[311/00015] train_loss: 0.024198 kl_loss: 0.203852 normal_loss: 0.018082\n",
      "[311/00115] train_loss: 0.024165 kl_loss: 0.204844 normal_loss: 0.018020\n",
      "[312/00071] train_loss: 0.024068 kl_loss: 0.203208 normal_loss: 0.017972\n",
      "[313/00027] train_loss: 0.024122 kl_loss: 0.203639 normal_loss: 0.018013\n",
      "[313/00127] train_loss: 0.024133 kl_loss: 0.204061 normal_loss: 0.018011\n",
      "[314/00083] train_loss: 0.023964 kl_loss: 0.202497 normal_loss: 0.017889\n",
      "[315/00039] train_loss: 0.024048 kl_loss: 0.202890 normal_loss: 0.017962\n",
      "[315/00139] train_loss: 0.024055 kl_loss: 0.203390 normal_loss: 0.017953\n",
      "[316/00095] train_loss: 0.024048 kl_loss: 0.202913 normal_loss: 0.017961\n",
      "[317/00051] train_loss: 0.023952 kl_loss: 0.202159 normal_loss: 0.017887\n",
      "[318/00007] train_loss: 0.024009 kl_loss: 0.202384 normal_loss: 0.017937\n",
      "[318/00107] train_loss: 0.023865 kl_loss: 0.202125 normal_loss: 0.017801\n",
      "[319/00063] train_loss: 0.023932 kl_loss: 0.202375 normal_loss: 0.017861\n",
      "[320/00019] train_loss: 0.023918 kl_loss: 0.201905 normal_loss: 0.017861\n",
      "[320/00119] train_loss: 0.023985 kl_loss: 0.201652 normal_loss: 0.017936\n",
      "[321/00075] train_loss: 0.024136 kl_loss: 0.202836 normal_loss: 0.018051\n",
      "[322/00031] train_loss: 0.023868 kl_loss: 0.200945 normal_loss: 0.017840\n",
      "[322/00131] train_loss: 0.023912 kl_loss: 0.201451 normal_loss: 0.017868\n",
      "[323/00087] train_loss: 0.023763 kl_loss: 0.200885 normal_loss: 0.017736\n",
      "[324/00043] train_loss: 0.023901 kl_loss: 0.201426 normal_loss: 0.017859\n",
      "[324/00143] train_loss: 0.023778 kl_loss: 0.200452 normal_loss: 0.017765\n",
      "[325/00099] train_loss: 0.023840 kl_loss: 0.201088 normal_loss: 0.017807\n",
      "[326/00055] train_loss: 0.023759 kl_loss: 0.200460 normal_loss: 0.017745\n",
      "[327/00011] train_loss: 0.023823 kl_loss: 0.200382 normal_loss: 0.017812\n",
      "[327/00111] train_loss: 0.023845 kl_loss: 0.200859 normal_loss: 0.017819\n",
      "[328/00067] train_loss: 0.023715 kl_loss: 0.198784 normal_loss: 0.017752\n",
      "[329/00023] train_loss: 0.023913 kl_loss: 0.201224 normal_loss: 0.017876\n",
      "[329/00123] train_loss: 0.023685 kl_loss: 0.198820 normal_loss: 0.017720\n",
      "[330/00079] train_loss: 0.023949 kl_loss: 0.200912 normal_loss: 0.017921\n",
      "[331/00035] train_loss: 0.023687 kl_loss: 0.199157 normal_loss: 0.017712\n",
      "[331/00135] train_loss: 0.023719 kl_loss: 0.199262 normal_loss: 0.017742\n",
      "[332/00091] train_loss: 0.023693 kl_loss: 0.199409 normal_loss: 0.017711\n",
      "[333/00047] train_loss: 0.023729 kl_loss: 0.200064 normal_loss: 0.017727\n",
      "[334/00003] train_loss: 0.023766 kl_loss: 0.198744 normal_loss: 0.017804\n",
      "[334/00103] train_loss: 0.023586 kl_loss: 0.198320 normal_loss: 0.017636\n",
      "[335/00059] train_loss: 0.023721 kl_loss: 0.199216 normal_loss: 0.017744\n",
      "[336/00015] train_loss: 0.023676 kl_loss: 0.199335 normal_loss: 0.017696\n",
      "[336/00115] train_loss: 0.023552 kl_loss: 0.197910 normal_loss: 0.017615\n",
      "[337/00071] train_loss: 0.023541 kl_loss: 0.198334 normal_loss: 0.017591\n",
      "[338/00027] train_loss: 0.023888 kl_loss: 0.199764 normal_loss: 0.017895\n",
      "[338/00127] train_loss: 0.023579 kl_loss: 0.197801 normal_loss: 0.017645\n",
      "[339/00083] train_loss: 0.023405 kl_loss: 0.197211 normal_loss: 0.017489\n",
      "[340/00039] train_loss: 0.023548 kl_loss: 0.198507 normal_loss: 0.017592\n",
      "[340/00139] train_loss: 0.023699 kl_loss: 0.198389 normal_loss: 0.017748\n",
      "[341/00095] train_loss: 0.023634 kl_loss: 0.197862 normal_loss: 0.017698\n",
      "[342/00051] train_loss: 0.023493 kl_loss: 0.196946 normal_loss: 0.017585\n",
      "[343/00007] train_loss: 0.023618 kl_loss: 0.198411 normal_loss: 0.017666\n",
      "[343/00107] train_loss: 0.023577 kl_loss: 0.197472 normal_loss: 0.017653\n",
      "[344/00063] train_loss: 0.023545 kl_loss: 0.197557 normal_loss: 0.017618\n",
      "[345/00019] train_loss: 0.023567 kl_loss: 0.197877 normal_loss: 0.017631\n",
      "[345/00119] train_loss: 0.023429 kl_loss: 0.195947 normal_loss: 0.017551\n",
      "[346/00075] train_loss: 0.023508 kl_loss: 0.197179 normal_loss: 0.017592\n",
      "[347/00031] train_loss: 0.023629 kl_loss: 0.197706 normal_loss: 0.017698\n",
      "[347/00131] train_loss: 0.023494 kl_loss: 0.196893 normal_loss: 0.017587\n",
      "[348/00087] train_loss: 0.023222 kl_loss: 0.196164 normal_loss: 0.017337\n",
      "[349/00043] train_loss: 0.023421 kl_loss: 0.196220 normal_loss: 0.017534\n",
      "[349/00143] train_loss: 0.023565 kl_loss: 0.197328 normal_loss: 0.017645\n",
      "[350/00099] train_loss: 0.023481 kl_loss: 0.196853 normal_loss: 0.017575\n",
      "[351/00055] train_loss: 0.023330 kl_loss: 0.195445 normal_loss: 0.017467\n",
      "[352/00011] train_loss: 0.023658 kl_loss: 0.197283 normal_loss: 0.017739\n",
      "[352/00111] train_loss: 0.023399 kl_loss: 0.196094 normal_loss: 0.017517\n",
      "[353/00067] train_loss: 0.023294 kl_loss: 0.195462 normal_loss: 0.017431\n",
      "[354/00023] train_loss: 0.023416 kl_loss: 0.195724 normal_loss: 0.017544\n",
      "[354/00123] train_loss: 0.023328 kl_loss: 0.195663 normal_loss: 0.017458\n",
      "[355/00079] train_loss: 0.023285 kl_loss: 0.196136 normal_loss: 0.017401\n",
      "[356/00035] train_loss: 0.023321 kl_loss: 0.195617 normal_loss: 0.017452\n",
      "[356/00135] train_loss: 0.023325 kl_loss: 0.195808 normal_loss: 0.017451\n",
      "[357/00091] train_loss: 0.023297 kl_loss: 0.195258 normal_loss: 0.017439\n",
      "[358/00047] train_loss: 0.023330 kl_loss: 0.195501 normal_loss: 0.017465\n",
      "[359/00003] train_loss: 0.023253 kl_loss: 0.194951 normal_loss: 0.017405\n",
      "[359/00103] train_loss: 0.023419 kl_loss: 0.196083 normal_loss: 0.017536\n",
      "[360/00059] train_loss: 0.023051 kl_loss: 0.193883 normal_loss: 0.017234\n",
      "[361/00015] train_loss: 0.023425 kl_loss: 0.195308 normal_loss: 0.017566\n",
      "[361/00115] train_loss: 0.023231 kl_loss: 0.194961 normal_loss: 0.017382\n",
      "[362/00071] train_loss: 0.023222 kl_loss: 0.193704 normal_loss: 0.017410\n",
      "[363/00027] train_loss: 0.023347 kl_loss: 0.195136 normal_loss: 0.017493\n",
      "[363/00127] train_loss: 0.023265 kl_loss: 0.195020 normal_loss: 0.017414\n",
      "[364/00083] train_loss: 0.023159 kl_loss: 0.193931 normal_loss: 0.017341\n",
      "[365/00039] train_loss: 0.023293 kl_loss: 0.194702 normal_loss: 0.017452\n",
      "[365/00139] train_loss: 0.023216 kl_loss: 0.194446 normal_loss: 0.017383\n",
      "[366/00095] train_loss: 0.023076 kl_loss: 0.193655 normal_loss: 0.017266\n",
      "[367/00051] train_loss: 0.023278 kl_loss: 0.195438 normal_loss: 0.017415\n",
      "[368/00007] train_loss: 0.023156 kl_loss: 0.193450 normal_loss: 0.017352\n",
      "[368/00107] train_loss: 0.023244 kl_loss: 0.194582 normal_loss: 0.017406\n",
      "[369/00063] train_loss: 0.023104 kl_loss: 0.193907 normal_loss: 0.017287\n",
      "[370/00019] train_loss: 0.023023 kl_loss: 0.193275 normal_loss: 0.017225\n",
      "[370/00119] train_loss: 0.023214 kl_loss: 0.194238 normal_loss: 0.017387\n",
      "[371/00075] train_loss: 0.023061 kl_loss: 0.193546 normal_loss: 0.017255\n",
      "[372/00031] train_loss: 0.023192 kl_loss: 0.193838 normal_loss: 0.017377\n",
      "[372/00131] train_loss: 0.023012 kl_loss: 0.192917 normal_loss: 0.017224\n",
      "[373/00087] train_loss: 0.023143 kl_loss: 0.193714 normal_loss: 0.017332\n",
      "[374/00043] train_loss: 0.023115 kl_loss: 0.193455 normal_loss: 0.017311\n",
      "[374/00143] train_loss: 0.023052 kl_loss: 0.192903 normal_loss: 0.017265\n",
      "[375/00099] train_loss: 0.023113 kl_loss: 0.193644 normal_loss: 0.017304\n",
      "[376/00055] train_loss: 0.022992 kl_loss: 0.192444 normal_loss: 0.017219\n",
      "[377/00011] train_loss: 0.023061 kl_loss: 0.192967 normal_loss: 0.017272\n",
      "[377/00111] train_loss: 0.023069 kl_loss: 0.193089 normal_loss: 0.017276\n",
      "[378/00067] train_loss: 0.022987 kl_loss: 0.192952 normal_loss: 0.017198\n",
      "[379/00023] train_loss: 0.022954 kl_loss: 0.192106 normal_loss: 0.017191\n",
      "[379/00123] train_loss: 0.023006 kl_loss: 0.192995 normal_loss: 0.017216\n",
      "[380/00079] train_loss: 0.022822 kl_loss: 0.191314 normal_loss: 0.017082\n",
      "[381/00035] train_loss: 0.023122 kl_loss: 0.194082 normal_loss: 0.017299\n",
      "[381/00135] train_loss: 0.022952 kl_loss: 0.191663 normal_loss: 0.017202\n",
      "[382/00091] train_loss: 0.023028 kl_loss: 0.192409 normal_loss: 0.017256\n",
      "[383/00047] train_loss: 0.022879 kl_loss: 0.191905 normal_loss: 0.017122\n",
      "[384/00003] train_loss: 0.022903 kl_loss: 0.192419 normal_loss: 0.017130\n",
      "[384/00103] train_loss: 0.023010 kl_loss: 0.191870 normal_loss: 0.017254\n",
      "[385/00059] train_loss: 0.022780 kl_loss: 0.191082 normal_loss: 0.017048\n",
      "[386/00015] train_loss: 0.023072 kl_loss: 0.193117 normal_loss: 0.017278\n",
      "[386/00115] train_loss: 0.022933 kl_loss: 0.192125 normal_loss: 0.017170\n",
      "[387/00071] train_loss: 0.022850 kl_loss: 0.191522 normal_loss: 0.017105\n",
      "[388/00027] train_loss: 0.022966 kl_loss: 0.191281 normal_loss: 0.017228\n",
      "[388/00127] train_loss: 0.022900 kl_loss: 0.191856 normal_loss: 0.017144\n",
      "[389/00083] train_loss: 0.022934 kl_loss: 0.191053 normal_loss: 0.017202\n",
      "[390/00039] train_loss: 0.022759 kl_loss: 0.191818 normal_loss: 0.017005\n",
      "[390/00139] train_loss: 0.022839 kl_loss: 0.191163 normal_loss: 0.017104\n",
      "[391/00095] train_loss: 0.022744 kl_loss: 0.190619 normal_loss: 0.017025\n",
      "[392/00051] train_loss: 0.022901 kl_loss: 0.191961 normal_loss: 0.017142\n",
      "[393/00007] train_loss: 0.022788 kl_loss: 0.190457 normal_loss: 0.017075\n",
      "[393/00107] train_loss: 0.022814 kl_loss: 0.191419 normal_loss: 0.017072\n",
      "[394/00063] train_loss: 0.022862 kl_loss: 0.190974 normal_loss: 0.017133\n",
      "[395/00019] train_loss: 0.022748 kl_loss: 0.190363 normal_loss: 0.017037\n",
      "[395/00119] train_loss: 0.022788 kl_loss: 0.190707 normal_loss: 0.017067\n",
      "[396/00075] train_loss: 0.022797 kl_loss: 0.190515 normal_loss: 0.017082\n",
      "[397/00031] train_loss: 0.022768 kl_loss: 0.190447 normal_loss: 0.017054\n",
      "[397/00131] train_loss: 0.022722 kl_loss: 0.190531 normal_loss: 0.017007\n",
      "[398/00087] train_loss: 0.022763 kl_loss: 0.190554 normal_loss: 0.017046\n",
      "[399/00043] train_loss: 0.022662 kl_loss: 0.190057 normal_loss: 0.016960\n",
      "[399/00143] train_loss: 0.022722 kl_loss: 0.190911 normal_loss: 0.016995\n",
      "[400/00099] train_loss: 0.022561 kl_loss: 0.190097 normal_loss: 0.016858\n",
      "[401/00055] train_loss: 0.022505 kl_loss: 0.190792 normal_loss: 0.016781\n",
      "[402/00011] train_loss: 0.022392 kl_loss: 0.189382 normal_loss: 0.016710\n",
      "[402/00111] train_loss: 0.022491 kl_loss: 0.189716 normal_loss: 0.016799\n",
      "[403/00067] train_loss: 0.022474 kl_loss: 0.190060 normal_loss: 0.016772\n",
      "[404/00023] train_loss: 0.022464 kl_loss: 0.189802 normal_loss: 0.016770\n",
      "[404/00123] train_loss: 0.022410 kl_loss: 0.189954 normal_loss: 0.016711\n",
      "[405/00079] train_loss: 0.022578 kl_loss: 0.190434 normal_loss: 0.016865\n",
      "[406/00035] train_loss: 0.022320 kl_loss: 0.189081 normal_loss: 0.016647\n",
      "[406/00135] train_loss: 0.022404 kl_loss: 0.189412 normal_loss: 0.016722\n",
      "[407/00091] train_loss: 0.022400 kl_loss: 0.189280 normal_loss: 0.016721\n",
      "[408/00047] train_loss: 0.022494 kl_loss: 0.189588 normal_loss: 0.016806\n",
      "[409/00003] train_loss: 0.022339 kl_loss: 0.189578 normal_loss: 0.016651\n",
      "[409/00103] train_loss: 0.022335 kl_loss: 0.189351 normal_loss: 0.016654\n",
      "[410/00059] train_loss: 0.022323 kl_loss: 0.189422 normal_loss: 0.016641\n",
      "[411/00015] train_loss: 0.022273 kl_loss: 0.188585 normal_loss: 0.016615\n",
      "[411/00115] train_loss: 0.022324 kl_loss: 0.188983 normal_loss: 0.016654\n",
      "[412/00071] train_loss: 0.022373 kl_loss: 0.189524 normal_loss: 0.016687\n",
      "[413/00027] train_loss: 0.022359 kl_loss: 0.189217 normal_loss: 0.016683\n",
      "[413/00127] train_loss: 0.022279 kl_loss: 0.188477 normal_loss: 0.016625\n",
      "[414/00083] train_loss: 0.022327 kl_loss: 0.189474 normal_loss: 0.016643\n",
      "[415/00039] train_loss: 0.022309 kl_loss: 0.188325 normal_loss: 0.016660\n",
      "[415/00139] train_loss: 0.022338 kl_loss: 0.188862 normal_loss: 0.016672\n",
      "[416/00095] train_loss: 0.022258 kl_loss: 0.188331 normal_loss: 0.016608\n",
      "[417/00051] train_loss: 0.022304 kl_loss: 0.188763 normal_loss: 0.016641\n",
      "[418/00007] train_loss: 0.022271 kl_loss: 0.188720 normal_loss: 0.016609\n",
      "[418/00107] train_loss: 0.022337 kl_loss: 0.188901 normal_loss: 0.016670\n",
      "[419/00063] train_loss: 0.022280 kl_loss: 0.188336 normal_loss: 0.016630\n",
      "[420/00019] train_loss: 0.022135 kl_loss: 0.187387 normal_loss: 0.016513\n",
      "[420/00119] train_loss: 0.022232 kl_loss: 0.188145 normal_loss: 0.016588\n",
      "[421/00075] train_loss: 0.022266 kl_loss: 0.187994 normal_loss: 0.016626\n",
      "[422/00031] train_loss: 0.022303 kl_loss: 0.188796 normal_loss: 0.016639\n",
      "[422/00131] train_loss: 0.022333 kl_loss: 0.188736 normal_loss: 0.016671\n",
      "[423/00087] train_loss: 0.022106 kl_loss: 0.187116 normal_loss: 0.016493\n",
      "[424/00043] train_loss: 0.022145 kl_loss: 0.187878 normal_loss: 0.016508\n",
      "[424/00143] train_loss: 0.022292 kl_loss: 0.188544 normal_loss: 0.016635\n",
      "[425/00099] train_loss: 0.022300 kl_loss: 0.188459 normal_loss: 0.016646\n",
      "[426/00055] train_loss: 0.022178 kl_loss: 0.187420 normal_loss: 0.016556\n",
      "[427/00011] train_loss: 0.022176 kl_loss: 0.187702 normal_loss: 0.016545\n",
      "[427/00111] train_loss: 0.022152 kl_loss: 0.187054 normal_loss: 0.016541\n",
      "[428/00067] train_loss: 0.022231 kl_loss: 0.188269 normal_loss: 0.016583\n",
      "[429/00023] train_loss: 0.022189 kl_loss: 0.187843 normal_loss: 0.016554\n",
      "[429/00123] train_loss: 0.022228 kl_loss: 0.187243 normal_loss: 0.016611\n",
      "[430/00079] train_loss: 0.022188 kl_loss: 0.188092 normal_loss: 0.016546\n",
      "[431/00035] train_loss: 0.022029 kl_loss: 0.186430 normal_loss: 0.016436\n",
      "[431/00135] train_loss: 0.022190 kl_loss: 0.187648 normal_loss: 0.016561\n",
      "[432/00091] train_loss: 0.022190 kl_loss: 0.187926 normal_loss: 0.016552\n",
      "[433/00047] train_loss: 0.022086 kl_loss: 0.186318 normal_loss: 0.016497\n",
      "[434/00003] train_loss: 0.022212 kl_loss: 0.188116 normal_loss: 0.016569\n",
      "[434/00103] train_loss: 0.022086 kl_loss: 0.186788 normal_loss: 0.016483\n",
      "[435/00059] train_loss: 0.022109 kl_loss: 0.187238 normal_loss: 0.016492\n",
      "[436/00015] train_loss: 0.022145 kl_loss: 0.187229 normal_loss: 0.016528\n",
      "[436/00115] train_loss: 0.022181 kl_loss: 0.187450 normal_loss: 0.016557\n",
      "[437/00071] train_loss: 0.022083 kl_loss: 0.186664 normal_loss: 0.016483\n",
      "[438/00027] train_loss: 0.022055 kl_loss: 0.186443 normal_loss: 0.016462\n",
      "[438/00127] train_loss: 0.022130 kl_loss: 0.187047 normal_loss: 0.016518\n",
      "[439/00083] train_loss: 0.022026 kl_loss: 0.186225 normal_loss: 0.016439\n",
      "[440/00039] train_loss: 0.022049 kl_loss: 0.186846 normal_loss: 0.016444\n",
      "[440/00139] train_loss: 0.022130 kl_loss: 0.186890 normal_loss: 0.016524\n",
      "[441/00095] train_loss: 0.022077 kl_loss: 0.186826 normal_loss: 0.016472\n",
      "[442/00051] train_loss: 0.021973 kl_loss: 0.186448 normal_loss: 0.016379\n",
      "[443/00007] train_loss: 0.022140 kl_loss: 0.186537 normal_loss: 0.016544\n",
      "[443/00107] train_loss: 0.022051 kl_loss: 0.186653 normal_loss: 0.016451\n",
      "[444/00063] train_loss: 0.021973 kl_loss: 0.185874 normal_loss: 0.016397\n",
      "[445/00019] train_loss: 0.022032 kl_loss: 0.186739 normal_loss: 0.016430\n",
      "[445/00119] train_loss: 0.022079 kl_loss: 0.186063 normal_loss: 0.016497\n",
      "[446/00075] train_loss: 0.022013 kl_loss: 0.185932 normal_loss: 0.016435\n",
      "[447/00031] train_loss: 0.022108 kl_loss: 0.186881 normal_loss: 0.016502\n",
      "[447/00131] train_loss: 0.021976 kl_loss: 0.185799 normal_loss: 0.016402\n",
      "[448/00087] train_loss: 0.022026 kl_loss: 0.186048 normal_loss: 0.016445\n",
      "[449/00043] train_loss: 0.021982 kl_loss: 0.185845 normal_loss: 0.016407\n",
      "[449/00143] train_loss: 0.021996 kl_loss: 0.185980 normal_loss: 0.016416\n",
      "[450/00099] train_loss: 0.022074 kl_loss: 0.186424 normal_loss: 0.016481\n",
      "[451/00055] train_loss: 0.021908 kl_loss: 0.184933 normal_loss: 0.016360\n",
      "[452/00011] train_loss: 0.021993 kl_loss: 0.186364 normal_loss: 0.016403\n",
      "[452/00111] train_loss: 0.021934 kl_loss: 0.185584 normal_loss: 0.016366\n",
      "[453/00067] train_loss: 0.021984 kl_loss: 0.185617 normal_loss: 0.016415\n",
      "[454/00023] train_loss: 0.022026 kl_loss: 0.186226 normal_loss: 0.016439\n",
      "[454/00123] train_loss: 0.021856 kl_loss: 0.184810 normal_loss: 0.016311\n",
      "[455/00079] train_loss: 0.021894 kl_loss: 0.185068 normal_loss: 0.016342\n",
      "[456/00035] train_loss: 0.022045 kl_loss: 0.186409 normal_loss: 0.016453\n",
      "[456/00135] train_loss: 0.021964 kl_loss: 0.185454 normal_loss: 0.016400\n",
      "[457/00091] train_loss: 0.021972 kl_loss: 0.185193 normal_loss: 0.016416\n",
      "[458/00047] train_loss: 0.021982 kl_loss: 0.185788 normal_loss: 0.016408\n",
      "[459/00003] train_loss: 0.021959 kl_loss: 0.185342 normal_loss: 0.016399\n",
      "[459/00103] train_loss: 0.021894 kl_loss: 0.184836 normal_loss: 0.016349\n",
      "[460/00059] train_loss: 0.021993 kl_loss: 0.185171 normal_loss: 0.016438\n",
      "[461/00015] train_loss: 0.021861 kl_loss: 0.185393 normal_loss: 0.016299\n",
      "[461/00115] train_loss: 0.021954 kl_loss: 0.185715 normal_loss: 0.016383\n",
      "[462/00071] train_loss: 0.021833 kl_loss: 0.184116 normal_loss: 0.016310\n",
      "[463/00027] train_loss: 0.021944 kl_loss: 0.185551 normal_loss: 0.016377\n",
      "[463/00127] train_loss: 0.021849 kl_loss: 0.184731 normal_loss: 0.016307\n",
      "[464/00083] train_loss: 0.021891 kl_loss: 0.184687 normal_loss: 0.016351\n",
      "[465/00039] train_loss: 0.021941 kl_loss: 0.185631 normal_loss: 0.016372\n",
      "[465/00139] train_loss: 0.021906 kl_loss: 0.184625 normal_loss: 0.016368\n",
      "[466/00095] train_loss: 0.021953 kl_loss: 0.185550 normal_loss: 0.016387\n",
      "[467/00051] train_loss: 0.021810 kl_loss: 0.184397 normal_loss: 0.016278\n",
      "[468/00007] train_loss: 0.021814 kl_loss: 0.184562 normal_loss: 0.016277\n",
      "[468/00107] train_loss: 0.021920 kl_loss: 0.185123 normal_loss: 0.016366\n",
      "[469/00063] train_loss: 0.021827 kl_loss: 0.184790 normal_loss: 0.016283\n",
      "[470/00019] train_loss: 0.021789 kl_loss: 0.184026 normal_loss: 0.016268\n",
      "[470/00119] train_loss: 0.021824 kl_loss: 0.184361 normal_loss: 0.016293\n",
      "[471/00075] train_loss: 0.021855 kl_loss: 0.184993 normal_loss: 0.016305\n",
      "[472/00031] train_loss: 0.021765 kl_loss: 0.184106 normal_loss: 0.016242\n",
      "[472/00131] train_loss: 0.021845 kl_loss: 0.184786 normal_loss: 0.016301\n",
      "[473/00087] train_loss: 0.021777 kl_loss: 0.184066 normal_loss: 0.016255\n",
      "[474/00043] train_loss: 0.021815 kl_loss: 0.184141 normal_loss: 0.016291\n",
      "[474/00143] train_loss: 0.021831 kl_loss: 0.184477 normal_loss: 0.016297\n",
      "[475/00099] train_loss: 0.021783 kl_loss: 0.184439 normal_loss: 0.016250\n",
      "[476/00055] train_loss: 0.021674 kl_loss: 0.183516 normal_loss: 0.016168\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msofa_chair_vad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOFA & CHAIR VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_chair_vad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa_chair',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3173\n",
      "Training params: 3\n",
      "[001/00049] train_loss: 0.158797 kl_loss: 0.485190 normal_loss: 0.144241\n",
      "[003/00049] train_loss: 0.103193 kl_loss: 0.413880 normal_loss: 0.090777\n",
      "[005/00049] train_loss: 0.092658 kl_loss: 0.348836 normal_loss: 0.082193\n",
      "[007/00049] train_loss: 0.089486 kl_loss: 0.300002 normal_loss: 0.080486\n",
      "[009/00049] train_loss: 0.084510 kl_loss: 0.266648 normal_loss: 0.076511\n",
      "[011/00049] train_loss: 0.080834 kl_loss: 0.244796 normal_loss: 0.073490\n",
      "[013/00049] train_loss: 0.075477 kl_loss: 0.232309 normal_loss: 0.068508\n",
      "[015/00049] train_loss: 0.072623 kl_loss: 0.224983 normal_loss: 0.065873\n",
      "[017/00049] train_loss: 0.070084 kl_loss: 0.216381 normal_loss: 0.063592\n",
      "[019/00049] train_loss: 0.068342 kl_loss: 0.208961 normal_loss: 0.062073\n",
      "[021/00049] train_loss: 0.065641 kl_loss: 0.202974 normal_loss: 0.059552\n",
      "[023/00049] train_loss: 0.062942 kl_loss: 0.197744 normal_loss: 0.057010\n",
      "[025/00049] train_loss: 0.061899 kl_loss: 0.192079 normal_loss: 0.056137\n",
      "[027/00049] train_loss: 0.059484 kl_loss: 0.187513 normal_loss: 0.053859\n",
      "[029/00049] train_loss: 0.058626 kl_loss: 0.183452 normal_loss: 0.053122\n",
      "[031/00049] train_loss: 0.058840 kl_loss: 0.180400 normal_loss: 0.053428\n",
      "[033/00049] train_loss: 0.057099 kl_loss: 0.178188 normal_loss: 0.051754\n",
      "[035/00049] train_loss: 0.055559 kl_loss: 0.176012 normal_loss: 0.050278\n",
      "[037/00049] train_loss: 0.053583 kl_loss: 0.174599 normal_loss: 0.048345\n",
      "[039/00049] train_loss: 0.054169 kl_loss: 0.173451 normal_loss: 0.048966\n",
      "[041/00049] train_loss: 0.052052 kl_loss: 0.172690 normal_loss: 0.046871\n",
      "[043/00049] train_loss: 0.052532 kl_loss: 0.171810 normal_loss: 0.047378\n",
      "[045/00049] train_loss: 0.050681 kl_loss: 0.170677 normal_loss: 0.045561\n",
      "[047/00049] train_loss: 0.049992 kl_loss: 0.170478 normal_loss: 0.044878\n",
      "[049/00049] train_loss: 0.049948 kl_loss: 0.170287 normal_loss: 0.044839\n",
      "[051/00049] train_loss: 0.048990 kl_loss: 0.170063 normal_loss: 0.043888\n",
      "[053/00049] train_loss: 0.048732 kl_loss: 0.169346 normal_loss: 0.043652\n",
      "[055/00049] train_loss: 0.048494 kl_loss: 0.169516 normal_loss: 0.043409\n",
      "[057/00049] train_loss: 0.047484 kl_loss: 0.169542 normal_loss: 0.042398\n",
      "[059/00049] train_loss: 0.045750 kl_loss: 0.169900 normal_loss: 0.040653\n",
      "[061/00049] train_loss: 0.046257 kl_loss: 0.170086 normal_loss: 0.041154\n",
      "[063/00049] train_loss: 0.045244 kl_loss: 0.170302 normal_loss: 0.040135\n",
      "[065/00049] train_loss: 0.045412 kl_loss: 0.170895 normal_loss: 0.040286\n",
      "[067/00049] train_loss: 0.044639 kl_loss: 0.171221 normal_loss: 0.039503\n",
      "[069/00049] train_loss: 0.044389 kl_loss: 0.171689 normal_loss: 0.039239\n",
      "[071/00049] train_loss: 0.044083 kl_loss: 0.171934 normal_loss: 0.038925\n",
      "[073/00049] train_loss: 0.043876 kl_loss: 0.173409 normal_loss: 0.038674\n",
      "[075/00049] train_loss: 0.042309 kl_loss: 0.174524 normal_loss: 0.037073\n",
      "[077/00049] train_loss: 0.042777 kl_loss: 0.175583 normal_loss: 0.037509\n",
      "[079/00049] train_loss: 0.042217 kl_loss: 0.176266 normal_loss: 0.036929\n",
      "[081/00049] train_loss: 0.041699 kl_loss: 0.177738 normal_loss: 0.036367\n",
      "[083/00049] train_loss: 0.041630 kl_loss: 0.179094 normal_loss: 0.036257\n",
      "[085/00049] train_loss: 0.040626 kl_loss: 0.180166 normal_loss: 0.035221\n",
      "[087/00049] train_loss: 0.040599 kl_loss: 0.181578 normal_loss: 0.035151\n",
      "[089/00049] train_loss: 0.040178 kl_loss: 0.182782 normal_loss: 0.034694\n",
      "[091/00049] train_loss: 0.039949 kl_loss: 0.184032 normal_loss: 0.034428\n",
      "[093/00049] train_loss: 0.039943 kl_loss: 0.184855 normal_loss: 0.034397\n",
      "[095/00049] train_loss: 0.039653 kl_loss: 0.185723 normal_loss: 0.034081\n",
      "[097/00049] train_loss: 0.039297 kl_loss: 0.186599 normal_loss: 0.033699\n",
      "[099/00049] train_loss: 0.039506 kl_loss: 0.188096 normal_loss: 0.033864\n",
      "[101/00049] train_loss: 0.035651 kl_loss: 0.188075 normal_loss: 0.030009\n",
      "[103/00049] train_loss: 0.035288 kl_loss: 0.186099 normal_loss: 0.029705\n",
      "[105/00049] train_loss: 0.034492 kl_loss: 0.184361 normal_loss: 0.028961\n",
      "[107/00049] train_loss: 0.034181 kl_loss: 0.182726 normal_loss: 0.028699\n",
      "[109/00049] train_loss: 0.034218 kl_loss: 0.181609 normal_loss: 0.028770\n",
      "[111/00049] train_loss: 0.033851 kl_loss: 0.180504 normal_loss: 0.028436\n",
      "[113/00049] train_loss: 0.033729 kl_loss: 0.179692 normal_loss: 0.028339\n",
      "[115/00049] train_loss: 0.033467 kl_loss: 0.178709 normal_loss: 0.028106\n",
      "[117/00049] train_loss: 0.033524 kl_loss: 0.177987 normal_loss: 0.028185\n",
      "[119/00049] train_loss: 0.033528 kl_loss: 0.177737 normal_loss: 0.028196\n",
      "[121/00049] train_loss: 0.033043 kl_loss: 0.177363 normal_loss: 0.027722\n",
      "[123/00049] train_loss: 0.032972 kl_loss: 0.177073 normal_loss: 0.027660\n",
      "[125/00049] train_loss: 0.032669 kl_loss: 0.176627 normal_loss: 0.027370\n",
      "[127/00049] train_loss: 0.032384 kl_loss: 0.176546 normal_loss: 0.027088\n",
      "[129/00049] train_loss: 0.032324 kl_loss: 0.176525 normal_loss: 0.027028\n",
      "[131/00049] train_loss: 0.032250 kl_loss: 0.176592 normal_loss: 0.026952\n",
      "[133/00049] train_loss: 0.032227 kl_loss: 0.176547 normal_loss: 0.026931\n",
      "[135/00049] train_loss: 0.031908 kl_loss: 0.176931 normal_loss: 0.026600\n",
      "[137/00049] train_loss: 0.032233 kl_loss: 0.177011 normal_loss: 0.026923\n",
      "[139/00049] train_loss: 0.032038 kl_loss: 0.177213 normal_loss: 0.026722\n",
      "[141/00049] train_loss: 0.031605 kl_loss: 0.177699 normal_loss: 0.026274\n",
      "[143/00049] train_loss: 0.031743 kl_loss: 0.177965 normal_loss: 0.026404\n",
      "[145/00049] train_loss: 0.031229 kl_loss: 0.178326 normal_loss: 0.025879\n",
      "[147/00049] train_loss: 0.031140 kl_loss: 0.178554 normal_loss: 0.025784\n",
      "[149/00049] train_loss: 0.030914 kl_loss: 0.179021 normal_loss: 0.025544\n",
      "[151/00049] train_loss: 0.030670 kl_loss: 0.179293 normal_loss: 0.025291\n",
      "[153/00049] train_loss: 0.030855 kl_loss: 0.179572 normal_loss: 0.025468\n",
      "[155/00049] train_loss: 0.030587 kl_loss: 0.179908 normal_loss: 0.025189\n",
      "[157/00049] train_loss: 0.029987 kl_loss: 0.180173 normal_loss: 0.024582\n",
      "[159/00049] train_loss: 0.030099 kl_loss: 0.180335 normal_loss: 0.024689\n",
      "[161/00049] train_loss: 0.030461 kl_loss: 0.180657 normal_loss: 0.025041\n",
      "[163/00049] train_loss: 0.030057 kl_loss: 0.181056 normal_loss: 0.024626\n",
      "[165/00049] train_loss: 0.029992 kl_loss: 0.181596 normal_loss: 0.024544\n",
      "[167/00049] train_loss: 0.029740 kl_loss: 0.181741 normal_loss: 0.024287\n",
      "[169/00049] train_loss: 0.029392 kl_loss: 0.181966 normal_loss: 0.023933\n",
      "[171/00049] train_loss: 0.029154 kl_loss: 0.182248 normal_loss: 0.023687\n",
      "[173/00049] train_loss: 0.029491 kl_loss: 0.182706 normal_loss: 0.024010\n",
      "[175/00049] train_loss: 0.029005 kl_loss: 0.182898 normal_loss: 0.023518\n",
      "[177/00049] train_loss: 0.028615 kl_loss: 0.183165 normal_loss: 0.023120\n",
      "[179/00049] train_loss: 0.028678 kl_loss: 0.183436 normal_loss: 0.023175\n",
      "[181/00049] train_loss: 0.028636 kl_loss: 0.183565 normal_loss: 0.023129\n",
      "[183/00049] train_loss: 0.028922 kl_loss: 0.183896 normal_loss: 0.023405\n",
      "[185/00049] train_loss: 0.028800 kl_loss: 0.184217 normal_loss: 0.023274\n",
      "[187/00049] train_loss: 0.028287 kl_loss: 0.184347 normal_loss: 0.022757\n",
      "[189/00049] train_loss: 0.028455 kl_loss: 0.184610 normal_loss: 0.022917\n",
      "[191/00049] train_loss: 0.028019 kl_loss: 0.184813 normal_loss: 0.022474\n",
      "[193/00049] train_loss: 0.027900 kl_loss: 0.184912 normal_loss: 0.022352\n",
      "[195/00049] train_loss: 0.027723 kl_loss: 0.185343 normal_loss: 0.022162\n",
      "[197/00049] train_loss: 0.027717 kl_loss: 0.185384 normal_loss: 0.022156\n",
      "[199/00049] train_loss: 0.027726 kl_loss: 0.185875 normal_loss: 0.022149\n",
      "[201/00049] train_loss: 0.026158 kl_loss: 0.185587 normal_loss: 0.020591\n",
      "[203/00049] train_loss: 0.025869 kl_loss: 0.185479 normal_loss: 0.020305\n",
      "[205/00049] train_loss: 0.025548 kl_loss: 0.184956 normal_loss: 0.019999\n",
      "[207/00049] train_loss: 0.025465 kl_loss: 0.184499 normal_loss: 0.019930\n",
      "[209/00049] train_loss: 0.025254 kl_loss: 0.184125 normal_loss: 0.019730\n",
      "[211/00049] train_loss: 0.025229 kl_loss: 0.183703 normal_loss: 0.019718\n",
      "[213/00049] train_loss: 0.025097 kl_loss: 0.183236 normal_loss: 0.019600\n",
      "[215/00049] train_loss: 0.024943 kl_loss: 0.182732 normal_loss: 0.019461\n",
      "[217/00049] train_loss: 0.025139 kl_loss: 0.182408 normal_loss: 0.019667\n",
      "[219/00049] train_loss: 0.025077 kl_loss: 0.182216 normal_loss: 0.019610\n",
      "[221/00049] train_loss: 0.025002 kl_loss: 0.181818 normal_loss: 0.019547\n",
      "[223/00049] train_loss: 0.024814 kl_loss: 0.181485 normal_loss: 0.019370\n",
      "[225/00049] train_loss: 0.024834 kl_loss: 0.181221 normal_loss: 0.019397\n",
      "[227/00049] train_loss: 0.024689 kl_loss: 0.180899 normal_loss: 0.019262\n",
      "[229/00049] train_loss: 0.024377 kl_loss: 0.180745 normal_loss: 0.018954\n",
      "[231/00049] train_loss: 0.024501 kl_loss: 0.180540 normal_loss: 0.019085\n",
      "[233/00049] train_loss: 0.024324 kl_loss: 0.180141 normal_loss: 0.018920\n",
      "[235/00049] train_loss: 0.024363 kl_loss: 0.179955 normal_loss: 0.018964\n",
      "[237/00049] train_loss: 0.024252 kl_loss: 0.179787 normal_loss: 0.018858\n",
      "[239/00049] train_loss: 0.024158 kl_loss: 0.179605 normal_loss: 0.018770\n",
      "[241/00049] train_loss: 0.024240 kl_loss: 0.179561 normal_loss: 0.018853\n",
      "[243/00049] train_loss: 0.024350 kl_loss: 0.179312 normal_loss: 0.018971\n",
      "[245/00049] train_loss: 0.024075 kl_loss: 0.179325 normal_loss: 0.018695\n",
      "[247/00049] train_loss: 0.024145 kl_loss: 0.179106 normal_loss: 0.018772\n",
      "[249/00049] train_loss: 0.023969 kl_loss: 0.178880 normal_loss: 0.018602\n",
      "[251/00049] train_loss: 0.024001 kl_loss: 0.178768 normal_loss: 0.018638\n",
      "[253/00049] train_loss: 0.023967 kl_loss: 0.178838 normal_loss: 0.018602\n",
      "[255/00049] train_loss: 0.023779 kl_loss: 0.178616 normal_loss: 0.018421\n",
      "[257/00049] train_loss: 0.023683 kl_loss: 0.178582 normal_loss: 0.018326\n",
      "[259/00049] train_loss: 0.023678 kl_loss: 0.178373 normal_loss: 0.018327\n",
      "[261/00049] train_loss: 0.023760 kl_loss: 0.178394 normal_loss: 0.018408\n",
      "[263/00049] train_loss: 0.023516 kl_loss: 0.178227 normal_loss: 0.018169\n",
      "[265/00049] train_loss: 0.023459 kl_loss: 0.178157 normal_loss: 0.018114\n",
      "[267/00049] train_loss: 0.023535 kl_loss: 0.178088 normal_loss: 0.018192\n",
      "[269/00049] train_loss: 0.023216 kl_loss: 0.178185 normal_loss: 0.017870\n",
      "[271/00049] train_loss: 0.023478 kl_loss: 0.178087 normal_loss: 0.018135\n",
      "[273/00049] train_loss: 0.023274 kl_loss: 0.178033 normal_loss: 0.017933\n",
      "[275/00049] train_loss: 0.023182 kl_loss: 0.177997 normal_loss: 0.017842\n",
      "[277/00049] train_loss: 0.023134 kl_loss: 0.177804 normal_loss: 0.017800\n",
      "[279/00049] train_loss: 0.023227 kl_loss: 0.177872 normal_loss: 0.017891\n",
      "[281/00049] train_loss: 0.023134 kl_loss: 0.177822 normal_loss: 0.017799\n",
      "[283/00049] train_loss: 0.022995 kl_loss: 0.177784 normal_loss: 0.017662\n",
      "[285/00049] train_loss: 0.022884 kl_loss: 0.177783 normal_loss: 0.017551\n",
      "[287/00049] train_loss: 0.023202 kl_loss: 0.177633 normal_loss: 0.017873\n",
      "[289/00049] train_loss: 0.023050 kl_loss: 0.177741 normal_loss: 0.017718\n",
      "[291/00049] train_loss: 0.022876 kl_loss: 0.177732 normal_loss: 0.017544\n",
      "[293/00049] train_loss: 0.022834 kl_loss: 0.177653 normal_loss: 0.017504\n",
      "[295/00049] train_loss: 0.022793 kl_loss: 0.177580 normal_loss: 0.017466\n",
      "[297/00049] train_loss: 0.022545 kl_loss: 0.177588 normal_loss: 0.017217\n",
      "[299/00049] train_loss: 0.022638 kl_loss: 0.177423 normal_loss: 0.017316\n",
      "[301/00049] train_loss: 0.022014 kl_loss: 0.177131 normal_loss: 0.016700\n",
      "[303/00049] train_loss: 0.021783 kl_loss: 0.177116 normal_loss: 0.016470\n",
      "[305/00049] train_loss: 0.021748 kl_loss: 0.176914 normal_loss: 0.016441\n",
      "[307/00049] train_loss: 0.021618 kl_loss: 0.176799 normal_loss: 0.016314\n",
      "[309/00049] train_loss: 0.021609 kl_loss: 0.176716 normal_loss: 0.016308\n",
      "[311/00049] train_loss: 0.021615 kl_loss: 0.176545 normal_loss: 0.016319\n",
      "[313/00049] train_loss: 0.021493 kl_loss: 0.176303 normal_loss: 0.016204\n",
      "[315/00049] train_loss: 0.021467 kl_loss: 0.176164 normal_loss: 0.016182\n",
      "[317/00049] train_loss: 0.021416 kl_loss: 0.175951 normal_loss: 0.016137\n",
      "[319/00049] train_loss: 0.021430 kl_loss: 0.175806 normal_loss: 0.016156\n",
      "[321/00049] train_loss: 0.021467 kl_loss: 0.175710 normal_loss: 0.016195\n",
      "[323/00049] train_loss: 0.021447 kl_loss: 0.175533 normal_loss: 0.016181\n",
      "[325/00049] train_loss: 0.021262 kl_loss: 0.175355 normal_loss: 0.016001\n",
      "[327/00049] train_loss: 0.021281 kl_loss: 0.175218 normal_loss: 0.016025\n",
      "[329/00049] train_loss: 0.021234 kl_loss: 0.174976 normal_loss: 0.015985\n",
      "[331/00049] train_loss: 0.021301 kl_loss: 0.174921 normal_loss: 0.016053\n",
      "[333/00049] train_loss: 0.021245 kl_loss: 0.174771 normal_loss: 0.016002\n",
      "[335/00049] train_loss: 0.021169 kl_loss: 0.174648 normal_loss: 0.015930\n",
      "[337/00049] train_loss: 0.021251 kl_loss: 0.174502 normal_loss: 0.016016\n",
      "[339/00049] train_loss: 0.021105 kl_loss: 0.174483 normal_loss: 0.015871\n",
      "[341/00049] train_loss: 0.021088 kl_loss: 0.174287 normal_loss: 0.015859\n",
      "[343/00049] train_loss: 0.021080 kl_loss: 0.174137 normal_loss: 0.015856\n",
      "[345/00049] train_loss: 0.021112 kl_loss: 0.174035 normal_loss: 0.015891\n",
      "[347/00049] train_loss: 0.021022 kl_loss: 0.173902 normal_loss: 0.015805\n",
      "[349/00049] train_loss: 0.021061 kl_loss: 0.173786 normal_loss: 0.015847\n",
      "[351/00049] train_loss: 0.020983 kl_loss: 0.173591 normal_loss: 0.015775\n",
      "[353/00049] train_loss: 0.020949 kl_loss: 0.173379 normal_loss: 0.015748\n",
      "[355/00049] train_loss: 0.020869 kl_loss: 0.173346 normal_loss: 0.015668\n",
      "[357/00049] train_loss: 0.020854 kl_loss: 0.173260 normal_loss: 0.015656\n",
      "[359/00049] train_loss: 0.020884 kl_loss: 0.173135 normal_loss: 0.015690\n",
      "[361/00049] train_loss: 0.020913 kl_loss: 0.173051 normal_loss: 0.015722\n",
      "[363/00049] train_loss: 0.020771 kl_loss: 0.172923 normal_loss: 0.015583\n",
      "[365/00049] train_loss: 0.020787 kl_loss: 0.172795 normal_loss: 0.015603\n",
      "[367/00049] train_loss: 0.020746 kl_loss: 0.172787 normal_loss: 0.015562\n",
      "[369/00049] train_loss: 0.020866 kl_loss: 0.172658 normal_loss: 0.015686\n",
      "[371/00049] train_loss: 0.020671 kl_loss: 0.172478 normal_loss: 0.015497\n",
      "[373/00049] train_loss: 0.020683 kl_loss: 0.172489 normal_loss: 0.015508\n",
      "[375/00049] train_loss: 0.020624 kl_loss: 0.172283 normal_loss: 0.015455\n",
      "[377/00049] train_loss: 0.020498 kl_loss: 0.172187 normal_loss: 0.015333\n",
      "[379/00049] train_loss: 0.020644 kl_loss: 0.172089 normal_loss: 0.015481\n",
      "[381/00049] train_loss: 0.020661 kl_loss: 0.171948 normal_loss: 0.015503\n",
      "[383/00049] train_loss: 0.020562 kl_loss: 0.171919 normal_loss: 0.015404\n",
      "[385/00049] train_loss: 0.020541 kl_loss: 0.171867 normal_loss: 0.015385\n",
      "[387/00049] train_loss: 0.020526 kl_loss: 0.171841 normal_loss: 0.015371\n",
      "[389/00049] train_loss: 0.020425 kl_loss: 0.171752 normal_loss: 0.015273\n",
      "[391/00049] train_loss: 0.020395 kl_loss: 0.171685 normal_loss: 0.015244\n",
      "[393/00049] train_loss: 0.020512 kl_loss: 0.171605 normal_loss: 0.015364\n",
      "[395/00049] train_loss: 0.020410 kl_loss: 0.171481 normal_loss: 0.015266\n",
      "[397/00049] train_loss: 0.020266 kl_loss: 0.171439 normal_loss: 0.015123\n",
      "[399/00049] train_loss: 0.020305 kl_loss: 0.171254 normal_loss: 0.015167\n",
      "[401/00049] train_loss: 0.020047 kl_loss: 0.171241 normal_loss: 0.014910\n",
      "[403/00049] train_loss: 0.020031 kl_loss: 0.171195 normal_loss: 0.014896\n",
      "[405/00049] train_loss: 0.019981 kl_loss: 0.171212 normal_loss: 0.014844\n",
      "[407/00049] train_loss: 0.019939 kl_loss: 0.171080 normal_loss: 0.014807\n",
      "[409/00049] train_loss: 0.019964 kl_loss: 0.170919 normal_loss: 0.014837\n",
      "[411/00049] train_loss: 0.019873 kl_loss: 0.170838 normal_loss: 0.014748\n",
      "[413/00049] train_loss: 0.019919 kl_loss: 0.170830 normal_loss: 0.014794\n",
      "[415/00049] train_loss: 0.019831 kl_loss: 0.170729 normal_loss: 0.014709\n",
      "[417/00049] train_loss: 0.019866 kl_loss: 0.170656 normal_loss: 0.014746\n",
      "[419/00049] train_loss: 0.019943 kl_loss: 0.170548 normal_loss: 0.014826\n",
      "[421/00049] train_loss: 0.019831 kl_loss: 0.170525 normal_loss: 0.014715\n",
      "[423/00049] train_loss: 0.019792 kl_loss: 0.170445 normal_loss: 0.014679\n",
      "[425/00049] train_loss: 0.019797 kl_loss: 0.170346 normal_loss: 0.014687\n",
      "[427/00049] train_loss: 0.019769 kl_loss: 0.170243 normal_loss: 0.014662\n",
      "[429/00049] train_loss: 0.019775 kl_loss: 0.170280 normal_loss: 0.014666\n",
      "[431/00049] train_loss: 0.019739 kl_loss: 0.170085 normal_loss: 0.014636\n",
      "[433/00049] train_loss: 0.019728 kl_loss: 0.170046 normal_loss: 0.014627\n",
      "[435/00049] train_loss: 0.019741 kl_loss: 0.170012 normal_loss: 0.014641\n",
      "[437/00049] train_loss: 0.019713 kl_loss: 0.169901 normal_loss: 0.014616\n",
      "[439/00049] train_loss: 0.019623 kl_loss: 0.169828 normal_loss: 0.014528\n",
      "[441/00049] train_loss: 0.019677 kl_loss: 0.169907 normal_loss: 0.014580\n",
      "[443/00049] train_loss: 0.019659 kl_loss: 0.169708 normal_loss: 0.014568\n",
      "[445/00049] train_loss: 0.019646 kl_loss: 0.169616 normal_loss: 0.014558\n",
      "[447/00049] train_loss: 0.019608 kl_loss: 0.169558 normal_loss: 0.014522\n",
      "[449/00049] train_loss: 0.019661 kl_loss: 0.169399 normal_loss: 0.014579\n",
      "[451/00049] train_loss: 0.019577 kl_loss: 0.169463 normal_loss: 0.014493\n",
      "[453/00049] train_loss: 0.019569 kl_loss: 0.169361 normal_loss: 0.014489\n",
      "[455/00049] train_loss: 0.019589 kl_loss: 0.169272 normal_loss: 0.014511\n",
      "[457/00049] train_loss: 0.019573 kl_loss: 0.169153 normal_loss: 0.014498\n",
      "[459/00049] train_loss: 0.019558 kl_loss: 0.169156 normal_loss: 0.014483\n",
      "[461/00049] train_loss: 0.019515 kl_loss: 0.169094 normal_loss: 0.014442\n",
      "[463/00049] train_loss: 0.019538 kl_loss: 0.168979 normal_loss: 0.014469\n",
      "[465/00049] train_loss: 0.019515 kl_loss: 0.168962 normal_loss: 0.014446\n",
      "[467/00049] train_loss: 0.019466 kl_loss: 0.168890 normal_loss: 0.014399\n",
      "[469/00049] train_loss: 0.019453 kl_loss: 0.168773 normal_loss: 0.014390\n",
      "[471/00049] train_loss: 0.019428 kl_loss: 0.168749 normal_loss: 0.014365\n",
      "[473/00049] train_loss: 0.019454 kl_loss: 0.168576 normal_loss: 0.014397\n",
      "[475/00049] train_loss: 0.019458 kl_loss: 0.168565 normal_loss: 0.014401\n",
      "[477/00049] train_loss: 0.019497 kl_loss: 0.168535 normal_loss: 0.014441\n",
      "[479/00049] train_loss: 0.019410 kl_loss: 0.168581 normal_loss: 0.014353\n",
      "[481/00049] train_loss: 0.019414 kl_loss: 0.168393 normal_loss: 0.014362\n",
      "[483/00049] train_loss: 0.019320 kl_loss: 0.168440 normal_loss: 0.014267\n",
      "[485/00049] train_loss: 0.019430 kl_loss: 0.168287 normal_loss: 0.014381\n",
      "[487/00049] train_loss: 0.019338 kl_loss: 0.168266 normal_loss: 0.014290\n",
      "[489/00049] train_loss: 0.019372 kl_loss: 0.168235 normal_loss: 0.014325\n",
      "[491/00049] train_loss: 0.019382 kl_loss: 0.168193 normal_loss: 0.014336\n",
      "[493/00049] train_loss: 0.019283 kl_loss: 0.168007 normal_loss: 0.014243\n",
      "[495/00049] train_loss: 0.019289 kl_loss: 0.167999 normal_loss: 0.014249\n",
      "[497/00049] train_loss: 0.019290 kl_loss: 0.167932 normal_loss: 0.014252\n",
      "[499/00049] train_loss: 0.019259 kl_loss: 0.167835 normal_loss: 0.014224\n"
     ]
    }
   ],
   "source": [
    "# SOFA VAD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_vad_0_03kl',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3173\n",
      "Training params: 2\n",
      "[001/00049] train_loss: 0.147046 kl_loss: 0.000000 normal_loss: 0.147046\n",
      "[003/00049] train_loss: 0.086997 kl_loss: 0.000000 normal_loss: 0.086997\n",
      "[005/00049] train_loss: 0.078271 kl_loss: 0.000000 normal_loss: 0.078271\n",
      "[007/00049] train_loss: 0.072125 kl_loss: 0.000000 normal_loss: 0.072125\n",
      "[009/00049] train_loss: 0.063552 kl_loss: 0.000000 normal_loss: 0.063552\n",
      "[011/00049] train_loss: 0.060275 kl_loss: 0.000000 normal_loss: 0.060275\n",
      "[013/00049] train_loss: 0.056823 kl_loss: 0.000000 normal_loss: 0.056823\n",
      "[015/00049] train_loss: 0.056097 kl_loss: 0.000000 normal_loss: 0.056097\n",
      "[017/00049] train_loss: 0.051553 kl_loss: 0.000000 normal_loss: 0.051553\n",
      "[019/00049] train_loss: 0.050349 kl_loss: 0.000000 normal_loss: 0.050349\n",
      "[021/00049] train_loss: 0.047069 kl_loss: 0.000000 normal_loss: 0.047069\n",
      "[023/00049] train_loss: 0.044120 kl_loss: 0.000000 normal_loss: 0.044120\n",
      "[025/00049] train_loss: 0.043236 kl_loss: 0.000000 normal_loss: 0.043236\n",
      "[027/00049] train_loss: 0.041510 kl_loss: 0.000000 normal_loss: 0.041510\n",
      "[029/00049] train_loss: 0.040672 kl_loss: 0.000000 normal_loss: 0.040672\n",
      "[031/00049] train_loss: 0.039042 kl_loss: 0.000000 normal_loss: 0.039042\n",
      "[033/00049] train_loss: 0.037533 kl_loss: 0.000000 normal_loss: 0.037533\n",
      "[035/00049] train_loss: 0.035808 kl_loss: 0.000000 normal_loss: 0.035808\n",
      "[037/00049] train_loss: 0.034947 kl_loss: 0.000000 normal_loss: 0.034947\n",
      "[039/00049] train_loss: 0.033185 kl_loss: 0.000000 normal_loss: 0.033185\n",
      "[041/00049] train_loss: 0.032495 kl_loss: 0.000000 normal_loss: 0.032495\n",
      "[043/00049] train_loss: 0.031494 kl_loss: 0.000000 normal_loss: 0.031494\n",
      "[045/00049] train_loss: 0.030708 kl_loss: 0.000000 normal_loss: 0.030708\n",
      "[047/00049] train_loss: 0.030595 kl_loss: 0.000000 normal_loss: 0.030595\n",
      "[049/00049] train_loss: 0.029956 kl_loss: 0.000000 normal_loss: 0.029956\n",
      "[051/00049] train_loss: 0.028813 kl_loss: 0.000000 normal_loss: 0.028813\n",
      "[053/00049] train_loss: 0.027714 kl_loss: 0.000000 normal_loss: 0.027714\n",
      "[055/00049] train_loss: 0.027332 kl_loss: 0.000000 normal_loss: 0.027332\n",
      "[057/00049] train_loss: 0.027308 kl_loss: 0.000000 normal_loss: 0.027308\n",
      "[059/00049] train_loss: 0.026427 kl_loss: 0.000000 normal_loss: 0.026427\n",
      "[061/00049] train_loss: 0.026049 kl_loss: 0.000000 normal_loss: 0.026049\n",
      "[063/00049] train_loss: 0.024755 kl_loss: 0.000000 normal_loss: 0.024755\n",
      "[065/00049] train_loss: 0.024271 kl_loss: 0.000000 normal_loss: 0.024271\n",
      "[067/00049] train_loss: 0.024266 kl_loss: 0.000000 normal_loss: 0.024266\n",
      "[069/00049] train_loss: 0.024478 kl_loss: 0.000000 normal_loss: 0.024478\n",
      "[071/00049] train_loss: 0.024100 kl_loss: 0.000000 normal_loss: 0.024100\n",
      "[073/00049] train_loss: 0.022835 kl_loss: 0.000000 normal_loss: 0.022835\n",
      "[075/00049] train_loss: 0.022686 kl_loss: 0.000000 normal_loss: 0.022686\n",
      "[077/00049] train_loss: 0.023004 kl_loss: 0.000000 normal_loss: 0.023004\n",
      "[079/00049] train_loss: 0.022281 kl_loss: 0.000000 normal_loss: 0.022281\n",
      "[081/00049] train_loss: 0.021605 kl_loss: 0.000000 normal_loss: 0.021605\n",
      "[083/00049] train_loss: 0.020298 kl_loss: 0.000000 normal_loss: 0.020298\n",
      "[085/00049] train_loss: 0.020655 kl_loss: 0.000000 normal_loss: 0.020655\n",
      "[087/00049] train_loss: 0.021509 kl_loss: 0.000000 normal_loss: 0.021509\n",
      "[089/00049] train_loss: 0.022649 kl_loss: 0.000000 normal_loss: 0.022649\n",
      "[091/00049] train_loss: 0.020297 kl_loss: 0.000000 normal_loss: 0.020297\n",
      "[093/00049] train_loss: 0.021210 kl_loss: 0.000000 normal_loss: 0.021210\n",
      "[095/00049] train_loss: 0.019058 kl_loss: 0.000000 normal_loss: 0.019058\n",
      "[097/00049] train_loss: 0.019488 kl_loss: 0.000000 normal_loss: 0.019488\n",
      "[099/00049] train_loss: 0.018620 kl_loss: 0.000000 normal_loss: 0.018620\n",
      "[101/00049] train_loss: 0.016498 kl_loss: 0.000000 normal_loss: 0.016498\n",
      "[103/00049] train_loss: 0.014815 kl_loss: 0.000000 normal_loss: 0.014815\n",
      "[105/00049] train_loss: 0.014853 kl_loss: 0.000000 normal_loss: 0.014853\n",
      "[107/00049] train_loss: 0.015392 kl_loss: 0.000000 normal_loss: 0.015392\n",
      "[109/00049] train_loss: 0.016997 kl_loss: 0.000000 normal_loss: 0.016997\n",
      "[111/00049] train_loss: 0.014858 kl_loss: 0.000000 normal_loss: 0.014858\n",
      "[113/00049] train_loss: 0.014544 kl_loss: 0.000000 normal_loss: 0.014544\n",
      "[115/00049] train_loss: 0.014657 kl_loss: 0.000000 normal_loss: 0.014657\n",
      "[117/00049] train_loss: 0.014345 kl_loss: 0.000000 normal_loss: 0.014345\n",
      "[119/00049] train_loss: 0.013949 kl_loss: 0.000000 normal_loss: 0.013949\n",
      "[121/00049] train_loss: 0.014472 kl_loss: 0.000000 normal_loss: 0.014472\n",
      "[123/00049] train_loss: 0.015266 kl_loss: 0.000000 normal_loss: 0.015266\n",
      "[125/00049] train_loss: 0.014614 kl_loss: 0.000000 normal_loss: 0.014614\n",
      "[127/00049] train_loss: 0.013951 kl_loss: 0.000000 normal_loss: 0.013951\n",
      "[129/00049] train_loss: 0.014564 kl_loss: 0.000000 normal_loss: 0.014564\n",
      "[131/00049] train_loss: 0.013915 kl_loss: 0.000000 normal_loss: 0.013915\n",
      "[133/00049] train_loss: 0.013754 kl_loss: 0.000000 normal_loss: 0.013754\n",
      "[135/00049] train_loss: 0.013780 kl_loss: 0.000000 normal_loss: 0.013780\n",
      "[137/00049] train_loss: 0.013444 kl_loss: 0.000000 normal_loss: 0.013444\n",
      "[139/00049] train_loss: 0.013381 kl_loss: 0.000000 normal_loss: 0.013381\n",
      "[141/00049] train_loss: 0.013236 kl_loss: 0.000000 normal_loss: 0.013236\n",
      "[143/00049] train_loss: 0.013419 kl_loss: 0.000000 normal_loss: 0.013419\n",
      "[145/00049] train_loss: 0.013631 kl_loss: 0.000000 normal_loss: 0.013631\n",
      "[147/00049] train_loss: 0.013256 kl_loss: 0.000000 normal_loss: 0.013256\n",
      "[149/00049] train_loss: 0.013501 kl_loss: 0.000000 normal_loss: 0.013501\n",
      "[151/00049] train_loss: 0.013316 kl_loss: 0.000000 normal_loss: 0.013316\n",
      "[153/00049] train_loss: 0.012754 kl_loss: 0.000000 normal_loss: 0.012754\n",
      "[155/00049] train_loss: 0.012450 kl_loss: 0.000000 normal_loss: 0.012450\n",
      "[157/00049] train_loss: 0.013149 kl_loss: 0.000000 normal_loss: 0.013149\n",
      "[159/00049] train_loss: 0.012789 kl_loss: 0.000000 normal_loss: 0.012789\n",
      "[161/00049] train_loss: 0.013052 kl_loss: 0.000000 normal_loss: 0.013052\n",
      "[163/00049] train_loss: 0.012452 kl_loss: 0.000000 normal_loss: 0.012452\n",
      "[165/00049] train_loss: 0.012844 kl_loss: 0.000000 normal_loss: 0.012844\n",
      "[167/00049] train_loss: 0.012598 kl_loss: 0.000000 normal_loss: 0.012598\n",
      "[169/00049] train_loss: 0.012433 kl_loss: 0.000000 normal_loss: 0.012433\n",
      "[171/00049] train_loss: 0.012264 kl_loss: 0.000000 normal_loss: 0.012264\n",
      "[173/00049] train_loss: 0.012649 kl_loss: 0.000000 normal_loss: 0.012649\n",
      "[175/00049] train_loss: 0.012375 kl_loss: 0.000000 normal_loss: 0.012375\n",
      "[177/00049] train_loss: 0.012224 kl_loss: 0.000000 normal_loss: 0.012224\n",
      "[179/00049] train_loss: 0.012347 kl_loss: 0.000000 normal_loss: 0.012347\n",
      "[181/00049] train_loss: 0.012235 kl_loss: 0.000000 normal_loss: 0.012235\n",
      "[183/00049] train_loss: 0.012057 kl_loss: 0.000000 normal_loss: 0.012057\n",
      "[185/00049] train_loss: 0.012528 kl_loss: 0.000000 normal_loss: 0.012528\n",
      "[187/00049] train_loss: 0.012125 kl_loss: 0.000000 normal_loss: 0.012125\n",
      "[189/00049] train_loss: 0.012086 kl_loss: 0.000000 normal_loss: 0.012086\n",
      "[191/00049] train_loss: 0.012190 kl_loss: 0.000000 normal_loss: 0.012190\n",
      "[193/00049] train_loss: 0.012084 kl_loss: 0.000000 normal_loss: 0.012084\n",
      "[195/00049] train_loss: 0.011601 kl_loss: 0.000000 normal_loss: 0.011601\n",
      "[197/00049] train_loss: 0.011606 kl_loss: 0.000000 normal_loss: 0.011606\n",
      "[199/00049] train_loss: 0.011835 kl_loss: 0.000000 normal_loss: 0.011835\n",
      "[201/00049] train_loss: 0.010245 kl_loss: 0.000000 normal_loss: 0.010245\n",
      "[203/00049] train_loss: 0.009693 kl_loss: 0.000000 normal_loss: 0.009693\n",
      "[205/00049] train_loss: 0.009638 kl_loss: 0.000000 normal_loss: 0.009638\n",
      "[207/00049] train_loss: 0.009711 kl_loss: 0.000000 normal_loss: 0.009711\n",
      "[209/00049] train_loss: 0.009666 kl_loss: 0.000000 normal_loss: 0.009666\n",
      "[211/00049] train_loss: 0.009929 kl_loss: 0.000000 normal_loss: 0.009929\n",
      "[213/00049] train_loss: 0.009723 kl_loss: 0.000000 normal_loss: 0.009723\n",
      "[215/00049] train_loss: 0.009604 kl_loss: 0.000000 normal_loss: 0.009604\n",
      "[217/00049] train_loss: 0.009741 kl_loss: 0.000000 normal_loss: 0.009741\n",
      "[219/00049] train_loss: 0.009573 kl_loss: 0.000000 normal_loss: 0.009573\n",
      "[221/00049] train_loss: 0.009610 kl_loss: 0.000000 normal_loss: 0.009610\n",
      "[223/00049] train_loss: 0.009645 kl_loss: 0.000000 normal_loss: 0.009645\n",
      "[225/00049] train_loss: 0.010178 kl_loss: 0.000000 normal_loss: 0.010178\n",
      "[227/00049] train_loss: 0.009635 kl_loss: 0.000000 normal_loss: 0.009635\n",
      "[229/00049] train_loss: 0.009561 kl_loss: 0.000000 normal_loss: 0.009561\n",
      "[231/00049] train_loss: 0.009556 kl_loss: 0.000000 normal_loss: 0.009556\n",
      "[233/00049] train_loss: 0.009592 kl_loss: 0.000000 normal_loss: 0.009592\n",
      "[235/00049] train_loss: 0.009393 kl_loss: 0.000000 normal_loss: 0.009393\n",
      "[237/00049] train_loss: 0.009423 kl_loss: 0.000000 normal_loss: 0.009423\n",
      "[239/00049] train_loss: 0.009530 kl_loss: 0.000000 normal_loss: 0.009530\n",
      "[241/00049] train_loss: 0.009409 kl_loss: 0.000000 normal_loss: 0.009409\n",
      "[243/00049] train_loss: 0.009378 kl_loss: 0.000000 normal_loss: 0.009378\n",
      "[245/00049] train_loss: 0.009269 kl_loss: 0.000000 normal_loss: 0.009269\n",
      "[247/00049] train_loss: 0.009874 kl_loss: 0.000000 normal_loss: 0.009874\n",
      "[249/00049] train_loss: 0.009428 kl_loss: 0.000000 normal_loss: 0.009428\n",
      "[251/00049] train_loss: 0.009331 kl_loss: 0.000000 normal_loss: 0.009331\n",
      "[253/00049] train_loss: 0.009261 kl_loss: 0.000000 normal_loss: 0.009261\n",
      "[255/00049] train_loss: 0.009186 kl_loss: 0.000000 normal_loss: 0.009186\n",
      "[257/00049] train_loss: 0.009217 kl_loss: 0.000000 normal_loss: 0.009217\n",
      "[259/00049] train_loss: 0.009274 kl_loss: 0.000000 normal_loss: 0.009274\n",
      "[261/00049] train_loss: 0.009253 kl_loss: 0.000000 normal_loss: 0.009253\n",
      "[263/00049] train_loss: 0.009154 kl_loss: 0.000000 normal_loss: 0.009154\n",
      "[265/00049] train_loss: 0.009312 kl_loss: 0.000000 normal_loss: 0.009312\n",
      "[267/00049] train_loss: 0.009182 kl_loss: 0.000000 normal_loss: 0.009182\n",
      "[269/00049] train_loss: 0.009103 kl_loss: 0.000000 normal_loss: 0.009103\n",
      "[271/00049] train_loss: 0.009021 kl_loss: 0.000000 normal_loss: 0.009021\n",
      "[273/00049] train_loss: 0.009119 kl_loss: 0.000000 normal_loss: 0.009119\n",
      "[275/00049] train_loss: 0.009051 kl_loss: 0.000000 normal_loss: 0.009051\n",
      "[277/00049] train_loss: 0.009004 kl_loss: 0.000000 normal_loss: 0.009004\n",
      "[279/00049] train_loss: 0.009004 kl_loss: 0.000000 normal_loss: 0.009004\n",
      "[281/00049] train_loss: 0.009110 kl_loss: 0.000000 normal_loss: 0.009110\n",
      "[283/00049] train_loss: 0.008988 kl_loss: 0.000000 normal_loss: 0.008988\n",
      "[285/00049] train_loss: 0.009043 kl_loss: 0.000000 normal_loss: 0.009043\n",
      "[287/00049] train_loss: 0.009085 kl_loss: 0.000000 normal_loss: 0.009085\n",
      "[289/00049] train_loss: 0.009061 kl_loss: 0.000000 normal_loss: 0.009061\n",
      "[291/00049] train_loss: 0.009048 kl_loss: 0.000000 normal_loss: 0.009048\n",
      "[293/00049] train_loss: 0.009074 kl_loss: 0.000000 normal_loss: 0.009074\n",
      "[295/00049] train_loss: 0.008822 kl_loss: 0.000000 normal_loss: 0.008822\n",
      "[297/00049] train_loss: 0.008993 kl_loss: 0.000000 normal_loss: 0.008993\n",
      "[299/00049] train_loss: 0.009084 kl_loss: 0.000000 normal_loss: 0.009084\n",
      "[301/00049] train_loss: 0.008269 kl_loss: 0.000000 normal_loss: 0.008269\n",
      "[303/00049] train_loss: 0.008235 kl_loss: 0.000000 normal_loss: 0.008235\n",
      "[305/00049] train_loss: 0.007957 kl_loss: 0.000000 normal_loss: 0.007957\n",
      "[307/00049] train_loss: 0.007999 kl_loss: 0.000000 normal_loss: 0.007999\n",
      "[309/00049] train_loss: 0.008008 kl_loss: 0.000000 normal_loss: 0.008008\n",
      "[311/00049] train_loss: 0.007940 kl_loss: 0.000000 normal_loss: 0.007940\n",
      "[313/00049] train_loss: 0.007987 kl_loss: 0.000000 normal_loss: 0.007987\n",
      "[315/00049] train_loss: 0.007912 kl_loss: 0.000000 normal_loss: 0.007912\n",
      "[317/00049] train_loss: 0.007917 kl_loss: 0.000000 normal_loss: 0.007917\n",
      "[319/00049] train_loss: 0.008026 kl_loss: 0.000000 normal_loss: 0.008026\n",
      "[321/00049] train_loss: 0.007986 kl_loss: 0.000000 normal_loss: 0.007986\n",
      "[323/00049] train_loss: 0.008022 kl_loss: 0.000000 normal_loss: 0.008022\n",
      "[325/00049] train_loss: 0.007902 kl_loss: 0.000000 normal_loss: 0.007902\n",
      "[327/00049] train_loss: 0.007901 kl_loss: 0.000000 normal_loss: 0.007901\n",
      "[329/00049] train_loss: 0.007892 kl_loss: 0.000000 normal_loss: 0.007892\n",
      "[331/00049] train_loss: 0.007906 kl_loss: 0.000000 normal_loss: 0.007906\n",
      "[333/00049] train_loss: 0.007951 kl_loss: 0.000000 normal_loss: 0.007951\n",
      "[335/00049] train_loss: 0.007999 kl_loss: 0.000000 normal_loss: 0.007999\n",
      "[337/00049] train_loss: 0.007971 kl_loss: 0.000000 normal_loss: 0.007971\n",
      "[339/00049] train_loss: 0.007929 kl_loss: 0.000000 normal_loss: 0.007929\n",
      "[341/00049] train_loss: 0.007864 kl_loss: 0.000000 normal_loss: 0.007864\n",
      "[343/00049] train_loss: 0.007930 kl_loss: 0.000000 normal_loss: 0.007930\n",
      "[345/00049] train_loss: 0.007838 kl_loss: 0.000000 normal_loss: 0.007838\n",
      "[347/00049] train_loss: 0.007828 kl_loss: 0.000000 normal_loss: 0.007828\n",
      "[349/00049] train_loss: 0.007752 kl_loss: 0.000000 normal_loss: 0.007752\n",
      "[351/00049] train_loss: 0.007833 kl_loss: 0.000000 normal_loss: 0.007833\n",
      "[353/00049] train_loss: 0.007786 kl_loss: 0.000000 normal_loss: 0.007786\n",
      "[355/00049] train_loss: 0.007723 kl_loss: 0.000000 normal_loss: 0.007723\n",
      "[357/00049] train_loss: 0.007965 kl_loss: 0.000000 normal_loss: 0.007965\n",
      "[359/00049] train_loss: 0.007852 kl_loss: 0.000000 normal_loss: 0.007852\n",
      "[361/00049] train_loss: 0.007754 kl_loss: 0.000000 normal_loss: 0.007754\n",
      "[363/00049] train_loss: 0.007941 kl_loss: 0.000000 normal_loss: 0.007941\n",
      "[365/00049] train_loss: 0.007849 kl_loss: 0.000000 normal_loss: 0.007849\n",
      "[367/00049] train_loss: 0.007733 kl_loss: 0.000000 normal_loss: 0.007733\n",
      "[369/00049] train_loss: 0.007711 kl_loss: 0.000000 normal_loss: 0.007711\n",
      "[371/00049] train_loss: 0.007828 kl_loss: 0.000000 normal_loss: 0.007828\n",
      "[373/00049] train_loss: 0.007737 kl_loss: 0.000000 normal_loss: 0.007737\n",
      "[375/00049] train_loss: 0.007653 kl_loss: 0.000000 normal_loss: 0.007653\n",
      "[377/00049] train_loss: 0.007701 kl_loss: 0.000000 normal_loss: 0.007701\n",
      "[379/00049] train_loss: 0.007686 kl_loss: 0.000000 normal_loss: 0.007686\n",
      "[381/00049] train_loss: 0.007610 kl_loss: 0.000000 normal_loss: 0.007610\n",
      "[383/00049] train_loss: 0.007665 kl_loss: 0.000000 normal_loss: 0.007665\n",
      "[385/00049] train_loss: 0.007645 kl_loss: 0.000000 normal_loss: 0.007645\n",
      "[387/00049] train_loss: 0.007624 kl_loss: 0.000000 normal_loss: 0.007624\n",
      "[389/00049] train_loss: 0.007573 kl_loss: 0.000000 normal_loss: 0.007573\n",
      "[391/00049] train_loss: 0.007549 kl_loss: 0.000000 normal_loss: 0.007549\n",
      "[393/00049] train_loss: 0.007596 kl_loss: 0.000000 normal_loss: 0.007596\n",
      "[395/00049] train_loss: 0.007568 kl_loss: 0.000000 normal_loss: 0.007568\n",
      "[397/00049] train_loss: 0.007679 kl_loss: 0.000000 normal_loss: 0.007679\n",
      "[399/00049] train_loss: 0.007571 kl_loss: 0.000000 normal_loss: 0.007571\n",
      "[401/00049] train_loss: 0.007387 kl_loss: 0.000000 normal_loss: 0.007387\n",
      "[403/00049] train_loss: 0.007264 kl_loss: 0.000000 normal_loss: 0.007264\n",
      "[405/00049] train_loss: 0.007265 kl_loss: 0.000000 normal_loss: 0.007265\n",
      "[407/00049] train_loss: 0.007248 kl_loss: 0.000000 normal_loss: 0.007248\n",
      "[409/00049] train_loss: 0.007270 kl_loss: 0.000000 normal_loss: 0.007270\n",
      "[411/00049] train_loss: 0.007240 kl_loss: 0.000000 normal_loss: 0.007240\n",
      "[413/00049] train_loss: 0.007286 kl_loss: 0.000000 normal_loss: 0.007286\n",
      "[415/00049] train_loss: 0.007280 kl_loss: 0.000000 normal_loss: 0.007280\n",
      "[417/00049] train_loss: 0.007265 kl_loss: 0.000000 normal_loss: 0.007265\n",
      "[419/00049] train_loss: 0.007276 kl_loss: 0.000000 normal_loss: 0.007276\n",
      "[421/00049] train_loss: 0.007284 kl_loss: 0.000000 normal_loss: 0.007284\n",
      "[423/00049] train_loss: 0.007252 kl_loss: 0.000000 normal_loss: 0.007252\n",
      "[425/00049] train_loss: 0.007216 kl_loss: 0.000000 normal_loss: 0.007216\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtraining\u001b[39;00m \u001b[39mimport\u001b[39;00m train\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=2'>3</a>\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mexperiment_name\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39msofa_ad\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=19'>20</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdecoder_var\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=20'>21</a>\u001b[0m }\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000006?line=21'>22</a>\u001b[0m train\u001b[39m.\u001b[39;49mmain(config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:186\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=182'>183</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=184'>185</a>\u001b[0m \u001b[39m# Start training\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=185'>186</a>\u001b[0m train(model, train_dataloader, latent_vectors, latent_log_var, device, config)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\Documents\\uni\\ADL4CV\\adl4cv-vad\\training\\train.py:98\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=95'>96</a>\u001b[0m     loss \u001b[39m=\u001b[39m reconstruction_loss\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=96'>97</a>\u001b[0m \u001b[39m# Compute gradients\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=97'>98</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=99'>100</a>\u001b[0m \u001b[39m# Update network parameters\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/Documents/uni/ADL4CV/adl4cv-vad/training/train.py?line=100'>101</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Munzer Dwedari\\miniconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer%20Dwedari/miniconda3/envs/adl4cv/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# SOFA AD\n",
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'sofa_ad',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : False,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'sofa',\n",
    "    'decoder_var' : False\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#                   #\n",
    "#    VISUALIZING    #\n",
    "#                   #\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "\n",
    "def visualize_dataset_sample(filter_class, index):\n",
    "    dataset = ShapeNet('train', filter_class = filter_class)\n",
    "    sample = dataset[index]\n",
    "    input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "    visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "\n",
    "def visualize_ad(experiment, index):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x = latent_vectors[index].unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_vad(experiment, index):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    log_vars = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x = latent_vectors[index]\n",
    "    Dist = dist.Normal(x, torch.exp(log_vars[index]))\n",
    "    x_vad = Dist.rsample().unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x_vad)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_vad_norm(experiment):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Sample\n",
    "    # Dist = dist.Normal(torch.zeros(256), torch.ones(256))\n",
    "    x_vad = torch.rand(256).unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x_vad)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "def visualize_interpolation_ad(experiment, index1, index2, a1=0.5, a2=0.5):\n",
    "    # Load model\n",
    "    model = ThreeDEPNDecoder()\n",
    "    model.load_state_dict(torch.load(f\"runs/{experiment}/model_best.ckpt\", map_location='cpu'))\n",
    "    # Load latent codes\n",
    "    latent_vectors = torch.load(f\"runs/{experiment}/latent_best.pt\", map_location = 'cpu')\n",
    "    # Sample\n",
    "    x1 = latent_vectors[index1]\n",
    "    x2 = latent_vectors[index2]\n",
    "    x = (a1*x1 + a2*x2).unsqueeze(0)\n",
    "    # Forward pass\n",
    "    output_meshes_int = model(x)\n",
    "    # Visualize\n",
    "    output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "    visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f95d5d044a452a99b0e81a60752709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "index1 = 77\n",
    "index2 = 66\n",
    "a1 = 0.52\n",
    "a2 = 1 - a1\n",
    "# experiment = \"sofa_ad\"\n",
    "# filter_class = \"sofa\"\n",
    "experiment = \"sofa_vad_0_03kl\"\n",
    "filter_class = \"sofa_chair\"\n",
    "visualize_vad_norm(experiment)\n",
    "#-------\n",
    "# visualize_vad(experiment, index)\n",
    "# visualize_ad(experiment, index)\n",
    "# visualize_dataset_sample(filter_class, index)\n",
    "#-------\n",
    "# visualize_interpolation_ad(experiment, index1, index2, a1, a2)\n",
    "# visualize_ad(experiment, index1)\n",
    "# visualize_ad(experiment, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d00862b34921449e983f816a3f5dffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc318834a3ea4d7e8763838ee854af57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6e37737e142a18ba21a339a0c4f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "name_temp = 'sofa_ad'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "Dist = dist.Normal(latent_vectors[1002], torch.exp(log_vars[1002]))\n",
    "Dist2 = dist.Normal(latent_vectors[1000], torch.exp(log_vars[1000]))\n",
    "#x_vad_n = torch.randn(256, device='cpu')\n",
    "x_vad = Dist.rsample()#.unsqueeze(0)\n",
    "x_vad2 = Dist2.rsample()#.unsqueeze(0)\n",
    "\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((x_vad*a1 + x_vad2*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad2.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca351708eaa4addbfc5b3ae6957994c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d6af7f7ae4b758d8db8fa80246362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c37264fb2946c5989b9589d5329c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c0b1f2ca8f437286882ddd1b59d2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[5]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "# load model\n",
    "name_temp = 'generalization_airplane_no_vad'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "\n",
    "# load latent codes and latent variances\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "\n",
    "x_vad_n = x_vad_n.unsqueeze(0)\n",
    "output_meshes = model(latent_vectors[5].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(latent_vectors[105].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((latent_vectors[f1]*a1 + latent_vectors[f2]*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62a9db08bae3858a47e0749c92dd3faa3fcc5b478149a89cc50b2ccab095deb2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
