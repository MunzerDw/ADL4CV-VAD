{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 263168  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 65792   \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | bottleneck.4 | Linear           | 131584  \n",
      "6  | bottleneck.5 | ReLU             | 0       \n",
      "7  | decoder1     | Sequential       | 8389376 \n",
      "8  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "9  | decoder1.1   | BatchNorm3d      | 512     \n",
      "10 | decoder1.2   | ReLU             | 0       \n",
      "11 | decoder2     | Sequential       | 2097536 \n",
      "12 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "13 | decoder2.1   | BatchNorm3d      | 256     \n",
      "14 | decoder2.2   | ReLU             | 0       \n",
      "15 | decoder3     | Sequential       | 524480  \n",
      "16 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "17 | decoder3.1   | BatchNorm3d      | 128     \n",
      "18 | decoder3.2   | ReLU             | 0       \n",
      "19 | decoder4     | Sequential       | 4097    \n",
      "20 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "21 | TOTAL        | ThreeDEPNDecoder | 11278657\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 4045\n",
      "Length of overfit set: 64\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class = 'airplane')\n",
    "overfit_dataset = ShapeNet('overfit')\n",
    "\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "# Get length, which is a call to __len__ function\n",
    "print(f'Length of overfit set: {len(overfit_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014b5ddc4f024de3bb8d91b88e5b9fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[101]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 3\n",
      "[001/00004] train_loss: 0.010115 kl_loss: 0.007572 normal_loss: 0.002543\n",
      "[001/00004] train_loss: 0.010374 kl_loss: 0.007749 normal_loss: 0.002626\n",
      "[002/00004] train_loss: 0.008716 kl_loss: 0.007542 normal_loss: 0.001174\n",
      "[002/00004] train_loss: 0.008393 kl_loss: 0.007422 normal_loss: 0.000971\n",
      "[003/00004] train_loss: 0.008175 kl_loss: 0.007302 normal_loss: 0.000872\n",
      "[003/00004] train_loss: 0.008076 kl_loss: 0.007325 normal_loss: 0.000750\n",
      "[004/00004] train_loss: 0.007750 kl_loss: 0.007125 normal_loss: 0.000625\n",
      "[004/00004] train_loss: 0.007689 kl_loss: 0.007174 normal_loss: 0.000515\n"
     ]
    }
   ],
   "source": [
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'overfit',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 4,\n",
    "    'print_every_n': 1,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 1,\n",
    "    'resume_ckpt': None\n",
    "\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 2\n",
      "[001/00002] train_loss: 0.009291 kl_loss: 0.009158 normal_loss: 0.000133\n",
      "[002/00002] train_loss: 0.009056 kl_loss: 0.008923 normal_loss: 0.000134\n"
     ]
    }
   ],
   "source": [
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'overfit_test',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': True,\n",
    "    'batch_size': 32,\n",
    "    'resume_ckpt': None,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 2,\n",
    "    'print_every_n': 1,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': True,\n",
    "    'kl_weight': 1,\n",
    "    'resume_ckpt': 'overfit_train'\n",
    "\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Training params: 3\n",
      "[000/00099] train_loss: 0.113204 kl_loss: 0.500511 normal_loss: 0.108199\n",
      "[001/00072] train_loss: 0.047127 kl_loss: 0.467388 normal_loss: 0.042453\n",
      "[002/00045] train_loss: 0.041506 kl_loss: 0.430261 normal_loss: 0.037203\n",
      "[003/00018] train_loss: 0.037069 kl_loss: 0.396226 normal_loss: 0.033107\n",
      "[003/00118] train_loss: 0.035432 kl_loss: 0.364296 normal_loss: 0.031789\n",
      "[004/00091] train_loss: 0.033546 kl_loss: 0.342165 normal_loss: 0.030125\n",
      "[005/00064] train_loss: 0.032608 kl_loss: 0.332488 normal_loss: 0.029283\n",
      "[006/00037] train_loss: 0.030671 kl_loss: 0.321053 normal_loss: 0.027461\n",
      "[007/00010] train_loss: 0.031114 kl_loss: 0.307378 normal_loss: 0.028040\n",
      "[007/00110] train_loss: 0.029205 kl_loss: 0.293948 normal_loss: 0.026265\n",
      "[008/00083] train_loss: 0.029286 kl_loss: 0.278934 normal_loss: 0.026496\n",
      "[009/00056] train_loss: 0.031165 kl_loss: 0.268189 normal_loss: 0.028483\n",
      "[010/00029] train_loss: 0.033534 kl_loss: 0.255888 normal_loss: 0.030975\n",
      "[011/00002] train_loss: 0.031743 kl_loss: 0.246048 normal_loss: 0.029282\n",
      "[011/00102] train_loss: 0.030723 kl_loss: 0.236032 normal_loss: 0.028363\n",
      "[012/00075] train_loss: 0.029806 kl_loss: 0.228096 normal_loss: 0.027525\n",
      "[013/00048] train_loss: 0.029251 kl_loss: 0.218850 normal_loss: 0.027062\n",
      "[014/00021] train_loss: 0.028778 kl_loss: 0.211228 normal_loss: 0.026666\n",
      "[014/00121] train_loss: 0.027980 kl_loss: 0.203864 normal_loss: 0.025941\n",
      "[015/00094] train_loss: 0.027647 kl_loss: 0.194282 normal_loss: 0.025704\n",
      "[016/00067] train_loss: 0.027335 kl_loss: 0.189829 normal_loss: 0.025436\n",
      "[017/00040] train_loss: 0.027157 kl_loss: 0.184219 normal_loss: 0.025314\n",
      "[018/00013] train_loss: 0.026925 kl_loss: 0.177658 normal_loss: 0.025148\n",
      "[018/00113] train_loss: 0.025329 kl_loss: 0.172484 normal_loss: 0.023604\n",
      "[019/00086] train_loss: 0.026682 kl_loss: 0.167228 normal_loss: 0.025010\n",
      "[020/00059] train_loss: 0.023553 kl_loss: 0.161689 normal_loss: 0.021936\n",
      "[021/00032] train_loss: 0.022170 kl_loss: 0.159516 normal_loss: 0.020575\n",
      "[022/00005] train_loss: 0.022787 kl_loss: 0.154518 normal_loss: 0.021242\n",
      "[022/00105] train_loss: 0.022470 kl_loss: 0.149157 normal_loss: 0.020978\n",
      "[023/00078] train_loss: 0.022523 kl_loss: 0.146799 normal_loss: 0.021055\n",
      "[024/00051] train_loss: 0.022545 kl_loss: 0.142253 normal_loss: 0.021123\n",
      "[025/00024] train_loss: 0.022303 kl_loss: 0.140638 normal_loss: 0.020896\n",
      "[025/00124] train_loss: 0.022153 kl_loss: 0.137594 normal_loss: 0.020777\n",
      "[026/00097] train_loss: 0.022261 kl_loss: 0.134072 normal_loss: 0.020920\n",
      "[027/00070] train_loss: 0.022029 kl_loss: 0.131442 normal_loss: 0.020714\n",
      "[028/00043] train_loss: 0.022076 kl_loss: 0.129929 normal_loss: 0.020777\n",
      "[029/00016] train_loss: 0.021884 kl_loss: 0.126925 normal_loss: 0.020615\n",
      "[029/00116] train_loss: 0.021815 kl_loss: 0.124310 normal_loss: 0.020572\n",
      "[030/00089] train_loss: 0.022029 kl_loss: 0.122768 normal_loss: 0.020801\n",
      "[031/00062] train_loss: 0.021882 kl_loss: 0.120960 normal_loss: 0.020672\n",
      "[032/00035] train_loss: 0.022151 kl_loss: 0.119330 normal_loss: 0.020958\n",
      "[033/00008] train_loss: 0.021785 kl_loss: 0.118649 normal_loss: 0.020598\n",
      "[033/00108] train_loss: 0.021899 kl_loss: 0.115706 normal_loss: 0.020742\n",
      "[034/00081] train_loss: 0.021895 kl_loss: 0.114956 normal_loss: 0.020746\n",
      "[035/00054] train_loss: 0.021315 kl_loss: 0.113062 normal_loss: 0.020185\n",
      "[036/00027] train_loss: 0.021655 kl_loss: 0.112665 normal_loss: 0.020528\n",
      "[037/00000] train_loss: 0.021633 kl_loss: 0.110221 normal_loss: 0.020531\n",
      "[037/00100] train_loss: 0.021455 kl_loss: 0.109428 normal_loss: 0.020360\n",
      "[038/00073] train_loss: 0.021342 kl_loss: 0.107976 normal_loss: 0.020263\n",
      "[039/00046] train_loss: 0.021517 kl_loss: 0.105972 normal_loss: 0.020457\n",
      "[040/00019] train_loss: 0.021119 kl_loss: 0.106198 normal_loss: 0.020057\n",
      "[040/00119] train_loss: 0.019638 kl_loss: 0.104522 normal_loss: 0.018593\n",
      "[041/00092] train_loss: 0.019546 kl_loss: 0.102342 normal_loss: 0.018523\n",
      "[042/00065] train_loss: 0.019605 kl_loss: 0.102119 normal_loss: 0.018584\n",
      "[043/00038] train_loss: 0.019620 kl_loss: 0.101487 normal_loss: 0.018606\n",
      "[044/00011] train_loss: 0.019498 kl_loss: 0.099230 normal_loss: 0.018506\n",
      "[044/00111] train_loss: 0.019807 kl_loss: 0.098969 normal_loss: 0.018817\n",
      "[045/00084] train_loss: 0.019471 kl_loss: 0.097707 normal_loss: 0.018494\n",
      "[046/00057] train_loss: 0.019415 kl_loss: 0.096459 normal_loss: 0.018451\n",
      "[047/00030] train_loss: 0.019524 kl_loss: 0.096045 normal_loss: 0.018563\n",
      "[048/00003] train_loss: 0.019625 kl_loss: 0.095593 normal_loss: 0.018669\n",
      "[048/00103] train_loss: 0.019508 kl_loss: 0.094237 normal_loss: 0.018566\n",
      "[049/00076] train_loss: 0.019650 kl_loss: 0.092595 normal_loss: 0.018724\n",
      "[050/00049] train_loss: 0.019563 kl_loss: 0.092823 normal_loss: 0.018634\n",
      "[051/00022] train_loss: 0.019539 kl_loss: 0.092177 normal_loss: 0.018617\n",
      "[051/00122] train_loss: 0.019074 kl_loss: 0.091251 normal_loss: 0.018162\n",
      "[052/00095] train_loss: 0.019216 kl_loss: 0.090467 normal_loss: 0.018312\n",
      "[053/00068] train_loss: 0.019661 kl_loss: 0.089034 normal_loss: 0.018770\n",
      "[054/00041] train_loss: 0.019623 kl_loss: 0.088693 normal_loss: 0.018736\n",
      "[055/00014] train_loss: 0.019376 kl_loss: 0.088557 normal_loss: 0.018490\n",
      "[055/00114] train_loss: 0.019418 kl_loss: 0.087595 normal_loss: 0.018542\n",
      "[056/00087] train_loss: 0.019411 kl_loss: 0.085759 normal_loss: 0.018553\n",
      "[057/00060] train_loss: 0.019468 kl_loss: 0.086756 normal_loss: 0.018600\n",
      "[058/00033] train_loss: 0.019499 kl_loss: 0.086753 normal_loss: 0.018631\n",
      "[059/00006] train_loss: 0.019360 kl_loss: 0.084600 normal_loss: 0.018514\n",
      "[059/00106] train_loss: 0.019076 kl_loss: 0.084773 normal_loss: 0.018228\n",
      "[060/00079] train_loss: 0.018701 kl_loss: 0.084482 normal_loss: 0.017856\n",
      "[061/00052] train_loss: 0.018406 kl_loss: 0.083050 normal_loss: 0.017575\n",
      "[062/00025] train_loss: 0.018370 kl_loss: 0.083336 normal_loss: 0.017537\n",
      "[062/00125] train_loss: 0.018492 kl_loss: 0.082643 normal_loss: 0.017666\n",
      "[063/00098] train_loss: 0.018530 kl_loss: 0.082069 normal_loss: 0.017709\n",
      "[064/00071] train_loss: 0.018363 kl_loss: 0.082493 normal_loss: 0.017538\n",
      "[065/00044] train_loss: 0.018276 kl_loss: 0.081027 normal_loss: 0.017466\n",
      "[066/00017] train_loss: 0.018645 kl_loss: 0.081349 normal_loss: 0.017832\n",
      "[066/00117] train_loss: 0.018177 kl_loss: 0.080398 normal_loss: 0.017373\n",
      "[067/00090] train_loss: 0.018204 kl_loss: 0.080885 normal_loss: 0.017395\n",
      "[068/00063] train_loss: 0.018525 kl_loss: 0.079279 normal_loss: 0.017732\n",
      "[069/00036] train_loss: 0.018485 kl_loss: 0.079851 normal_loss: 0.017687\n",
      "[070/00009] train_loss: 0.018367 kl_loss: 0.080301 normal_loss: 0.017564\n",
      "[070/00109] train_loss: 0.018411 kl_loss: 0.078554 normal_loss: 0.017625\n",
      "[071/00082] train_loss: 0.018383 kl_loss: 0.079539 normal_loss: 0.017588\n",
      "[072/00055] train_loss: 0.018616 kl_loss: 0.077898 normal_loss: 0.017837\n",
      "[073/00028] train_loss: 0.018380 kl_loss: 0.077945 normal_loss: 0.017601\n",
      "[074/00001] train_loss: 0.018312 kl_loss: 0.077755 normal_loss: 0.017534\n",
      "[074/00101] train_loss: 0.018341 kl_loss: 0.077542 normal_loss: 0.017566\n",
      "[075/00074] train_loss: 0.018250 kl_loss: 0.076740 normal_loss: 0.017483\n",
      "[076/00047] train_loss: 0.018370 kl_loss: 0.076938 normal_loss: 0.017601\n",
      "[077/00020] train_loss: 0.018453 kl_loss: 0.076442 normal_loss: 0.017689\n",
      "[077/00120] train_loss: 0.018219 kl_loss: 0.076812 normal_loss: 0.017451\n",
      "[078/00093] train_loss: 0.018442 kl_loss: 0.075608 normal_loss: 0.017686\n",
      "[079/00066] train_loss: 0.018176 kl_loss: 0.075919 normal_loss: 0.017417\n",
      "[080/00039] train_loss: 0.017998 kl_loss: 0.075740 normal_loss: 0.017241\n",
      "[081/00012] train_loss: 0.017985 kl_loss: 0.074665 normal_loss: 0.017239\n",
      "[081/00112] train_loss: 0.017970 kl_loss: 0.075106 normal_loss: 0.017219\n",
      "[082/00085] train_loss: 0.018010 kl_loss: 0.075031 normal_loss: 0.017259\n",
      "[083/00058] train_loss: 0.017513 kl_loss: 0.074410 normal_loss: 0.016769\n",
      "[084/00031] train_loss: 0.018035 kl_loss: 0.074921 normal_loss: 0.017286\n",
      "[085/00004] train_loss: 0.017885 kl_loss: 0.074433 normal_loss: 0.017141\n",
      "[085/00104] train_loss: 0.018009 kl_loss: 0.073763 normal_loss: 0.017271\n",
      "[086/00077] train_loss: 0.017720 kl_loss: 0.073494 normal_loss: 0.016985\n",
      "[087/00050] train_loss: 0.017928 kl_loss: 0.074736 normal_loss: 0.017180\n",
      "[088/00023] train_loss: 0.017815 kl_loss: 0.073928 normal_loss: 0.017076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[088/00123] train_loss: 0.017917 kl_loss: 0.073647 normal_loss: 0.017181\n",
      "[089/00096] train_loss: 0.017808 kl_loss: 0.072722 normal_loss: 0.017081\n",
      "[090/00069] train_loss: 0.017798 kl_loss: 0.073440 normal_loss: 0.017063\n",
      "[091/00042] train_loss: 0.018076 kl_loss: 0.074233 normal_loss: 0.017334\n",
      "[092/00015] train_loss: 0.017761 kl_loss: 0.072420 normal_loss: 0.017037\n",
      "[092/00115] train_loss: 0.017928 kl_loss: 0.072796 normal_loss: 0.017200\n",
      "[093/00088] train_loss: 0.017804 kl_loss: 0.073089 normal_loss: 0.017073\n",
      "[094/00061] train_loss: 0.017892 kl_loss: 0.072936 normal_loss: 0.017162\n",
      "[095/00034] train_loss: 0.017920 kl_loss: 0.070905 normal_loss: 0.017211\n",
      "[096/00007] train_loss: 0.017763 kl_loss: 0.073296 normal_loss: 0.017030\n",
      "[096/00107] train_loss: 0.017951 kl_loss: 0.072174 normal_loss: 0.017229\n",
      "[097/00080] train_loss: 0.017646 kl_loss: 0.071963 normal_loss: 0.016926\n",
      "[098/00053] train_loss: 0.018194 kl_loss: 0.072043 normal_loss: 0.017474\n",
      "[099/00026] train_loss: 0.017738 kl_loss: 0.071641 normal_loss: 0.017022\n",
      "[099/00126] train_loss: 0.017663 kl_loss: 0.071110 normal_loss: 0.016952\n",
      "[100/00099] train_loss: 0.017640 kl_loss: 0.071415 normal_loss: 0.016926\n",
      "[101/00072] train_loss: 0.017929 kl_loss: 0.071238 normal_loss: 0.017217\n",
      "[102/00045] train_loss: 0.017331 kl_loss: 0.070995 normal_loss: 0.016621\n",
      "[103/00018] train_loss: 0.017652 kl_loss: 0.071237 normal_loss: 0.016939\n",
      "[103/00118] train_loss: 0.017632 kl_loss: 0.071299 normal_loss: 0.016919\n",
      "[104/00091] train_loss: 0.017681 kl_loss: 0.070633 normal_loss: 0.016975\n",
      "[105/00064] train_loss: 0.017759 kl_loss: 0.071272 normal_loss: 0.017046\n",
      "[106/00037] train_loss: 0.017653 kl_loss: 0.070629 normal_loss: 0.016946\n",
      "[107/00010] train_loss: 0.017764 kl_loss: 0.070653 normal_loss: 0.017057\n",
      "[107/00110] train_loss: 0.017645 kl_loss: 0.070683 normal_loss: 0.016938\n",
      "[108/00083] train_loss: 0.017722 kl_loss: 0.071175 normal_loss: 0.017010\n",
      "[109/00056] train_loss: 0.017606 kl_loss: 0.070104 normal_loss: 0.016905\n",
      "[110/00029] train_loss: 0.017536 kl_loss: 0.070891 normal_loss: 0.016827\n",
      "[111/00002] train_loss: 0.017787 kl_loss: 0.070343 normal_loss: 0.017084\n",
      "[111/00102] train_loss: 0.017569 kl_loss: 0.069812 normal_loss: 0.016871\n",
      "[112/00075] train_loss: 0.017764 kl_loss: 0.069994 normal_loss: 0.017064\n",
      "[113/00048] train_loss: 0.017590 kl_loss: 0.071001 normal_loss: 0.016880\n",
      "[114/00021] train_loss: 0.017572 kl_loss: 0.069803 normal_loss: 0.016874\n",
      "[114/00121] train_loss: 0.017492 kl_loss: 0.070012 normal_loss: 0.016792\n",
      "[115/00094] train_loss: 0.017829 kl_loss: 0.070137 normal_loss: 0.017127\n",
      "[116/00067] train_loss: 0.017423 kl_loss: 0.069253 normal_loss: 0.016730\n",
      "[117/00040] train_loss: 0.017476 kl_loss: 0.069665 normal_loss: 0.016780\n",
      "[118/00013] train_loss: 0.017917 kl_loss: 0.069893 normal_loss: 0.017218\n",
      "[118/00113] train_loss: 0.017545 kl_loss: 0.069847 normal_loss: 0.016846\n",
      "[119/00086] train_loss: 0.017830 kl_loss: 0.069843 normal_loss: 0.017132\n",
      "[120/00059] train_loss: 0.017328 kl_loss: 0.069515 normal_loss: 0.016633\n",
      "[121/00032] train_loss: 0.017436 kl_loss: 0.069094 normal_loss: 0.016745\n",
      "[122/00005] train_loss: 0.017773 kl_loss: 0.069806 normal_loss: 0.017075\n",
      "[122/00105] train_loss: 0.017397 kl_loss: 0.068582 normal_loss: 0.016711\n",
      "[123/00078] train_loss: 0.017579 kl_loss: 0.069353 normal_loss: 0.016886\n",
      "[124/00051] train_loss: 0.017546 kl_loss: 0.069385 normal_loss: 0.016852\n",
      "[125/00024] train_loss: 0.017513 kl_loss: 0.069866 normal_loss: 0.016814\n",
      "[125/00124] train_loss: 0.017566 kl_loss: 0.069235 normal_loss: 0.016873\n",
      "[126/00097] train_loss: 0.017355 kl_loss: 0.068814 normal_loss: 0.016667\n",
      "[127/00070] train_loss: 0.017611 kl_loss: 0.068989 normal_loss: 0.016921\n",
      "[128/00043] train_loss: 0.017598 kl_loss: 0.069045 normal_loss: 0.016907\n",
      "[129/00016] train_loss: 0.017546 kl_loss: 0.069344 normal_loss: 0.016853\n",
      "[129/00116] train_loss: 0.017574 kl_loss: 0.069007 normal_loss: 0.016884\n",
      "[130/00089] train_loss: 0.017270 kl_loss: 0.069398 normal_loss: 0.016576\n",
      "[131/00062] train_loss: 0.017562 kl_loss: 0.068206 normal_loss: 0.016880\n",
      "[132/00035] train_loss: 0.017594 kl_loss: 0.069215 normal_loss: 0.016902\n",
      "[133/00008] train_loss: 0.017647 kl_loss: 0.068507 normal_loss: 0.016962\n",
      "[133/00108] train_loss: 0.017401 kl_loss: 0.069531 normal_loss: 0.016706\n",
      "[134/00081] train_loss: 0.017578 kl_loss: 0.068602 normal_loss: 0.016892\n",
      "[135/00054] train_loss: 0.017207 kl_loss: 0.068567 normal_loss: 0.016522\n",
      "[136/00027] train_loss: 0.017790 kl_loss: 0.068665 normal_loss: 0.017103\n",
      "[137/00000] train_loss: 0.017369 kl_loss: 0.068716 normal_loss: 0.016682\n",
      "[137/00100] train_loss: 0.017431 kl_loss: 0.069187 normal_loss: 0.016739\n",
      "[138/00073] train_loss: 0.017576 kl_loss: 0.068542 normal_loss: 0.016891\n",
      "[139/00046] train_loss: 0.017712 kl_loss: 0.067795 normal_loss: 0.017034\n",
      "[140/00019] train_loss: 0.017375 kl_loss: 0.069219 normal_loss: 0.016683\n",
      "[140/00119] train_loss: 0.017483 kl_loss: 0.067656 normal_loss: 0.016806\n",
      "[141/00092] train_loss: 0.017502 kl_loss: 0.068594 normal_loss: 0.016816\n",
      "[142/00065] train_loss: 0.017406 kl_loss: 0.068421 normal_loss: 0.016722\n",
      "[143/00038] train_loss: 0.017533 kl_loss: 0.069445 normal_loss: 0.016838\n",
      "[144/00011] train_loss: 0.017218 kl_loss: 0.067369 normal_loss: 0.016545\n",
      "[144/00111] train_loss: 0.017478 kl_loss: 0.068271 normal_loss: 0.016795\n",
      "[145/00084] train_loss: 0.017585 kl_loss: 0.068683 normal_loss: 0.016898\n",
      "[146/00057] train_loss: 0.017446 kl_loss: 0.067642 normal_loss: 0.016770\n",
      "[147/00030] train_loss: 0.017396 kl_loss: 0.068889 normal_loss: 0.016708\n",
      "[148/00003] train_loss: 0.017590 kl_loss: 0.068065 normal_loss: 0.016910\n",
      "[148/00103] train_loss: 0.017499 kl_loss: 0.068563 normal_loss: 0.016814\n",
      "[149/00076] train_loss: 0.017423 kl_loss: 0.068092 normal_loss: 0.016743\n",
      "[150/00049] train_loss: 0.017378 kl_loss: 0.067550 normal_loss: 0.016702\n",
      "[151/00022] train_loss: 0.017492 kl_loss: 0.069771 normal_loss: 0.016794\n",
      "[151/00122] train_loss: 0.017396 kl_loss: 0.067357 normal_loss: 0.016723\n",
      "[152/00095] train_loss: 0.017386 kl_loss: 0.069085 normal_loss: 0.016695\n",
      "[153/00068] train_loss: 0.017497 kl_loss: 0.068439 normal_loss: 0.016813\n",
      "[154/00041] train_loss: 0.017494 kl_loss: 0.066885 normal_loss: 0.016825\n",
      "[155/00014] train_loss: 0.017426 kl_loss: 0.068492 normal_loss: 0.016741\n",
      "[155/00114] train_loss: 0.017485 kl_loss: 0.068217 normal_loss: 0.016803\n",
      "[156/00087] train_loss: 0.017645 kl_loss: 0.067153 normal_loss: 0.016973\n",
      "[157/00060] train_loss: 0.017482 kl_loss: 0.068683 normal_loss: 0.016795\n",
      "[158/00033] train_loss: 0.017297 kl_loss: 0.068495 normal_loss: 0.016612\n",
      "[159/00006] train_loss: 0.017388 kl_loss: 0.067859 normal_loss: 0.016710\n",
      "[159/00106] train_loss: 0.017390 kl_loss: 0.068207 normal_loss: 0.016708\n",
      "[160/00079] train_loss: 0.017592 kl_loss: 0.067789 normal_loss: 0.016914\n",
      "[161/00052] train_loss: 0.017531 kl_loss: 0.067454 normal_loss: 0.016856\n",
      "[162/00025] train_loss: 0.017286 kl_loss: 0.068780 normal_loss: 0.016598\n",
      "[162/00125] train_loss: 0.017454 kl_loss: 0.067602 normal_loss: 0.016778\n",
      "[163/00098] train_loss: 0.017496 kl_loss: 0.067395 normal_loss: 0.016822\n",
      "[164/00071] train_loss: 0.017315 kl_loss: 0.068360 normal_loss: 0.016632\n",
      "[165/00044] train_loss: 0.017504 kl_loss: 0.068136 normal_loss: 0.016822\n",
      "[166/00017] train_loss: 0.017532 kl_loss: 0.068322 normal_loss: 0.016849\n",
      "[166/00117] train_loss: 0.017331 kl_loss: 0.067530 normal_loss: 0.016655\n",
      "[167/00090] train_loss: 0.017481 kl_loss: 0.068253 normal_loss: 0.016799\n",
      "[168/00063] train_loss: 0.017447 kl_loss: 0.067558 normal_loss: 0.016772\n",
      "[169/00036] train_loss: 0.017392 kl_loss: 0.068134 normal_loss: 0.016711\n",
      "[170/00009] train_loss: 0.017406 kl_loss: 0.067730 normal_loss: 0.016729\n",
      "[170/00109] train_loss: 0.017457 kl_loss: 0.068020 normal_loss: 0.016777\n",
      "[171/00082] train_loss: 0.017394 kl_loss: 0.067758 normal_loss: 0.016716\n",
      "[172/00055] train_loss: 0.017521 kl_loss: 0.068208 normal_loss: 0.016839\n",
      "[173/00028] train_loss: 0.017279 kl_loss: 0.067710 normal_loss: 0.016602\n",
      "[174/00001] train_loss: 0.017498 kl_loss: 0.067780 normal_loss: 0.016820\n",
      "[174/00101] train_loss: 0.017436 kl_loss: 0.068403 normal_loss: 0.016752\n",
      "[175/00074] train_loss: 0.017439 kl_loss: 0.066910 normal_loss: 0.016770\n",
      "[176/00047] train_loss: 0.017404 kl_loss: 0.067799 normal_loss: 0.016726\n",
      "[177/00020] train_loss: 0.017398 kl_loss: 0.067770 normal_loss: 0.016720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[177/00120] train_loss: 0.017386 kl_loss: 0.068216 normal_loss: 0.016704\n",
      "[178/00093] train_loss: 0.017285 kl_loss: 0.068315 normal_loss: 0.016602\n",
      "[179/00066] train_loss: 0.017466 kl_loss: 0.066803 normal_loss: 0.016798\n",
      "[180/00039] train_loss: 0.017578 kl_loss: 0.068516 normal_loss: 0.016893\n",
      "[181/00012] train_loss: 0.017308 kl_loss: 0.067903 normal_loss: 0.016629\n",
      "[181/00112] train_loss: 0.017378 kl_loss: 0.067897 normal_loss: 0.016700\n",
      "[182/00085] train_loss: 0.017435 kl_loss: 0.066644 normal_loss: 0.016768\n",
      "[183/00058] train_loss: 0.017426 kl_loss: 0.068389 normal_loss: 0.016743\n",
      "[184/00031] train_loss: 0.017402 kl_loss: 0.067653 normal_loss: 0.016726\n",
      "[185/00004] train_loss: 0.017537 kl_loss: 0.068059 normal_loss: 0.016856\n",
      "[185/00104] train_loss: 0.017496 kl_loss: 0.067481 normal_loss: 0.016822\n",
      "[186/00077] train_loss: 0.017286 kl_loss: 0.067342 normal_loss: 0.016612\n",
      "[187/00050] train_loss: 0.017214 kl_loss: 0.067333 normal_loss: 0.016541\n",
      "[188/00023] train_loss: 0.017648 kl_loss: 0.068016 normal_loss: 0.016968\n",
      "[188/00123] train_loss: 0.017252 kl_loss: 0.068119 normal_loss: 0.016571\n",
      "[189/00096] train_loss: 0.017132 kl_loss: 0.067696 normal_loss: 0.016455\n",
      "[190/00069] train_loss: 0.017691 kl_loss: 0.067978 normal_loss: 0.017011\n",
      "[191/00042] train_loss: 0.017189 kl_loss: 0.067489 normal_loss: 0.016514\n",
      "[192/00015] train_loss: 0.017423 kl_loss: 0.068022 normal_loss: 0.016742\n",
      "[192/00115] train_loss: 0.017397 kl_loss: 0.067500 normal_loss: 0.016722\n",
      "[193/00088] train_loss: 0.017436 kl_loss: 0.066976 normal_loss: 0.016767\n",
      "[194/00061] train_loss: 0.017342 kl_loss: 0.068359 normal_loss: 0.016659\n",
      "[195/00034] train_loss: 0.017390 kl_loss: 0.067525 normal_loss: 0.016715\n",
      "[196/00007] train_loss: 0.017380 kl_loss: 0.067474 normal_loss: 0.016705\n",
      "[196/00107] train_loss: 0.017303 kl_loss: 0.067698 normal_loss: 0.016626\n",
      "[197/00080] train_loss: 0.017537 kl_loss: 0.068519 normal_loss: 0.016852\n",
      "[198/00053] train_loss: 0.017481 kl_loss: 0.067325 normal_loss: 0.016808\n",
      "[199/00026] train_loss: 0.017228 kl_loss: 0.066478 normal_loss: 0.016563\n",
      "[199/00126] train_loss: 0.017414 kl_loss: 0.068512 normal_loss: 0.016729\n",
      "[200/00099] train_loss: 0.017495 kl_loss: 0.067748 normal_loss: 0.016817\n",
      "[201/00072] train_loss: 0.017378 kl_loss: 0.068058 normal_loss: 0.016698\n",
      "[202/00045] train_loss: 0.017427 kl_loss: 0.066972 normal_loss: 0.016758\n",
      "[203/00018] train_loss: 0.017177 kl_loss: 0.067860 normal_loss: 0.016499\n",
      "[203/00118] train_loss: 0.017566 kl_loss: 0.067400 normal_loss: 0.016892\n",
      "[204/00091] train_loss: 0.017232 kl_loss: 0.068179 normal_loss: 0.016551\n",
      "[205/00064] train_loss: 0.017670 kl_loss: 0.067609 normal_loss: 0.016994\n",
      "[206/00037] train_loss: 0.016968 kl_loss: 0.067545 normal_loss: 0.016292\n",
      "[207/00010] train_loss: 0.017642 kl_loss: 0.066699 normal_loss: 0.016975\n",
      "[207/00110] train_loss: 0.017450 kl_loss: 0.067745 normal_loss: 0.016772\n",
      "[208/00083] train_loss: 0.017370 kl_loss: 0.067453 normal_loss: 0.016695\n",
      "[209/00056] train_loss: 0.017466 kl_loss: 0.068156 normal_loss: 0.016785\n",
      "[210/00029] train_loss: 0.017417 kl_loss: 0.067649 normal_loss: 0.016741\n",
      "[211/00002] train_loss: 0.017285 kl_loss: 0.067410 normal_loss: 0.016611\n",
      "[211/00102] train_loss: 0.017303 kl_loss: 0.067457 normal_loss: 0.016628\n",
      "[212/00075] train_loss: 0.017475 kl_loss: 0.066717 normal_loss: 0.016808\n",
      "[213/00048] train_loss: 0.017304 kl_loss: 0.069029 normal_loss: 0.016614\n",
      "[214/00021] train_loss: 0.017495 kl_loss: 0.067276 normal_loss: 0.016822\n",
      "[214/00121] train_loss: 0.017314 kl_loss: 0.067392 normal_loss: 0.016640\n",
      "[215/00094] train_loss: 0.017363 kl_loss: 0.067404 normal_loss: 0.016689\n",
      "[216/00067] train_loss: 0.017444 kl_loss: 0.067992 normal_loss: 0.016764\n",
      "[217/00040] train_loss: 0.017550 kl_loss: 0.068153 normal_loss: 0.016869\n",
      "[218/00013] train_loss: 0.017379 kl_loss: 0.067182 normal_loss: 0.016707\n",
      "[218/00113] train_loss: 0.017519 kl_loss: 0.067389 normal_loss: 0.016845\n",
      "[219/00086] train_loss: 0.017596 kl_loss: 0.068279 normal_loss: 0.016913\n",
      "[220/00059] train_loss: 0.017200 kl_loss: 0.067694 normal_loss: 0.016523\n",
      "[221/00032] train_loss: 0.017263 kl_loss: 0.066547 normal_loss: 0.016597\n",
      "[222/00005] train_loss: 0.017488 kl_loss: 0.067572 normal_loss: 0.016812\n",
      "[222/00105] train_loss: 0.017424 kl_loss: 0.067100 normal_loss: 0.016753\n",
      "[223/00078] train_loss: 0.017458 kl_loss: 0.068502 normal_loss: 0.016773\n",
      "[224/00051] train_loss: 0.017286 kl_loss: 0.067230 normal_loss: 0.016613\n",
      "[225/00024] train_loss: 0.017557 kl_loss: 0.068076 normal_loss: 0.016877\n",
      "[225/00124] train_loss: 0.017336 kl_loss: 0.067067 normal_loss: 0.016665\n",
      "[226/00097] train_loss: 0.017398 kl_loss: 0.067330 normal_loss: 0.016725\n",
      "[227/00070] train_loss: 0.017427 kl_loss: 0.067516 normal_loss: 0.016752\n",
      "[228/00043] train_loss: 0.017382 kl_loss: 0.067813 normal_loss: 0.016704\n",
      "[229/00016] train_loss: 0.017372 kl_loss: 0.067760 normal_loss: 0.016694\n",
      "[229/00116] train_loss: 0.017338 kl_loss: 0.067302 normal_loss: 0.016665\n",
      "[230/00089] train_loss: 0.017443 kl_loss: 0.067836 normal_loss: 0.016764\n",
      "[231/00062] train_loss: 0.017388 kl_loss: 0.068003 normal_loss: 0.016708\n",
      "[232/00035] train_loss: 0.017488 kl_loss: 0.067050 normal_loss: 0.016818\n",
      "[233/00008] train_loss: 0.017367 kl_loss: 0.067230 normal_loss: 0.016695\n",
      "[233/00108] train_loss: 0.017347 kl_loss: 0.067883 normal_loss: 0.016668\n",
      "[234/00081] train_loss: 0.017390 kl_loss: 0.066956 normal_loss: 0.016721\n",
      "[235/00054] train_loss: 0.017532 kl_loss: 0.068193 normal_loss: 0.016850\n",
      "[236/00027] train_loss: 0.017214 kl_loss: 0.067406 normal_loss: 0.016540\n",
      "[237/00000] train_loss: 0.017446 kl_loss: 0.067608 normal_loss: 0.016770\n",
      "[237/00100] train_loss: 0.017347 kl_loss: 0.067852 normal_loss: 0.016669\n",
      "[238/00073] train_loss: 0.017187 kl_loss: 0.067450 normal_loss: 0.016512\n",
      "[239/00046] train_loss: 0.017753 kl_loss: 0.067796 normal_loss: 0.017075\n",
      "[240/00019] train_loss: 0.017308 kl_loss: 0.066987 normal_loss: 0.016638\n",
      "[240/00119] train_loss: 0.017213 kl_loss: 0.067193 normal_loss: 0.016542\n",
      "[241/00092] train_loss: 0.017398 kl_loss: 0.067676 normal_loss: 0.016721\n",
      "[242/00065] train_loss: 0.017410 kl_loss: 0.067686 normal_loss: 0.016733\n",
      "[243/00038] train_loss: 0.017518 kl_loss: 0.067357 normal_loss: 0.016845\n",
      "[244/00011] train_loss: 0.017279 kl_loss: 0.067302 normal_loss: 0.016606\n",
      "[244/00111] train_loss: 0.017456 kl_loss: 0.068054 normal_loss: 0.016775\n",
      "[245/00084] train_loss: 0.017448 kl_loss: 0.066783 normal_loss: 0.016780\n",
      "[246/00057] train_loss: 0.017246 kl_loss: 0.067792 normal_loss: 0.016568\n",
      "[247/00030] train_loss: 0.017418 kl_loss: 0.067553 normal_loss: 0.016742\n",
      "[248/00003] train_loss: 0.017530 kl_loss: 0.067565 normal_loss: 0.016854\n",
      "[248/00103] train_loss: 0.017319 kl_loss: 0.067667 normal_loss: 0.016642\n",
      "[249/00076] train_loss: 0.017493 kl_loss: 0.067595 normal_loss: 0.016817\n",
      "[250/00049] train_loss: 0.017516 kl_loss: 0.068317 normal_loss: 0.016833\n",
      "[251/00022] train_loss: 0.017118 kl_loss: 0.066794 normal_loss: 0.016450\n",
      "[251/00122] train_loss: 0.017469 kl_loss: 0.067650 normal_loss: 0.016793\n",
      "[252/00095] train_loss: 0.017294 kl_loss: 0.067083 normal_loss: 0.016623\n",
      "[253/00068] train_loss: 0.017404 kl_loss: 0.067660 normal_loss: 0.016728\n",
      "[254/00041] train_loss: 0.017616 kl_loss: 0.067565 normal_loss: 0.016940\n",
      "[255/00014] train_loss: 0.017525 kl_loss: 0.067711 normal_loss: 0.016848\n",
      "[255/00114] train_loss: 0.017331 kl_loss: 0.067663 normal_loss: 0.016654\n",
      "[256/00087] train_loss: 0.017206 kl_loss: 0.066864 normal_loss: 0.016538\n",
      "[257/00060] train_loss: 0.017434 kl_loss: 0.067661 normal_loss: 0.016758\n",
      "[258/00033] train_loss: 0.017383 kl_loss: 0.067742 normal_loss: 0.016705\n",
      "[259/00006] train_loss: 0.017533 kl_loss: 0.067402 normal_loss: 0.016859\n",
      "[259/00106] train_loss: 0.017256 kl_loss: 0.068028 normal_loss: 0.016576\n",
      "[260/00079] train_loss: 0.017374 kl_loss: 0.066423 normal_loss: 0.016709\n",
      "[261/00052] train_loss: 0.017526 kl_loss: 0.068286 normal_loss: 0.016843\n",
      "[262/00025] train_loss: 0.017460 kl_loss: 0.067946 normal_loss: 0.016780\n",
      "[262/00125] train_loss: 0.017377 kl_loss: 0.066946 normal_loss: 0.016708\n",
      "[263/00098] train_loss: 0.017541 kl_loss: 0.067538 normal_loss: 0.016865\n",
      "[264/00071] train_loss: 0.017144 kl_loss: 0.067304 normal_loss: 0.016471\n",
      "[265/00044] train_loss: 0.017617 kl_loss: 0.067264 normal_loss: 0.016945\n",
      "[266/00017] train_loss: 0.017270 kl_loss: 0.067849 normal_loss: 0.016591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266/00117] train_loss: 0.017354 kl_loss: 0.067723 normal_loss: 0.016677\n",
      "[267/00090] train_loss: 0.017493 kl_loss: 0.067932 normal_loss: 0.016814\n",
      "[268/00063] train_loss: 0.017315 kl_loss: 0.067601 normal_loss: 0.016639\n",
      "[269/00036] train_loss: 0.017308 kl_loss: 0.066964 normal_loss: 0.016638\n",
      "[270/00009] train_loss: 0.017504 kl_loss: 0.067672 normal_loss: 0.016827\n",
      "[270/00109] train_loss: 0.017456 kl_loss: 0.067879 normal_loss: 0.016777\n",
      "[271/00082] train_loss: 0.017513 kl_loss: 0.067897 normal_loss: 0.016834\n",
      "[272/00055] train_loss: 0.017239 kl_loss: 0.066431 normal_loss: 0.016575\n",
      "[273/00028] train_loss: 0.017445 kl_loss: 0.069203 normal_loss: 0.016753\n",
      "[274/00001] train_loss: 0.017517 kl_loss: 0.066355 normal_loss: 0.016854\n",
      "[274/00101] train_loss: 0.017507 kl_loss: 0.067196 normal_loss: 0.016835\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneralization_airplane_vad_deeper\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_var\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m }\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\excercises\\adl4cv\\adl4cv-vad-main\\adl4cv-vad-main\\training\\train.py:167\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    164\u001b[0m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_log_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\excercises\\adl4cv\\adl4cv-vad-main\\adl4cv-vad-main\\training\\train.py:58\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Move batch to device, set optimizer gradients to zero, perform forward pass\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     ShapeNet\u001b[38;5;241m.\u001b[39mmove_batch_to_device(batch, device)\n\u001b[1;32m---> 58\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_var\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# Create distribution from latent codes and laten variances (each sample has its own distribution)\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         Dist \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mNormal(latent_vectors[batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]], torch\u001b[38;5;241m.\u001b[39mexp(latent_log_var[batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m]]))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\optim\\optimizer.py:222\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    220\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m foreach \u001b[38;5;129;01mor\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse):\n\u001b[1;32m--> 222\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     per_device_and_dtype_grads[p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdevice][p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype]\u001b[38;5;241m.\u001b[39mappend(p\u001b[38;5;241m.\u001b[39mgrad)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'generalization_airplane_vad_deeper',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.01,\n",
    "    'resume_ckpt': None,\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading saved model, latent codes, and latent variances...\n",
      "Training params: 2\n",
      "[000/00099] train_loss: 0.017962 kl_loss: 0.014923 normal_loss: 0.016470\n",
      "[001/00072] train_loss: 0.017872 kl_loss: 0.014774 normal_loss: 0.016395\n",
      "[002/00045] train_loss: 0.017851 kl_loss: 0.014575 normal_loss: 0.016393\n",
      "[003/00018] train_loss: 0.017669 kl_loss: 0.014604 normal_loss: 0.016208\n",
      "[003/00118] train_loss: 0.017860 kl_loss: 0.014563 normal_loss: 0.016404\n",
      "[004/00091] train_loss: 0.017988 kl_loss: 0.014671 normal_loss: 0.016521\n",
      "[005/00064] train_loss: 0.017824 kl_loss: 0.014523 normal_loss: 0.016372\n",
      "[006/00037] train_loss: 0.017844 kl_loss: 0.014805 normal_loss: 0.016364\n",
      "[007/00010] train_loss: 0.018040 kl_loss: 0.014678 normal_loss: 0.016572\n",
      "[007/00110] train_loss: 0.017887 kl_loss: 0.014810 normal_loss: 0.016406\n",
      "[008/00083] train_loss: 0.017823 kl_loss: 0.014727 normal_loss: 0.016350\n",
      "[009/00056] train_loss: 0.018025 kl_loss: 0.014937 normal_loss: 0.016531\n",
      "[010/00029] train_loss: 0.017877 kl_loss: 0.014945 normal_loss: 0.016383\n",
      "[011/00002] train_loss: 0.018025 kl_loss: 0.014998 normal_loss: 0.016525\n",
      "[011/00102] train_loss: 0.017919 kl_loss: 0.015045 normal_loss: 0.016415\n",
      "[012/00075] train_loss: 0.017901 kl_loss: 0.015330 normal_loss: 0.016368\n",
      "[013/00048] train_loss: 0.018050 kl_loss: 0.015047 normal_loss: 0.016546\n",
      "[014/00021] train_loss: 0.017993 kl_loss: 0.015163 normal_loss: 0.016477\n",
      "[014/00121] train_loss: 0.017970 kl_loss: 0.015372 normal_loss: 0.016432\n",
      "[015/00094] train_loss: 0.017915 kl_loss: 0.015484 normal_loss: 0.016366\n",
      "[016/00067] train_loss: 0.017815 kl_loss: 0.015168 normal_loss: 0.016298\n",
      "[017/00040] train_loss: 0.017849 kl_loss: 0.015421 normal_loss: 0.016307\n",
      "[018/00013] train_loss: 0.018107 kl_loss: 0.015541 normal_loss: 0.016553\n",
      "[018/00113] train_loss: 0.017876 kl_loss: 0.015604 normal_loss: 0.016315\n",
      "[019/00086] train_loss: 0.017839 kl_loss: 0.015504 normal_loss: 0.016289\n",
      "[020/00059] train_loss: 0.017964 kl_loss: 0.015482 normal_loss: 0.016416\n",
      "[021/00032] train_loss: 0.017746 kl_loss: 0.015423 normal_loss: 0.016204\n",
      "[022/00005] train_loss: 0.017959 kl_loss: 0.015657 normal_loss: 0.016393\n",
      "[022/00105] train_loss: 0.017965 kl_loss: 0.015615 normal_loss: 0.016403\n",
      "[023/00078] train_loss: 0.018133 kl_loss: 0.015609 normal_loss: 0.016572\n",
      "[024/00051] train_loss: 0.017910 kl_loss: 0.015394 normal_loss: 0.016371\n",
      "[025/00024] train_loss: 0.017873 kl_loss: 0.015539 normal_loss: 0.016319\n",
      "[025/00124] train_loss: 0.017792 kl_loss: 0.015627 normal_loss: 0.016229\n",
      "[026/00097] train_loss: 0.017860 kl_loss: 0.015590 normal_loss: 0.016301\n",
      "[027/00070] train_loss: 0.017891 kl_loss: 0.015576 normal_loss: 0.016333\n",
      "[028/00043] train_loss: 0.018065 kl_loss: 0.015520 normal_loss: 0.016513\n",
      "[029/00016] train_loss: 0.017851 kl_loss: 0.015640 normal_loss: 0.016287\n",
      "[029/00116] train_loss: 0.017894 kl_loss: 0.015502 normal_loss: 0.016344\n",
      "[030/00089] train_loss: 0.017755 kl_loss: 0.015421 normal_loss: 0.016213\n",
      "[031/00062] train_loss: 0.017983 kl_loss: 0.015664 normal_loss: 0.016416\n",
      "[032/00035] train_loss: 0.017877 kl_loss: 0.015515 normal_loss: 0.016326\n",
      "[033/00008] train_loss: 0.017724 kl_loss: 0.015493 normal_loss: 0.016175\n",
      "[033/00108] train_loss: 0.017846 kl_loss: 0.015555 normal_loss: 0.016291\n",
      "[034/00081] train_loss: 0.017939 kl_loss: 0.015595 normal_loss: 0.016380\n",
      "[035/00054] train_loss: 0.017774 kl_loss: 0.015520 normal_loss: 0.016222\n",
      "[036/00027] train_loss: 0.017872 kl_loss: 0.015589 normal_loss: 0.016313\n",
      "[037/00000] train_loss: 0.017834 kl_loss: 0.015440 normal_loss: 0.016290\n",
      "[037/00100] train_loss: 0.017964 kl_loss: 0.015612 normal_loss: 0.016403\n",
      "[038/00073] train_loss: 0.017662 kl_loss: 0.015405 normal_loss: 0.016121\n",
      "[039/00046] train_loss: 0.017959 kl_loss: 0.015659 normal_loss: 0.016393\n",
      "[040/00019] train_loss: 0.017758 kl_loss: 0.015460 normal_loss: 0.016212\n",
      "[040/00119] train_loss: 0.017774 kl_loss: 0.015583 normal_loss: 0.016216\n",
      "[041/00092] train_loss: 0.017951 kl_loss: 0.015534 normal_loss: 0.016397\n",
      "[042/00065] train_loss: 0.017691 kl_loss: 0.015303 normal_loss: 0.016161\n",
      "[043/00038] train_loss: 0.017904 kl_loss: 0.015572 normal_loss: 0.016347\n",
      "[044/00011] train_loss: 0.017860 kl_loss: 0.015631 normal_loss: 0.016297\n",
      "[044/00111] train_loss: 0.017876 kl_loss: 0.015505 normal_loss: 0.016325\n",
      "[045/00084] train_loss: 0.017837 kl_loss: 0.015486 normal_loss: 0.016289\n",
      "[046/00057] train_loss: 0.018068 kl_loss: 0.015531 normal_loss: 0.016515\n",
      "[047/00030] train_loss: 0.017405 kl_loss: 0.015324 normal_loss: 0.015872\n",
      "[048/00003] train_loss: 0.017975 kl_loss: 0.015489 normal_loss: 0.016426\n",
      "[048/00103] train_loss: 0.017903 kl_loss: 0.015612 normal_loss: 0.016342\n",
      "[049/00076] train_loss: 0.017688 kl_loss: 0.015255 normal_loss: 0.016162\n",
      "[050/00049] train_loss: 0.017951 kl_loss: 0.015448 normal_loss: 0.016406\n",
      "[051/00022] train_loss: 0.017909 kl_loss: 0.015668 normal_loss: 0.016342\n",
      "[051/00122] train_loss: 0.017815 kl_loss: 0.015370 normal_loss: 0.016278\n",
      "[052/00095] train_loss: 0.017948 kl_loss: 0.015474 normal_loss: 0.016401\n",
      "[053/00068] train_loss: 0.017848 kl_loss: 0.015434 normal_loss: 0.016304\n",
      "[054/00041] train_loss: 0.017716 kl_loss: 0.015222 normal_loss: 0.016194\n",
      "[055/00014] train_loss: 0.017722 kl_loss: 0.015622 normal_loss: 0.016160\n",
      "[055/00114] train_loss: 0.017870 kl_loss: 0.015421 normal_loss: 0.016328\n",
      "[056/00087] train_loss: 0.018015 kl_loss: 0.015528 normal_loss: 0.016462\n",
      "[057/00060] train_loss: 0.017633 kl_loss: 0.015216 normal_loss: 0.016112\n",
      "[058/00033] train_loss: 0.017857 kl_loss: 0.015575 normal_loss: 0.016299\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [97]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneralization_airplane_vad_test\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# change this to cpu if you do not have a GPU\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecoder_var\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m }\n\u001b[1;32m---> 21\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\excercises\\adl4cv\\adl4cv-vad-main\\adl4cv-vad-main\\training\\train.py:167\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    164\u001b[0m Path(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_name\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 167\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_log_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\excercises\\adl4cv\\adl4cv-vad-main\\adl4cv-vad-main\\training\\train.py:55\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataloader, latent_vectors, latent_log_var, device, config)\u001b[0m\n\u001b[0;32m     53\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;66;03m# Move batch to device, set optimizer gradients to zero, perform forward pass\u001b[39;00m\n\u001b[0;32m     57\u001b[0m         ShapeNet\u001b[38;5;241m.\u001b[39mmove_batch_to_device(batch, device)\n\u001b[0;32m     58\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:368\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:314\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:927\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    920\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    921\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m    924\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m    925\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m--> 927\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\adl4cv\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from training import train\n",
    "config = {\n",
    "    'experiment_name': 'generalization_airplane_vad_test',\n",
    "    'device': 'cuda:0',  # change this to cpu if you do not have a GPU\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 32,\n",
    "    'learning_rate_model': 0.001,\n",
    "    'learning_rate_code': 0.001,\n",
    "    'learning_rate_log_var':0.001,\n",
    "    'max_epochs': 500,\n",
    "    'print_every_n': 100,\n",
    "    'validate_every_n': 250,\n",
    "    'latent_code_length' : 256,\n",
    "    'vad_free' : True,\n",
    "    'test': True,\n",
    "    'kl_weight': 0.1,\n",
    "    'resume_ckpt': 'generalization_airplane_vad',\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf606ee49b9d4e178468d03cfef24073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fb4b8170ac4e1ca09f42b93b999b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e31c42b9a884953afac64cc36904851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "\n",
    "# load model\n",
    "name_temp = 'generalization_airplane_vad_lowkl'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "Dist = dist.Normal(latent_vectors[1002], torch.exp(log_vars[1002]))\n",
    "Dist2 = dist.Normal(latent_vectors[1000], torch.exp(log_vars[1000]))\n",
    "#x_vad_n = torch.randn(256, device='cpu')\n",
    "x_vad = Dist.rsample()#.unsqueeze(0)\n",
    "x_vad2 = Dist2.rsample()#.unsqueeze(0)\n",
    "\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((x_vad*a1 + x_vad2*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(x_vad2.unsqueeze(0))\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca351708eaa4addbfc5b3ae6957994c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7d6af7f7ae4b758d8db8fa80246362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c37264fb2946c5989b9589d5329c1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c0b1f2ca8f437286882ddd1b59d2b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import torch.distributions as dist\n",
    "import torch\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = train_dataset[5]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)\n",
    "# load model\n",
    "name_temp = 'generalization_airplane_no_vad'\n",
    "model = ThreeDEPNDecoder()\n",
    "model.load_state_dict(torch.load(f\"runs/{name_temp}/model_best.ckpt\", map_location='cpu'))\n",
    "\n",
    "# load latent codes and latent variances\n",
    "latent_vectors = torch.load(f\"runs/{name_temp}/latent_best.pt\", map_location = 'cpu')\n",
    "log_vars = torch.load(f\"runs/{name_temp}/log_var_best.pt\", map_location = 'cpu')\n",
    "\n",
    "x_vad_n = x_vad_n.unsqueeze(0)\n",
    "output_meshes = model(latent_vectors[5].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "\n",
    "output_meshes = model(latent_vectors[105].unsqueeze(0))\n",
    "\n",
    "# Visualize\n",
    "output_mesh = marching_cubes(output_meshes[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh[0], output_mesh[1], flip_axes=True)\n",
    "a1 = 0.5\n",
    "a2 = 0.5\n",
    "f1 = 5\n",
    "f2 = 105\n",
    "output_meshes_int = model((latent_vectors[f1]*a1 + latent_vectors[f2]*a2).unsqueeze(0))\n",
    "output_mesh_int = marching_cubes(output_meshes_int[0].detach().numpy(), level=1)\n",
    "visualize_mesh(output_mesh_int[0], output_mesh_int[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed4ba241db2bd7fdff0e4e773891b91e31df4401ac731abb861e2ca41e0b1ff4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
