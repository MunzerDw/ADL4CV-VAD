{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   | Name         | Type             | Params  \n",
      "-----------------------------------------------------\n",
      "0  | bottleneck   | Sequential       | 197376  \n",
      "1  | bottleneck.0 | Linear           | 65792   \n",
      "2  | bottleneck.1 | ReLU             | 0       \n",
      "3  | bottleneck.2 | Linear           | 131584  \n",
      "4  | bottleneck.3 | ReLU             | 0       \n",
      "5  | decoder1     | Sequential       | 8389376 \n",
      "6  | decoder1.0   | ConvTranspose3d  | 8388864 \n",
      "7  | decoder1.1   | BatchNorm3d      | 512     \n",
      "8  | decoder1.2   | ReLU             | 0       \n",
      "9  | decoder2     | Sequential       | 2097536 \n",
      "10 | decoder2.0   | ConvTranspose3d  | 2097280 \n",
      "11 | decoder2.1   | BatchNorm3d      | 256     \n",
      "12 | decoder2.2   | ReLU             | 0       \n",
      "13 | decoder3     | Sequential       | 524480  \n",
      "14 | decoder3.0   | ConvTranspose3d  | 524352  \n",
      "15 | decoder3.1   | BatchNorm3d      | 128     \n",
      "16 | decoder3.2   | ReLU             | 0       \n",
      "17 | decoder4     | Sequential       | 4097    \n",
      "18 | decoder4.0   | ConvTranspose3d  | 4097    \n",
      "19 | TOTAL        | ThreeDEPNDecoder | 11212865\n"
     ]
    }
   ],
   "source": [
    "from model.threedepn import ThreeDEPNDecoder\n",
    "from util.model import summarize_model\n",
    "\n",
    "threedepn = ThreeDEPNDecoder()\n",
    "print(summarize_model(threedepn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 1854\n",
      "Length of val set: 232\n",
      "Length of test set: 232\n"
     ]
    }
   ],
   "source": [
    "from data.shapenet import ShapeNet\n",
    "\n",
    "# Create a dataset with train split\n",
    "train_dataset = ShapeNet('train', filter_class='lamp')\n",
    "val_dataset = ShapeNet('val', filter_class='lamp')\n",
    "test_dataset = ShapeNet('test', filter_class='lamp')\n",
    "\n",
    "print(f'Length of train set: {len(train_dataset)}')  # expected output: 153540\n",
    "print(f'Length of val set: {len(val_dataset)}')  # expected output: 153540\n",
    "print(f'Length of test set: {len(test_dataset)}')  # expected output: 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target DF: (32, 32, 32)\n",
      "Target DF: <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa21d48a0f58406cb7b1bb8b27c36862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "\n",
    "sample = test_dataset[231]\n",
    "print(f'Target DF: {sample[\"target_df\"].shape}')  # expected output: (32, 32, 32)\n",
    "print(f'Target DF: {type(sample[\"target_df\"])}')  # expected output: <class 'numpy.ndarray'>\n",
    "\n",
    "input_mesh = marching_cubes(sample['target_df'], level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "#                #\n",
    "#    TRAINING    #\n",
    "#                #\n",
    "##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 4800\n",
      "Training params: 3\n",
      "[001/00024] train_loss: 0.174892 kl_loss: 0.490081 normal_loss: 0.160189\n",
      "[002/00049] train_loss: 0.132340 kl_loss: 0.436066 normal_loss: 0.119258\n",
      "[003/00074] train_loss: 0.125638 kl_loss: 0.381073 normal_loss: 0.114205\n",
      "[005/00024] train_loss: 0.120445 kl_loss: 0.337990 normal_loss: 0.110305\n",
      "[006/00049] train_loss: 0.114292 kl_loss: 0.321254 normal_loss: 0.104654\n",
      "[007/00074] train_loss: 0.111616 kl_loss: 0.307387 normal_loss: 0.102394\n",
      "[009/00024] train_loss: 0.108841 kl_loss: 0.294496 normal_loss: 0.100006\n",
      "[010/00049] train_loss: 0.105368 kl_loss: 0.289730 normal_loss: 0.096676\n",
      "[011/00074] train_loss: 0.100769 kl_loss: 0.285123 normal_loss: 0.092216\n",
      "[013/00024] train_loss: 0.099397 kl_loss: 0.280643 normal_loss: 0.090978\n",
      "[014/00049] train_loss: 0.098232 kl_loss: 0.276889 normal_loss: 0.089926\n",
      "[015/00074] train_loss: 0.095730 kl_loss: 0.270200 normal_loss: 0.087624\n",
      "[017/00024] train_loss: 0.093486 kl_loss: 0.264647 normal_loss: 0.085546\n",
      "[018/00049] train_loss: 0.092436 kl_loss: 0.258839 normal_loss: 0.084671\n",
      "[019/00074] train_loss: 0.088407 kl_loss: 0.256938 normal_loss: 0.080699\n",
      "[021/00024] train_loss: 0.087249 kl_loss: 0.253677 normal_loss: 0.079638\n",
      "[022/00049] train_loss: 0.086192 kl_loss: 0.254677 normal_loss: 0.078552\n",
      "[023/00074] train_loss: 0.084293 kl_loss: 0.254422 normal_loss: 0.076660\n",
      "[025/00024] train_loss: 0.081814 kl_loss: 0.254448 normal_loss: 0.074180\n",
      "[026/00049] train_loss: 0.079240 kl_loss: 0.254694 normal_loss: 0.071599\n",
      "[027/00074] train_loss: 0.077124 kl_loss: 0.253976 normal_loss: 0.069505\n",
      "[029/00024] train_loss: 0.076036 kl_loss: 0.254828 normal_loss: 0.068392\n",
      "[030/00049] train_loss: 0.074288 kl_loss: 0.255469 normal_loss: 0.066624\n",
      "[031/00074] train_loss: 0.072499 kl_loss: 0.255532 normal_loss: 0.064833\n",
      "[033/00024] train_loss: 0.070929 kl_loss: 0.255621 normal_loss: 0.063261\n",
      "[034/00049] train_loss: 0.070606 kl_loss: 0.255193 normal_loss: 0.062950\n",
      "[035/00074] train_loss: 0.068869 kl_loss: 0.255988 normal_loss: 0.061189\n",
      "[037/00024] train_loss: 0.067432 kl_loss: 0.255820 normal_loss: 0.059757\n",
      "[038/00049] train_loss: 0.066676 kl_loss: 0.255743 normal_loss: 0.059003\n",
      "[039/00074] train_loss: 0.064758 kl_loss: 0.257484 normal_loss: 0.057034\n",
      "[041/00024] train_loss: 0.064306 kl_loss: 0.256404 normal_loss: 0.056614\n",
      "[042/00049] train_loss: 0.063757 kl_loss: 0.258473 normal_loss: 0.056003\n",
      "[043/00074] train_loss: 0.062041 kl_loss: 0.258648 normal_loss: 0.054282\n",
      "[045/00024] train_loss: 0.061458 kl_loss: 0.258406 normal_loss: 0.053706\n",
      "[046/00049] train_loss: 0.060709 kl_loss: 0.258261 normal_loss: 0.052961\n",
      "[047/00074] train_loss: 0.060697 kl_loss: 0.260772 normal_loss: 0.052874\n",
      "[049/00024] train_loss: 0.059282 kl_loss: 0.259287 normal_loss: 0.051504\n",
      "[050/00049] train_loss: 0.058983 kl_loss: 0.260820 normal_loss: 0.051158\n",
      "[051/00074] train_loss: 0.057937 kl_loss: 0.261466 normal_loss: 0.050093\n",
      "[053/00024] train_loss: 0.057863 kl_loss: 0.260407 normal_loss: 0.050051\n",
      "[054/00049] train_loss: 0.057139 kl_loss: 0.260861 normal_loss: 0.049313\n",
      "[055/00074] train_loss: 0.056672 kl_loss: 0.261321 normal_loss: 0.048832\n",
      "[057/00024] train_loss: 0.055989 kl_loss: 0.263042 normal_loss: 0.048098\n",
      "[058/00049] train_loss: 0.054970 kl_loss: 0.260226 normal_loss: 0.047163\n",
      "[059/00074] train_loss: 0.053983 kl_loss: 0.260783 normal_loss: 0.046160\n",
      "[061/00024] train_loss: 0.054232 kl_loss: 0.260735 normal_loss: 0.046410\n",
      "[062/00049] train_loss: 0.053969 kl_loss: 0.262384 normal_loss: 0.046098\n",
      "[063/00074] train_loss: 0.053358 kl_loss: 0.262429 normal_loss: 0.045485\n",
      "[065/00024] train_loss: 0.052727 kl_loss: 0.262076 normal_loss: 0.044865\n",
      "[066/00049] train_loss: 0.052276 kl_loss: 0.263354 normal_loss: 0.044375\n",
      "[067/00074] train_loss: 0.051968 kl_loss: 0.261539 normal_loss: 0.044122\n",
      "[069/00024] train_loss: 0.050955 kl_loss: 0.261803 normal_loss: 0.043101\n",
      "[070/00049] train_loss: 0.051605 kl_loss: 0.263014 normal_loss: 0.043715\n",
      "[071/00074] train_loss: 0.050581 kl_loss: 0.263007 normal_loss: 0.042691\n",
      "[073/00024] train_loss: 0.049700 kl_loss: 0.263164 normal_loss: 0.041805\n",
      "[074/00049] train_loss: 0.049855 kl_loss: 0.264417 normal_loss: 0.041922\n",
      "[075/00074] train_loss: 0.049819 kl_loss: 0.263295 normal_loss: 0.041920\n",
      "[077/00024] train_loss: 0.049048 kl_loss: 0.264191 normal_loss: 0.041123\n",
      "[078/00049] train_loss: 0.049619 kl_loss: 0.265862 normal_loss: 0.041643\n",
      "[079/00074] train_loss: 0.047374 kl_loss: 0.264688 normal_loss: 0.039434\n",
      "[081/00024] train_loss: 0.047280 kl_loss: 0.265212 normal_loss: 0.039324\n",
      "[082/00049] train_loss: 0.047587 kl_loss: 0.266925 normal_loss: 0.039579\n",
      "[083/00074] train_loss: 0.047574 kl_loss: 0.264865 normal_loss: 0.039628\n",
      "[085/00024] train_loss: 0.046838 kl_loss: 0.267317 normal_loss: 0.038819\n",
      "[086/00049] train_loss: 0.046833 kl_loss: 0.266354 normal_loss: 0.038842\n",
      "[087/00074] train_loss: 0.046381 kl_loss: 0.267950 normal_loss: 0.038343\n",
      "[089/00024] train_loss: 0.045782 kl_loss: 0.268734 normal_loss: 0.037720\n",
      "[090/00049] train_loss: 0.045119 kl_loss: 0.266882 normal_loss: 0.037113\n",
      "[091/00074] train_loss: 0.045390 kl_loss: 0.268916 normal_loss: 0.037322\n",
      "[093/00024] train_loss: 0.045118 kl_loss: 0.270442 normal_loss: 0.037005\n",
      "[094/00049] train_loss: 0.044979 kl_loss: 0.268700 normal_loss: 0.036918\n",
      "[095/00074] train_loss: 0.044646 kl_loss: 0.270150 normal_loss: 0.036542\n",
      "[097/00024] train_loss: 0.043913 kl_loss: 0.270343 normal_loss: 0.035802\n",
      "[098/00049] train_loss: 0.043556 kl_loss: 0.270991 normal_loss: 0.035426\n",
      "[099/00074] train_loss: 0.043976 kl_loss: 0.271826 normal_loss: 0.035821\n",
      "[101/00024] train_loss: 0.040866 kl_loss: 0.270648 normal_loss: 0.032747\n",
      "[102/00049] train_loss: 0.040264 kl_loss: 0.269128 normal_loss: 0.032190\n",
      "[103/00074] train_loss: 0.039768 kl_loss: 0.268168 normal_loss: 0.031723\n",
      "[105/00024] train_loss: 0.039066 kl_loss: 0.264381 normal_loss: 0.031135\n",
      "[106/00049] train_loss: 0.038838 kl_loss: 0.263555 normal_loss: 0.030931\n",
      "[107/00074] train_loss: 0.038834 kl_loss: 0.261769 normal_loss: 0.030981\n",
      "[109/00024] train_loss: 0.039143 kl_loss: 0.259602 normal_loss: 0.031355\n",
      "[110/00049] train_loss: 0.037891 kl_loss: 0.257870 normal_loss: 0.030155\n",
      "[111/00074] train_loss: 0.037594 kl_loss: 0.257189 normal_loss: 0.029879\n",
      "[113/00024] train_loss: 0.037587 kl_loss: 0.255492 normal_loss: 0.029922\n",
      "[114/00049] train_loss: 0.037455 kl_loss: 0.254733 normal_loss: 0.029813\n",
      "[115/00074] train_loss: 0.037031 kl_loss: 0.252814 normal_loss: 0.029447\n",
      "[117/00024] train_loss: 0.037176 kl_loss: 0.253521 normal_loss: 0.029570\n",
      "[118/00049] train_loss: 0.036934 kl_loss: 0.250646 normal_loss: 0.029414\n",
      "[119/00074] train_loss: 0.036753 kl_loss: 0.250522 normal_loss: 0.029237\n",
      "[121/00024] train_loss: 0.036512 kl_loss: 0.250226 normal_loss: 0.029006\n",
      "[122/00049] train_loss: 0.036236 kl_loss: 0.249277 normal_loss: 0.028758\n",
      "[123/00074] train_loss: 0.035969 kl_loss: 0.248836 normal_loss: 0.028504\n",
      "[125/00024] train_loss: 0.035802 kl_loss: 0.248304 normal_loss: 0.028353\n",
      "[126/00049] train_loss: 0.036165 kl_loss: 0.247400 normal_loss: 0.028743\n",
      "[127/00074] train_loss: 0.035757 kl_loss: 0.247988 normal_loss: 0.028317\n",
      "[129/00024] train_loss: 0.035309 kl_loss: 0.246471 normal_loss: 0.027915\n",
      "[130/00049] train_loss: 0.035760 kl_loss: 0.248361 normal_loss: 0.028309\n",
      "[131/00074] train_loss: 0.035305 kl_loss: 0.245684 normal_loss: 0.027934\n",
      "[133/00024] train_loss: 0.034776 kl_loss: 0.246465 normal_loss: 0.027382\n",
      "[134/00049] train_loss: 0.034982 kl_loss: 0.245875 normal_loss: 0.027606\n",
      "[135/00074] train_loss: 0.034794 kl_loss: 0.245889 normal_loss: 0.027417\n",
      "[137/00024] train_loss: 0.034413 kl_loss: 0.245675 normal_loss: 0.027043\n",
      "[138/00049] train_loss: 0.034583 kl_loss: 0.245289 normal_loss: 0.027224\n",
      "[139/00074] train_loss: 0.034626 kl_loss: 0.245882 normal_loss: 0.027250\n",
      "[141/00024] train_loss: 0.034148 kl_loss: 0.245362 normal_loss: 0.026787\n",
      "[142/00049] train_loss: 0.034071 kl_loss: 0.246039 normal_loss: 0.026690\n",
      "[143/00074] train_loss: 0.033902 kl_loss: 0.244751 normal_loss: 0.026560\n",
      "[145/00024] train_loss: 0.034007 kl_loss: 0.245583 normal_loss: 0.026639\n",
      "[146/00049] train_loss: 0.033530 kl_loss: 0.245005 normal_loss: 0.026180\n",
      "[147/00074] train_loss: 0.033834 kl_loss: 0.244886 normal_loss: 0.026487\n",
      "[149/00024] train_loss: 0.033265 kl_loss: 0.244892 normal_loss: 0.025918\n",
      "[150/00049] train_loss: 0.033512 kl_loss: 0.245592 normal_loss: 0.026145\n",
      "[151/00074] train_loss: 0.033499 kl_loss: 0.244744 normal_loss: 0.026157\n",
      "[153/00024] train_loss: 0.033194 kl_loss: 0.244520 normal_loss: 0.025859\n",
      "[154/00049] train_loss: 0.033109 kl_loss: 0.245902 normal_loss: 0.025732\n",
      "[155/00074] train_loss: 0.033072 kl_loss: 0.244738 normal_loss: 0.025730\n",
      "[157/00024] train_loss: 0.032455 kl_loss: 0.244832 normal_loss: 0.025110\n",
      "[158/00049] train_loss: 0.032753 kl_loss: 0.244818 normal_loss: 0.025409\n",
      "[159/00074] train_loss: 0.032495 kl_loss: 0.245129 normal_loss: 0.025141\n",
      "[161/00024] train_loss: 0.032390 kl_loss: 0.244666 normal_loss: 0.025050\n",
      "[162/00049] train_loss: 0.032211 kl_loss: 0.245785 normal_loss: 0.024837\n",
      "[163/00074] train_loss: 0.032201 kl_loss: 0.244260 normal_loss: 0.024874\n",
      "[165/00024] train_loss: 0.032173 kl_loss: 0.244655 normal_loss: 0.024833\n",
      "[166/00049] train_loss: 0.031716 kl_loss: 0.245027 normal_loss: 0.024365\n",
      "[167/00074] train_loss: 0.031911 kl_loss: 0.245092 normal_loss: 0.024558\n",
      "[169/00024] train_loss: 0.031737 kl_loss: 0.243893 normal_loss: 0.024420\n",
      "[170/00049] train_loss: 0.031802 kl_loss: 0.245084 normal_loss: 0.024450\n",
      "[171/00074] train_loss: 0.031713 kl_loss: 0.244163 normal_loss: 0.024388\n",
      "[173/00024] train_loss: 0.031698 kl_loss: 0.244088 normal_loss: 0.024375\n",
      "[174/00049] train_loss: 0.031391 kl_loss: 0.245343 normal_loss: 0.024031\n",
      "[175/00074] train_loss: 0.031279 kl_loss: 0.243580 normal_loss: 0.023972\n",
      "[177/00024] train_loss: 0.031200 kl_loss: 0.243366 normal_loss: 0.023899\n",
      "[178/00049] train_loss: 0.031024 kl_loss: 0.245067 normal_loss: 0.023672\n",
      "[179/00074] train_loss: 0.030998 kl_loss: 0.244773 normal_loss: 0.023655\n",
      "[181/00024] train_loss: 0.030758 kl_loss: 0.244572 normal_loss: 0.023421\n",
      "[182/00049] train_loss: 0.030649 kl_loss: 0.243400 normal_loss: 0.023347\n",
      "[183/00074] train_loss: 0.030816 kl_loss: 0.244918 normal_loss: 0.023468\n",
      "[185/00024] train_loss: 0.030634 kl_loss: 0.244057 normal_loss: 0.023312\n",
      "[186/00049] train_loss: 0.030357 kl_loss: 0.244013 normal_loss: 0.023037\n",
      "[187/00074] train_loss: 0.030291 kl_loss: 0.243796 normal_loss: 0.022977\n",
      "[189/00024] train_loss: 0.030413 kl_loss: 0.243026 normal_loss: 0.023122\n",
      "[190/00049] train_loss: 0.030127 kl_loss: 0.244983 normal_loss: 0.022778\n",
      "[191/00074] train_loss: 0.030046 kl_loss: 0.242863 normal_loss: 0.022761\n",
      "[193/00024] train_loss: 0.030183 kl_loss: 0.243121 normal_loss: 0.022889\n",
      "[194/00049] train_loss: 0.030239 kl_loss: 0.244363 normal_loss: 0.022908\n",
      "[195/00074] train_loss: 0.029644 kl_loss: 0.242149 normal_loss: 0.022379\n",
      "[197/00024] train_loss: 0.029739 kl_loss: 0.242861 normal_loss: 0.022453\n",
      "[198/00049] train_loss: 0.029596 kl_loss: 0.242039 normal_loss: 0.022335\n",
      "[199/00074] train_loss: 0.029936 kl_loss: 0.243140 normal_loss: 0.022642\n",
      "[201/00024] train_loss: 0.028365 kl_loss: 0.242353 normal_loss: 0.021095\n",
      "[202/00049] train_loss: 0.028126 kl_loss: 0.241914 normal_loss: 0.020868\n",
      "[203/00074] train_loss: 0.027948 kl_loss: 0.241209 normal_loss: 0.020712\n",
      "[205/00024] train_loss: 0.027705 kl_loss: 0.240216 normal_loss: 0.020499\n",
      "[206/00049] train_loss: 0.027613 kl_loss: 0.240551 normal_loss: 0.020396\n",
      "[207/00074] train_loss: 0.027582 kl_loss: 0.239021 normal_loss: 0.020412\n",
      "[209/00024] train_loss: 0.027441 kl_loss: 0.239027 normal_loss: 0.020270\n",
      "[210/00049] train_loss: 0.027514 kl_loss: 0.238359 normal_loss: 0.020363\n",
      "[211/00074] train_loss: 0.027295 kl_loss: 0.237041 normal_loss: 0.020183\n",
      "[213/00024] train_loss: 0.027096 kl_loss: 0.237360 normal_loss: 0.019975\n",
      "[214/00049] train_loss: 0.027155 kl_loss: 0.236342 normal_loss: 0.020064\n",
      "[215/00074] train_loss: 0.027130 kl_loss: 0.235596 normal_loss: 0.020062\n",
      "[217/00024] train_loss: 0.026966 kl_loss: 0.235251 normal_loss: 0.019909\n",
      "[218/00049] train_loss: 0.027145 kl_loss: 0.234903 normal_loss: 0.020098\n",
      "[219/00074] train_loss: 0.026975 kl_loss: 0.234319 normal_loss: 0.019945\n",
      "[221/00024] train_loss: 0.026682 kl_loss: 0.233247 normal_loss: 0.019685\n",
      "[222/00049] train_loss: 0.026925 kl_loss: 0.234304 normal_loss: 0.019896\n",
      "[223/00074] train_loss: 0.026694 kl_loss: 0.232766 normal_loss: 0.019711\n",
      "[225/00024] train_loss: 0.026709 kl_loss: 0.232389 normal_loss: 0.019737\n",
      "[226/00049] train_loss: 0.026757 kl_loss: 0.232677 normal_loss: 0.019776\n",
      "[227/00074] train_loss: 0.026617 kl_loss: 0.231222 normal_loss: 0.019680\n",
      "[229/00024] train_loss: 0.026397 kl_loss: 0.230518 normal_loss: 0.019481\n",
      "[230/00049] train_loss: 0.026721 kl_loss: 0.231827 normal_loss: 0.019766\n",
      "[231/00074] train_loss: 0.026388 kl_loss: 0.230178 normal_loss: 0.019483\n",
      "[233/00024] train_loss: 0.026621 kl_loss: 0.230346 normal_loss: 0.019711\n",
      "[234/00049] train_loss: 0.026201 kl_loss: 0.229287 normal_loss: 0.019323\n",
      "[235/00074] train_loss: 0.026301 kl_loss: 0.229453 normal_loss: 0.019417\n",
      "[237/00024] train_loss: 0.026173 kl_loss: 0.229047 normal_loss: 0.019302\n",
      "[238/00049] train_loss: 0.026208 kl_loss: 0.228442 normal_loss: 0.019355\n",
      "[239/00074] train_loss: 0.026304 kl_loss: 0.228430 normal_loss: 0.019451\n",
      "[241/00024] train_loss: 0.026035 kl_loss: 0.227904 normal_loss: 0.019198\n",
      "[242/00049] train_loss: 0.026125 kl_loss: 0.228290 normal_loss: 0.019276\n",
      "[243/00074] train_loss: 0.025884 kl_loss: 0.226843 normal_loss: 0.019078\n",
      "[245/00024] train_loss: 0.025881 kl_loss: 0.226629 normal_loss: 0.019082\n",
      "[246/00049] train_loss: 0.026054 kl_loss: 0.226853 normal_loss: 0.019248\n",
      "[247/00074] train_loss: 0.025904 kl_loss: 0.226843 normal_loss: 0.019099\n",
      "[249/00024] train_loss: 0.025985 kl_loss: 0.226385 normal_loss: 0.019194\n",
      "[250/00049] train_loss: 0.025780 kl_loss: 0.225546 normal_loss: 0.019014\n",
      "[251/00074] train_loss: 0.025792 kl_loss: 0.226062 normal_loss: 0.019010\n",
      "[253/00024] train_loss: 0.025550 kl_loss: 0.225326 normal_loss: 0.018790\n",
      "[254/00049] train_loss: 0.025897 kl_loss: 0.225917 normal_loss: 0.019120\n",
      "[255/00074] train_loss: 0.025683 kl_loss: 0.224511 normal_loss: 0.018948\n",
      "[257/00024] train_loss: 0.025506 kl_loss: 0.224743 normal_loss: 0.018764\n",
      "[258/00049] train_loss: 0.025623 kl_loss: 0.225119 normal_loss: 0.018869\n",
      "[259/00074] train_loss: 0.025474 kl_loss: 0.224090 normal_loss: 0.018751\n",
      "[261/00024] train_loss: 0.025563 kl_loss: 0.223864 normal_loss: 0.018847\n",
      "[262/00049] train_loss: 0.025527 kl_loss: 0.223889 normal_loss: 0.018810\n",
      "[263/00074] train_loss: 0.025393 kl_loss: 0.224403 normal_loss: 0.018661\n",
      "[265/00024] train_loss: 0.025334 kl_loss: 0.223542 normal_loss: 0.018627\n",
      "[266/00049] train_loss: 0.025371 kl_loss: 0.224200 normal_loss: 0.018645\n",
      "[267/00074] train_loss: 0.025261 kl_loss: 0.222587 normal_loss: 0.018583\n",
      "[269/00024] train_loss: 0.025215 kl_loss: 0.223304 normal_loss: 0.018516\n",
      "[270/00049] train_loss: 0.025273 kl_loss: 0.223197 normal_loss: 0.018577\n",
      "[271/00074] train_loss: 0.025157 kl_loss: 0.222075 normal_loss: 0.018495\n",
      "[273/00024] train_loss: 0.025165 kl_loss: 0.222927 normal_loss: 0.018477\n",
      "[274/00049] train_loss: 0.025227 kl_loss: 0.222343 normal_loss: 0.018556\n",
      "[275/00074] train_loss: 0.025021 kl_loss: 0.221727 normal_loss: 0.018370\n",
      "[277/00024] train_loss: 0.024904 kl_loss: 0.221421 normal_loss: 0.018261\n",
      "[278/00049] train_loss: 0.025021 kl_loss: 0.222707 normal_loss: 0.018340\n",
      "[279/00074] train_loss: 0.024954 kl_loss: 0.221295 normal_loss: 0.018315\n",
      "[281/00024] train_loss: 0.024861 kl_loss: 0.221201 normal_loss: 0.018225\n",
      "[282/00049] train_loss: 0.025013 kl_loss: 0.221496 normal_loss: 0.018368\n",
      "[283/00074] train_loss: 0.024911 kl_loss: 0.221088 normal_loss: 0.018278\n",
      "[285/00024] train_loss: 0.024782 kl_loss: 0.220012 normal_loss: 0.018182\n",
      "[286/00049] train_loss: 0.024833 kl_loss: 0.222010 normal_loss: 0.018172\n",
      "[287/00074] train_loss: 0.024752 kl_loss: 0.220194 normal_loss: 0.018146\n",
      "[289/00024] train_loss: 0.024704 kl_loss: 0.220417 normal_loss: 0.018091\n",
      "[290/00049] train_loss: 0.024855 kl_loss: 0.220451 normal_loss: 0.018241\n",
      "[291/00074] train_loss: 0.024714 kl_loss: 0.219942 normal_loss: 0.018116\n",
      "[293/00024] train_loss: 0.024689 kl_loss: 0.219475 normal_loss: 0.018104\n",
      "[294/00049] train_loss: 0.024520 kl_loss: 0.219567 normal_loss: 0.017933\n",
      "[295/00074] train_loss: 0.024565 kl_loss: 0.220370 normal_loss: 0.017954\n",
      "[297/00024] train_loss: 0.024379 kl_loss: 0.219451 normal_loss: 0.017796\n",
      "[298/00049] train_loss: 0.024613 kl_loss: 0.219160 normal_loss: 0.018038\n",
      "[299/00074] train_loss: 0.024380 kl_loss: 0.219168 normal_loss: 0.017805\n",
      "[301/00024] train_loss: 0.023887 kl_loss: 0.218494 normal_loss: 0.017332\n",
      "[302/00049] train_loss: 0.023824 kl_loss: 0.218764 normal_loss: 0.017261\n",
      "[303/00074] train_loss: 0.023759 kl_loss: 0.218637 normal_loss: 0.017200\n",
      "[305/00024] train_loss: 0.023879 kl_loss: 0.218211 normal_loss: 0.017332\n",
      "[306/00049] train_loss: 0.023673 kl_loss: 0.218225 normal_loss: 0.017126\n",
      "[307/00074] train_loss: 0.023518 kl_loss: 0.217601 normal_loss: 0.016990\n",
      "[309/00024] train_loss: 0.023565 kl_loss: 0.217703 normal_loss: 0.017034\n",
      "[310/00049] train_loss: 0.023577 kl_loss: 0.217409 normal_loss: 0.017055\n",
      "[311/00074] train_loss: 0.023480 kl_loss: 0.217030 normal_loss: 0.016969\n",
      "[313/00024] train_loss: 0.023448 kl_loss: 0.216555 normal_loss: 0.016951\n",
      "[314/00049] train_loss: 0.023492 kl_loss: 0.217448 normal_loss: 0.016969\n",
      "[315/00074] train_loss: 0.023238 kl_loss: 0.216190 normal_loss: 0.016752\n",
      "[317/00024] train_loss: 0.023377 kl_loss: 0.216454 normal_loss: 0.016883\n",
      "[318/00049] train_loss: 0.023275 kl_loss: 0.216392 normal_loss: 0.016783\n",
      "[319/00074] train_loss: 0.023241 kl_loss: 0.215408 normal_loss: 0.016779\n",
      "[321/00024] train_loss: 0.023389 kl_loss: 0.215411 normal_loss: 0.016927\n",
      "[322/00049] train_loss: 0.023153 kl_loss: 0.214950 normal_loss: 0.016705\n",
      "[323/00074] train_loss: 0.023332 kl_loss: 0.215919 normal_loss: 0.016854\n",
      "[325/00024] train_loss: 0.023182 kl_loss: 0.214750 normal_loss: 0.016740\n",
      "[326/00049] train_loss: 0.023176 kl_loss: 0.215049 normal_loss: 0.016725\n",
      "[327/00074] train_loss: 0.023083 kl_loss: 0.214469 normal_loss: 0.016649\n",
      "[329/00024] train_loss: 0.023191 kl_loss: 0.214763 normal_loss: 0.016748\n",
      "[330/00049] train_loss: 0.023111 kl_loss: 0.213843 normal_loss: 0.016695\n",
      "[331/00074] train_loss: 0.022987 kl_loss: 0.213846 normal_loss: 0.016572\n",
      "[333/00024] train_loss: 0.023089 kl_loss: 0.213943 normal_loss: 0.016670\n",
      "[334/00049] train_loss: 0.023029 kl_loss: 0.213347 normal_loss: 0.016629\n",
      "[335/00074] train_loss: 0.023064 kl_loss: 0.213314 normal_loss: 0.016664\n",
      "[337/00024] train_loss: 0.023004 kl_loss: 0.212998 normal_loss: 0.016614\n",
      "[338/00049] train_loss: 0.022922 kl_loss: 0.213071 normal_loss: 0.016530\n",
      "[339/00074] train_loss: 0.022941 kl_loss: 0.212942 normal_loss: 0.016553\n",
      "[341/00024] train_loss: 0.022984 kl_loss: 0.212984 normal_loss: 0.016594\n",
      "[342/00049] train_loss: 0.022793 kl_loss: 0.212279 normal_loss: 0.016424\n",
      "[343/00074] train_loss: 0.022935 kl_loss: 0.212014 normal_loss: 0.016575\n",
      "[345/00024] train_loss: 0.022906 kl_loss: 0.211796 normal_loss: 0.016553\n",
      "[346/00049] train_loss: 0.022788 kl_loss: 0.212692 normal_loss: 0.016407\n",
      "[347/00074] train_loss: 0.022839 kl_loss: 0.211211 normal_loss: 0.016502\n",
      "[349/00024] train_loss: 0.022807 kl_loss: 0.211428 normal_loss: 0.016464\n",
      "[350/00049] train_loss: 0.022796 kl_loss: 0.210938 normal_loss: 0.016468\n",
      "[351/00074] train_loss: 0.022814 kl_loss: 0.211802 normal_loss: 0.016460\n",
      "[353/00024] train_loss: 0.022778 kl_loss: 0.210579 normal_loss: 0.016461\n",
      "[354/00049] train_loss: 0.022840 kl_loss: 0.211791 normal_loss: 0.016486\n",
      "[355/00074] train_loss: 0.022698 kl_loss: 0.210397 normal_loss: 0.016386\n",
      "[357/00024] train_loss: 0.022640 kl_loss: 0.210929 normal_loss: 0.016312\n",
      "[358/00049] train_loss: 0.022720 kl_loss: 0.210452 normal_loss: 0.016406\n",
      "[359/00074] train_loss: 0.022724 kl_loss: 0.210078 normal_loss: 0.016422\n",
      "[361/00024] train_loss: 0.022595 kl_loss: 0.209432 normal_loss: 0.016312\n",
      "[362/00049] train_loss: 0.022797 kl_loss: 0.210985 normal_loss: 0.016468\n",
      "[363/00074] train_loss: 0.022465 kl_loss: 0.209557 normal_loss: 0.016178\n",
      "[365/00024] train_loss: 0.022533 kl_loss: 0.209364 normal_loss: 0.016252\n",
      "[366/00049] train_loss: 0.022595 kl_loss: 0.210094 normal_loss: 0.016292\n",
      "[367/00074] train_loss: 0.022558 kl_loss: 0.209129 normal_loss: 0.016284\n",
      "[369/00024] train_loss: 0.022481 kl_loss: 0.208824 normal_loss: 0.016216\n",
      "[370/00049] train_loss: 0.022609 kl_loss: 0.209931 normal_loss: 0.016311\n",
      "[371/00074] train_loss: 0.022402 kl_loss: 0.208589 normal_loss: 0.016144\n",
      "[373/00024] train_loss: 0.022453 kl_loss: 0.208737 normal_loss: 0.016191\n",
      "[374/00049] train_loss: 0.022372 kl_loss: 0.208736 normal_loss: 0.016109\n",
      "[375/00074] train_loss: 0.022523 kl_loss: 0.208582 normal_loss: 0.016266\n",
      "[377/00024] train_loss: 0.022450 kl_loss: 0.208206 normal_loss: 0.016204\n",
      "[378/00049] train_loss: 0.022622 kl_loss: 0.209314 normal_loss: 0.016343\n",
      "[379/00074] train_loss: 0.022319 kl_loss: 0.207440 normal_loss: 0.016096\n",
      "[381/00024] train_loss: 0.022435 kl_loss: 0.208007 normal_loss: 0.016195\n",
      "[382/00049] train_loss: 0.022363 kl_loss: 0.207981 normal_loss: 0.016124\n",
      "[383/00074] train_loss: 0.022331 kl_loss: 0.207948 normal_loss: 0.016093\n",
      "[385/00024] train_loss: 0.022316 kl_loss: 0.207788 normal_loss: 0.016082\n",
      "[386/00049] train_loss: 0.022384 kl_loss: 0.207591 normal_loss: 0.016156\n",
      "[387/00074] train_loss: 0.022253 kl_loss: 0.207370 normal_loss: 0.016032\n",
      "[389/00024] train_loss: 0.022272 kl_loss: 0.207564 normal_loss: 0.016045\n",
      "[390/00049] train_loss: 0.022223 kl_loss: 0.206771 normal_loss: 0.016019\n",
      "[391/00074] train_loss: 0.022326 kl_loss: 0.207258 normal_loss: 0.016108\n",
      "[393/00024] train_loss: 0.022220 kl_loss: 0.206437 normal_loss: 0.016027\n",
      "[394/00049] train_loss: 0.022154 kl_loss: 0.206962 normal_loss: 0.015946\n",
      "[395/00074] train_loss: 0.022164 kl_loss: 0.207133 normal_loss: 0.015950\n",
      "[397/00024] train_loss: 0.022116 kl_loss: 0.206633 normal_loss: 0.015917\n",
      "[398/00049] train_loss: 0.022172 kl_loss: 0.205975 normal_loss: 0.015993\n",
      "[399/00074] train_loss: 0.022228 kl_loss: 0.206768 normal_loss: 0.016025\n",
      "[401/00024] train_loss: 0.021846 kl_loss: 0.206329 normal_loss: 0.015657\n",
      "[402/00049] train_loss: 0.021862 kl_loss: 0.206115 normal_loss: 0.015679\n",
      "[403/00074] train_loss: 0.021834 kl_loss: 0.205958 normal_loss: 0.015655\n",
      "[405/00024] train_loss: 0.021846 kl_loss: 0.206251 normal_loss: 0.015658\n",
      "[406/00049] train_loss: 0.021789 kl_loss: 0.205511 normal_loss: 0.015623\n",
      "[407/00074] train_loss: 0.021776 kl_loss: 0.205777 normal_loss: 0.015603\n",
      "[409/00024] train_loss: 0.021766 kl_loss: 0.206015 normal_loss: 0.015586\n",
      "[410/00049] train_loss: 0.021658 kl_loss: 0.205029 normal_loss: 0.015508\n",
      "[411/00074] train_loss: 0.021740 kl_loss: 0.205569 normal_loss: 0.015573\n",
      "[413/00024] train_loss: 0.021690 kl_loss: 0.205328 normal_loss: 0.015531\n",
      "[414/00049] train_loss: 0.021792 kl_loss: 0.205185 normal_loss: 0.015636\n",
      "[415/00074] train_loss: 0.021628 kl_loss: 0.205166 normal_loss: 0.015473\n",
      "[417/00024] train_loss: 0.021692 kl_loss: 0.205218 normal_loss: 0.015535\n",
      "[418/00049] train_loss: 0.021681 kl_loss: 0.204842 normal_loss: 0.015536\n",
      "[419/00074] train_loss: 0.021652 kl_loss: 0.204752 normal_loss: 0.015510\n",
      "[421/00024] train_loss: 0.021682 kl_loss: 0.205234 normal_loss: 0.015525\n",
      "[422/00049] train_loss: 0.021541 kl_loss: 0.204037 normal_loss: 0.015420\n",
      "[423/00074] train_loss: 0.021552 kl_loss: 0.204700 normal_loss: 0.015411\n",
      "[425/00024] train_loss: 0.021557 kl_loss: 0.203737 normal_loss: 0.015445\n",
      "[426/00049] train_loss: 0.021699 kl_loss: 0.205080 normal_loss: 0.015546\n",
      "[427/00074] train_loss: 0.021545 kl_loss: 0.204213 normal_loss: 0.015418\n",
      "[429/00024] train_loss: 0.021558 kl_loss: 0.204007 normal_loss: 0.015438\n",
      "[430/00049] train_loss: 0.021491 kl_loss: 0.204114 normal_loss: 0.015368\n",
      "[431/00074] train_loss: 0.021550 kl_loss: 0.204040 normal_loss: 0.015429\n",
      "[433/00024] train_loss: 0.021438 kl_loss: 0.203875 normal_loss: 0.015321\n",
      "[434/00049] train_loss: 0.021515 kl_loss: 0.204141 normal_loss: 0.015391\n",
      "[435/00074] train_loss: 0.021469 kl_loss: 0.203228 normal_loss: 0.015372\n",
      "[437/00024] train_loss: 0.021417 kl_loss: 0.203026 normal_loss: 0.015326\n",
      "[438/00049] train_loss: 0.021596 kl_loss: 0.204353 normal_loss: 0.015465\n",
      "[439/00074] train_loss: 0.021435 kl_loss: 0.202999 normal_loss: 0.015345\n",
      "[441/00024] train_loss: 0.021528 kl_loss: 0.203233 normal_loss: 0.015431\n",
      "[442/00049] train_loss: 0.021367 kl_loss: 0.203640 normal_loss: 0.015258\n",
      "[443/00074] train_loss: 0.021458 kl_loss: 0.202731 normal_loss: 0.015376\n",
      "[445/00024] train_loss: 0.021478 kl_loss: 0.203220 normal_loss: 0.015381\n",
      "[446/00049] train_loss: 0.021326 kl_loss: 0.202820 normal_loss: 0.015241\n",
      "[447/00074] train_loss: 0.021467 kl_loss: 0.202786 normal_loss: 0.015383\n",
      "[449/00024] train_loss: 0.021345 kl_loss: 0.202232 normal_loss: 0.015278\n",
      "[450/00049] train_loss: 0.021477 kl_loss: 0.203616 normal_loss: 0.015369\n",
      "[451/00074] train_loss: 0.021379 kl_loss: 0.202200 normal_loss: 0.015313\n",
      "[453/00024] train_loss: 0.021384 kl_loss: 0.202428 normal_loss: 0.015311\n",
      "[454/00049] train_loss: 0.021382 kl_loss: 0.202840 normal_loss: 0.015297\n",
      "[455/00074] train_loss: 0.021338 kl_loss: 0.201962 normal_loss: 0.015279\n",
      "[457/00024] train_loss: 0.021283 kl_loss: 0.202147 normal_loss: 0.015219\n",
      "[458/00049] train_loss: 0.021349 kl_loss: 0.202340 normal_loss: 0.015279\n",
      "[459/00074] train_loss: 0.021337 kl_loss: 0.201964 normal_loss: 0.015278\n",
      "[461/00024] train_loss: 0.021310 kl_loss: 0.202002 normal_loss: 0.015250\n",
      "[462/00049] train_loss: 0.021254 kl_loss: 0.202131 normal_loss: 0.015190\n",
      "[463/00074] train_loss: 0.021286 kl_loss: 0.201547 normal_loss: 0.015240\n",
      "[465/00024] train_loss: 0.021309 kl_loss: 0.201603 normal_loss: 0.015261\n",
      "[466/00049] train_loss: 0.021242 kl_loss: 0.201679 normal_loss: 0.015192\n",
      "[467/00074] train_loss: 0.021276 kl_loss: 0.201633 normal_loss: 0.015227\n",
      "[469/00024] train_loss: 0.021220 kl_loss: 0.201043 normal_loss: 0.015189\n",
      "[470/00049] train_loss: 0.021255 kl_loss: 0.201439 normal_loss: 0.015212\n",
      "[471/00074] train_loss: 0.021218 kl_loss: 0.201696 normal_loss: 0.015167\n",
      "[473/00024] train_loss: 0.021254 kl_loss: 0.201145 normal_loss: 0.015219\n",
      "[474/00049] train_loss: 0.021216 kl_loss: 0.201628 normal_loss: 0.015167\n",
      "[475/00074] train_loss: 0.021153 kl_loss: 0.200619 normal_loss: 0.015134\n",
      "[477/00024] train_loss: 0.021238 kl_loss: 0.201046 normal_loss: 0.015207\n",
      "[478/00049] train_loss: 0.021244 kl_loss: 0.201006 normal_loss: 0.015214\n",
      "[479/00074] train_loss: 0.021087 kl_loss: 0.200628 normal_loss: 0.015068\n",
      "[481/00024] train_loss: 0.021132 kl_loss: 0.201054 normal_loss: 0.015101\n",
      "[482/00049] train_loss: 0.021130 kl_loss: 0.200274 normal_loss: 0.015122\n",
      "[483/00074] train_loss: 0.021118 kl_loss: 0.200577 normal_loss: 0.015100\n",
      "[485/00024] train_loss: 0.021146 kl_loss: 0.200867 normal_loss: 0.015120\n",
      "[486/00049] train_loss: 0.021200 kl_loss: 0.200058 normal_loss: 0.015198\n",
      "[487/00074] train_loss: 0.021142 kl_loss: 0.200203 normal_loss: 0.015136\n",
      "[489/00024] train_loss: 0.021204 kl_loss: 0.200736 normal_loss: 0.015182\n",
      "[490/00049] train_loss: 0.021008 kl_loss: 0.199201 normal_loss: 0.015032\n",
      "[491/00074] train_loss: 0.021129 kl_loss: 0.200511 normal_loss: 0.015113\n",
      "[493/00024] train_loss: 0.021141 kl_loss: 0.199473 normal_loss: 0.015156\n",
      "[494/00049] train_loss: 0.021062 kl_loss: 0.200225 normal_loss: 0.015055\n",
      "[495/00074] train_loss: 0.021050 kl_loss: 0.200075 normal_loss: 0.015048\n",
      "[497/00024] train_loss: 0.021090 kl_loss: 0.199556 normal_loss: 0.015103\n",
      "[498/00049] train_loss: 0.021104 kl_loss: 0.199845 normal_loss: 0.015109\n",
      "[499/00074] train_loss: 0.021055 kl_loss: 0.199724 normal_loss: 0.015063\n",
      "[501/00024] train_loss: 0.020867 kl_loss: 0.199146 normal_loss: 0.014893\n",
      "[502/00049] train_loss: 0.021022 kl_loss: 0.200012 normal_loss: 0.015021\n",
      "[503/00074] train_loss: 0.020960 kl_loss: 0.199438 normal_loss: 0.014977\n",
      "[505/00024] train_loss: 0.020816 kl_loss: 0.198775 normal_loss: 0.014852\n",
      "[506/00049] train_loss: 0.020899 kl_loss: 0.199797 normal_loss: 0.014905\n",
      "[507/00074] train_loss: 0.020932 kl_loss: 0.199611 normal_loss: 0.014944\n",
      "[509/00024] train_loss: 0.020859 kl_loss: 0.199057 normal_loss: 0.014888\n",
      "[510/00049] train_loss: 0.020796 kl_loss: 0.198895 normal_loss: 0.014829\n",
      "[511/00074] train_loss: 0.020952 kl_loss: 0.199830 normal_loss: 0.014957\n",
      "[513/00024] train_loss: 0.020801 kl_loss: 0.199066 normal_loss: 0.014829\n",
      "[514/00049] train_loss: 0.020887 kl_loss: 0.199198 normal_loss: 0.014911\n",
      "[515/00074] train_loss: 0.020924 kl_loss: 0.199105 normal_loss: 0.014951\n",
      "[517/00024] train_loss: 0.020872 kl_loss: 0.199122 normal_loss: 0.014898\n",
      "[518/00049] train_loss: 0.020732 kl_loss: 0.198605 normal_loss: 0.014774\n",
      "[519/00074] train_loss: 0.020852 kl_loss: 0.199239 normal_loss: 0.014875\n",
      "[521/00024] train_loss: 0.020752 kl_loss: 0.198637 normal_loss: 0.014793\n",
      "[522/00049] train_loss: 0.020878 kl_loss: 0.199402 normal_loss: 0.014896\n",
      "[523/00074] train_loss: 0.020779 kl_loss: 0.198507 normal_loss: 0.014824\n",
      "[525/00024] train_loss: 0.020735 kl_loss: 0.198145 normal_loss: 0.014790\n",
      "[526/00049] train_loss: 0.020828 kl_loss: 0.199416 normal_loss: 0.014846\n",
      "[527/00074] train_loss: 0.020745 kl_loss: 0.198575 normal_loss: 0.014788\n",
      "[529/00024] train_loss: 0.020776 kl_loss: 0.198502 normal_loss: 0.014821\n",
      "[530/00049] train_loss: 0.020796 kl_loss: 0.198781 normal_loss: 0.014832\n",
      "[531/00074] train_loss: 0.020779 kl_loss: 0.198412 normal_loss: 0.014827\n",
      "[533/00024] train_loss: 0.020660 kl_loss: 0.198547 normal_loss: 0.014704\n",
      "[534/00049] train_loss: 0.020760 kl_loss: 0.198292 normal_loss: 0.014811\n",
      "[535/00074] train_loss: 0.020770 kl_loss: 0.198420 normal_loss: 0.014818\n",
      "[537/00024] train_loss: 0.020818 kl_loss: 0.198693 normal_loss: 0.014857\n",
      "[538/00049] train_loss: 0.020666 kl_loss: 0.197532 normal_loss: 0.014740\n",
      "[539/00074] train_loss: 0.020751 kl_loss: 0.198600 normal_loss: 0.014793\n",
      "[541/00024] train_loss: 0.020721 kl_loss: 0.198675 normal_loss: 0.014760\n",
      "[542/00049] train_loss: 0.020659 kl_loss: 0.197726 normal_loss: 0.014727\n",
      "[543/00074] train_loss: 0.020734 kl_loss: 0.198003 normal_loss: 0.014794\n",
      "[545/00024] train_loss: 0.020783 kl_loss: 0.198565 normal_loss: 0.014826\n",
      "[546/00049] train_loss: 0.020532 kl_loss: 0.197043 normal_loss: 0.014621\n",
      "[547/00074] train_loss: 0.020791 kl_loss: 0.198400 normal_loss: 0.014839\n",
      "[549/00024] train_loss: 0.020715 kl_loss: 0.198159 normal_loss: 0.014770\n",
      "[550/00049] train_loss: 0.020583 kl_loss: 0.197325 normal_loss: 0.014663\n",
      "[551/00074] train_loss: 0.020652 kl_loss: 0.198122 normal_loss: 0.014709\n",
      "[553/00024] train_loss: 0.020638 kl_loss: 0.197717 normal_loss: 0.014707\n",
      "[554/00049] train_loss: 0.020672 kl_loss: 0.197613 normal_loss: 0.014744\n",
      "[555/00074] train_loss: 0.020659 kl_loss: 0.197814 normal_loss: 0.014725\n",
      "[557/00024] train_loss: 0.020713 kl_loss: 0.197701 normal_loss: 0.014782\n",
      "[558/00049] train_loss: 0.020620 kl_loss: 0.197689 normal_loss: 0.014690\n",
      "[559/00074] train_loss: 0.020685 kl_loss: 0.197318 normal_loss: 0.014766\n",
      "[561/00024] train_loss: 0.020642 kl_loss: 0.197090 normal_loss: 0.014729\n",
      "[562/00049] train_loss: 0.020666 kl_loss: 0.197715 normal_loss: 0.014734\n",
      "[563/00074] train_loss: 0.020689 kl_loss: 0.197489 normal_loss: 0.014765\n",
      "[565/00024] train_loss: 0.020625 kl_loss: 0.197636 normal_loss: 0.014696\n",
      "[566/00049] train_loss: 0.020652 kl_loss: 0.197723 normal_loss: 0.014720\n",
      "[567/00074] train_loss: 0.020619 kl_loss: 0.196559 normal_loss: 0.014723\n",
      "[569/00024] train_loss: 0.020658 kl_loss: 0.196935 normal_loss: 0.014750\n",
      "[570/00049] train_loss: 0.020631 kl_loss: 0.197086 normal_loss: 0.014718\n",
      "[571/00074] train_loss: 0.020648 kl_loss: 0.197531 normal_loss: 0.014722\n",
      "[573/00024] train_loss: 0.020506 kl_loss: 0.196687 normal_loss: 0.014606\n",
      "[574/00049] train_loss: 0.020638 kl_loss: 0.197686 normal_loss: 0.014707\n",
      "[575/00074] train_loss: 0.020640 kl_loss: 0.196786 normal_loss: 0.014736\n",
      "[577/00024] train_loss: 0.020566 kl_loss: 0.196443 normal_loss: 0.014672\n",
      "[578/00049] train_loss: 0.020662 kl_loss: 0.197458 normal_loss: 0.014738\n",
      "[579/00074] train_loss: 0.020643 kl_loss: 0.196904 normal_loss: 0.014736\n",
      "[581/00024] train_loss: 0.020601 kl_loss: 0.196884 normal_loss: 0.014695\n",
      "[582/00049] train_loss: 0.020561 kl_loss: 0.196573 normal_loss: 0.014663\n",
      "[583/00074] train_loss: 0.020628 kl_loss: 0.197005 normal_loss: 0.014718\n",
      "[585/00024] train_loss: 0.020614 kl_loss: 0.197116 normal_loss: 0.014701\n",
      "[586/00049] train_loss: 0.020417 kl_loss: 0.195875 normal_loss: 0.014541\n",
      "[587/00074] train_loss: 0.020604 kl_loss: 0.197111 normal_loss: 0.014690\n",
      "[589/00024] train_loss: 0.020478 kl_loss: 0.196459 normal_loss: 0.014584\n",
      "[590/00049] train_loss: 0.020558 kl_loss: 0.197025 normal_loss: 0.014647\n",
      "[591/00074] train_loss: 0.020477 kl_loss: 0.196214 normal_loss: 0.014590\n",
      "[593/00024] train_loss: 0.020568 kl_loss: 0.196622 normal_loss: 0.014669\n",
      "[594/00049] train_loss: 0.020423 kl_loss: 0.196098 normal_loss: 0.014540\n",
      "[595/00074] train_loss: 0.020649 kl_loss: 0.196596 normal_loss: 0.014751\n",
      "[597/00024] train_loss: 0.020505 kl_loss: 0.196590 normal_loss: 0.014607\n",
      "[598/00049] train_loss: 0.020628 kl_loss: 0.195949 normal_loss: 0.014749\n",
      "[599/00074] train_loss: 0.020505 kl_loss: 0.196435 normal_loss: 0.014612\n",
      "[601/00024] train_loss: 0.020463 kl_loss: 0.196560 normal_loss: 0.014566\n",
      "[602/00049] train_loss: 0.020333 kl_loss: 0.195197 normal_loss: 0.014477\n",
      "[603/00074] train_loss: 0.020581 kl_loss: 0.196936 normal_loss: 0.014673\n",
      "[605/00024] train_loss: 0.020452 kl_loss: 0.196201 normal_loss: 0.014566\n",
      "[606/00049] train_loss: 0.020403 kl_loss: 0.195823 normal_loss: 0.014528\n",
      "[607/00074] train_loss: 0.020441 kl_loss: 0.196481 normal_loss: 0.014546\n",
      "[609/00024] train_loss: 0.020408 kl_loss: 0.195816 normal_loss: 0.014533\n",
      "[610/00049] train_loss: 0.020363 kl_loss: 0.196021 normal_loss: 0.014482\n",
      "[611/00074] train_loss: 0.020517 kl_loss: 0.196472 normal_loss: 0.014623\n",
      "[613/00024] train_loss: 0.020450 kl_loss: 0.195831 normal_loss: 0.014575\n",
      "[614/00049] train_loss: 0.020390 kl_loss: 0.196079 normal_loss: 0.014508\n",
      "[615/00074] train_loss: 0.020427 kl_loss: 0.196207 normal_loss: 0.014540\n",
      "[617/00024] train_loss: 0.020403 kl_loss: 0.196027 normal_loss: 0.014522\n",
      "[618/00049] train_loss: 0.020375 kl_loss: 0.195802 normal_loss: 0.014501\n",
      "[619/00074] train_loss: 0.020464 kl_loss: 0.196102 normal_loss: 0.014581\n",
      "[621/00024] train_loss: 0.020364 kl_loss: 0.196311 normal_loss: 0.014474\n",
      "[622/00049] train_loss: 0.020250 kl_loss: 0.194966 normal_loss: 0.014401\n",
      "[623/00074] train_loss: 0.020480 kl_loss: 0.196445 normal_loss: 0.014587\n",
      "[625/00024] train_loss: 0.020381 kl_loss: 0.195608 normal_loss: 0.014513\n",
      "[626/00049] train_loss: 0.020425 kl_loss: 0.196387 normal_loss: 0.014534\n",
      "[627/00074] train_loss: 0.020383 kl_loss: 0.195525 normal_loss: 0.014517\n",
      "[629/00024] train_loss: 0.020377 kl_loss: 0.195966 normal_loss: 0.014499\n",
      "[630/00049] train_loss: 0.020358 kl_loss: 0.195389 normal_loss: 0.014496\n",
      "[631/00074] train_loss: 0.020303 kl_loss: 0.195968 normal_loss: 0.014424\n",
      "[633/00024] train_loss: 0.020409 kl_loss: 0.195458 normal_loss: 0.014546\n",
      "[634/00049] train_loss: 0.020328 kl_loss: 0.195521 normal_loss: 0.014462\n",
      "[635/00074] train_loss: 0.020389 kl_loss: 0.196140 normal_loss: 0.014505\n",
      "[637/00024] train_loss: 0.020368 kl_loss: 0.195322 normal_loss: 0.014509\n",
      "[638/00049] train_loss: 0.020404 kl_loss: 0.195839 normal_loss: 0.014529\n",
      "[639/00074] train_loss: 0.020421 kl_loss: 0.195762 normal_loss: 0.014549\n",
      "[641/00024] train_loss: 0.020298 kl_loss: 0.195330 normal_loss: 0.014438\n",
      "[642/00049] train_loss: 0.020332 kl_loss: 0.195873 normal_loss: 0.014456\n",
      "[643/00074] train_loss: 0.020370 kl_loss: 0.195514 normal_loss: 0.014505\n",
      "[645/00024] train_loss: 0.020226 kl_loss: 0.195076 normal_loss: 0.014374\n",
      "[646/00049] train_loss: 0.020365 kl_loss: 0.195778 normal_loss: 0.014491\n",
      "[647/00074] train_loss: 0.020325 kl_loss: 0.195656 normal_loss: 0.014455\n",
      "[649/00024] train_loss: 0.020344 kl_loss: 0.195326 normal_loss: 0.014484\n",
      "[650/00049] train_loss: 0.020435 kl_loss: 0.196102 normal_loss: 0.014552\n",
      "[651/00074] train_loss: 0.020225 kl_loss: 0.194869 normal_loss: 0.014379\n",
      "[653/00024] train_loss: 0.020325 kl_loss: 0.194900 normal_loss: 0.014478\n",
      "[654/00049] train_loss: 0.020411 kl_loss: 0.196295 normal_loss: 0.014522\n",
      "[655/00074] train_loss: 0.020269 kl_loss: 0.194889 normal_loss: 0.014423\n",
      "[657/00024] train_loss: 0.020318 kl_loss: 0.195256 normal_loss: 0.014460\n",
      "[658/00049] train_loss: 0.020309 kl_loss: 0.195347 normal_loss: 0.014448\n",
      "[659/00074] train_loss: 0.020316 kl_loss: 0.195272 normal_loss: 0.014458\n",
      "[661/00024] train_loss: 0.020369 kl_loss: 0.195804 normal_loss: 0.014494\n",
      "[662/00049] train_loss: 0.020242 kl_loss: 0.195087 normal_loss: 0.014389\n",
      "[663/00074] train_loss: 0.020282 kl_loss: 0.194765 normal_loss: 0.014439\n",
      "[665/00024] train_loss: 0.020317 kl_loss: 0.194911 normal_loss: 0.014470\n",
      "[666/00049] train_loss: 0.020382 kl_loss: 0.195379 normal_loss: 0.014521\n",
      "[667/00074] train_loss: 0.020295 kl_loss: 0.195174 normal_loss: 0.014440\n",
      "[669/00024] train_loss: 0.020432 kl_loss: 0.195145 normal_loss: 0.014578\n",
      "[670/00049] train_loss: 0.020231 kl_loss: 0.195257 normal_loss: 0.014374\n",
      "[671/00074] train_loss: 0.020365 kl_loss: 0.194878 normal_loss: 0.014519\n",
      "[673/00024] train_loss: 0.020208 kl_loss: 0.194888 normal_loss: 0.014362\n",
      "[674/00049] train_loss: 0.020345 kl_loss: 0.195332 normal_loss: 0.014485\n",
      "[675/00074] train_loss: 0.020351 kl_loss: 0.194861 normal_loss: 0.014505\n",
      "[677/00024] train_loss: 0.020348 kl_loss: 0.195671 normal_loss: 0.014478\n",
      "[678/00049] train_loss: 0.020175 kl_loss: 0.194157 normal_loss: 0.014350\n",
      "[679/00074] train_loss: 0.020266 kl_loss: 0.195066 normal_loss: 0.014414\n",
      "[681/00024] train_loss: 0.020306 kl_loss: 0.195275 normal_loss: 0.014448\n",
      "[682/00049] train_loss: 0.020119 kl_loss: 0.194539 normal_loss: 0.014283\n",
      "[683/00074] train_loss: 0.020323 kl_loss: 0.194875 normal_loss: 0.014477\n",
      "[685/00024] train_loss: 0.020330 kl_loss: 0.195337 normal_loss: 0.014470\n",
      "[686/00049] train_loss: 0.020214 kl_loss: 0.194426 normal_loss: 0.014381\n",
      "[687/00074] train_loss: 0.020275 kl_loss: 0.194727 normal_loss: 0.014433\n",
      "[689/00024] train_loss: 0.020334 kl_loss: 0.194920 normal_loss: 0.014487\n",
      "[690/00049] train_loss: 0.020143 kl_loss: 0.194847 normal_loss: 0.014297\n",
      "[691/00074] train_loss: 0.020235 kl_loss: 0.194524 normal_loss: 0.014399\n",
      "[693/00024] train_loss: 0.020292 kl_loss: 0.195029 normal_loss: 0.014441\n",
      "[694/00049] train_loss: 0.020287 kl_loss: 0.194329 normal_loss: 0.014457\n",
      "[695/00074] train_loss: 0.020258 kl_loss: 0.194739 normal_loss: 0.014416\n",
      "[697/00024] train_loss: 0.020274 kl_loss: 0.194684 normal_loss: 0.014433\n",
      "[698/00049] train_loss: 0.020247 kl_loss: 0.194592 normal_loss: 0.014409\n",
      "[699/00074] train_loss: 0.020213 kl_loss: 0.194635 normal_loss: 0.014374\n",
      "[701/00024] train_loss: 0.020193 kl_loss: 0.194627 normal_loss: 0.014354\n",
      "[702/00049] train_loss: 0.020228 kl_loss: 0.194778 normal_loss: 0.014385\n",
      "[703/00074] train_loss: 0.020176 kl_loss: 0.194354 normal_loss: 0.014345\n",
      "[705/00024] train_loss: 0.020156 kl_loss: 0.194294 normal_loss: 0.014327\n",
      "[706/00049] train_loss: 0.020166 kl_loss: 0.194568 normal_loss: 0.014329\n",
      "[707/00074] train_loss: 0.020253 kl_loss: 0.194793 normal_loss: 0.014409\n",
      "[709/00024] train_loss: 0.020209 kl_loss: 0.194604 normal_loss: 0.014371\n",
      "[710/00049] train_loss: 0.020080 kl_loss: 0.194288 normal_loss: 0.014252\n",
      "[711/00074] train_loss: 0.020297 kl_loss: 0.194669 normal_loss: 0.014457\n",
      "[713/00024] train_loss: 0.020115 kl_loss: 0.193964 normal_loss: 0.014296\n",
      "[714/00049] train_loss: 0.020321 kl_loss: 0.195719 normal_loss: 0.014449\n",
      "[715/00074] train_loss: 0.020084 kl_loss: 0.193777 normal_loss: 0.014270\n",
      "[717/00024] train_loss: 0.020262 kl_loss: 0.194355 normal_loss: 0.014432\n",
      "[718/00049] train_loss: 0.020154 kl_loss: 0.194474 normal_loss: 0.014320\n",
      "[719/00074] train_loss: 0.020237 kl_loss: 0.194539 normal_loss: 0.014401\n",
      "[721/00024] train_loss: 0.020168 kl_loss: 0.194612 normal_loss: 0.014330\n",
      "[722/00049] train_loss: 0.020134 kl_loss: 0.193987 normal_loss: 0.014314\n",
      "[723/00074] train_loss: 0.020162 kl_loss: 0.194678 normal_loss: 0.014321\n",
      "[725/00024] train_loss: 0.020215 kl_loss: 0.194343 normal_loss: 0.014385\n",
      "[726/00049] train_loss: 0.020126 kl_loss: 0.194225 normal_loss: 0.014299\n",
      "[727/00074] train_loss: 0.020178 kl_loss: 0.194607 normal_loss: 0.014340\n",
      "[729/00024] train_loss: 0.020180 kl_loss: 0.194192 normal_loss: 0.014355\n",
      "[730/00049] train_loss: 0.020194 kl_loss: 0.194564 normal_loss: 0.014357\n",
      "[731/00074] train_loss: 0.020175 kl_loss: 0.194315 normal_loss: 0.014345\n",
      "[733/00024] train_loss: 0.020225 kl_loss: 0.194217 normal_loss: 0.014398\n",
      "[734/00049] train_loss: 0.020157 kl_loss: 0.194242 normal_loss: 0.014330\n",
      "[735/00074] train_loss: 0.020213 kl_loss: 0.194517 normal_loss: 0.014378\n",
      "[737/00024] train_loss: 0.020195 kl_loss: 0.194655 normal_loss: 0.014355\n",
      "[738/00049] train_loss: 0.020158 kl_loss: 0.194322 normal_loss: 0.014329\n",
      "[739/00074] train_loss: 0.020168 kl_loss: 0.193908 normal_loss: 0.014351\n",
      "[741/00024] train_loss: 0.020173 kl_loss: 0.194147 normal_loss: 0.014349\n",
      "[742/00049] train_loss: 0.020144 kl_loss: 0.194226 normal_loss: 0.014317\n",
      "[743/00074] train_loss: 0.020207 kl_loss: 0.194410 normal_loss: 0.014374\n",
      "[745/00024] train_loss: 0.020117 kl_loss: 0.194102 normal_loss: 0.014294\n",
      "[746/00049] train_loss: 0.020154 kl_loss: 0.194361 normal_loss: 0.014323\n",
      "[747/00074] train_loss: 0.020240 kl_loss: 0.194211 normal_loss: 0.014413\n",
      "[749/00024] train_loss: 0.020172 kl_loss: 0.194120 normal_loss: 0.014348\n",
      "[750/00049] train_loss: 0.020175 kl_loss: 0.194082 normal_loss: 0.014352\n",
      "[751/00074] train_loss: 0.020171 kl_loss: 0.194377 normal_loss: 0.014339\n",
      "[753/00024] train_loss: 0.020183 kl_loss: 0.194017 normal_loss: 0.014363\n",
      "[754/00049] train_loss: 0.020203 kl_loss: 0.194315 normal_loss: 0.014373\n",
      "[755/00074] train_loss: 0.020218 kl_loss: 0.194158 normal_loss: 0.014393\n",
      "[757/00024] train_loss: 0.020159 kl_loss: 0.194500 normal_loss: 0.014324\n",
      "[758/00049] train_loss: 0.020141 kl_loss: 0.194249 normal_loss: 0.014314\n",
      "[759/00074] train_loss: 0.020099 kl_loss: 0.193649 normal_loss: 0.014290\n",
      "[761/00024] train_loss: 0.020156 kl_loss: 0.194249 normal_loss: 0.014329\n",
      "[762/00049] train_loss: 0.020115 kl_loss: 0.194093 normal_loss: 0.014293\n",
      "[763/00074] train_loss: 0.020153 kl_loss: 0.193960 normal_loss: 0.014334\n",
      "[765/00024] train_loss: 0.020115 kl_loss: 0.194034 normal_loss: 0.014294\n",
      "[766/00049] train_loss: 0.020166 kl_loss: 0.193755 normal_loss: 0.014353\n",
      "[767/00074] train_loss: 0.020179 kl_loss: 0.194418 normal_loss: 0.014347\n",
      "[769/00024] train_loss: 0.020115 kl_loss: 0.194143 normal_loss: 0.014291\n",
      "[770/00049] train_loss: 0.020078 kl_loss: 0.193751 normal_loss: 0.014266\n",
      "[771/00074] train_loss: 0.020155 kl_loss: 0.194224 normal_loss: 0.014328\n",
      "[773/00024] train_loss: 0.020136 kl_loss: 0.194567 normal_loss: 0.014299\n",
      "[774/00049] train_loss: 0.020103 kl_loss: 0.193471 normal_loss: 0.014299\n",
      "[775/00074] train_loss: 0.020082 kl_loss: 0.193979 normal_loss: 0.014263\n",
      "[777/00024] train_loss: 0.020099 kl_loss: 0.194129 normal_loss: 0.014275\n",
      "[778/00049] train_loss: 0.020153 kl_loss: 0.194130 normal_loss: 0.014329\n",
      "[779/00074] train_loss: 0.020080 kl_loss: 0.193648 normal_loss: 0.014271\n",
      "[781/00024] train_loss: 0.020162 kl_loss: 0.193756 normal_loss: 0.014349\n",
      "[782/00049] train_loss: 0.020095 kl_loss: 0.193751 normal_loss: 0.014282\n",
      "[783/00074] train_loss: 0.020104 kl_loss: 0.194299 normal_loss: 0.014275\n",
      "[785/00024] train_loss: 0.020120 kl_loss: 0.193952 normal_loss: 0.014301\n",
      "[786/00049] train_loss: 0.020112 kl_loss: 0.193940 normal_loss: 0.014294\n",
      "[787/00074] train_loss: 0.020107 kl_loss: 0.193816 normal_loss: 0.014292\n",
      "[789/00024] train_loss: 0.020104 kl_loss: 0.193697 normal_loss: 0.014293\n",
      "[790/00049] train_loss: 0.020187 kl_loss: 0.194185 normal_loss: 0.014361\n",
      "[791/00074] train_loss: 0.020160 kl_loss: 0.193733 normal_loss: 0.014348\n",
      "[793/00024] train_loss: 0.020094 kl_loss: 0.193634 normal_loss: 0.014285\n",
      "[794/00049] train_loss: 0.020143 kl_loss: 0.194045 normal_loss: 0.014321\n",
      "[795/00074] train_loss: 0.020111 kl_loss: 0.193844 normal_loss: 0.014296\n",
      "[797/00024] train_loss: 0.020106 kl_loss: 0.193996 normal_loss: 0.014286\n",
      "[798/00049] train_loss: 0.020119 kl_loss: 0.194089 normal_loss: 0.014296\n",
      "[799/00074] train_loss: 0.020094 kl_loss: 0.193340 normal_loss: 0.014294\n",
      "[801/00024] train_loss: 0.020241 kl_loss: 0.194594 normal_loss: 0.014403\n",
      "[802/00049] train_loss: 0.019942 kl_loss: 0.192910 normal_loss: 0.014154\n",
      "[803/00074] train_loss: 0.020150 kl_loss: 0.193846 normal_loss: 0.014335\n",
      "[805/00024] train_loss: 0.020122 kl_loss: 0.193991 normal_loss: 0.014303\n",
      "[806/00049] train_loss: 0.020129 kl_loss: 0.193652 normal_loss: 0.014319\n",
      "[807/00074] train_loss: 0.020058 kl_loss: 0.193663 normal_loss: 0.014248\n",
      "[809/00024] train_loss: 0.020142 kl_loss: 0.193809 normal_loss: 0.014328\n",
      "[810/00049] train_loss: 0.020057 kl_loss: 0.193673 normal_loss: 0.014247\n",
      "[811/00074] train_loss: 0.020098 kl_loss: 0.193776 normal_loss: 0.014285\n",
      "[813/00024] train_loss: 0.020013 kl_loss: 0.193511 normal_loss: 0.014207\n",
      "[814/00049] train_loss: 0.020049 kl_loss: 0.194021 normal_loss: 0.014228\n",
      "[815/00074] train_loss: 0.020065 kl_loss: 0.193674 normal_loss: 0.014254\n",
      "[817/00024] train_loss: 0.020118 kl_loss: 0.194032 normal_loss: 0.014297\n",
      "[818/00049] train_loss: 0.020043 kl_loss: 0.193627 normal_loss: 0.014234\n",
      "[819/00074] train_loss: 0.020028 kl_loss: 0.193493 normal_loss: 0.014223\n",
      "[821/00024] train_loss: 0.020003 kl_loss: 0.193334 normal_loss: 0.014203\n",
      "[822/00049] train_loss: 0.020090 kl_loss: 0.194254 normal_loss: 0.014262\n",
      "[823/00074] train_loss: 0.020026 kl_loss: 0.193510 normal_loss: 0.014220\n",
      "[825/00024] train_loss: 0.020044 kl_loss: 0.193749 normal_loss: 0.014232\n",
      "[826/00049] train_loss: 0.020141 kl_loss: 0.194008 normal_loss: 0.014321\n",
      "[827/00074] train_loss: 0.020092 kl_loss: 0.193290 normal_loss: 0.014293\n",
      "[829/00024] train_loss: 0.019978 kl_loss: 0.193130 normal_loss: 0.014184\n",
      "[830/00049] train_loss: 0.020211 kl_loss: 0.194171 normal_loss: 0.014386\n",
      "[831/00074] train_loss: 0.020040 kl_loss: 0.193695 normal_loss: 0.014229\n",
      "[833/00024] train_loss: 0.019976 kl_loss: 0.193230 normal_loss: 0.014179\n",
      "[834/00049] train_loss: 0.020044 kl_loss: 0.193938 normal_loss: 0.014226\n",
      "[835/00074] train_loss: 0.020124 kl_loss: 0.193775 normal_loss: 0.014310\n",
      "[837/00024] train_loss: 0.019992 kl_loss: 0.193028 normal_loss: 0.014202\n",
      "[838/00049] train_loss: 0.020055 kl_loss: 0.193800 normal_loss: 0.014241\n",
      "[839/00074] train_loss: 0.020079 kl_loss: 0.194061 normal_loss: 0.014258\n",
      "[841/00024] train_loss: 0.020059 kl_loss: 0.193901 normal_loss: 0.014242\n",
      "[842/00049] train_loss: 0.020079 kl_loss: 0.193453 normal_loss: 0.014276\n",
      "[843/00074] train_loss: 0.020075 kl_loss: 0.193484 normal_loss: 0.014270\n",
      "[845/00024] train_loss: 0.020040 kl_loss: 0.193634 normal_loss: 0.014231\n",
      "[846/00049] train_loss: 0.019952 kl_loss: 0.193051 normal_loss: 0.014160\n",
      "[847/00074] train_loss: 0.020088 kl_loss: 0.194106 normal_loss: 0.014265\n",
      "[849/00024] train_loss: 0.020049 kl_loss: 0.193551 normal_loss: 0.014243\n",
      "[850/00049] train_loss: 0.020152 kl_loss: 0.193711 normal_loss: 0.014340\n",
      "[851/00074] train_loss: 0.019991 kl_loss: 0.193477 normal_loss: 0.014187\n",
      "[853/00024] train_loss: 0.020103 kl_loss: 0.193726 normal_loss: 0.014291\n",
      "[854/00049] train_loss: 0.020008 kl_loss: 0.193351 normal_loss: 0.014207\n",
      "[855/00074] train_loss: 0.020097 kl_loss: 0.193612 normal_loss: 0.014289\n",
      "[857/00024] train_loss: 0.020012 kl_loss: 0.193592 normal_loss: 0.014204\n",
      "[858/00049] train_loss: 0.020089 kl_loss: 0.193205 normal_loss: 0.014292\n",
      "[859/00074] train_loss: 0.020084 kl_loss: 0.193842 normal_loss: 0.014269\n",
      "[861/00024] train_loss: 0.020084 kl_loss: 0.193742 normal_loss: 0.014272\n",
      "[862/00049] train_loss: 0.020145 kl_loss: 0.193872 normal_loss: 0.014329\n",
      "[863/00074] train_loss: 0.020014 kl_loss: 0.192979 normal_loss: 0.014224\n",
      "[865/00024] train_loss: 0.019979 kl_loss: 0.193379 normal_loss: 0.014178\n",
      "[866/00049] train_loss: 0.020003 kl_loss: 0.193493 normal_loss: 0.014198\n",
      "[867/00074] train_loss: 0.020026 kl_loss: 0.193673 normal_loss: 0.014216\n",
      "[869/00024] train_loss: 0.020009 kl_loss: 0.193352 normal_loss: 0.014208\n",
      "[870/00049] train_loss: 0.019995 kl_loss: 0.193377 normal_loss: 0.014194\n",
      "[871/00074] train_loss: 0.020136 kl_loss: 0.193763 normal_loss: 0.014323\n",
      "[873/00024] train_loss: 0.020140 kl_loss: 0.193629 normal_loss: 0.014331\n",
      "[874/00049] train_loss: 0.020063 kl_loss: 0.193385 normal_loss: 0.014261\n",
      "[875/00074] train_loss: 0.020063 kl_loss: 0.193434 normal_loss: 0.014260\n",
      "[877/00024] train_loss: 0.020036 kl_loss: 0.193763 normal_loss: 0.014223\n",
      "[878/00049] train_loss: 0.020013 kl_loss: 0.192639 normal_loss: 0.014234\n",
      "[879/00074] train_loss: 0.020074 kl_loss: 0.193996 normal_loss: 0.014254\n",
      "[881/00024] train_loss: 0.019997 kl_loss: 0.193466 normal_loss: 0.014193\n",
      "[882/00049] train_loss: 0.020022 kl_loss: 0.193484 normal_loss: 0.014217\n",
      "[883/00074] train_loss: 0.020013 kl_loss: 0.193396 normal_loss: 0.014211\n",
      "[885/00024] train_loss: 0.020040 kl_loss: 0.193677 normal_loss: 0.014230\n",
      "[886/00049] train_loss: 0.019930 kl_loss: 0.192563 normal_loss: 0.014153\n",
      "[887/00074] train_loss: 0.020117 kl_loss: 0.194054 normal_loss: 0.014295\n",
      "[889/00024] train_loss: 0.020100 kl_loss: 0.193542 normal_loss: 0.014293\n",
      "[890/00049] train_loss: 0.020066 kl_loss: 0.193462 normal_loss: 0.014262\n",
      "[891/00074] train_loss: 0.019901 kl_loss: 0.193240 normal_loss: 0.014104\n",
      "[893/00024] train_loss: 0.020037 kl_loss: 0.193780 normal_loss: 0.014223\n",
      "[894/00049] train_loss: 0.020023 kl_loss: 0.192545 normal_loss: 0.014247\n",
      "[895/00074] train_loss: 0.020092 kl_loss: 0.193865 normal_loss: 0.014276\n",
      "[897/00024] train_loss: 0.020027 kl_loss: 0.193576 normal_loss: 0.014220\n",
      "[898/00049] train_loss: 0.020073 kl_loss: 0.193765 normal_loss: 0.014260\n",
      "[899/00074] train_loss: 0.019953 kl_loss: 0.192802 normal_loss: 0.014169\n",
      "[901/00024] train_loss: 0.019984 kl_loss: 0.193133 normal_loss: 0.014190\n",
      "[902/00049] train_loss: 0.020089 kl_loss: 0.193890 normal_loss: 0.014272\n",
      "[903/00074] train_loss: 0.020027 kl_loss: 0.193080 normal_loss: 0.014235\n",
      "[905/00024] train_loss: 0.020022 kl_loss: 0.193668 normal_loss: 0.014212\n",
      "[906/00049] train_loss: 0.020043 kl_loss: 0.193208 normal_loss: 0.014247\n",
      "[907/00074] train_loss: 0.019957 kl_loss: 0.193203 normal_loss: 0.014161\n",
      "[909/00024] train_loss: 0.020000 kl_loss: 0.193040 normal_loss: 0.014209\n",
      "[910/00049] train_loss: 0.020037 kl_loss: 0.193968 normal_loss: 0.014218\n",
      "[911/00074] train_loss: 0.019981 kl_loss: 0.193045 normal_loss: 0.014189\n",
      "[913/00024] train_loss: 0.019936 kl_loss: 0.193292 normal_loss: 0.014137\n",
      "[914/00049] train_loss: 0.020073 kl_loss: 0.193320 normal_loss: 0.014273\n",
      "[915/00074] train_loss: 0.020028 kl_loss: 0.193415 normal_loss: 0.014226\n",
      "[917/00024] train_loss: 0.019969 kl_loss: 0.192881 normal_loss: 0.014182\n",
      "[918/00049] train_loss: 0.020063 kl_loss: 0.193580 normal_loss: 0.014256\n",
      "[919/00074] train_loss: 0.020016 kl_loss: 0.193540 normal_loss: 0.014210\n",
      "[921/00024] train_loss: 0.019985 kl_loss: 0.193071 normal_loss: 0.014193\n",
      "[922/00049] train_loss: 0.020056 kl_loss: 0.194212 normal_loss: 0.014230\n",
      "[923/00074] train_loss: 0.019983 kl_loss: 0.192692 normal_loss: 0.014202\n",
      "[925/00024] train_loss: 0.020011 kl_loss: 0.193348 normal_loss: 0.014210\n",
      "[926/00049] train_loss: 0.020042 kl_loss: 0.193352 normal_loss: 0.014242\n",
      "[927/00074] train_loss: 0.019968 kl_loss: 0.193249 normal_loss: 0.014170\n",
      "[929/00024] train_loss: 0.020015 kl_loss: 0.193404 normal_loss: 0.014213\n",
      "[930/00049] train_loss: 0.020019 kl_loss: 0.193408 normal_loss: 0.014217\n",
      "[931/00074] train_loss: 0.019960 kl_loss: 0.193112 normal_loss: 0.014167\n",
      "[933/00024] train_loss: 0.020011 kl_loss: 0.193215 normal_loss: 0.014215\n",
      "[934/00049] train_loss: 0.020048 kl_loss: 0.193000 normal_loss: 0.014258\n",
      "[935/00074] train_loss: 0.020045 kl_loss: 0.193684 normal_loss: 0.014234\n",
      "[937/00024] train_loss: 0.020024 kl_loss: 0.193692 normal_loss: 0.014213\n",
      "[938/00049] train_loss: 0.019959 kl_loss: 0.192996 normal_loss: 0.014169\n",
      "[939/00074] train_loss: 0.020020 kl_loss: 0.193185 normal_loss: 0.014224\n",
      "[941/00024] train_loss: 0.019975 kl_loss: 0.193138 normal_loss: 0.014181\n",
      "[942/00049] train_loss: 0.020161 kl_loss: 0.194152 normal_loss: 0.014336\n",
      "[943/00074] train_loss: 0.019945 kl_loss: 0.192558 normal_loss: 0.014168\n",
      "[945/00024] train_loss: 0.020017 kl_loss: 0.193436 normal_loss: 0.014214\n",
      "[946/00049] train_loss: 0.019974 kl_loss: 0.193163 normal_loss: 0.014180\n",
      "[947/00074] train_loss: 0.020027 kl_loss: 0.193224 normal_loss: 0.014230\n",
      "[949/00024] train_loss: 0.019964 kl_loss: 0.193080 normal_loss: 0.014171\n",
      "[950/00049] train_loss: 0.020010 kl_loss: 0.193575 normal_loss: 0.014203\n",
      "[951/00074] train_loss: 0.020001 kl_loss: 0.193143 normal_loss: 0.014207\n",
      "[953/00024] train_loss: 0.020018 kl_loss: 0.193378 normal_loss: 0.014217\n",
      "[954/00049] train_loss: 0.019995 kl_loss: 0.193140 normal_loss: 0.014201\n",
      "[955/00074] train_loss: 0.020038 kl_loss: 0.193256 normal_loss: 0.014241\n",
      "[957/00024] train_loss: 0.020070 kl_loss: 0.193412 normal_loss: 0.014267\n",
      "[958/00049] train_loss: 0.019998 kl_loss: 0.193447 normal_loss: 0.014194\n",
      "[959/00074] train_loss: 0.019898 kl_loss: 0.192888 normal_loss: 0.014112\n",
      "[961/00024] train_loss: 0.020041 kl_loss: 0.193520 normal_loss: 0.014235\n",
      "[962/00049] train_loss: 0.020066 kl_loss: 0.193122 normal_loss: 0.014272\n",
      "[963/00074] train_loss: 0.019951 kl_loss: 0.193080 normal_loss: 0.014159\n",
      "[965/00024] train_loss: 0.019992 kl_loss: 0.193377 normal_loss: 0.014191\n",
      "[966/00049] train_loss: 0.020081 kl_loss: 0.192797 normal_loss: 0.014298\n",
      "[967/00074] train_loss: 0.020029 kl_loss: 0.193526 normal_loss: 0.014223\n",
      "[969/00024] train_loss: 0.019948 kl_loss: 0.192808 normal_loss: 0.014164\n",
      "[970/00049] train_loss: 0.020094 kl_loss: 0.193778 normal_loss: 0.014280\n",
      "[971/00074] train_loss: 0.019986 kl_loss: 0.193090 normal_loss: 0.014194\n",
      "[973/00024] train_loss: 0.020022 kl_loss: 0.193404 normal_loss: 0.014220\n",
      "[974/00049] train_loss: 0.020069 kl_loss: 0.193760 normal_loss: 0.014256\n",
      "[975/00074] train_loss: 0.019924 kl_loss: 0.192485 normal_loss: 0.014150\n",
      "[977/00024] train_loss: 0.019879 kl_loss: 0.192911 normal_loss: 0.014091\n",
      "[978/00049] train_loss: 0.019998 kl_loss: 0.193281 normal_loss: 0.014199\n",
      "[979/00074] train_loss: 0.020127 kl_loss: 0.193433 normal_loss: 0.014324\n",
      "[981/00024] train_loss: 0.020012 kl_loss: 0.193133 normal_loss: 0.014218\n",
      "[982/00049] train_loss: 0.020021 kl_loss: 0.193629 normal_loss: 0.014212\n",
      "[983/00074] train_loss: 0.019990 kl_loss: 0.192839 normal_loss: 0.014205\n",
      "[985/00024] train_loss: 0.020006 kl_loss: 0.193032 normal_loss: 0.014215\n",
      "[986/00049] train_loss: 0.020066 kl_loss: 0.193607 normal_loss: 0.014258\n",
      "[987/00074] train_loss: 0.019985 kl_loss: 0.192938 normal_loss: 0.014197\n",
      "[989/00024] train_loss: 0.020090 kl_loss: 0.193012 normal_loss: 0.014300\n",
      "[990/00049] train_loss: 0.019922 kl_loss: 0.193236 normal_loss: 0.014125\n",
      "[991/00074] train_loss: 0.020017 kl_loss: 0.193304 normal_loss: 0.014218\n",
      "[993/00024] train_loss: 0.019976 kl_loss: 0.193014 normal_loss: 0.014185\n",
      "[994/00049] train_loss: 0.019999 kl_loss: 0.193457 normal_loss: 0.014196\n",
      "[995/00074] train_loss: 0.019977 kl_loss: 0.193056 normal_loss: 0.014185\n",
      "[997/00024] train_loss: 0.020016 kl_loss: 0.193137 normal_loss: 0.014222\n",
      "[998/00049] train_loss: 0.020060 kl_loss: 0.193294 normal_loss: 0.014261\n",
      "[999/00074] train_loss: 0.020022 kl_loss: 0.193071 normal_loss: 0.014230\n"
     ]
    }
   ],
   "source": [
    "# TABLE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'table_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.01,\n",
    "    'resume_ckpt': 'table_vad',\n",
    "    'filter_class': 'table',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Data length 3236\n",
      "Training params: 3\n",
      "[001/00048] train_loss: 0.123281 kl_loss: 0.483260 normal_loss: 0.108783\n",
      "[003/00046] train_loss: 0.058216 kl_loss: 0.408448 normal_loss: 0.045963\n",
      "[005/00044] train_loss: 0.048277 kl_loss: 0.339225 normal_loss: 0.038100\n",
      "[007/00042] train_loss: 0.043332 kl_loss: 0.281702 normal_loss: 0.034881\n",
      "[009/00040] train_loss: 0.040835 kl_loss: 0.237563 normal_loss: 0.033708\n",
      "[011/00038] train_loss: 0.037442 kl_loss: 0.206272 normal_loss: 0.031254\n",
      "[013/00036] train_loss: 0.037738 kl_loss: 0.180581 normal_loss: 0.032321\n",
      "[015/00034] train_loss: 0.037525 kl_loss: 0.159066 normal_loss: 0.032753\n",
      "[017/00032] train_loss: 0.035151 kl_loss: 0.138642 normal_loss: 0.030991\n",
      "[019/00030] train_loss: 0.032855 kl_loss: 0.121132 normal_loss: 0.029221\n",
      "[021/00028] train_loss: 0.031596 kl_loss: 0.106062 normal_loss: 0.028415\n",
      "[023/00026] train_loss: 0.031367 kl_loss: 0.093590 normal_loss: 0.028559\n",
      "[025/00024] train_loss: 0.030193 kl_loss: 0.084878 normal_loss: 0.027646\n",
      "[027/00022] train_loss: 0.028837 kl_loss: 0.079463 normal_loss: 0.026453\n",
      "[029/00020] train_loss: 0.028069 kl_loss: 0.074174 normal_loss: 0.025843\n",
      "[031/00018] train_loss: 0.027340 kl_loss: 0.070937 normal_loss: 0.025212\n",
      "[033/00016] train_loss: 0.027017 kl_loss: 0.068229 normal_loss: 0.024971\n",
      "[035/00014] train_loss: 0.026327 kl_loss: 0.066868 normal_loss: 0.024321\n",
      "[037/00012] train_loss: 0.025666 kl_loss: 0.066231 normal_loss: 0.023679\n",
      "[039/00010] train_loss: 0.025655 kl_loss: 0.065307 normal_loss: 0.023696\n",
      "[041/00008] train_loss: 0.025184 kl_loss: 0.065364 normal_loss: 0.023223\n",
      "[043/00006] train_loss: 0.024622 kl_loss: 0.064790 normal_loss: 0.022679\n",
      "[045/00004] train_loss: 0.023864 kl_loss: 0.064390 normal_loss: 0.021933\n",
      "[047/00002] train_loss: 0.023980 kl_loss: 0.064336 normal_loss: 0.022050\n",
      "[049/00000] train_loss: 0.023291 kl_loss: 0.064666 normal_loss: 0.021351\n",
      "[050/00049] train_loss: 0.023538 kl_loss: 0.065183 normal_loss: 0.021582\n",
      "[052/00047] train_loss: 0.023211 kl_loss: 0.065701 normal_loss: 0.021240\n",
      "[054/00045] train_loss: 0.022869 kl_loss: 0.066324 normal_loss: 0.020879\n",
      "[056/00043] train_loss: 0.022078 kl_loss: 0.066855 normal_loss: 0.020073\n",
      "[058/00041] train_loss: 0.022401 kl_loss: 0.067648 normal_loss: 0.020371\n",
      "[060/00039] train_loss: 0.022297 kl_loss: 0.068219 normal_loss: 0.020251\n",
      "[062/00037] train_loss: 0.022074 kl_loss: 0.069174 normal_loss: 0.019999\n",
      "[064/00035] train_loss: 0.021796 kl_loss: 0.070519 normal_loss: 0.019681\n",
      "[066/00033] train_loss: 0.021495 kl_loss: 0.070835 normal_loss: 0.019370\n",
      "[068/00031] train_loss: 0.021391 kl_loss: 0.071516 normal_loss: 0.019245\n",
      "[070/00029] train_loss: 0.021168 kl_loss: 0.072700 normal_loss: 0.018987\n",
      "[072/00027] train_loss: 0.020404 kl_loss: 0.073127 normal_loss: 0.018210\n",
      "[074/00025] train_loss: 0.020932 kl_loss: 0.073822 normal_loss: 0.018718\n",
      "[076/00023] train_loss: 0.020752 kl_loss: 0.074692 normal_loss: 0.018511\n",
      "[078/00021] train_loss: 0.020451 kl_loss: 0.075436 normal_loss: 0.018188\n",
      "[080/00019] train_loss: 0.020260 kl_loss: 0.077184 normal_loss: 0.017945\n",
      "[082/00017] train_loss: 0.020579 kl_loss: 0.078270 normal_loss: 0.018231\n",
      "[084/00015] train_loss: 0.020485 kl_loss: 0.079256 normal_loss: 0.018107\n",
      "[086/00013] train_loss: 0.020216 kl_loss: 0.080080 normal_loss: 0.017814\n",
      "[088/00011] train_loss: 0.020044 kl_loss: 0.081005 normal_loss: 0.017614\n",
      "[090/00009] train_loss: 0.019958 kl_loss: 0.082519 normal_loss: 0.017482\n",
      "[092/00007] train_loss: 0.019611 kl_loss: 0.082833 normal_loss: 0.017126\n",
      "[094/00005] train_loss: 0.019396 kl_loss: 0.082805 normal_loss: 0.016912\n",
      "[096/00003] train_loss: 0.019586 kl_loss: 0.082925 normal_loss: 0.017098\n",
      "[098/00001] train_loss: 0.019539 kl_loss: 0.083698 normal_loss: 0.017028\n",
      "[099/00050] train_loss: 0.019431 kl_loss: 0.084527 normal_loss: 0.016895\n",
      "[101/00048] train_loss: 0.016137 kl_loss: 0.084342 normal_loss: 0.013607\n",
      "[103/00046] train_loss: 0.015878 kl_loss: 0.081747 normal_loss: 0.013425\n",
      "[105/00044] train_loss: 0.016311 kl_loss: 0.080375 normal_loss: 0.013899\n",
      "[107/00042] train_loss: 0.016164 kl_loss: 0.078614 normal_loss: 0.013806\n",
      "[109/00040] train_loss: 0.015907 kl_loss: 0.077417 normal_loss: 0.013585\n",
      "[111/00038] train_loss: 0.016142 kl_loss: 0.076648 normal_loss: 0.013843\n",
      "[113/00036] train_loss: 0.015895 kl_loss: 0.075597 normal_loss: 0.013627\n",
      "[115/00034] train_loss: 0.015558 kl_loss: 0.075616 normal_loss: 0.013290\n",
      "[117/00032] train_loss: 0.015798 kl_loss: 0.075015 normal_loss: 0.013548\n",
      "[119/00030] train_loss: 0.015943 kl_loss: 0.075278 normal_loss: 0.013685\n",
      "[121/00028] train_loss: 0.015878 kl_loss: 0.074895 normal_loss: 0.013631\n",
      "[123/00026] train_loss: 0.015787 kl_loss: 0.074942 normal_loss: 0.013539\n",
      "[125/00024] train_loss: 0.015714 kl_loss: 0.075259 normal_loss: 0.013456\n",
      "[127/00022] train_loss: 0.015694 kl_loss: 0.075069 normal_loss: 0.013442\n",
      "[129/00020] train_loss: 0.015743 kl_loss: 0.075456 normal_loss: 0.013479\n",
      "[131/00018] train_loss: 0.015672 kl_loss: 0.075651 normal_loss: 0.013402\n",
      "[133/00016] train_loss: 0.015384 kl_loss: 0.075740 normal_loss: 0.013112\n",
      "[135/00014] train_loss: 0.015309 kl_loss: 0.075951 normal_loss: 0.013030\n",
      "[137/00012] train_loss: 0.015617 kl_loss: 0.076199 normal_loss: 0.013331\n",
      "[139/00010] train_loss: 0.015624 kl_loss: 0.076195 normal_loss: 0.013338\n",
      "[141/00008] train_loss: 0.015546 kl_loss: 0.076654 normal_loss: 0.013247\n",
      "[143/00006] train_loss: 0.015394 kl_loss: 0.076934 normal_loss: 0.013086\n",
      "[145/00004] train_loss: 0.015283 kl_loss: 0.077193 normal_loss: 0.012967\n",
      "[147/00002] train_loss: 0.015167 kl_loss: 0.077352 normal_loss: 0.012846\n",
      "[149/00000] train_loss: 0.015349 kl_loss: 0.077572 normal_loss: 0.013022\n",
      "[150/00049] train_loss: 0.015362 kl_loss: 0.077925 normal_loss: 0.013025\n",
      "[152/00047] train_loss: 0.015312 kl_loss: 0.078197 normal_loss: 0.012966\n",
      "[154/00045] train_loss: 0.015367 kl_loss: 0.078893 normal_loss: 0.013000\n",
      "[156/00043] train_loss: 0.015003 kl_loss: 0.079300 normal_loss: 0.012624\n",
      "[158/00041] train_loss: 0.015073 kl_loss: 0.079605 normal_loss: 0.012684\n",
      "[160/00039] train_loss: 0.015159 kl_loss: 0.080134 normal_loss: 0.012755\n",
      "[162/00037] train_loss: 0.015079 kl_loss: 0.080053 normal_loss: 0.012677\n",
      "[164/00035] train_loss: 0.014880 kl_loss: 0.080438 normal_loss: 0.012467\n",
      "[166/00033] train_loss: 0.015061 kl_loss: 0.080238 normal_loss: 0.012654\n",
      "[168/00031] train_loss: 0.015001 kl_loss: 0.080687 normal_loss: 0.012581\n",
      "[170/00029] train_loss: 0.014903 kl_loss: 0.080848 normal_loss: 0.012478\n",
      "[172/00027] train_loss: 0.014898 kl_loss: 0.081430 normal_loss: 0.012455\n",
      "[174/00025] train_loss: 0.014949 kl_loss: 0.081818 normal_loss: 0.012495\n",
      "[176/00023] train_loss: 0.014538 kl_loss: 0.081797 normal_loss: 0.012085\n",
      "[178/00021] train_loss: 0.014703 kl_loss: 0.081721 normal_loss: 0.012251\n",
      "[180/00019] train_loss: 0.014880 kl_loss: 0.082627 normal_loss: 0.012401\n",
      "[182/00017] train_loss: 0.014784 kl_loss: 0.082875 normal_loss: 0.012298\n",
      "[184/00015] train_loss: 0.014759 kl_loss: 0.082948 normal_loss: 0.012271\n",
      "[186/00013] train_loss: 0.014540 kl_loss: 0.083153 normal_loss: 0.012046\n",
      "[188/00011] train_loss: 0.014684 kl_loss: 0.083610 normal_loss: 0.012176\n",
      "[190/00009] train_loss: 0.014513 kl_loss: 0.083187 normal_loss: 0.012018\n",
      "[192/00007] train_loss: 0.014597 kl_loss: 0.083941 normal_loss: 0.012079\n",
      "[194/00005] train_loss: 0.014540 kl_loss: 0.083986 normal_loss: 0.012021\n",
      "[196/00003] train_loss: 0.014529 kl_loss: 0.084212 normal_loss: 0.012002\n",
      "[198/00001] train_loss: 0.014417 kl_loss: 0.084217 normal_loss: 0.011891\n",
      "[199/00050] train_loss: 0.014301 kl_loss: 0.084539 normal_loss: 0.011764\n",
      "[201/00048] train_loss: 0.012898 kl_loss: 0.084843 normal_loss: 0.010353\n",
      "[203/00046] train_loss: 0.012806 kl_loss: 0.084286 normal_loss: 0.010277\n",
      "[205/00044] train_loss: 0.012837 kl_loss: 0.083817 normal_loss: 0.010323\n",
      "[207/00042] train_loss: 0.012890 kl_loss: 0.083879 normal_loss: 0.010373\n",
      "[209/00040] train_loss: 0.012567 kl_loss: 0.082784 normal_loss: 0.010083\n",
      "[211/00038] train_loss: 0.012791 kl_loss: 0.083246 normal_loss: 0.010294\n",
      "[213/00036] train_loss: 0.012531 kl_loss: 0.082665 normal_loss: 0.010051\n",
      "[215/00034] train_loss: 0.012784 kl_loss: 0.082080 normal_loss: 0.010322\n",
      "[217/00032] train_loss: 0.012708 kl_loss: 0.081745 normal_loss: 0.010255\n",
      "[219/00030] train_loss: 0.012722 kl_loss: 0.082180 normal_loss: 0.010257\n",
      "[221/00028] train_loss: 0.012745 kl_loss: 0.081655 normal_loss: 0.010296\n",
      "[223/00026] train_loss: 0.012527 kl_loss: 0.081535 normal_loss: 0.010081\n",
      "[225/00024] train_loss: 0.012570 kl_loss: 0.081441 normal_loss: 0.010127\n",
      "[227/00022] train_loss: 0.012582 kl_loss: 0.081458 normal_loss: 0.010138\n",
      "[229/00020] train_loss: 0.012546 kl_loss: 0.081220 normal_loss: 0.010110\n",
      "[231/00018] train_loss: 0.012659 kl_loss: 0.081104 normal_loss: 0.010225\n",
      "[233/00016] train_loss: 0.012447 kl_loss: 0.080595 normal_loss: 0.010029\n",
      "[235/00014] train_loss: 0.012611 kl_loss: 0.081027 normal_loss: 0.010181\n",
      "[237/00012] train_loss: 0.012478 kl_loss: 0.080871 normal_loss: 0.010052\n",
      "[239/00010] train_loss: 0.012246 kl_loss: 0.080841 normal_loss: 0.009820\n",
      "[241/00008] train_loss: 0.012441 kl_loss: 0.080733 normal_loss: 0.010019\n",
      "[243/00006] train_loss: 0.012459 kl_loss: 0.080782 normal_loss: 0.010035\n",
      "[245/00004] train_loss: 0.012373 kl_loss: 0.080956 normal_loss: 0.009945\n",
      "[247/00002] train_loss: 0.012549 kl_loss: 0.080865 normal_loss: 0.010123\n",
      "[249/00000] train_loss: 0.012500 kl_loss: 0.080889 normal_loss: 0.010074\n",
      "[250/00049] train_loss: 0.012234 kl_loss: 0.080982 normal_loss: 0.009805\n",
      "[252/00047] train_loss: 0.012366 kl_loss: 0.081033 normal_loss: 0.009935\n",
      "[254/00045] train_loss: 0.012365 kl_loss: 0.081088 normal_loss: 0.009932\n",
      "[256/00043] train_loss: 0.012298 kl_loss: 0.081213 normal_loss: 0.009862\n",
      "[258/00041] train_loss: 0.012277 kl_loss: 0.081401 normal_loss: 0.009835\n",
      "[260/00039] train_loss: 0.012212 kl_loss: 0.081385 normal_loss: 0.009770\n",
      "[262/00037] train_loss: 0.012208 kl_loss: 0.081452 normal_loss: 0.009764\n",
      "[264/00035] train_loss: 0.012232 kl_loss: 0.081642 normal_loss: 0.009782\n",
      "[266/00033] train_loss: 0.012309 kl_loss: 0.081664 normal_loss: 0.009859\n",
      "[268/00031] train_loss: 0.012265 kl_loss: 0.081677 normal_loss: 0.009815\n",
      "[270/00029] train_loss: 0.012293 kl_loss: 0.081754 normal_loss: 0.009841\n",
      "[272/00027] train_loss: 0.012253 kl_loss: 0.082282 normal_loss: 0.009785\n",
      "[274/00025] train_loss: 0.012159 kl_loss: 0.081988 normal_loss: 0.009699\n",
      "[276/00023] train_loss: 0.012072 kl_loss: 0.082235 normal_loss: 0.009605\n",
      "[278/00021] train_loss: 0.012047 kl_loss: 0.082258 normal_loss: 0.009579\n",
      "[280/00019] train_loss: 0.011902 kl_loss: 0.082081 normal_loss: 0.009440\n",
      "[282/00017] train_loss: 0.012177 kl_loss: 0.082685 normal_loss: 0.009697\n",
      "[284/00015] train_loss: 0.011821 kl_loss: 0.082825 normal_loss: 0.009337\n",
      "[286/00013] train_loss: 0.012198 kl_loss: 0.082548 normal_loss: 0.009722\n",
      "[288/00011] train_loss: 0.012034 kl_loss: 0.082828 normal_loss: 0.009549\n",
      "[290/00009] train_loss: 0.012008 kl_loss: 0.082803 normal_loss: 0.009524\n",
      "[292/00007] train_loss: 0.012070 kl_loss: 0.082984 normal_loss: 0.009580\n",
      "[294/00005] train_loss: 0.011988 kl_loss: 0.082841 normal_loss: 0.009503\n",
      "[296/00003] train_loss: 0.011808 kl_loss: 0.083123 normal_loss: 0.009314\n",
      "[298/00001] train_loss: 0.011867 kl_loss: 0.083315 normal_loss: 0.009368\n",
      "[299/00050] train_loss: 0.012009 kl_loss: 0.083330 normal_loss: 0.009509\n",
      "[301/00048] train_loss: 0.011305 kl_loss: 0.083321 normal_loss: 0.008806\n",
      "[303/00046] train_loss: 0.011210 kl_loss: 0.083282 normal_loss: 0.008712\n",
      "[305/00044] train_loss: 0.011153 kl_loss: 0.083381 normal_loss: 0.008651\n",
      "[307/00042] train_loss: 0.011087 kl_loss: 0.082742 normal_loss: 0.008605\n",
      "[309/00040] train_loss: 0.011132 kl_loss: 0.082944 normal_loss: 0.008644\n",
      "[311/00038] train_loss: 0.011118 kl_loss: 0.083172 normal_loss: 0.008623\n",
      "[313/00036] train_loss: 0.011042 kl_loss: 0.082823 normal_loss: 0.008557\n",
      "[315/00034] train_loss: 0.011144 kl_loss: 0.082639 normal_loss: 0.008664\n",
      "[317/00032] train_loss: 0.011094 kl_loss: 0.082820 normal_loss: 0.008609\n",
      "[319/00030] train_loss: 0.011039 kl_loss: 0.082581 normal_loss: 0.008562\n",
      "[321/00028] train_loss: 0.011162 kl_loss: 0.082640 normal_loss: 0.008682\n",
      "[323/00026] train_loss: 0.011087 kl_loss: 0.082165 normal_loss: 0.008622\n",
      "[325/00024] train_loss: 0.011173 kl_loss: 0.082919 normal_loss: 0.008686\n",
      "[327/00022] train_loss: 0.011047 kl_loss: 0.082457 normal_loss: 0.008573\n",
      "[329/00020] train_loss: 0.010921 kl_loss: 0.082187 normal_loss: 0.008455\n",
      "[331/00018] train_loss: 0.010975 kl_loss: 0.082076 normal_loss: 0.008513\n",
      "[333/00016] train_loss: 0.011085 kl_loss: 0.082660 normal_loss: 0.008605\n",
      "[335/00014] train_loss: 0.010997 kl_loss: 0.082128 normal_loss: 0.008533\n",
      "[337/00012] train_loss: 0.011039 kl_loss: 0.082295 normal_loss: 0.008570\n",
      "[339/00010] train_loss: 0.010953 kl_loss: 0.082055 normal_loss: 0.008492\n",
      "[341/00008] train_loss: 0.010951 kl_loss: 0.082263 normal_loss: 0.008483\n",
      "[343/00006] train_loss: 0.010946 kl_loss: 0.082347 normal_loss: 0.008475\n",
      "[345/00004] train_loss: 0.010889 kl_loss: 0.081916 normal_loss: 0.008431\n",
      "[347/00002] train_loss: 0.010956 kl_loss: 0.082466 normal_loss: 0.008483\n",
      "[349/00000] train_loss: 0.010915 kl_loss: 0.081934 normal_loss: 0.008457\n",
      "[350/00049] train_loss: 0.010931 kl_loss: 0.082108 normal_loss: 0.008468\n",
      "[352/00047] train_loss: 0.010950 kl_loss: 0.082090 normal_loss: 0.008487\n",
      "[354/00045] train_loss: 0.010855 kl_loss: 0.082147 normal_loss: 0.008391\n",
      "[356/00043] train_loss: 0.010951 kl_loss: 0.081921 normal_loss: 0.008494\n",
      "[358/00041] train_loss: 0.010938 kl_loss: 0.082137 normal_loss: 0.008474\n",
      "[360/00039] train_loss: 0.010778 kl_loss: 0.081735 normal_loss: 0.008326\n",
      "[362/00037] train_loss: 0.010841 kl_loss: 0.082320 normal_loss: 0.008371\n",
      "[364/00035] train_loss: 0.010814 kl_loss: 0.081935 normal_loss: 0.008356\n",
      "[366/00033] train_loss: 0.010836 kl_loss: 0.082051 normal_loss: 0.008374\n",
      "[368/00031] train_loss: 0.010844 kl_loss: 0.082195 normal_loss: 0.008378\n",
      "[370/00029] train_loss: 0.010758 kl_loss: 0.082535 normal_loss: 0.008282\n",
      "[372/00027] train_loss: 0.010809 kl_loss: 0.081936 normal_loss: 0.008351\n",
      "[374/00025] train_loss: 0.010768 kl_loss: 0.082159 normal_loss: 0.008303\n",
      "[376/00023] train_loss: 0.010772 kl_loss: 0.082418 normal_loss: 0.008299\n",
      "[378/00021] train_loss: 0.010816 kl_loss: 0.082187 normal_loss: 0.008351\n",
      "[380/00019] train_loss: 0.010743 kl_loss: 0.082393 normal_loss: 0.008272\n",
      "[382/00017] train_loss: 0.010733 kl_loss: 0.081946 normal_loss: 0.008275\n",
      "[384/00015] train_loss: 0.010811 kl_loss: 0.082667 normal_loss: 0.008331\n",
      "[386/00013] train_loss: 0.010755 kl_loss: 0.082052 normal_loss: 0.008293\n",
      "[388/00011] train_loss: 0.010701 kl_loss: 0.082310 normal_loss: 0.008232\n",
      "[390/00009] train_loss: 0.010617 kl_loss: 0.082543 normal_loss: 0.008140\n",
      "[392/00007] train_loss: 0.010770 kl_loss: 0.082226 normal_loss: 0.008303\n",
      "[394/00005] train_loss: 0.010734 kl_loss: 0.082398 normal_loss: 0.008263\n",
      "[396/00003] train_loss: 0.010807 kl_loss: 0.082431 normal_loss: 0.008334\n",
      "[398/00001] train_loss: 0.010626 kl_loss: 0.082630 normal_loss: 0.008147\n",
      "[399/00050] train_loss: 0.010628 kl_loss: 0.082610 normal_loss: 0.008150\n",
      "[401/00048] train_loss: 0.010439 kl_loss: 0.082619 normal_loss: 0.007960\n",
      "[403/00046] train_loss: 0.010366 kl_loss: 0.082825 normal_loss: 0.007881\n",
      "[405/00044] train_loss: 0.010366 kl_loss: 0.082502 normal_loss: 0.007891\n",
      "[407/00042] train_loss: 0.010267 kl_loss: 0.082202 normal_loss: 0.007801\n",
      "[409/00040] train_loss: 0.010429 kl_loss: 0.083010 normal_loss: 0.007939\n",
      "[411/00038] train_loss: 0.010352 kl_loss: 0.082463 normal_loss: 0.007878\n",
      "[413/00036] train_loss: 0.010351 kl_loss: 0.082272 normal_loss: 0.007883\n",
      "[415/00034] train_loss: 0.010300 kl_loss: 0.082472 normal_loss: 0.007826\n",
      "[417/00032] train_loss: 0.010340 kl_loss: 0.082407 normal_loss: 0.007868\n",
      "[419/00030] train_loss: 0.010308 kl_loss: 0.082491 normal_loss: 0.007833\n",
      "[421/00028] train_loss: 0.010295 kl_loss: 0.082026 normal_loss: 0.007835\n",
      "[423/00026] train_loss: 0.010397 kl_loss: 0.082963 normal_loss: 0.007908\n",
      "[425/00024] train_loss: 0.010383 kl_loss: 0.082390 normal_loss: 0.007911\n",
      "[427/00022] train_loss: 0.010266 kl_loss: 0.082357 normal_loss: 0.007795\n",
      "[429/00020] train_loss: 0.010275 kl_loss: 0.082406 normal_loss: 0.007803\n",
      "[431/00018] train_loss: 0.010269 kl_loss: 0.082375 normal_loss: 0.007797\n",
      "[433/00016] train_loss: 0.010274 kl_loss: 0.082137 normal_loss: 0.007810\n",
      "[435/00014] train_loss: 0.010288 kl_loss: 0.082295 normal_loss: 0.007819\n",
      "[437/00012] train_loss: 0.010339 kl_loss: 0.082204 normal_loss: 0.007873\n",
      "[439/00010] train_loss: 0.010235 kl_loss: 0.082479 normal_loss: 0.007761\n",
      "[441/00008] train_loss: 0.010263 kl_loss: 0.082046 normal_loss: 0.007801\n",
      "[443/00006] train_loss: 0.010239 kl_loss: 0.082388 normal_loss: 0.007768\n",
      "[445/00004] train_loss: 0.010292 kl_loss: 0.082202 normal_loss: 0.007826\n",
      "[447/00002] train_loss: 0.010208 kl_loss: 0.082265 normal_loss: 0.007740\n",
      "[449/00000] train_loss: 0.010182 kl_loss: 0.082204 normal_loss: 0.007716\n",
      "[450/00049] train_loss: 0.010220 kl_loss: 0.082270 normal_loss: 0.007752\n",
      "[452/00047] train_loss: 0.010258 kl_loss: 0.082167 normal_loss: 0.007793\n",
      "[454/00045] train_loss: 0.010173 kl_loss: 0.081949 normal_loss: 0.007715\n",
      "[456/00043] train_loss: 0.010227 kl_loss: 0.082584 normal_loss: 0.007750\n",
      "[458/00041] train_loss: 0.010252 kl_loss: 0.082348 normal_loss: 0.007781\n",
      "[460/00039] train_loss: 0.010209 kl_loss: 0.081847 normal_loss: 0.007753\n",
      "[462/00037] train_loss: 0.010209 kl_loss: 0.082545 normal_loss: 0.007733\n",
      "[464/00035] train_loss: 0.010115 kl_loss: 0.082063 normal_loss: 0.007653\n",
      "[466/00033] train_loss: 0.010147 kl_loss: 0.082298 normal_loss: 0.007679\n",
      "[468/00031] train_loss: 0.010163 kl_loss: 0.082199 normal_loss: 0.007697\n",
      "[470/00029] train_loss: 0.010151 kl_loss: 0.082330 normal_loss: 0.007681\n",
      "[472/00027] train_loss: 0.010182 kl_loss: 0.082072 normal_loss: 0.007720\n",
      "[474/00025] train_loss: 0.010166 kl_loss: 0.082367 normal_loss: 0.007695\n",
      "[476/00023] train_loss: 0.010150 kl_loss: 0.082304 normal_loss: 0.007681\n",
      "[478/00021] train_loss: 0.010177 kl_loss: 0.082148 normal_loss: 0.007712\n",
      "[480/00019] train_loss: 0.010110 kl_loss: 0.082136 normal_loss: 0.007646\n",
      "[482/00017] train_loss: 0.010108 kl_loss: 0.081939 normal_loss: 0.007649\n",
      "[484/00015] train_loss: 0.010099 kl_loss: 0.082169 normal_loss: 0.007634\n",
      "[486/00013] train_loss: 0.010171 kl_loss: 0.082066 normal_loss: 0.007709\n",
      "[488/00011] train_loss: 0.010061 kl_loss: 0.081971 normal_loss: 0.007602\n",
      "[490/00009] train_loss: 0.010164 kl_loss: 0.082759 normal_loss: 0.007681\n",
      "[492/00007] train_loss: 0.010066 kl_loss: 0.082005 normal_loss: 0.007606\n",
      "[494/00005] train_loss: 0.010054 kl_loss: 0.081989 normal_loss: 0.007594\n",
      "[496/00003] train_loss: 0.010058 kl_loss: 0.082341 normal_loss: 0.007588\n",
      "[498/00001] train_loss: 0.010096 kl_loss: 0.082431 normal_loss: 0.007623\n",
      "[499/00050] train_loss: 0.010075 kl_loss: 0.082073 normal_loss: 0.007613\n",
      "[501/00048] train_loss: 0.010022 kl_loss: 0.082274 normal_loss: 0.007554\n",
      "[503/00046] train_loss: 0.009900 kl_loss: 0.082034 normal_loss: 0.007439\n",
      "[505/00044] train_loss: 0.009963 kl_loss: 0.082314 normal_loss: 0.007494\n",
      "[507/00042] train_loss: 0.010010 kl_loss: 0.082493 normal_loss: 0.007536\n",
      "[509/00040] train_loss: 0.009968 kl_loss: 0.081873 normal_loss: 0.007511\n",
      "[511/00038] train_loss: 0.009947 kl_loss: 0.082312 normal_loss: 0.007477\n",
      "[513/00036] train_loss: 0.009978 kl_loss: 0.082126 normal_loss: 0.007514\n",
      "[515/00034] train_loss: 0.009999 kl_loss: 0.082255 normal_loss: 0.007531\n",
      "[517/00032] train_loss: 0.009907 kl_loss: 0.082019 normal_loss: 0.007446\n",
      "[519/00030] train_loss: 0.009918 kl_loss: 0.082096 normal_loss: 0.007455\n",
      "[521/00028] train_loss: 0.009945 kl_loss: 0.082299 normal_loss: 0.007476\n",
      "[523/00026] train_loss: 0.009890 kl_loss: 0.082225 normal_loss: 0.007423\n",
      "[525/00024] train_loss: 0.009896 kl_loss: 0.082169 normal_loss: 0.007431\n",
      "[527/00022] train_loss: 0.009992 kl_loss: 0.082315 normal_loss: 0.007522\n",
      "[529/00020] train_loss: 0.009913 kl_loss: 0.082370 normal_loss: 0.007442\n",
      "[531/00018] train_loss: 0.009937 kl_loss: 0.081664 normal_loss: 0.007487\n",
      "[533/00016] train_loss: 0.009967 kl_loss: 0.082168 normal_loss: 0.007502\n",
      "[535/00014] train_loss: 0.009886 kl_loss: 0.082159 normal_loss: 0.007421\n",
      "[537/00012] train_loss: 0.009840 kl_loss: 0.081969 normal_loss: 0.007381\n",
      "[539/00010] train_loss: 0.009963 kl_loss: 0.082494 normal_loss: 0.007488\n",
      "[541/00008] train_loss: 0.009879 kl_loss: 0.081928 normal_loss: 0.007421\n",
      "[543/00006] train_loss: 0.009902 kl_loss: 0.082147 normal_loss: 0.007438\n",
      "[545/00004] train_loss: 0.009857 kl_loss: 0.081927 normal_loss: 0.007399\n",
      "[547/00002] train_loss: 0.009903 kl_loss: 0.082103 normal_loss: 0.007440\n",
      "[549/00000] train_loss: 0.009852 kl_loss: 0.082101 normal_loss: 0.007389\n",
      "[550/00049] train_loss: 0.009935 kl_loss: 0.082136 normal_loss: 0.007471\n",
      "[552/00047] train_loss: 0.009895 kl_loss: 0.082188 normal_loss: 0.007430\n",
      "[554/00045] train_loss: 0.009887 kl_loss: 0.082037 normal_loss: 0.007426\n",
      "[556/00043] train_loss: 0.009866 kl_loss: 0.082100 normal_loss: 0.007403\n",
      "[558/00041] train_loss: 0.009853 kl_loss: 0.081820 normal_loss: 0.007398\n",
      "[560/00039] train_loss: 0.009863 kl_loss: 0.082073 normal_loss: 0.007400\n",
      "[562/00037] train_loss: 0.009847 kl_loss: 0.082068 normal_loss: 0.007385\n",
      "[564/00035] train_loss: 0.009923 kl_loss: 0.082097 normal_loss: 0.007461\n",
      "[566/00033] train_loss: 0.009859 kl_loss: 0.082118 normal_loss: 0.007395\n",
      "[568/00031] train_loss: 0.009840 kl_loss: 0.082167 normal_loss: 0.007375\n",
      "[570/00029] train_loss: 0.009829 kl_loss: 0.081653 normal_loss: 0.007379\n",
      "[572/00027] train_loss: 0.009892 kl_loss: 0.082705 normal_loss: 0.007411\n",
      "[574/00025] train_loss: 0.009755 kl_loss: 0.081575 normal_loss: 0.007308\n",
      "[576/00023] train_loss: 0.009838 kl_loss: 0.082082 normal_loss: 0.007375\n",
      "[578/00021] train_loss: 0.009841 kl_loss: 0.081891 normal_loss: 0.007385\n",
      "[580/00019] train_loss: 0.009831 kl_loss: 0.081984 normal_loss: 0.007371\n",
      "[582/00017] train_loss: 0.009840 kl_loss: 0.082361 normal_loss: 0.007369\n",
      "[584/00015] train_loss: 0.009737 kl_loss: 0.081624 normal_loss: 0.007288\n",
      "[586/00013] train_loss: 0.009834 kl_loss: 0.081986 normal_loss: 0.007375\n",
      "[588/00011] train_loss: 0.009826 kl_loss: 0.081973 normal_loss: 0.007366\n",
      "[590/00009] train_loss: 0.009801 kl_loss: 0.082303 normal_loss: 0.007332\n",
      "[592/00007] train_loss: 0.009792 kl_loss: 0.081890 normal_loss: 0.007335\n",
      "[594/00005] train_loss: 0.009792 kl_loss: 0.081925 normal_loss: 0.007334\n",
      "[596/00003] train_loss: 0.009780 kl_loss: 0.082052 normal_loss: 0.007319\n",
      "[598/00001] train_loss: 0.009807 kl_loss: 0.081907 normal_loss: 0.007350\n",
      "[599/00050] train_loss: 0.009830 kl_loss: 0.082025 normal_loss: 0.007369\n",
      "[601/00048] train_loss: 0.009761 kl_loss: 0.081919 normal_loss: 0.007303\n",
      "[603/00046] train_loss: 0.009727 kl_loss: 0.081870 normal_loss: 0.007271\n",
      "[605/00044] train_loss: 0.009750 kl_loss: 0.082065 normal_loss: 0.007288\n",
      "[607/00042] train_loss: 0.009692 kl_loss: 0.081656 normal_loss: 0.007242\n",
      "[609/00040] train_loss: 0.009776 kl_loss: 0.082065 normal_loss: 0.007314\n",
      "[611/00038] train_loss: 0.009787 kl_loss: 0.081946 normal_loss: 0.007329\n",
      "[613/00036] train_loss: 0.009745 kl_loss: 0.081674 normal_loss: 0.007295\n",
      "[615/00034] train_loss: 0.009750 kl_loss: 0.082105 normal_loss: 0.007286\n",
      "[617/00032] train_loss: 0.009736 kl_loss: 0.082278 normal_loss: 0.007268\n",
      "[619/00030] train_loss: 0.009709 kl_loss: 0.081662 normal_loss: 0.007260\n",
      "[621/00028] train_loss: 0.009683 kl_loss: 0.081848 normal_loss: 0.007227\n",
      "[623/00026] train_loss: 0.009759 kl_loss: 0.082264 normal_loss: 0.007291\n",
      "[625/00024] train_loss: 0.009734 kl_loss: 0.081795 normal_loss: 0.007280\n",
      "[627/00022] train_loss: 0.009765 kl_loss: 0.081861 normal_loss: 0.007309\n",
      "[629/00020] train_loss: 0.009681 kl_loss: 0.081697 normal_loss: 0.007231\n",
      "[631/00018] train_loss: 0.009797 kl_loss: 0.082429 normal_loss: 0.007324\n",
      "[633/00016] train_loss: 0.009650 kl_loss: 0.081589 normal_loss: 0.007202\n",
      "[635/00014] train_loss: 0.009722 kl_loss: 0.081962 normal_loss: 0.007263\n",
      "[637/00012] train_loss: 0.009744 kl_loss: 0.081715 normal_loss: 0.007293\n",
      "[639/00010] train_loss: 0.009763 kl_loss: 0.081915 normal_loss: 0.007305\n",
      "[641/00008] train_loss: 0.009694 kl_loss: 0.082045 normal_loss: 0.007233\n",
      "[643/00006] train_loss: 0.009702 kl_loss: 0.081770 normal_loss: 0.007249\n",
      "[645/00004] train_loss: 0.009684 kl_loss: 0.081747 normal_loss: 0.007231\n",
      "[647/00002] train_loss: 0.009717 kl_loss: 0.081909 normal_loss: 0.007259\n",
      "[649/00000] train_loss: 0.009756 kl_loss: 0.081973 normal_loss: 0.007297\n",
      "[650/00049] train_loss: 0.009731 kl_loss: 0.081935 normal_loss: 0.007273\n",
      "[652/00047] train_loss: 0.009699 kl_loss: 0.081997 normal_loss: 0.007240\n",
      "[654/00045] train_loss: 0.009664 kl_loss: 0.081579 normal_loss: 0.007216\n",
      "[656/00043] train_loss: 0.009690 kl_loss: 0.082149 normal_loss: 0.007226\n",
      "[658/00041] train_loss: 0.009691 kl_loss: 0.081697 normal_loss: 0.007241\n",
      "[660/00039] train_loss: 0.009733 kl_loss: 0.082317 normal_loss: 0.007264\n",
      "[662/00037] train_loss: 0.009672 kl_loss: 0.081467 normal_loss: 0.007228\n",
      "[664/00035] train_loss: 0.009677 kl_loss: 0.081896 normal_loss: 0.007220\n",
      "[666/00033] train_loss: 0.009760 kl_loss: 0.081992 normal_loss: 0.007300\n",
      "[668/00031] train_loss: 0.009651 kl_loss: 0.081755 normal_loss: 0.007198\n",
      "[670/00029] train_loss: 0.009691 kl_loss: 0.081621 normal_loss: 0.007243\n",
      "[672/00027] train_loss: 0.009685 kl_loss: 0.082257 normal_loss: 0.007218\n",
      "[674/00025] train_loss: 0.009681 kl_loss: 0.081888 normal_loss: 0.007225\n",
      "[676/00023] train_loss: 0.009690 kl_loss: 0.081644 normal_loss: 0.007241\n",
      "[678/00021] train_loss: 0.009695 kl_loss: 0.081863 normal_loss: 0.007239\n",
      "[680/00019] train_loss: 0.009691 kl_loss: 0.081758 normal_loss: 0.007239\n",
      "[682/00017] train_loss: 0.009657 kl_loss: 0.081717 normal_loss: 0.007206\n",
      "[684/00015] train_loss: 0.009692 kl_loss: 0.082154 normal_loss: 0.007228\n",
      "[686/00013] train_loss: 0.009678 kl_loss: 0.081961 normal_loss: 0.007219\n",
      "[688/00011] train_loss: 0.009686 kl_loss: 0.081825 normal_loss: 0.007231\n",
      "[690/00009] train_loss: 0.009632 kl_loss: 0.081696 normal_loss: 0.007182\n",
      "[692/00007] train_loss: 0.009695 kl_loss: 0.081898 normal_loss: 0.007238\n",
      "[694/00005] train_loss: 0.009638 kl_loss: 0.081391 normal_loss: 0.007196\n",
      "[696/00003] train_loss: 0.009695 kl_loss: 0.082233 normal_loss: 0.007228\n",
      "[698/00001] train_loss: 0.009659 kl_loss: 0.081733 normal_loss: 0.007207\n",
      "[699/00050] train_loss: 0.009658 kl_loss: 0.081828 normal_loss: 0.007203\n",
      "[701/00048] train_loss: 0.009611 kl_loss: 0.081684 normal_loss: 0.007161\n",
      "[703/00046] train_loss: 0.009639 kl_loss: 0.082077 normal_loss: 0.007176\n",
      "[705/00044] train_loss: 0.009585 kl_loss: 0.081722 normal_loss: 0.007133\n",
      "[707/00042] train_loss: 0.009685 kl_loss: 0.081981 normal_loss: 0.007226\n",
      "[709/00040] train_loss: 0.009629 kl_loss: 0.081768 normal_loss: 0.007176\n",
      "[711/00038] train_loss: 0.009619 kl_loss: 0.081610 normal_loss: 0.007171\n",
      "[713/00036] train_loss: 0.009697 kl_loss: 0.082327 normal_loss: 0.007227\n",
      "[715/00034] train_loss: 0.009608 kl_loss: 0.081454 normal_loss: 0.007164\n",
      "[717/00032] train_loss: 0.009620 kl_loss: 0.081720 normal_loss: 0.007169\n",
      "[719/00030] train_loss: 0.009666 kl_loss: 0.082044 normal_loss: 0.007205\n",
      "[721/00028] train_loss: 0.009622 kl_loss: 0.081917 normal_loss: 0.007164\n",
      "[723/00026] train_loss: 0.009654 kl_loss: 0.081738 normal_loss: 0.007202\n",
      "[725/00024] train_loss: 0.009624 kl_loss: 0.081706 normal_loss: 0.007173\n",
      "[727/00022] train_loss: 0.009647 kl_loss: 0.081784 normal_loss: 0.007194\n",
      "[729/00020] train_loss: 0.009624 kl_loss: 0.081751 normal_loss: 0.007171\n",
      "[731/00018] train_loss: 0.009646 kl_loss: 0.081941 normal_loss: 0.007188\n",
      "[733/00016] train_loss: 0.009671 kl_loss: 0.082264 normal_loss: 0.007203\n",
      "[735/00014] train_loss: 0.009609 kl_loss: 0.081721 normal_loss: 0.007157\n",
      "[737/00012] train_loss: 0.009582 kl_loss: 0.081467 normal_loss: 0.007138\n",
      "[739/00010] train_loss: 0.009678 kl_loss: 0.082073 normal_loss: 0.007216\n",
      "[741/00008] train_loss: 0.009670 kl_loss: 0.081591 normal_loss: 0.007223\n",
      "[743/00006] train_loss: 0.009651 kl_loss: 0.081969 normal_loss: 0.007192\n",
      "[745/00004] train_loss: 0.009643 kl_loss: 0.081678 normal_loss: 0.007193\n",
      "[747/00002] train_loss: 0.009623 kl_loss: 0.081603 normal_loss: 0.007175\n",
      "[749/00000] train_loss: 0.009605 kl_loss: 0.081993 normal_loss: 0.007145\n",
      "[750/00049] train_loss: 0.009630 kl_loss: 0.081862 normal_loss: 0.007174\n",
      "[752/00047] train_loss: 0.009644 kl_loss: 0.081711 normal_loss: 0.007193\n",
      "[754/00045] train_loss: 0.009634 kl_loss: 0.082012 normal_loss: 0.007174\n",
      "[756/00043] train_loss: 0.009591 kl_loss: 0.081691 normal_loss: 0.007141\n",
      "[758/00041] train_loss: 0.009668 kl_loss: 0.081854 normal_loss: 0.007212\n",
      "[760/00039] train_loss: 0.009604 kl_loss: 0.081642 normal_loss: 0.007155\n",
      "[762/00037] train_loss: 0.009623 kl_loss: 0.082017 normal_loss: 0.007163\n",
      "[764/00035] train_loss: 0.009541 kl_loss: 0.081378 normal_loss: 0.007099\n",
      "[766/00033] train_loss: 0.009673 kl_loss: 0.082245 normal_loss: 0.007205\n",
      "[768/00031] train_loss: 0.009555 kl_loss: 0.081263 normal_loss: 0.007117\n",
      "[770/00029] train_loss: 0.009698 kl_loss: 0.082103 normal_loss: 0.007235\n",
      "[772/00027] train_loss: 0.009660 kl_loss: 0.081835 normal_loss: 0.007205\n",
      "[774/00025] train_loss: 0.009575 kl_loss: 0.081590 normal_loss: 0.007127\n",
      "[776/00023] train_loss: 0.009597 kl_loss: 0.081800 normal_loss: 0.007143\n",
      "[778/00021] train_loss: 0.009622 kl_loss: 0.081789 normal_loss: 0.007168\n",
      "[780/00019] train_loss: 0.009594 kl_loss: 0.081871 normal_loss: 0.007138\n",
      "[782/00017] train_loss: 0.009595 kl_loss: 0.081851 normal_loss: 0.007140\n",
      "[784/00015] train_loss: 0.009597 kl_loss: 0.081953 normal_loss: 0.007139\n",
      "[786/00013] train_loss: 0.009617 kl_loss: 0.081577 normal_loss: 0.007170\n",
      "[788/00011] train_loss: 0.009585 kl_loss: 0.081555 normal_loss: 0.007138\n",
      "[790/00009] train_loss: 0.009647 kl_loss: 0.082068 normal_loss: 0.007185\n",
      "[792/00007] train_loss: 0.009580 kl_loss: 0.081509 normal_loss: 0.007135\n",
      "[794/00005] train_loss: 0.009639 kl_loss: 0.082055 normal_loss: 0.007177\n",
      "[796/00003] train_loss: 0.009590 kl_loss: 0.081648 normal_loss: 0.007140\n",
      "[798/00001] train_loss: 0.009583 kl_loss: 0.081900 normal_loss: 0.007126\n",
      "[799/00050] train_loss: 0.009584 kl_loss: 0.081724 normal_loss: 0.007133\n",
      "[801/00048] train_loss: 0.009585 kl_loss: 0.081713 normal_loss: 0.007134\n",
      "[803/00046] train_loss: 0.009580 kl_loss: 0.081828 normal_loss: 0.007125\n",
      "[805/00044] train_loss: 0.009536 kl_loss: 0.081508 normal_loss: 0.007091\n",
      "[807/00042] train_loss: 0.009590 kl_loss: 0.082091 normal_loss: 0.007127\n",
      "[809/00040] train_loss: 0.009617 kl_loss: 0.081535 normal_loss: 0.007171\n",
      "[811/00038] train_loss: 0.009595 kl_loss: 0.081779 normal_loss: 0.007141\n",
      "[813/00036] train_loss: 0.009599 kl_loss: 0.081745 normal_loss: 0.007146\n",
      "[815/00034] train_loss: 0.009591 kl_loss: 0.081959 normal_loss: 0.007132\n",
      "[817/00032] train_loss: 0.009584 kl_loss: 0.081510 normal_loss: 0.007138\n",
      "[819/00030] train_loss: 0.009574 kl_loss: 0.081671 normal_loss: 0.007124\n",
      "[821/00028] train_loss: 0.009614 kl_loss: 0.081877 normal_loss: 0.007158\n",
      "[823/00026] train_loss: 0.009574 kl_loss: 0.081917 normal_loss: 0.007117\n",
      "[825/00024] train_loss: 0.009590 kl_loss: 0.082059 normal_loss: 0.007128\n",
      "[827/00022] train_loss: 0.009594 kl_loss: 0.081505 normal_loss: 0.007149\n",
      "[829/00020] train_loss: 0.009537 kl_loss: 0.081642 normal_loss: 0.007088\n",
      "[831/00018] train_loss: 0.009616 kl_loss: 0.082020 normal_loss: 0.007155\n",
      "[833/00016] train_loss: 0.009552 kl_loss: 0.081607 normal_loss: 0.007104\n",
      "[835/00014] train_loss: 0.009609 kl_loss: 0.081647 normal_loss: 0.007159\n",
      "[837/00012] train_loss: 0.009570 kl_loss: 0.081651 normal_loss: 0.007120\n",
      "[839/00010] train_loss: 0.009621 kl_loss: 0.081874 normal_loss: 0.007164\n",
      "[841/00008] train_loss: 0.009637 kl_loss: 0.081974 normal_loss: 0.007177\n",
      "[843/00006] train_loss: 0.009547 kl_loss: 0.081636 normal_loss: 0.007098\n",
      "[845/00004] train_loss: 0.009578 kl_loss: 0.082140 normal_loss: 0.007114\n",
      "[847/00002] train_loss: 0.009582 kl_loss: 0.081740 normal_loss: 0.007130\n",
      "[849/00000] train_loss: 0.009561 kl_loss: 0.081724 normal_loss: 0.007109\n",
      "[850/00049] train_loss: 0.009598 kl_loss: 0.081763 normal_loss: 0.007145\n",
      "[852/00047] train_loss: 0.009601 kl_loss: 0.081644 normal_loss: 0.007152\n",
      "[854/00045] train_loss: 0.009600 kl_loss: 0.081869 normal_loss: 0.007144\n",
      "[856/00043] train_loss: 0.009566 kl_loss: 0.081754 normal_loss: 0.007113\n",
      "[858/00041] train_loss: 0.009535 kl_loss: 0.081590 normal_loss: 0.007088\n",
      "[860/00039] train_loss: 0.009580 kl_loss: 0.081484 normal_loss: 0.007135\n",
      "[862/00037] train_loss: 0.009627 kl_loss: 0.082405 normal_loss: 0.007155\n",
      "[864/00035] train_loss: 0.009538 kl_loss: 0.081539 normal_loss: 0.007092\n",
      "[866/00033] train_loss: 0.009637 kl_loss: 0.082054 normal_loss: 0.007175\n",
      "[868/00031] train_loss: 0.009563 kl_loss: 0.081448 normal_loss: 0.007120\n",
      "[870/00029] train_loss: 0.009582 kl_loss: 0.081764 normal_loss: 0.007129\n",
      "[872/00027] train_loss: 0.009582 kl_loss: 0.082055 normal_loss: 0.007120\n",
      "[874/00025] train_loss: 0.009553 kl_loss: 0.081781 normal_loss: 0.007099\n",
      "[876/00023] train_loss: 0.009518 kl_loss: 0.081333 normal_loss: 0.007078\n",
      "[878/00021] train_loss: 0.009547 kl_loss: 0.081774 normal_loss: 0.007094\n",
      "[880/00019] train_loss: 0.009579 kl_loss: 0.081868 normal_loss: 0.007123\n",
      "[882/00017] train_loss: 0.009541 kl_loss: 0.081481 normal_loss: 0.007097\n",
      "[884/00015] train_loss: 0.009595 kl_loss: 0.082122 normal_loss: 0.007131\n",
      "[886/00013] train_loss: 0.009483 kl_loss: 0.081240 normal_loss: 0.007046\n",
      "[888/00011] train_loss: 0.009573 kl_loss: 0.082148 normal_loss: 0.007108\n",
      "[890/00009] train_loss: 0.009568 kl_loss: 0.081731 normal_loss: 0.007116\n",
      "[892/00007] train_loss: 0.009568 kl_loss: 0.081753 normal_loss: 0.007115\n",
      "[894/00005] train_loss: 0.009604 kl_loss: 0.081817 normal_loss: 0.007149\n",
      "[896/00003] train_loss: 0.009561 kl_loss: 0.081726 normal_loss: 0.007110\n",
      "[898/00001] train_loss: 0.009573 kl_loss: 0.081791 normal_loss: 0.007119\n",
      "[899/00050] train_loss: 0.009561 kl_loss: 0.081741 normal_loss: 0.007109\n",
      "[901/00048] train_loss: 0.009544 kl_loss: 0.081746 normal_loss: 0.007091\n",
      "[903/00046] train_loss: 0.009556 kl_loss: 0.081645 normal_loss: 0.007107\n",
      "[905/00044] train_loss: 0.009555 kl_loss: 0.081879 normal_loss: 0.007098\n",
      "[907/00042] train_loss: 0.009561 kl_loss: 0.081648 normal_loss: 0.007111\n",
      "[909/00040] train_loss: 0.009562 kl_loss: 0.081817 normal_loss: 0.007107\n",
      "[911/00038] train_loss: 0.009496 kl_loss: 0.081658 normal_loss: 0.007047\n",
      "[913/00036] train_loss: 0.009574 kl_loss: 0.081825 normal_loss: 0.007119\n",
      "[915/00034] train_loss: 0.009514 kl_loss: 0.081603 normal_loss: 0.007066\n",
      "[917/00032] train_loss: 0.009578 kl_loss: 0.081973 normal_loss: 0.007118\n",
      "[919/00030] train_loss: 0.009537 kl_loss: 0.081506 normal_loss: 0.007092\n",
      "[921/00028] train_loss: 0.009536 kl_loss: 0.081725 normal_loss: 0.007084\n",
      "[923/00026] train_loss: 0.009570 kl_loss: 0.082093 normal_loss: 0.007107\n",
      "[925/00024] train_loss: 0.009548 kl_loss: 0.081363 normal_loss: 0.007107\n",
      "[927/00022] train_loss: 0.009537 kl_loss: 0.081735 normal_loss: 0.007085\n",
      "[929/00020] train_loss: 0.009522 kl_loss: 0.081594 normal_loss: 0.007074\n",
      "[931/00018] train_loss: 0.009526 kl_loss: 0.081755 normal_loss: 0.007074\n",
      "[933/00016] train_loss: 0.009536 kl_loss: 0.081845 normal_loss: 0.007081\n",
      "[935/00014] train_loss: 0.009527 kl_loss: 0.081564 normal_loss: 0.007080\n",
      "[937/00012] train_loss: 0.009597 kl_loss: 0.082061 normal_loss: 0.007136\n",
      "[939/00010] train_loss: 0.009534 kl_loss: 0.081557 normal_loss: 0.007088\n",
      "[941/00008] train_loss: 0.009585 kl_loss: 0.081944 normal_loss: 0.007126\n",
      "[943/00006] train_loss: 0.009554 kl_loss: 0.081809 normal_loss: 0.007100\n",
      "[945/00004] train_loss: 0.009544 kl_loss: 0.081873 normal_loss: 0.007088\n",
      "[947/00002] train_loss: 0.009547 kl_loss: 0.081614 normal_loss: 0.007098\n",
      "[949/00000] train_loss: 0.009533 kl_loss: 0.081692 normal_loss: 0.007082\n",
      "[950/00049] train_loss: 0.009544 kl_loss: 0.081817 normal_loss: 0.007090\n",
      "[952/00047] train_loss: 0.009582 kl_loss: 0.081688 normal_loss: 0.007131\n",
      "[954/00045] train_loss: 0.009570 kl_loss: 0.081756 normal_loss: 0.007118\n",
      "[956/00043] train_loss: 0.009578 kl_loss: 0.081945 normal_loss: 0.007120\n",
      "[958/00041] train_loss: 0.009540 kl_loss: 0.081405 normal_loss: 0.007098\n",
      "[960/00039] train_loss: 0.009518 kl_loss: 0.081699 normal_loss: 0.007067\n",
      "[962/00037] train_loss: 0.009522 kl_loss: 0.081848 normal_loss: 0.007067\n",
      "[964/00035] train_loss: 0.009580 kl_loss: 0.081808 normal_loss: 0.007126\n",
      "[966/00033] train_loss: 0.009510 kl_loss: 0.081633 normal_loss: 0.007061\n",
      "[968/00031] train_loss: 0.009585 kl_loss: 0.081903 normal_loss: 0.007128\n",
      "[970/00029] train_loss: 0.009591 kl_loss: 0.081807 normal_loss: 0.007137\n",
      "[972/00027] train_loss: 0.009503 kl_loss: 0.081346 normal_loss: 0.007063\n",
      "[974/00025] train_loss: 0.009509 kl_loss: 0.081832 normal_loss: 0.007054\n",
      "[976/00023] train_loss: 0.009499 kl_loss: 0.081628 normal_loss: 0.007050\n",
      "[978/00021] train_loss: 0.009598 kl_loss: 0.081923 normal_loss: 0.007141\n",
      "[980/00019] train_loss: 0.009537 kl_loss: 0.082170 normal_loss: 0.007072\n",
      "[982/00017] train_loss: 0.009495 kl_loss: 0.081454 normal_loss: 0.007051\n",
      "[984/00015] train_loss: 0.009568 kl_loss: 0.081835 normal_loss: 0.007113\n",
      "[986/00013] train_loss: 0.009495 kl_loss: 0.081369 normal_loss: 0.007054\n",
      "[988/00011] train_loss: 0.009575 kl_loss: 0.081940 normal_loss: 0.007117\n",
      "[990/00009] train_loss: 0.009552 kl_loss: 0.081702 normal_loss: 0.007101\n",
      "[992/00007] train_loss: 0.009529 kl_loss: 0.081720 normal_loss: 0.007077\n",
      "[994/00005] train_loss: 0.009563 kl_loss: 0.081774 normal_loss: 0.007109\n",
      "[996/00003] train_loss: 0.009517 kl_loss: 0.081812 normal_loss: 0.007062\n",
      "[998/00001] train_loss: 0.009541 kl_loss: 0.081828 normal_loss: 0.007086\n",
      "[999/00050] train_loss: 0.009498 kl_loss: 0.081657 normal_loss: 0.007048\n"
     ]
    }
   ],
   "source": [
    "# AIRPLANE VAD\n",
    "from scripts import train\n",
    "config = {\n",
    "    'experiment_name': 'airplane_vad',\n",
    "    'device': 'cuda:0',\n",
    "    'is_overfit': False,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate_model': 0.01,\n",
    "    'learning_rate_code': 0.01,\n",
    "    'learning_rate_log_var':0.01,\n",
    "    'max_epochs': 1000,\n",
    "    'print_every_n': 100,\n",
    "    'latent_code_length' : 256,\n",
    "    'scheduler_step_size': 100,\n",
    "    'vad_free' : True,\n",
    "    'test': False,\n",
    "    'kl_weight': 0.03,\n",
    "    'kl_weight_increase_every_epochs': 100,\n",
    "    'kl_weight_increase_value': 0.0,\n",
    "    'resume_ckpt': 'airplane_vad',\n",
    "    'filter_class': 'airplane',\n",
    "    'decoder_var' : True\n",
    "}\n",
    "train.main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "#                   #\n",
    "#    VISUALIZING    #\n",
    "#                   #\n",
    "#####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.visualize import visualize_dataset_sample, visualize_ad, visualize_vad, visualize_vad_norm, visualize_vad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88155f994cf44655b19dedd271bd5548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1f0dc1be6e4c8da7ade5d0ba0b9a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "experiment = \"airplane_vad\"\n",
    "experiment2 = \"sofa_ad\"\n",
    "# experiment2 = \"sofa_ad\"\n",
    "filter_class = \"airplane\"\n",
    "index = 4123\n",
    "index1 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "index2 = random.choice(range(len(ShapeNet('train', filter_class = filter_class))))\n",
    "a1 = 0.5\n",
    "a2 = 1 - a1\n",
    "#-------\n",
    "# visualize_vad_norm(experiment2)\n",
    "visualize_vad_norm(experiment)\n",
    "visualize_ad(experiment, index1)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_ad(experiment, index)\n",
    "#-------\n",
    "# visualize_vad_norm(experiment)\n",
    "# visualize_vad_norm(experiment2)\n",
    "# visualize_dataset_sample(filter_class, index)\n",
    "#-------\n",
    "# visualize_interpolation_ad(experiment, index1, index2, a1, a2)\n",
    "# visualize_ad(experiment, index1)\n",
    "# visualize_ad(experiment, index2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#                  #\n",
    "#    EVALUATION    #\n",
    "#                  #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate import generate_samples, convert_df_to_point_cloud, chamfer_distance, MMD, convert_set_to_point_cloud\n",
    "from util.visualization import visualize_mesh\n",
    "from skimage.measure import marching_cubes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 8.00 GiB total capacity; 6.48 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\index.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=0'>1</a>\u001b[0m \u001b[39m# MMD\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/index.ipynb#ch0000018?line=1'>2</a>\u001b[0m MMD(\u001b[39m'\u001b[39;49m\u001b[39mairplane_vad\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mairplane\u001b[39;49m\u001b[39m'\u001b[39;49m, n_samples\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcuda:0\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\evaluate.py:129\u001b[0m, in \u001b[0;36mMMD\u001b[1;34m(experiment, split, filter_class, n_samples, device)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=126'>127</a>\u001b[0m samples_point_clouds \u001b[39m=\u001b[39m convert_set_to_point_cloud(samples, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=127'>128</a>\u001b[0m val_point_clouds \u001b[39m=\u001b[39m convert_set_to_point_cloud(val, device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=128'>129</a>\u001b[0m mmd_value \u001b[39m=\u001b[39m _mmd(samples_point_clouds, val_point_clouds)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=129'>130</a>\u001b[0m \u001b[39mreturn\u001b[39;00m mmd_value\u001b[39m.\u001b[39mmean(), mmd_value\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\evaluate.py:109\u001b[0m, in \u001b[0;36m_mmd\u001b[1;34m(set1, set2)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=106'>107</a>\u001b[0m sample_distances \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=107'>108</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample2 \u001b[39min\u001b[39;00m set2:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=108'>109</a>\u001b[0m     cf_distance \u001b[39m=\u001b[39m chamfer_distance(sample, sample2)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=109'>110</a>\u001b[0m     sample_distances\u001b[39m.\u001b[39mappend(cf_distance)\n\u001b[0;32m    <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=110'>111</a>\u001b[0m mmd \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(sample_distances) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(sample_distances)\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\evaluate.py:91\u001b[0m, in \u001b[0;36mchamfer_distance\u001b[1;34m(point_cloud1, point_cloud2)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=83'>84</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=84'>85</a>\u001b[0m \u001b[39minput: \u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=85'>86</a>\u001b[0m \u001b[39m    point_cloud1: tensor (counts, 3)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=86'>87</a>\u001b[0m \u001b[39m    point_cloud2: tensor (counts, 3)\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=87'>88</a>\u001b[0m \u001b[39moutput: float\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=88'>89</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=89'>90</a>\u001b[0m dist \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=90'>91</a>\u001b[0m av_dist1 \u001b[39m=\u001b[39m _point_clouds_min_distance(point_cloud1, point_cloud2)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=91'>92</a>\u001b[0m av_dist2 \u001b[39m=\u001b[39m _point_clouds_min_distance(point_cloud2, point_cloud1)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=92'>93</a>\u001b[0m dist \u001b[39m=\u001b[39m dist \u001b[39m+\u001b[39m av_dist1 \u001b[39m+\u001b[39m av_dist2\n",
      "File \u001b[1;32mc:\\Users\\Munzer\\Documents\\uni\\ADL4CV\\adl4cv-vad\\scripts\\evaluate.py:71\u001b[0m, in \u001b[0;36m_point_clouds_min_distance\u001b[1;34m(point_cloud1, point_cloud2)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=67'>68</a>\u001b[0m num_point, num_features \u001b[39m=\u001b[39m point_cloud1\u001b[39m.\u001b[39mshape\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=68'>69</a>\u001b[0m expanded_array1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtile(point_cloud1, (num_point, \u001b[39m1\u001b[39m)) \u001b[39m# num_points * num_points, num_feature\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=69'>70</a>\u001b[0m expanded_array2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(\n\u001b[1;32m---> <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=70'>71</a>\u001b[0m     torch\u001b[39m.\u001b[39;49mtile(\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=71'>72</a>\u001b[0m         torch\u001b[39m.\u001b[39;49munsqueeze(point_cloud2, \u001b[39m1\u001b[39;49m), \n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=72'>73</a>\u001b[0m         (\u001b[39m1\u001b[39;49m, num_point, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=73'>74</a>\u001b[0m     ),\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=74'>75</a>\u001b[0m     (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, num_features)\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=75'>76</a>\u001b[0m ) \u001b[39m# num_points * num_points, num_feature\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=76'>77</a>\u001b[0m distances \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(expanded_array1\u001b[39m-\u001b[39mexpanded_array2, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# num_points * num_points\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/Munzer/Documents/uni/ADL4CV/adl4cv-vad/scripts/evaluate.py?line=77'>78</a>\u001b[0m distances \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(distances, (num_point, num_point)) \u001b[39m# num_points, num_points\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.12 GiB (GPU 0; 8.00 GiB total capacity; 6.48 GiB already allocated; 0 bytes free; 6.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# MMD\n",
    "MMD('airplane_vad', 'val', 'airplane', n_samples=10, device=torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMD\n",
    "\n",
    "# generate n new samples\n",
    "n = 10\n",
    "samples = generate_samples('airplane_vad', n)\n",
    "samples = samples.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get validation datasett\n",
    "val = []\n",
    "val_dataset = ShapeNet('val', filter_class='airplane')\n",
    "for data_dict in val_dataset:\n",
    "    target_df = torch.from_numpy(data_dict['target_df']).float()\n",
    "    val.append(target_df)\n",
    "val = torch.stack(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2eacb14e08743d7a97764ef425c46d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "input_mesh = marching_cubes(samples[8].detach().numpy(), level=1)\n",
    "visualize_mesh(input_mesh[0], input_mesh[1], flip_axes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8614)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate chamfer distance between 1 generated sample and 1 sample from val\n",
    "point_cloud = convert_df_to_point_cloud(samples[0])\n",
    "point_cloud2 = convert_df_to_point_cloud(val[0])\n",
    "chamfer_distance(point_cloud, point_cloud2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sets to pointclouds\n",
    "samples_point_clouds = convert_set_to_point_cloud(samples)\n",
    "val_point_clouds = convert_set_to_point_cloud(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 1.8070911169052124\n",
      "2: 1.7960222959518433\n",
      "3: 1.9385355710983276\n",
      "4: 3.0144054889678955\n",
      "5: 1.745727777481079\n",
      "6: 1.736878752708435\n",
      "7: 1.8000874519348145\n",
      "8: 1.786746621131897\n",
      "9: 2.1584560871124268\n",
      "10: 1.9656767845153809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.9750, device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate mmd between the 2 samples\n",
    "# Note: the final value changes a bit every time because every time a new point cloud is generated\n",
    "# Note: mmd function takes a lot of time, needs optimization\n",
    "mmd_value = mmd(samples_point_clouds, val_point_clouds)\n",
    "mmd_value.mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8edcb304c74d7de69396267fdd221ef1d3cdc7db9124f1020d58ca6af5038c14"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('adl4cv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
